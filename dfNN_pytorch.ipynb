{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix field reconstruction\n",
    "\n",
    "The matrix field reconstruction has one less Jacobian step than the vector field reconstruction which is why it is computationally more efficient.\n",
    "\n",
    "We use the following deterministic transformations to get from the NN output to a divergence-free vector field v:\n",
    "1. Parameterise the Skew-Symmetric decomposition of A\n",
    "    - U = NN(x) (non-zero values of the Upper Triangular U are of size N(N - 1)/(2) (x2))\n",
    "2. Construct anti-symmetric matrix A\n",
    "    - A = U - U.T \n",
    "3. Attain divergence-free vector field v via\n",
    "    - v = (div(A1), div(A2)), trace of the Jacobian\n",
    "\n",
    "see https://github.com/facebookresearch/neural-conservation-law/blob/main/pytorch/divfree.py\n",
    "\n",
    "Dimensionalities:\n",
    "- If our input in (4 x 4, 2) so flat that is (16, 2) coordinate pairs, U should be (6, 2, 2)\n",
    "- 6 = (sqrt(N)(sqrt(N) - 1)/2)\n",
    "\n",
    "NN batch-wise or not\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "- Do we use a NN that processes a batch or points individually? (Batch)\n",
    "- What is the output shape of u_v?\n",
    "    - For the dim = 2 case, do it is always (dim * (dim - 1) / 2), which is 1 because we only estimate the upper right corner of every (N) 2 x 2 matrix\n",
    "    - Can we build this directly into the net?\n",
    "\n",
    "The model seems to only be implemented under [jax > models.py > Divfree()](https://github.com/facebookresearch/neural-conservation-law/blob/20a403d00affad905d1c47b041bc60d0ff0ea360/jax/models.py#L118). DivfreeSparse() and DivFreeImplicit() are not used anywhere.\n",
    "\n",
    "The model is used in [jax > hh_experiment_DivFree.py](https://github.com/facebookresearch/neural-conservation-law/blob/20a403d00affad905d1c47b041bc60d0ff0ea360/jax/hh_experiment_DivFree.py#L53). Hodge decomp.\n",
    "\n",
    "dim = 10.  \n",
    "mlp = MLP(depth = layers, width = width, act = act, out_dim = **dim * (dim-1) // 2**, std = 1, bias = True)\n",
    "\n",
    "For dim = 2, at each point, each matrix A (2 x 2) is antisymm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "u_fn, params, _ = build_divfree_vector_field(self.module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim = 2, hidden_dim = 32):\n",
    "        super().__init__()\n",
    "        output_dim = int((input_dim * (input_dim - 1)) / 2)\n",
    "        print(output_dim)\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)  # Output shape: (4,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 2])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLP()\n",
    "\n",
    "N = 16  # N should be a perfect square\n",
    "N_side = int(N ** 0.5)\n",
    "dims = 2\n",
    "inputs = torch.randn(N, dims)  # Random (N, 2) inputs\n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# U_fill shape is (N, 1)\n",
    "U_fill = model(inputs)\n",
    "U = torch.zeros(N, dims, dims)\n",
    "# Fills all top right corners of the (N, 2, 2) tensor\n",
    "U[:, 0, 1] = U_fill.squeeze()\n",
    "# U is (N, 2, 2), so we need to swap the last two dims and then subtract\n",
    "A = U - U.transpose(1, 2)\n",
    "\n",
    "def compute_A(inputs):\n",
    "    # U_fill is (N, 1)\n",
    "    U_fill = model(inputs)\n",
    "    U = torch.triu(torch.ones(N, dims, dims), diagonal = 1)\n",
    "    U = U * U_fill.unsqueeze(1)\n",
    "    # U_empty = torch.ones(N, dims, dims)\n",
    "    # U = U_empty.clone()\n",
    "    # U[:, 0, 1] = U_fill.squeeze()\n",
    "    # make vmap work: use concat\n",
    "    # rows_2x2 = torch.cat([torch.zeros(U_fill.shape[0], 1), U_fill], dim = 1)\n",
    "    # U = torch.cat([rows_2x2.unsqueeze(1), torch.zeros(U_fill.shape[0], 1, 2)], dim = 1)\n",
    "    A = U - U.transpose(1, 2)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 2, 2])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows_2x2 = torch.cat([torch.zeros(U_fill.shape[0], 1), U_fill], dim = -1)\n",
    "torch.cat([rows_2x2.unsqueeze(1), torch.zeros(U_fill.shape[0], 1, 2)], dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# U = U.index_add(1, torch.tensor([0, 1]), U_fill.squeeze().unsqueeze(1).repeat(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jacobian\n",
    "\n",
    "- https://pytorch.org/docs/stable/generated/torch.func.jacrev.html#torch.func.jacrev\n",
    "- batched Jacobians via vmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 4, 4])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.func import jacrev, vmap\n",
    "x = torch.randn(5, 4)\n",
    "jacobian = vmap(jacrev(torch.sin))(x)\n",
    "jacobian.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 16, 2, 2, 2])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Issue with how we construct U \n",
    "jacobian = vmap(jacrev(compute_A))(inputs)\n",
    "jacobian.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 3 and 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[136], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Compute the Jacobian using autograd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Takes in function & input\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# torch.Size([16, 2, 2, 16, 2])\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# jacobian = torch.autograd.functional.jacobian(compute_A, inputs, vectorize = True)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m jacobian_func \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjacrev\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompute_A\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# trace_result = torch.trace(jacobian, dim1 = 1, dim2 = 2)\u001b[39;00m\n",
      "File \u001b[0;32m~/ice_thickness/.conda/lib/python3.9/site-packages/torch/_functorch/apis.py:203\u001b[0m, in \u001b[0;36mvmap.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvmap_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ice_thickness/.conda/lib/python3.9/site-packages/torch/_functorch/vmap.py:331\u001b[0m, in \u001b[0;36mvmap_impl\u001b[0;34m(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _chunked_vmap(\n\u001b[1;32m    321\u001b[0m         func,\n\u001b[1;32m    322\u001b[0m         flat_in_dims,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    328\u001b[0m     )\n\u001b[1;32m    330\u001b[0m \u001b[38;5;66;03m# If chunk_size is not specified.\u001b[39;00m\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_flat_vmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_in_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ice_thickness/.conda/lib/python3.9/site-packages/torch/_functorch/vmap.py:479\u001b[0m, in \u001b[0;36m_flat_vmap\u001b[0;34m(func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m vmap_increment_nesting(batch_size, randomness) \u001b[38;5;28;01mas\u001b[39;00m vmap_level:\n\u001b[1;32m    476\u001b[0m     batched_inputs \u001b[38;5;241m=\u001b[39m _create_batched_inputs(\n\u001b[1;32m    477\u001b[0m         flat_in_dims, flat_args, vmap_level, args_spec\n\u001b[1;32m    478\u001b[0m     )\n\u001b[0;32m--> 479\u001b[0m     batched_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatched_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)\n",
      "File \u001b[0;32m~/ice_thickness/.conda/lib/python3.9/site-packages/torch/_functorch/eager_transforms.py:604\u001b[0m, in \u001b[0;36mjacrev.<locals>.wrapper_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper_fn\u001b[39m(\u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m    603\u001b[0m     error_if_complex(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjacrev\u001b[39m\u001b[38;5;124m\"\u001b[39m, args, is_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 604\u001b[0m     vjp_out \u001b[38;5;241m=\u001b[39m \u001b[43m_vjp_with_argnums\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margnums\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margnums\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_aux\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    605\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_aux:\n\u001b[1;32m    606\u001b[0m         output, vjp_fn, aux \u001b[38;5;241m=\u001b[39m vjp_out\n",
      "File \u001b[0;32m~/ice_thickness/.conda/lib/python3.9/site-packages/torch/_functorch/vmap.py:48\u001b[0m, in \u001b[0;36mdoesnt_support_saved_tensors_hooks.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mdisable_saved_tensors_hooks(message):\n\u001b[0;32m---> 48\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ice_thickness/.conda/lib/python3.9/site-packages/torch/_functorch/eager_transforms.py:399\u001b[0m, in \u001b[0;36m_vjp_with_argnums\u001b[0;34m(func, argnums, has_aux, *primals)\u001b[0m\n\u001b[1;32m    397\u001b[0m     diff_primals \u001b[38;5;241m=\u001b[39m _slice_argnums(primals, argnums, as_tuple\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    398\u001b[0m     tree_map_(partial(_create_differentiable, level\u001b[38;5;241m=\u001b[39mlevel), diff_primals)\n\u001b[0;32m--> 399\u001b[0m primals_out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprimals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_aux:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(primals_out, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(primals_out) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n",
      "Cell \u001b[0;32mIn[99], line 15\u001b[0m, in \u001b[0;36mcompute_A\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m     11\u001b[0m U_fill \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# U = torch.zeros(N, dims, dims)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# U[:, 0, 1] = U_fill.squeeze()\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# make vmap work: use concat\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m rows_2x2 \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43mU_fill\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mU_fill\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m U \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([rows_2x2\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m), torch\u001b[38;5;241m.\u001b[39mzeros(U_fill\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)], dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     17\u001b[0m A \u001b[38;5;241m=\u001b[39m U \u001b[38;5;241m-\u001b[39m U\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 3 and 2"
     ]
    }
   ],
   "source": [
    "# Compute the Jacobian using autograd\n",
    "# Takes in function & input\n",
    "# torch.Size([16, 2, 2, 16, 2])\n",
    "# jacobian = torch.autograd.functional.jacobian(compute_A, inputs, vectorize = True)\n",
    "jacobian_func = torch.func.vmap(torch.func.jacrev(compute_A))(inputs)\n",
    "# trace_result = torch.trace(jacobian, dim1 = 1, dim2 = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 2, 2, 16, 2])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.func.jacrev(compute_A, chunk_size = 1)(inputs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  0.0000],\n",
       "         [ 0.0273,  0.0344]],\n",
       "\n",
       "        [[-0.0273, -0.0344],\n",
       "         [ 0.0000,  0.0000]]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.func.jacrev(compute_A)(inputs).diagonal(dim1 = 2, dim2 = 3).shape\n",
    "torch.func.jacrev(compute_A)(inputs)[0, :, :, 0]\n",
    "# torch.func.jacrev(compute_A)(inputs)[0, 0, 1, 0, 0]\n",
    "# torch.func.jacrev(compute_A)(inputs)[0, 1, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'diagonal'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mjacobian_func\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiagonal\u001b[49m(dim1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, dim2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39msum()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'diagonal'"
     ]
    }
   ],
   "source": [
    "jacobian_func.diagonal(dim1 = 1, dim2 = 2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jacobian.diagonal(dim1 = 1, dim2 = 2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 2, 2, 16, 2])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jacobian.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U_empty[:, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = jnp.zeros((N_side, N_side))\n",
    "idx = jnp.triu_indices(N, 1)\n",
    "U = U.at[idx].set(b) # go through via row\n",
    "A = U - U.T # miuns now multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([16, 2])\n",
      "Output shape: torch.Size([120, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import itertools\n",
    "import math\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim = 2, hidden_dim = 32, output_dim = 4):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)  # Output shape: (4,)\n",
    "\n",
    "def generate_pairwise_matrices(inputs, model):\n",
    "    \"\"\"\n",
    "    Takes (N, 2) inputs, applies the model, and outputs (sqrt(N)(sqrt(N)-1)/2, 2, 2) shaped tensor.\n",
    "    \"\"\"\n",
    "    N = inputs.shape[0]\n",
    "    \n",
    "    # Check that N is a perfect square for âˆšN pairs\n",
    "    sqrt_N = math.isqrt(N)\n",
    "    assert sqrt_N ** 2 == N, \"N should be a perfect square!\"\n",
    "\n",
    "    # Compute the number of pairwise combinations: sqrt(N) * (sqrt(N) - 1) / 2\n",
    "    num_pairs = (sqrt_N * (sqrt_N - 1)) // 2\n",
    "\n",
    "    pairs = list(itertools.combinations(range(N), 2))  # Generate all unique (i, j) pairs\n",
    "\n",
    "    outputs = []\n",
    "    for i, j in pairs:\n",
    "        pair_input = (inputs[i] + inputs[j]) / 2  # Combine inputs (simple average)\n",
    "        matrix_flat = model(pair_input)  # Get (4,) shaped output\n",
    "        matrix = matrix_flat.view(2, 2)  # Reshape to (2, 2)\n",
    "        outputs.append(matrix)\n",
    "\n",
    "    return torch.stack(outputs)  # Shape: (num_pairs, 2, 2)\n",
    "\n",
    "# Example usage\n",
    "N = 16  # N should be a perfect square\n",
    "inputs = torch.randn(N, 2)  # Random (N, 2) inputs\n",
    "\n",
    "model = MLP()\n",
    "output_matrices = generate_pairwise_matrices(inputs, model)\n",
    "\n",
    "print(\"Input shape:\", inputs.shape)  # (N, 2)\n",
    "print(\"Output shape:\", output_matrices.shape)  # (12, 2, 2) for N = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
