{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dfGP + dfNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "=== CONVERGENCE ===\n",
      "Training inputs shape: torch.Size([196, 2])\n",
      "Training observations shape: torch.Size([196, 2])\n",
      "Training inputs dtype: torch.float32\n",
      "\n",
      "=== BRANCHING ===\n",
      "Training inputs shape: torch.Size([196, 2])\n",
      "Training observations shape: torch.Size([196, 2])\n",
      "Training inputs dtype: torch.float32\n",
      "\n",
      "=== MERGE ===\n",
      "Training inputs shape: torch.Size([196, 2])\n",
      "Training observations shape: torch.Size([196, 2])\n",
      "Training inputs dtype: torch.float32\n",
      "\n",
      "=== DEFLECTION ===\n",
      "Training inputs shape: torch.Size([196, 2])\n",
      "Training observations shape: torch.Size([196, 2])\n",
      "Training inputs dtype: torch.float32\n",
      "\n",
      "=== RIDGE ===\n",
      "Training inputs shape: torch.Size([196, 2])\n",
      "Training observations shape: torch.Size([196, 2])\n",
      "Training inputs dtype: torch.float32\n",
      "\n",
      "=== Generating test data ===\n",
      "=== CONVERGENCE ===\n",
      "Test inputs shape: torch.Size([400, 2])\n",
      "Test observations shape: torch.Size([400, 2])\n",
      "Test inputs dtype: torch.float32\n",
      "\n",
      "=== BRANCHING ===\n",
      "Test inputs shape: torch.Size([400, 2])\n",
      "Test observations shape: torch.Size([400, 2])\n",
      "Test inputs dtype: torch.float32\n",
      "\n",
      "=== MERGE ===\n",
      "Test inputs shape: torch.Size([400, 2])\n",
      "Test observations shape: torch.Size([400, 2])\n",
      "Test inputs dtype: torch.float32\n",
      "\n",
      "=== DEFLECTION ===\n",
      "Test inputs shape: torch.Size([400, 2])\n",
      "Test observations shape: torch.Size([400, 2])\n",
      "Test inputs dtype: torch.float32\n",
      "\n",
      "=== RIDGE ===\n",
      "Test inputs shape: torch.Size([400, 2])\n",
      "Test observations shape: torch.Size([400, 2])\n",
      "Test inputs dtype: torch.float32\n",
      "\n",
      "\n",
      "Training for CONVERGENCE...\n",
      "\n",
      "--- Training Run 1/10 ---\n",
      "\n",
      "Start Training\n",
      "convergence dfGPdfNN Run 1/10, Epoch 1/1000, Training Loss (NLML): -872.5280, (RMSE): 0.0155\n",
      "convergence dfGPdfNN Run 1/10, Epoch 2/1000, Training Loss (NLML): -885.1465, (RMSE): 0.0103\n",
      "convergence dfGPdfNN Run 1/10, Epoch 3/1000, Training Loss (NLML): -890.2666, (RMSE): 0.0085\n",
      "convergence dfGPdfNN Run 1/10, Epoch 4/1000, Training Loss (NLML): -894.5923, (RMSE): 0.0065\n",
      "convergence dfGPdfNN Run 1/10, Epoch 5/1000, Training Loss (NLML): -898.1437, (RMSE): 0.0047\n",
      "convergence dfGPdfNN Run 1/10, Epoch 6/1000, Training Loss (NLML): -900.1156, (RMSE): 0.0043\n",
      "convergence dfGPdfNN Run 1/10, Epoch 7/1000, Training Loss (NLML): -902.0502, (RMSE): 0.0040\n",
      "convergence dfGPdfNN Run 1/10, Epoch 8/1000, Training Loss (NLML): -904.1273, (RMSE): 0.0033\n",
      "convergence dfGPdfNN Run 1/10, Epoch 9/1000, Training Loss (NLML): -905.9165, (RMSE): 0.0027\n",
      "convergence dfGPdfNN Run 1/10, Epoch 10/1000, Training Loss (NLML): -907.2411, (RMSE): 0.0029\n",
      "convergence dfGPdfNN Run 1/10, Epoch 11/1000, Training Loss (NLML): -908.5232, (RMSE): 0.0031\n",
      "convergence dfGPdfNN Run 1/10, Epoch 12/1000, Training Loss (NLML): -910.0233, (RMSE): 0.0030\n",
      "convergence dfGPdfNN Run 1/10, Epoch 13/1000, Training Loss (NLML): -911.3365, (RMSE): 0.0031\n",
      "convergence dfGPdfNN Run 1/10, Epoch 14/1000, Training Loss (NLML): -912.6949, (RMSE): 0.0028\n",
      "convergence dfGPdfNN Run 1/10, Epoch 15/1000, Training Loss (NLML): -914.0663, (RMSE): 0.0025\n",
      "convergence dfGPdfNN Run 1/10, Epoch 16/1000, Training Loss (NLML): -915.3617, (RMSE): 0.0022\n",
      "convergence dfGPdfNN Run 1/10, Epoch 17/1000, Training Loss (NLML): -916.5851, (RMSE): 0.0018\n",
      "convergence dfGPdfNN Run 1/10, Epoch 18/1000, Training Loss (NLML): -917.7350, (RMSE): 0.0016\n",
      "convergence dfGPdfNN Run 1/10, Epoch 19/1000, Training Loss (NLML): -918.7980, (RMSE): 0.0014\n",
      "convergence dfGPdfNN Run 1/10, Epoch 20/1000, Training Loss (NLML): -919.8313, (RMSE): 0.0013\n",
      "convergence dfGPdfNN Run 1/10, Epoch 21/1000, Training Loss (NLML): -920.7861, (RMSE): 0.0014\n",
      "convergence dfGPdfNN Run 1/10, Epoch 22/1000, Training Loss (NLML): -921.6843, (RMSE): 0.0015\n",
      "convergence dfGPdfNN Run 1/10, Epoch 23/1000, Training Loss (NLML): -922.5854, (RMSE): 0.0015\n",
      "convergence dfGPdfNN Run 1/10, Epoch 24/1000, Training Loss (NLML): -923.4846, (RMSE): 0.0015\n",
      "convergence dfGPdfNN Run 1/10, Epoch 25/1000, Training Loss (NLML): -924.1892, (RMSE): 0.0019\n",
      "convergence dfGPdfNN Run 1/10, Epoch 26/1000, Training Loss (NLML): -925.0154, (RMSE): 0.0019\n",
      "convergence dfGPdfNN Run 1/10, Epoch 27/1000, Training Loss (NLML): -925.8232, (RMSE): 0.0018\n",
      "convergence dfGPdfNN Run 1/10, Epoch 28/1000, Training Loss (NLML): -926.5934, (RMSE): 0.0017\n",
      "convergence dfGPdfNN Run 1/10, Epoch 29/1000, Training Loss (NLML): -927.4438, (RMSE): 0.0013\n",
      "convergence dfGPdfNN Run 1/10, Epoch 30/1000, Training Loss (NLML): -928.1686, (RMSE): 0.0013\n",
      "convergence dfGPdfNN Run 1/10, Epoch 31/1000, Training Loss (NLML): -928.8569, (RMSE): 0.0012\n",
      "convergence dfGPdfNN Run 1/10, Epoch 32/1000, Training Loss (NLML): -929.5293, (RMSE): 0.0012\n",
      "convergence dfGPdfNN Run 1/10, Epoch 33/1000, Training Loss (NLML): -930.1710, (RMSE): 0.0012\n",
      "convergence dfGPdfNN Run 1/10, Epoch 34/1000, Training Loss (NLML): -930.8048, (RMSE): 0.0012\n",
      "convergence dfGPdfNN Run 1/10, Epoch 35/1000, Training Loss (NLML): -931.4128, (RMSE): 0.0011\n",
      "convergence dfGPdfNN Run 1/10, Epoch 36/1000, Training Loss (NLML): -932.0092, (RMSE): 0.0012\n",
      "convergence dfGPdfNN Run 1/10, Epoch 37/1000, Training Loss (NLML): -932.5428, (RMSE): 0.0013\n",
      "convergence dfGPdfNN Run 1/10, Epoch 38/1000, Training Loss (NLML): -933.0991, (RMSE): 0.0013\n",
      "convergence dfGPdfNN Run 1/10, Epoch 39/1000, Training Loss (NLML): -933.6332, (RMSE): 0.0013\n",
      "convergence dfGPdfNN Run 1/10, Epoch 40/1000, Training Loss (NLML): -934.1771, (RMSE): 0.0013\n",
      "convergence dfGPdfNN Run 1/10, Epoch 41/1000, Training Loss (NLML): -934.6892, (RMSE): 0.0013\n",
      "convergence dfGPdfNN Run 1/10, Epoch 42/1000, Training Loss (NLML): -935.1902, (RMSE): 0.0014\n",
      "convergence dfGPdfNN Run 1/10, Epoch 43/1000, Training Loss (NLML): -935.6678, (RMSE): 0.0014\n",
      "convergence dfGPdfNN Run 1/10, Epoch 44/1000, Training Loss (NLML): -936.1560, (RMSE): 0.0015\n",
      "convergence dfGPdfNN Run 1/10, Epoch 45/1000, Training Loss (NLML): -936.5992, (RMSE): 0.0016\n",
      "convergence dfGPdfNN Run 1/10, Epoch 46/1000, Training Loss (NLML): -937.0496, (RMSE): 0.0016\n",
      "convergence dfGPdfNN Run 1/10, Epoch 47/1000, Training Loss (NLML): -935.7340, (RMSE): 0.0048\n",
      "convergence dfGPdfNN Run 1/10, Epoch 48/1000, Training Loss (NLML): -937.8529, (RMSE): 0.0019\n",
      "convergence dfGPdfNN Run 1/10, Epoch 49/1000, Training Loss (NLML): -938.2825, (RMSE): 0.0018\n",
      "convergence dfGPdfNN Run 1/10, Epoch 50/1000, Training Loss (NLML): -938.6755, (RMSE): 0.0018\n",
      "convergence dfGPdfNN Run 1/10, Epoch 51/1000, Training Loss (NLML): -939.0425, (RMSE): 0.0018\n",
      "convergence dfGPdfNN Run 1/10, Epoch 52/1000, Training Loss (NLML): -939.3997, (RMSE): 0.0018\n",
      "convergence dfGPdfNN Run 1/10, Epoch 53/1000, Training Loss (NLML): -939.7600, (RMSE): 0.0017\n",
      "convergence dfGPdfNN Run 1/10, Epoch 54/1000, Training Loss (NLML): -940.1453, (RMSE): 0.0016\n",
      "convergence dfGPdfNN Run 1/10, Epoch 55/1000, Training Loss (NLML): -940.4917, (RMSE): 0.0017\n",
      "convergence dfGPdfNN Run 1/10, Epoch 56/1000, Training Loss (NLML): -940.8496, (RMSE): 0.0018\n",
      "convergence dfGPdfNN Run 1/10, Epoch 57/1000, Training Loss (NLML): -941.2150, (RMSE): 0.0018\n",
      "convergence dfGPdfNN Run 1/10, Epoch 58/1000, Training Loss (NLML): -941.5638, (RMSE): 0.0017\n",
      "convergence dfGPdfNN Run 1/10, Epoch 59/1000, Training Loss (NLML): -941.8904, (RMSE): 0.0018\n",
      "convergence dfGPdfNN Run 1/10, Epoch 60/1000, Training Loss (NLML): -942.2291, (RMSE): 0.0019\n",
      "convergence dfGPdfNN Run 1/10, Epoch 61/1000, Training Loss (NLML): -942.5751, (RMSE): 0.0019\n",
      "convergence dfGPdfNN Run 1/10, Epoch 62/1000, Training Loss (NLML): -942.8931, (RMSE): 0.0019\n",
      "convergence dfGPdfNN Run 1/10, Epoch 63/1000, Training Loss (NLML): -943.1965, (RMSE): 0.0020\n",
      "convergence dfGPdfNN Run 1/10, Epoch 64/1000, Training Loss (NLML): -943.4868, (RMSE): 0.0021\n",
      "convergence dfGPdfNN Run 1/10, Epoch 65/1000, Training Loss (NLML): -928.2678, (RMSE): 0.0138\n",
      "convergence dfGPdfNN Run 1/10, Epoch 66/1000, Training Loss (NLML): -944.0713, (RMSE): 0.0021\n",
      "convergence dfGPdfNN Run 1/10, Epoch 67/1000, Training Loss (NLML): -944.3673, (RMSE): 0.0019\n",
      "convergence dfGPdfNN Run 1/10, Epoch 68/1000, Training Loss (NLML): -944.5959, (RMSE): 0.0019\n",
      "convergence dfGPdfNN Run 1/10, Epoch 69/1000, Training Loss (NLML): -944.8250, (RMSE): 0.0019\n",
      "convergence dfGPdfNN Run 1/10, Epoch 70/1000, Training Loss (NLML): -945.0681, (RMSE): 0.0019\n",
      "convergence dfGPdfNN Run 1/10, Epoch 71/1000, Training Loss (NLML): -945.3119, (RMSE): 0.0020\n",
      "convergence dfGPdfNN Run 1/10, Epoch 72/1000, Training Loss (NLML): -945.5769, (RMSE): 0.0020\n",
      "convergence dfGPdfNN Run 1/10, Epoch 73/1000, Training Loss (NLML): -945.8470, (RMSE): 0.0020\n",
      "convergence dfGPdfNN Run 1/10, Epoch 74/1000, Training Loss (NLML): -946.0728, (RMSE): 0.0022\n",
      "convergence dfGPdfNN Run 1/10, Epoch 75/1000, Training Loss (NLML): -946.2739, (RMSE): 0.0024\n",
      "convergence dfGPdfNN Run 1/10, Epoch 76/1000, Training Loss (NLML): -946.4851, (RMSE): 0.0025\n",
      "convergence dfGPdfNN Run 1/10, Epoch 77/1000, Training Loss (NLML): -946.6863, (RMSE): 0.0026\n",
      "convergence dfGPdfNN Run 1/10, Epoch 78/1000, Training Loss (NLML): -946.9506, (RMSE): 0.0026\n",
      "convergence dfGPdfNN Run 1/10, Epoch 79/1000, Training Loss (NLML): -947.1442, (RMSE): 0.0028\n",
      "convergence dfGPdfNN Run 1/10, Epoch 80/1000, Training Loss (NLML): -947.3558, (RMSE): 0.0029\n",
      "convergence dfGPdfNN Run 1/10, Epoch 81/1000, Training Loss (NLML): -947.6013, (RMSE): 0.0030\n",
      "convergence dfGPdfNN Run 1/10, Epoch 82/1000, Training Loss (NLML): -947.7679, (RMSE): 0.0031\n",
      "convergence dfGPdfNN Run 1/10, Epoch 83/1000, Training Loss (NLML): -948.0024, (RMSE): 0.0031\n",
      "convergence dfGPdfNN Run 1/10, Epoch 84/1000, Training Loss (NLML): -948.2893, (RMSE): 0.0028\n",
      "convergence dfGPdfNN Run 1/10, Epoch 85/1000, Training Loss (NLML): -948.5219, (RMSE): 0.0027\n",
      "convergence dfGPdfNN Run 1/10, Epoch 86/1000, Training Loss (NLML): -948.7347, (RMSE): 0.0026\n",
      "convergence dfGPdfNN Run 1/10, Epoch 87/1000, Training Loss (NLML): -948.9027, (RMSE): 0.0027\n",
      "convergence dfGPdfNN Run 1/10, Epoch 88/1000, Training Loss (NLML): -949.1082, (RMSE): 0.0027\n",
      "convergence dfGPdfNN Run 1/10, Epoch 89/1000, Training Loss (NLML): -949.3054, (RMSE): 0.0028\n",
      "convergence dfGPdfNN Run 1/10, Epoch 90/1000, Training Loss (NLML): -949.5554, (RMSE): 0.0028\n",
      "convergence dfGPdfNN Run 1/10, Epoch 91/1000, Training Loss (NLML): -949.7257, (RMSE): 0.0029\n",
      "convergence dfGPdfNN Run 1/10, Epoch 92/1000, Training Loss (NLML): -949.8590, (RMSE): 0.0031\n",
      "convergence dfGPdfNN Run 1/10, Epoch 93/1000, Training Loss (NLML): -950.0238, (RMSE): 0.0031\n",
      "convergence dfGPdfNN Run 1/10, Epoch 94/1000, Training Loss (NLML): -950.2152, (RMSE): 0.0032\n",
      "convergence dfGPdfNN Run 1/10, Epoch 95/1000, Training Loss (NLML): -950.3555, (RMSE): 0.0032\n",
      "convergence dfGPdfNN Run 1/10, Epoch 96/1000, Training Loss (NLML): -950.4774, (RMSE): 0.0033\n",
      "convergence dfGPdfNN Run 1/10, Epoch 97/1000, Training Loss (NLML): -950.7323, (RMSE): 0.0032\n",
      "convergence dfGPdfNN Run 1/10, Epoch 98/1000, Training Loss (NLML): -950.9408, (RMSE): 0.0032\n",
      "convergence dfGPdfNN Run 1/10, Epoch 99/1000, Training Loss (NLML): -951.1245, (RMSE): 0.0032\n",
      "convergence dfGPdfNN Run 1/10, Epoch 100/1000, Training Loss (NLML): -951.3751, (RMSE): 0.0030\n",
      "convergence dfGPdfNN Run 1/10, Epoch 101/1000, Training Loss (NLML): -951.5427, (RMSE): 0.0030\n",
      "convergence dfGPdfNN Run 1/10, Epoch 102/1000, Training Loss (NLML): -951.7643, (RMSE): 0.0030\n",
      "convergence dfGPdfNN Run 1/10, Epoch 103/1000, Training Loss (NLML): -951.9022, (RMSE): 0.0030\n",
      "convergence dfGPdfNN Run 1/10, Epoch 104/1000, Training Loss (NLML): -952.0138, (RMSE): 0.0032\n",
      "convergence dfGPdfNN Run 1/10, Epoch 105/1000, Training Loss (NLML): -952.1779, (RMSE): 0.0032\n",
      "convergence dfGPdfNN Run 1/10, Epoch 106/1000, Training Loss (NLML): -952.3486, (RMSE): 0.0033\n",
      "convergence dfGPdfNN Run 1/10, Epoch 107/1000, Training Loss (NLML): -952.4998, (RMSE): 0.0034\n",
      "convergence dfGPdfNN Run 1/10, Epoch 108/1000, Training Loss (NLML): -952.6793, (RMSE): 0.0033\n",
      "convergence dfGPdfNN Run 1/10, Epoch 109/1000, Training Loss (NLML): -952.8691, (RMSE): 0.0032\n",
      "convergence dfGPdfNN Run 1/10, Epoch 110/1000, Training Loss (NLML): -953.0526, (RMSE): 0.0032\n",
      "convergence dfGPdfNN Run 1/10, Epoch 111/1000, Training Loss (NLML): -953.2086, (RMSE): 0.0032\n",
      "convergence dfGPdfNN Run 1/10, Epoch 112/1000, Training Loss (NLML): -953.3541, (RMSE): 0.0032\n",
      "convergence dfGPdfNN Run 1/10, Epoch 113/1000, Training Loss (NLML): -953.4880, (RMSE): 0.0032\n",
      "convergence dfGPdfNN Run 1/10, Epoch 114/1000, Training Loss (NLML): -953.5613, (RMSE): 0.0034\n",
      "convergence dfGPdfNN Run 1/10, Epoch 115/1000, Training Loss (NLML): -953.6387, (RMSE): 0.0035\n",
      "convergence dfGPdfNN Run 1/10, Epoch 116/1000, Training Loss (NLML): -953.7372, (RMSE): 0.0035\n",
      "convergence dfGPdfNN Run 1/10, Epoch 117/1000, Training Loss (NLML): -953.8479, (RMSE): 0.0035\n",
      "convergence dfGPdfNN Run 1/10, Epoch 118/1000, Training Loss (NLML): -953.8728, (RMSE): 0.0036\n",
      "convergence dfGPdfNN Run 1/10, Epoch 119/1000, Training Loss (NLML): -953.9812, (RMSE): 0.0037\n",
      "convergence dfGPdfNN Run 1/10, Epoch 120/1000, Training Loss (NLML): -954.0942, (RMSE): 0.0037\n",
      "convergence dfGPdfNN Run 1/10, Epoch 121/1000, Training Loss (NLML): -954.1841, (RMSE): 0.0038\n",
      "convergence dfGPdfNN Run 1/10, Epoch 122/1000, Training Loss (NLML): -954.2836, (RMSE): 0.0038\n",
      "convergence dfGPdfNN Run 1/10, Epoch 123/1000, Training Loss (NLML): -954.3979, (RMSE): 0.0037\n",
      "convergence dfGPdfNN Run 1/10, Epoch 124/1000, Training Loss (NLML): -954.5569, (RMSE): 0.0036\n",
      "convergence dfGPdfNN Run 1/10, Epoch 125/1000, Training Loss (NLML): -954.6747, (RMSE): 0.0036\n",
      "convergence dfGPdfNN Run 1/10, Epoch 126/1000, Training Loss (NLML): -954.8046, (RMSE): 0.0036\n",
      "convergence dfGPdfNN Run 1/10, Epoch 127/1000, Training Loss (NLML): -954.9042, (RMSE): 0.0038\n",
      "convergence dfGPdfNN Run 1/10, Epoch 128/1000, Training Loss (NLML): -954.9624, (RMSE): 0.0039\n",
      "convergence dfGPdfNN Run 1/10, Epoch 129/1000, Training Loss (NLML): -955.0887, (RMSE): 0.0040\n",
      "convergence dfGPdfNN Run 1/10, Epoch 130/1000, Training Loss (NLML): -955.2014, (RMSE): 0.0040\n",
      "convergence dfGPdfNN Run 1/10, Epoch 131/1000, Training Loss (NLML): -955.2517, (RMSE): 0.0040\n",
      "convergence dfGPdfNN Run 1/10, Epoch 132/1000, Training Loss (NLML): -955.3351, (RMSE): 0.0040\n",
      "convergence dfGPdfNN Run 1/10, Epoch 133/1000, Training Loss (NLML): -955.3635, (RMSE): 0.0039\n",
      "convergence dfGPdfNN Run 1/10, Epoch 134/1000, Training Loss (NLML): -955.3728, (RMSE): 0.0039\n",
      "convergence dfGPdfNN Run 1/10, Epoch 135/1000, Training Loss (NLML): -955.4326, (RMSE): 0.0039\n",
      "convergence dfGPdfNN Run 1/10, Epoch 136/1000, Training Loss (NLML): -955.3627, (RMSE): 0.0041\n",
      "convergence dfGPdfNN Run 1/10, Epoch 137/1000, Training Loss (NLML): -955.3574, (RMSE): 0.0043\n",
      "convergence dfGPdfNN Run 1/10, Epoch 138/1000, Training Loss (NLML): -955.3610, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 139/1000, Training Loss (NLML): -955.4832, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 140/1000, Training Loss (NLML): -955.6537, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 141/1000, Training Loss (NLML): -955.6326, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 142/1000, Training Loss (NLML): -955.5953, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 143/1000, Training Loss (NLML): -955.7168, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 144/1000, Training Loss (NLML): -955.7642, (RMSE): 0.0043\n",
      "convergence dfGPdfNN Run 1/10, Epoch 145/1000, Training Loss (NLML): -955.8892, (RMSE): 0.0042\n",
      "convergence dfGPdfNN Run 1/10, Epoch 146/1000, Training Loss (NLML): -955.9125, (RMSE): 0.0042\n",
      "convergence dfGPdfNN Run 1/10, Epoch 147/1000, Training Loss (NLML): -955.9355, (RMSE): 0.0043\n",
      "convergence dfGPdfNN Run 1/10, Epoch 148/1000, Training Loss (NLML): -956.0265, (RMSE): 0.0043\n",
      "convergence dfGPdfNN Run 1/10, Epoch 149/1000, Training Loss (NLML): -956.0170, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 150/1000, Training Loss (NLML): -956.0149, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 151/1000, Training Loss (NLML): -956.0156, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 152/1000, Training Loss (NLML): -956.0548, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 153/1000, Training Loss (NLML): -956.0385, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 154/1000, Training Loss (NLML): -956.0299, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 155/1000, Training Loss (NLML): -956.0150, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 156/1000, Training Loss (NLML): -956.0261, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 157/1000, Training Loss (NLML): -956.0974, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 158/1000, Training Loss (NLML): -956.1682, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 159/1000, Training Loss (NLML): -956.3142, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 160/1000, Training Loss (NLML): -956.3485, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 161/1000, Training Loss (NLML): -956.3938, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 162/1000, Training Loss (NLML): -956.2919, (RMSE): 0.0046\n",
      "convergence dfGPdfNN Run 1/10, Epoch 163/1000, Training Loss (NLML): -956.2678, (RMSE): 0.0046\n",
      "convergence dfGPdfNN Run 1/10, Epoch 164/1000, Training Loss (NLML): -956.2120, (RMSE): 0.0046\n",
      "convergence dfGPdfNN Run 1/10, Epoch 165/1000, Training Loss (NLML): -956.2002, (RMSE): 0.0046\n",
      "convergence dfGPdfNN Run 1/10, Epoch 166/1000, Training Loss (NLML): -956.2208, (RMSE): 0.0046\n",
      "convergence dfGPdfNN Run 1/10, Epoch 167/1000, Training Loss (NLML): -956.3088, (RMSE): 0.0046\n",
      "convergence dfGPdfNN Run 1/10, Epoch 168/1000, Training Loss (NLML): -956.2639, (RMSE): 0.0048\n",
      "convergence dfGPdfNN Run 1/10, Epoch 169/1000, Training Loss (NLML): -956.5691, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 170/1000, Training Loss (NLML): -956.6096, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 171/1000, Training Loss (NLML): -956.4233, (RMSE): 0.0047\n",
      "convergence dfGPdfNN Run 1/10, Epoch 172/1000, Training Loss (NLML): -956.1414, (RMSE): 0.0050\n",
      "convergence dfGPdfNN Run 1/10, Epoch 173/1000, Training Loss (NLML): -956.0785, (RMSE): 0.0049\n",
      "convergence dfGPdfNN Run 1/10, Epoch 174/1000, Training Loss (NLML): -956.1338, (RMSE): 0.0047\n",
      "convergence dfGPdfNN Run 1/10, Epoch 175/1000, Training Loss (NLML): -956.1337, (RMSE): 0.0047\n",
      "convergence dfGPdfNN Run 1/10, Epoch 176/1000, Training Loss (NLML): -956.1571, (RMSE): 0.0047\n",
      "convergence dfGPdfNN Run 1/10, Epoch 177/1000, Training Loss (NLML): -956.1993, (RMSE): 0.0047\n",
      "convergence dfGPdfNN Run 1/10, Epoch 178/1000, Training Loss (NLML): -956.2482, (RMSE): 0.0047\n",
      "convergence dfGPdfNN Run 1/10, Epoch 179/1000, Training Loss (NLML): -956.2887, (RMSE): 0.0048\n",
      "convergence dfGPdfNN Run 1/10, Epoch 180/1000, Training Loss (NLML): -956.2223, (RMSE): 0.0049\n",
      "convergence dfGPdfNN Run 1/10, Epoch 181/1000, Training Loss (NLML): -956.3541, (RMSE): 0.0050\n",
      "convergence dfGPdfNN Run 1/10, Epoch 182/1000, Training Loss (NLML): -956.4540, (RMSE): 0.0049\n",
      "convergence dfGPdfNN Run 1/10, Epoch 183/1000, Training Loss (NLML): -956.5464, (RMSE): 0.0048\n",
      "convergence dfGPdfNN Run 1/10, Epoch 184/1000, Training Loss (NLML): -956.5823, (RMSE): 0.0048\n",
      "convergence dfGPdfNN Run 1/10, Epoch 185/1000, Training Loss (NLML): -956.5422, (RMSE): 0.0048\n",
      "convergence dfGPdfNN Run 1/10, Epoch 186/1000, Training Loss (NLML): -956.5061, (RMSE): 0.0048\n",
      "convergence dfGPdfNN Run 1/10, Epoch 187/1000, Training Loss (NLML): -956.4307, (RMSE): 0.0048\n",
      "convergence dfGPdfNN Run 1/10, Epoch 188/1000, Training Loss (NLML): -956.5018, (RMSE): 0.0048\n",
      "convergence dfGPdfNN Run 1/10, Epoch 189/1000, Training Loss (NLML): -956.5309, (RMSE): 0.0048\n",
      "convergence dfGPdfNN Run 1/10, Epoch 190/1000, Training Loss (NLML): -956.6790, (RMSE): 0.0047\n",
      "convergence dfGPdfNN Run 1/10, Epoch 191/1000, Training Loss (NLML): -956.7841, (RMSE): 0.0047\n",
      "convergence dfGPdfNN Run 1/10, Epoch 192/1000, Training Loss (NLML): -956.9186, (RMSE): 0.0046\n",
      "convergence dfGPdfNN Run 1/10, Epoch 193/1000, Training Loss (NLML): -957.0262, (RMSE): 0.0046\n",
      "convergence dfGPdfNN Run 1/10, Epoch 194/1000, Training Loss (NLML): -957.1609, (RMSE): 0.0046\n",
      "convergence dfGPdfNN Run 1/10, Epoch 195/1000, Training Loss (NLML): -957.2275, (RMSE): 0.0046\n",
      "convergence dfGPdfNN Run 1/10, Epoch 196/1000, Training Loss (NLML): -957.2933, (RMSE): 0.0046\n",
      "convergence dfGPdfNN Run 1/10, Epoch 197/1000, Training Loss (NLML): -957.1798, (RMSE): 0.0048\n",
      "convergence dfGPdfNN Run 1/10, Epoch 198/1000, Training Loss (NLML): -957.0366, (RMSE): 0.0049\n",
      "convergence dfGPdfNN Run 1/10, Epoch 199/1000, Training Loss (NLML): -957.0278, (RMSE): 0.0049\n",
      "convergence dfGPdfNN Run 1/10, Epoch 200/1000, Training Loss (NLML): -956.9595, (RMSE): 0.0048\n",
      "convergence dfGPdfNN Run 1/10, Epoch 201/1000, Training Loss (NLML): -956.6886, (RMSE): 0.0049\n",
      "convergence dfGPdfNN Run 1/10, Epoch 202/1000, Training Loss (NLML): -956.6676, (RMSE): 0.0048\n",
      "convergence dfGPdfNN Run 1/10, Epoch 203/1000, Training Loss (NLML): -956.6550, (RMSE): 0.0048\n",
      "convergence dfGPdfNN Run 1/10, Epoch 204/1000, Training Loss (NLML): -956.6270, (RMSE): 0.0048\n",
      "convergence dfGPdfNN Run 1/10, Epoch 205/1000, Training Loss (NLML): -956.5840, (RMSE): 0.0048\n",
      "convergence dfGPdfNN Run 1/10, Epoch 206/1000, Training Loss (NLML): -956.6144, (RMSE): 0.0048\n",
      "convergence dfGPdfNN Run 1/10, Epoch 207/1000, Training Loss (NLML): -956.5504, (RMSE): 0.0049\n",
      "convergence dfGPdfNN Run 1/10, Epoch 208/1000, Training Loss (NLML): -956.5908, (RMSE): 0.0048\n",
      "convergence dfGPdfNN Run 1/10, Epoch 209/1000, Training Loss (NLML): -956.6241, (RMSE): 0.0048\n",
      "convergence dfGPdfNN Run 1/10, Epoch 210/1000, Training Loss (NLML): -956.6344, (RMSE): 0.0048\n",
      "convergence dfGPdfNN Run 1/10, Epoch 211/1000, Training Loss (NLML): -956.5892, (RMSE): 0.0049\n",
      "convergence dfGPdfNN Run 1/10, Epoch 212/1000, Training Loss (NLML): -956.6171, (RMSE): 0.0049\n",
      "convergence dfGPdfNN Run 1/10, Epoch 213/1000, Training Loss (NLML): -956.6389, (RMSE): 0.0049\n",
      "convergence dfGPdfNN Run 1/10, Epoch 214/1000, Training Loss (NLML): -956.6597, (RMSE): 0.0049\n",
      "convergence dfGPdfNN Run 1/10, Epoch 215/1000, Training Loss (NLML): -956.7866, (RMSE): 0.0047\n",
      "convergence dfGPdfNN Run 1/10, Epoch 216/1000, Training Loss (NLML): -956.8689, (RMSE): 0.0047\n",
      "convergence dfGPdfNN Run 1/10, Epoch 217/1000, Training Loss (NLML): -956.8540, (RMSE): 0.0047\n",
      "convergence dfGPdfNN Run 1/10, Epoch 218/1000, Training Loss (NLML): -956.7307, (RMSE): 0.0049\n",
      "convergence dfGPdfNN Run 1/10, Epoch 219/1000, Training Loss (NLML): -956.4622, (RMSE): 0.0052\n",
      "convergence dfGPdfNN Run 1/10, Epoch 220/1000, Training Loss (NLML): -956.5688, (RMSE): 0.0051\n",
      "convergence dfGPdfNN Run 1/10, Epoch 221/1000, Training Loss (NLML): -956.6970, (RMSE): 0.0049\n",
      "convergence dfGPdfNN Run 1/10, Epoch 222/1000, Training Loss (NLML): -956.7999, (RMSE): 0.0049\n",
      "convergence dfGPdfNN Run 1/10, Epoch 223/1000, Training Loss (NLML): -956.7765, (RMSE): 0.0049\n",
      "convergence dfGPdfNN Run 1/10, Epoch 224/1000, Training Loss (NLML): -956.8833, (RMSE): 0.0048\n",
      "convergence dfGPdfNN Run 1/10, Epoch 225/1000, Training Loss (NLML): -957.0195, (RMSE): 0.0047\n",
      "convergence dfGPdfNN Run 1/10, Epoch 226/1000, Training Loss (NLML): -957.1750, (RMSE): 0.0047\n",
      "convergence dfGPdfNN Run 1/10, Epoch 227/1000, Training Loss (NLML): -957.2867, (RMSE): 0.0046\n",
      "convergence dfGPdfNN Run 1/10, Epoch 228/1000, Training Loss (NLML): -957.4563, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 229/1000, Training Loss (NLML): -957.5436, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 230/1000, Training Loss (NLML): -957.5709, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 231/1000, Training Loss (NLML): -957.5338, (RMSE): 0.0046\n",
      "convergence dfGPdfNN Run 1/10, Epoch 232/1000, Training Loss (NLML): -957.4305, (RMSE): 0.0048\n",
      "convergence dfGPdfNN Run 1/10, Epoch 233/1000, Training Loss (NLML): -957.2900, (RMSE): 0.0050\n",
      "convergence dfGPdfNN Run 1/10, Epoch 234/1000, Training Loss (NLML): -957.2496, (RMSE): 0.0051\n",
      "convergence dfGPdfNN Run 1/10, Epoch 235/1000, Training Loss (NLML): -957.5381, (RMSE): 0.0048\n",
      "convergence dfGPdfNN Run 1/10, Epoch 236/1000, Training Loss (NLML): -957.6986, (RMSE): 0.0046\n",
      "convergence dfGPdfNN Run 1/10, Epoch 237/1000, Training Loss (NLML): -957.6816, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 238/1000, Training Loss (NLML): -957.5123, (RMSE): 0.0047\n",
      "convergence dfGPdfNN Run 1/10, Epoch 239/1000, Training Loss (NLML): -957.3258, (RMSE): 0.0048\n",
      "convergence dfGPdfNN Run 1/10, Epoch 240/1000, Training Loss (NLML): -957.1769, (RMSE): 0.0048\n",
      "convergence dfGPdfNN Run 1/10, Epoch 241/1000, Training Loss (NLML): -957.2217, (RMSE): 0.0048\n",
      "convergence dfGPdfNN Run 1/10, Epoch 242/1000, Training Loss (NLML): -957.2343, (RMSE): 0.0048\n",
      "convergence dfGPdfNN Run 1/10, Epoch 243/1000, Training Loss (NLML): -957.2931, (RMSE): 0.0047\n",
      "convergence dfGPdfNN Run 1/10, Epoch 244/1000, Training Loss (NLML): -957.3259, (RMSE): 0.0047\n",
      "convergence dfGPdfNN Run 1/10, Epoch 245/1000, Training Loss (NLML): -957.2842, (RMSE): 0.0048\n",
      "convergence dfGPdfNN Run 1/10, Epoch 246/1000, Training Loss (NLML): -957.2780, (RMSE): 0.0048\n",
      "convergence dfGPdfNN Run 1/10, Epoch 247/1000, Training Loss (NLML): -957.3481, (RMSE): 0.0048\n",
      "convergence dfGPdfNN Run 1/10, Epoch 248/1000, Training Loss (NLML): -957.3660, (RMSE): 0.0048\n",
      "convergence dfGPdfNN Run 1/10, Epoch 249/1000, Training Loss (NLML): -957.3879, (RMSE): 0.0047\n",
      "convergence dfGPdfNN Run 1/10, Epoch 250/1000, Training Loss (NLML): -957.5665, (RMSE): 0.0046\n",
      "convergence dfGPdfNN Run 1/10, Epoch 251/1000, Training Loss (NLML): -957.6351, (RMSE): 0.0046\n",
      "convergence dfGPdfNN Run 1/10, Epoch 252/1000, Training Loss (NLML): -957.7704, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 253/1000, Training Loss (NLML): -957.8214, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 254/1000, Training Loss (NLML): -957.9052, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 255/1000, Training Loss (NLML): -957.9435, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 256/1000, Training Loss (NLML): -957.9431, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 257/1000, Training Loss (NLML): -957.9299, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 258/1000, Training Loss (NLML): -957.8928, (RMSE): 0.0046\n",
      "convergence dfGPdfNN Run 1/10, Epoch 259/1000, Training Loss (NLML): -957.8020, (RMSE): 0.0047\n",
      "convergence dfGPdfNN Run 1/10, Epoch 260/1000, Training Loss (NLML): -957.8214, (RMSE): 0.0047\n",
      "convergence dfGPdfNN Run 1/10, Epoch 261/1000, Training Loss (NLML): -957.9572, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 262/1000, Training Loss (NLML): -958.0355, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 263/1000, Training Loss (NLML): -958.0897, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 264/1000, Training Loss (NLML): -958.1146, (RMSE): 0.0043\n",
      "convergence dfGPdfNN Run 1/10, Epoch 265/1000, Training Loss (NLML): -958.0903, (RMSE): 0.0043\n",
      "convergence dfGPdfNN Run 1/10, Epoch 266/1000, Training Loss (NLML): -958.1265, (RMSE): 0.0043\n",
      "convergence dfGPdfNN Run 1/10, Epoch 267/1000, Training Loss (NLML): -958.0261, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 268/1000, Training Loss (NLML): -957.8077, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 269/1000, Training Loss (NLML): -957.7916, (RMSE): 0.0046\n",
      "convergence dfGPdfNN Run 1/10, Epoch 270/1000, Training Loss (NLML): -957.8206, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 271/1000, Training Loss (NLML): -957.8230, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 272/1000, Training Loss (NLML): -957.8241, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 273/1000, Training Loss (NLML): -957.6819, (RMSE): 0.0047\n",
      "convergence dfGPdfNN Run 1/10, Epoch 274/1000, Training Loss (NLML): -957.7014, (RMSE): 0.0047\n",
      "convergence dfGPdfNN Run 1/10, Epoch 275/1000, Training Loss (NLML): -957.7025, (RMSE): 0.0047\n",
      "convergence dfGPdfNN Run 1/10, Epoch 276/1000, Training Loss (NLML): -957.7888, (RMSE): 0.0046\n",
      "convergence dfGPdfNN Run 1/10, Epoch 277/1000, Training Loss (NLML): -957.8101, (RMSE): 0.0046\n",
      "convergence dfGPdfNN Run 1/10, Epoch 278/1000, Training Loss (NLML): -957.9169, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 279/1000, Training Loss (NLML): -958.0576, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 280/1000, Training Loss (NLML): -958.1838, (RMSE): 0.0043\n",
      "convergence dfGPdfNN Run 1/10, Epoch 281/1000, Training Loss (NLML): -958.2427, (RMSE): 0.0043\n",
      "convergence dfGPdfNN Run 1/10, Epoch 282/1000, Training Loss (NLML): -958.2931, (RMSE): 0.0042\n",
      "convergence dfGPdfNN Run 1/10, Epoch 283/1000, Training Loss (NLML): -958.3361, (RMSE): 0.0042\n",
      "convergence dfGPdfNN Run 1/10, Epoch 284/1000, Training Loss (NLML): -958.3489, (RMSE): 0.0042\n",
      "convergence dfGPdfNN Run 1/10, Epoch 285/1000, Training Loss (NLML): -958.3766, (RMSE): 0.0042\n",
      "convergence dfGPdfNN Run 1/10, Epoch 286/1000, Training Loss (NLML): -958.3822, (RMSE): 0.0042\n",
      "convergence dfGPdfNN Run 1/10, Epoch 287/1000, Training Loss (NLML): -958.3397, (RMSE): 0.0043\n",
      "convergence dfGPdfNN Run 1/10, Epoch 288/1000, Training Loss (NLML): -958.3308, (RMSE): 0.0043\n",
      "convergence dfGPdfNN Run 1/10, Epoch 289/1000, Training Loss (NLML): -958.3754, (RMSE): 0.0043\n",
      "convergence dfGPdfNN Run 1/10, Epoch 290/1000, Training Loss (NLML): -958.3905, (RMSE): 0.0043\n",
      "convergence dfGPdfNN Run 1/10, Epoch 291/1000, Training Loss (NLML): -958.3646, (RMSE): 0.0043\n",
      "convergence dfGPdfNN Run 1/10, Epoch 292/1000, Training Loss (NLML): -958.3667, (RMSE): 0.0042\n",
      "convergence dfGPdfNN Run 1/10, Epoch 293/1000, Training Loss (NLML): -958.2125, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 294/1000, Training Loss (NLML): -958.1373, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 295/1000, Training Loss (NLML): -958.1033, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 296/1000, Training Loss (NLML): -958.0590, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 297/1000, Training Loss (NLML): -958.0173, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 298/1000, Training Loss (NLML): -958.0402, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 299/1000, Training Loss (NLML): -958.0566, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 300/1000, Training Loss (NLML): -958.0824, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 301/1000, Training Loss (NLML): -958.0847, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 302/1000, Training Loss (NLML): -958.0973, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 303/1000, Training Loss (NLML): -958.0994, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 304/1000, Training Loss (NLML): -958.2604, (RMSE): 0.0043\n",
      "convergence dfGPdfNN Run 1/10, Epoch 305/1000, Training Loss (NLML): -958.3208, (RMSE): 0.0043\n",
      "convergence dfGPdfNN Run 1/10, Epoch 306/1000, Training Loss (NLML): -958.4329, (RMSE): 0.0042\n",
      "convergence dfGPdfNN Run 1/10, Epoch 307/1000, Training Loss (NLML): -958.4663, (RMSE): 0.0042\n",
      "convergence dfGPdfNN Run 1/10, Epoch 308/1000, Training Loss (NLML): -958.5215, (RMSE): 0.0041\n",
      "convergence dfGPdfNN Run 1/10, Epoch 309/1000, Training Loss (NLML): -958.5977, (RMSE): 0.0041\n",
      "convergence dfGPdfNN Run 1/10, Epoch 310/1000, Training Loss (NLML): -958.6090, (RMSE): 0.0041\n",
      "convergence dfGPdfNN Run 1/10, Epoch 311/1000, Training Loss (NLML): -958.6300, (RMSE): 0.0041\n",
      "convergence dfGPdfNN Run 1/10, Epoch 312/1000, Training Loss (NLML): -958.6125, (RMSE): 0.0041\n",
      "convergence dfGPdfNN Run 1/10, Epoch 313/1000, Training Loss (NLML): -958.5535, (RMSE): 0.0042\n",
      "convergence dfGPdfNN Run 1/10, Epoch 314/1000, Training Loss (NLML): -958.5470, (RMSE): 0.0043\n",
      "convergence dfGPdfNN Run 1/10, Epoch 315/1000, Training Loss (NLML): -958.6036, (RMSE): 0.0042\n",
      "convergence dfGPdfNN Run 1/10, Epoch 316/1000, Training Loss (NLML): -958.6169, (RMSE): 0.0042\n",
      "convergence dfGPdfNN Run 1/10, Epoch 317/1000, Training Loss (NLML): -958.6537, (RMSE): 0.0041\n",
      "convergence dfGPdfNN Run 1/10, Epoch 318/1000, Training Loss (NLML): -958.7084, (RMSE): 0.0040\n",
      "convergence dfGPdfNN Run 1/10, Epoch 319/1000, Training Loss (NLML): -958.6823, (RMSE): 0.0041\n",
      "convergence dfGPdfNN Run 1/10, Epoch 320/1000, Training Loss (NLML): -958.6368, (RMSE): 0.0041\n",
      "convergence dfGPdfNN Run 1/10, Epoch 321/1000, Training Loss (NLML): -958.4847, (RMSE): 0.0042\n",
      "convergence dfGPdfNN Run 1/10, Epoch 322/1000, Training Loss (NLML): -958.3591, (RMSE): 0.0043\n",
      "convergence dfGPdfNN Run 1/10, Epoch 323/1000, Training Loss (NLML): -958.3224, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 324/1000, Training Loss (NLML): -958.2877, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 325/1000, Training Loss (NLML): -958.3014, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 326/1000, Training Loss (NLML): -958.2433, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 327/1000, Training Loss (NLML): -958.2104, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 328/1000, Training Loss (NLML): -958.1250, (RMSE): 0.0046\n",
      "convergence dfGPdfNN Run 1/10, Epoch 329/1000, Training Loss (NLML): -958.1497, (RMSE): 0.0046\n",
      "convergence dfGPdfNN Run 1/10, Epoch 330/1000, Training Loss (NLML): -958.2528, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 331/1000, Training Loss (NLML): -958.2866, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 332/1000, Training Loss (NLML): -958.2664, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 333/1000, Training Loss (NLML): -958.2817, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 334/1000, Training Loss (NLML): -958.2887, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 335/1000, Training Loss (NLML): -958.2988, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 336/1000, Training Loss (NLML): -958.3062, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 337/1000, Training Loss (NLML): -958.3149, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 338/1000, Training Loss (NLML): -958.2576, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 339/1000, Training Loss (NLML): -958.2649, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 340/1000, Training Loss (NLML): -958.2787, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 341/1000, Training Loss (NLML): -958.2877, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 342/1000, Training Loss (NLML): -958.2997, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 343/1000, Training Loss (NLML): -958.3130, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 344/1000, Training Loss (NLML): -958.3225, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 345/1000, Training Loss (NLML): -958.3442, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 346/1000, Training Loss (NLML): -958.3091, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 347/1000, Training Loss (NLML): -958.3215, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 348/1000, Training Loss (NLML): -958.3317, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 349/1000, Training Loss (NLML): -958.3446, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 350/1000, Training Loss (NLML): -958.3566, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 351/1000, Training Loss (NLML): -958.3542, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 352/1000, Training Loss (NLML): -958.3641, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 353/1000, Training Loss (NLML): -958.3801, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 354/1000, Training Loss (NLML): -958.4231, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 355/1000, Training Loss (NLML): -958.4393, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 356/1000, Training Loss (NLML): -958.4498, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 357/1000, Training Loss (NLML): -958.4475, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 358/1000, Training Loss (NLML): -958.4535, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 359/1000, Training Loss (NLML): -958.4562, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 360/1000, Training Loss (NLML): -958.4640, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 361/1000, Training Loss (NLML): -958.4948, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 362/1000, Training Loss (NLML): -958.5244, (RMSE): 0.0043\n",
      "convergence dfGPdfNN Run 1/10, Epoch 363/1000, Training Loss (NLML): -958.5389, (RMSE): 0.0043\n",
      "convergence dfGPdfNN Run 1/10, Epoch 364/1000, Training Loss (NLML): -958.5559, (RMSE): 0.0043\n",
      "convergence dfGPdfNN Run 1/10, Epoch 365/1000, Training Loss (NLML): -958.5773, (RMSE): 0.0043\n",
      "convergence dfGPdfNN Run 1/10, Epoch 366/1000, Training Loss (NLML): -958.5837, (RMSE): 0.0043\n",
      "convergence dfGPdfNN Run 1/10, Epoch 367/1000, Training Loss (NLML): -958.5891, (RMSE): 0.0043\n",
      "convergence dfGPdfNN Run 1/10, Epoch 368/1000, Training Loss (NLML): -958.6405, (RMSE): 0.0042\n",
      "Early stopping triggered after 368 epochs.\n",
      "\n",
      "--- Training Run 2/10 ---\n",
      "\n",
      "Start Training\n",
      "convergence dfGPdfNN Run 2/10, Epoch 1/1000, Training Loss (NLML): -884.1392\n",
      "convergence dfGPdfNN Run 2/10, Epoch 2/1000, Training Loss (NLML): -888.4862\n",
      "convergence dfGPdfNN Run 2/10, Epoch 3/1000, Training Loss (NLML): -891.8903\n",
      "convergence dfGPdfNN Run 2/10, Epoch 4/1000, Training Loss (NLML): -894.4111\n",
      "convergence dfGPdfNN Run 2/10, Epoch 5/1000, Training Loss (NLML): -896.5400\n",
      "convergence dfGPdfNN Run 2/10, Epoch 6/1000, Training Loss (NLML): -898.5034\n",
      "convergence dfGPdfNN Run 2/10, Epoch 7/1000, Training Loss (NLML): -900.2649\n",
      "convergence dfGPdfNN Run 2/10, Epoch 8/1000, Training Loss (NLML): -900.9675\n",
      "convergence dfGPdfNN Run 2/10, Epoch 9/1000, Training Loss (NLML): -903.3693\n",
      "convergence dfGPdfNN Run 2/10, Epoch 10/1000, Training Loss (NLML): -904.2816\n",
      "convergence dfGPdfNN Run 2/10, Epoch 11/1000, Training Loss (NLML): -905.8193\n",
      "convergence dfGPdfNN Run 2/10, Epoch 12/1000, Training Loss (NLML): -907.9288\n",
      "convergence dfGPdfNN Run 2/10, Epoch 13/1000, Training Loss (NLML): -909.6349\n",
      "convergence dfGPdfNN Run 2/10, Epoch 14/1000, Training Loss (NLML): -911.1056\n",
      "convergence dfGPdfNN Run 2/10, Epoch 15/1000, Training Loss (NLML): -912.5812\n",
      "convergence dfGPdfNN Run 2/10, Epoch 16/1000, Training Loss (NLML): -913.8862\n",
      "convergence dfGPdfNN Run 2/10, Epoch 17/1000, Training Loss (NLML): -915.0968\n",
      "convergence dfGPdfNN Run 2/10, Epoch 18/1000, Training Loss (NLML): -916.2499\n",
      "convergence dfGPdfNN Run 2/10, Epoch 19/1000, Training Loss (NLML): -917.1807\n",
      "convergence dfGPdfNN Run 2/10, Epoch 20/1000, Training Loss (NLML): -917.5493\n",
      "convergence dfGPdfNN Run 2/10, Epoch 21/1000, Training Loss (NLML): -918.6295\n",
      "convergence dfGPdfNN Run 2/10, Epoch 22/1000, Training Loss (NLML): -919.8401\n",
      "convergence dfGPdfNN Run 2/10, Epoch 23/1000, Training Loss (NLML): -920.8618\n",
      "convergence dfGPdfNN Run 2/10, Epoch 24/1000, Training Loss (NLML): -921.7246\n",
      "convergence dfGPdfNN Run 2/10, Epoch 25/1000, Training Loss (NLML): -922.7374\n",
      "convergence dfGPdfNN Run 2/10, Epoch 26/1000, Training Loss (NLML): -923.7542\n",
      "convergence dfGPdfNN Run 2/10, Epoch 27/1000, Training Loss (NLML): -924.7489\n",
      "convergence dfGPdfNN Run 2/10, Epoch 28/1000, Training Loss (NLML): -925.6122\n",
      "convergence dfGPdfNN Run 2/10, Epoch 29/1000, Training Loss (NLML): -926.4144\n",
      "convergence dfGPdfNN Run 2/10, Epoch 30/1000, Training Loss (NLML): -927.1986\n",
      "convergence dfGPdfNN Run 2/10, Epoch 31/1000, Training Loss (NLML): -927.8845\n",
      "convergence dfGPdfNN Run 2/10, Epoch 32/1000, Training Loss (NLML): -928.6355\n",
      "convergence dfGPdfNN Run 2/10, Epoch 33/1000, Training Loss (NLML): -929.3392\n",
      "convergence dfGPdfNN Run 2/10, Epoch 34/1000, Training Loss (NLML): -930.0126\n",
      "convergence dfGPdfNN Run 2/10, Epoch 35/1000, Training Loss (NLML): -930.6610\n",
      "convergence dfGPdfNN Run 2/10, Epoch 36/1000, Training Loss (NLML): -931.2747\n",
      "convergence dfGPdfNN Run 2/10, Epoch 37/1000, Training Loss (NLML): -931.8641\n",
      "convergence dfGPdfNN Run 2/10, Epoch 38/1000, Training Loss (NLML): -932.4507\n",
      "convergence dfGPdfNN Run 2/10, Epoch 39/1000, Training Loss (NLML): -933.0111\n",
      "convergence dfGPdfNN Run 2/10, Epoch 40/1000, Training Loss (NLML): -932.6791\n",
      "convergence dfGPdfNN Run 2/10, Epoch 41/1000, Training Loss (NLML): -932.7017\n",
      "convergence dfGPdfNN Run 2/10, Epoch 42/1000, Training Loss (NLML): -933.4495\n",
      "convergence dfGPdfNN Run 2/10, Epoch 43/1000, Training Loss (NLML): -934.3143\n",
      "convergence dfGPdfNN Run 2/10, Epoch 44/1000, Training Loss (NLML): -935.1333\n",
      "convergence dfGPdfNN Run 2/10, Epoch 45/1000, Training Loss (NLML): -935.8229\n",
      "convergence dfGPdfNN Run 2/10, Epoch 46/1000, Training Loss (NLML): -935.4401\n",
      "convergence dfGPdfNN Run 2/10, Epoch 47/1000, Training Loss (NLML): -935.8433\n",
      "convergence dfGPdfNN Run 2/10, Epoch 48/1000, Training Loss (NLML): -936.5574\n",
      "convergence dfGPdfNN Run 2/10, Epoch 49/1000, Training Loss (NLML): -937.1697\n",
      "convergence dfGPdfNN Run 2/10, Epoch 50/1000, Training Loss (NLML): -937.4215\n",
      "convergence dfGPdfNN Run 2/10, Epoch 51/1000, Training Loss (NLML): -938.2341\n",
      "convergence dfGPdfNN Run 2/10, Epoch 52/1000, Training Loss (NLML): -938.6252\n",
      "convergence dfGPdfNN Run 2/10, Epoch 53/1000, Training Loss (NLML): -938.9757\n",
      "convergence dfGPdfNN Run 2/10, Epoch 54/1000, Training Loss (NLML): -939.3551\n",
      "convergence dfGPdfNN Run 2/10, Epoch 55/1000, Training Loss (NLML): -939.7488\n",
      "convergence dfGPdfNN Run 2/10, Epoch 56/1000, Training Loss (NLML): -940.1638\n",
      "convergence dfGPdfNN Run 2/10, Epoch 57/1000, Training Loss (NLML): -940.3937\n",
      "convergence dfGPdfNN Run 2/10, Epoch 58/1000, Training Loss (NLML): -940.7716\n",
      "convergence dfGPdfNN Run 2/10, Epoch 59/1000, Training Loss (NLML): -941.1379\n",
      "convergence dfGPdfNN Run 2/10, Epoch 60/1000, Training Loss (NLML): -941.5439\n",
      "convergence dfGPdfNN Run 2/10, Epoch 61/1000, Training Loss (NLML): -941.8960\n",
      "convergence dfGPdfNN Run 2/10, Epoch 62/1000, Training Loss (NLML): -942.2166\n",
      "convergence dfGPdfNN Run 2/10, Epoch 63/1000, Training Loss (NLML): -942.5187\n",
      "convergence dfGPdfNN Run 2/10, Epoch 64/1000, Training Loss (NLML): -942.6304\n",
      "convergence dfGPdfNN Run 2/10, Epoch 65/1000, Training Loss (NLML): -942.6572\n",
      "convergence dfGPdfNN Run 2/10, Epoch 66/1000, Training Loss (NLML): -943.0045\n",
      "convergence dfGPdfNN Run 2/10, Epoch 67/1000, Training Loss (NLML): -943.4823\n",
      "convergence dfGPdfNN Run 2/10, Epoch 68/1000, Training Loss (NLML): -943.8713\n",
      "convergence dfGPdfNN Run 2/10, Epoch 69/1000, Training Loss (NLML): -944.1642\n",
      "convergence dfGPdfNN Run 2/10, Epoch 70/1000, Training Loss (NLML): -944.4237\n",
      "convergence dfGPdfNN Run 2/10, Epoch 71/1000, Training Loss (NLML): -944.6307\n",
      "convergence dfGPdfNN Run 2/10, Epoch 72/1000, Training Loss (NLML): -944.9451\n",
      "convergence dfGPdfNN Run 2/10, Epoch 73/1000, Training Loss (NLML): -945.2374\n",
      "convergence dfGPdfNN Run 2/10, Epoch 74/1000, Training Loss (NLML): -945.4055\n",
      "convergence dfGPdfNN Run 2/10, Epoch 75/1000, Training Loss (NLML): -945.8262\n",
      "convergence dfGPdfNN Run 2/10, Epoch 76/1000, Training Loss (NLML): -941.4781\n",
      "convergence dfGPdfNN Run 2/10, Epoch 77/1000, Training Loss (NLML): -938.9109\n",
      "convergence dfGPdfNN Run 2/10, Epoch 78/1000, Training Loss (NLML): -935.8883\n",
      "convergence dfGPdfNN Run 2/10, Epoch 79/1000, Training Loss (NLML): -937.2950\n",
      "convergence dfGPdfNN Run 2/10, Epoch 80/1000, Training Loss (NLML): -940.6377\n",
      "convergence dfGPdfNN Run 2/10, Epoch 81/1000, Training Loss (NLML): -943.8778\n",
      "convergence dfGPdfNN Run 2/10, Epoch 82/1000, Training Loss (NLML): -946.1470\n",
      "convergence dfGPdfNN Run 2/10, Epoch 83/1000, Training Loss (NLML): -946.9830\n",
      "convergence dfGPdfNN Run 2/10, Epoch 84/1000, Training Loss (NLML): -946.6055\n",
      "convergence dfGPdfNN Run 2/10, Epoch 85/1000, Training Loss (NLML): -946.2336\n",
      "convergence dfGPdfNN Run 2/10, Epoch 86/1000, Training Loss (NLML): -946.4335\n",
      "convergence dfGPdfNN Run 2/10, Epoch 87/1000, Training Loss (NLML): -947.0017\n",
      "convergence dfGPdfNN Run 2/10, Epoch 88/1000, Training Loss (NLML): -947.5354\n",
      "convergence dfGPdfNN Run 2/10, Epoch 89/1000, Training Loss (NLML): -947.8118\n",
      "convergence dfGPdfNN Run 2/10, Epoch 90/1000, Training Loss (NLML): -947.6787\n",
      "convergence dfGPdfNN Run 2/10, Epoch 91/1000, Training Loss (NLML): -948.5481\n",
      "convergence dfGPdfNN Run 2/10, Epoch 92/1000, Training Loss (NLML): -948.8546\n",
      "convergence dfGPdfNN Run 2/10, Epoch 93/1000, Training Loss (NLML): -949.0793\n",
      "convergence dfGPdfNN Run 2/10, Epoch 94/1000, Training Loss (NLML): -949.2687\n",
      "convergence dfGPdfNN Run 2/10, Epoch 95/1000, Training Loss (NLML): -949.1324\n",
      "convergence dfGPdfNN Run 2/10, Epoch 96/1000, Training Loss (NLML): -949.0305\n",
      "convergence dfGPdfNN Run 2/10, Epoch 97/1000, Training Loss (NLML): -946.2759\n",
      "convergence dfGPdfNN Run 2/10, Epoch 98/1000, Training Loss (NLML): -942.7100\n",
      "convergence dfGPdfNN Run 2/10, Epoch 99/1000, Training Loss (NLML): -945.7913\n",
      "convergence dfGPdfNN Run 2/10, Epoch 100/1000, Training Loss (NLML): -947.8796\n",
      "convergence dfGPdfNN Run 2/10, Epoch 101/1000, Training Loss (NLML): -948.5135\n",
      "convergence dfGPdfNN Run 2/10, Epoch 102/1000, Training Loss (NLML): -948.7762\n",
      "convergence dfGPdfNN Run 2/10, Epoch 103/1000, Training Loss (NLML): -949.3511\n",
      "convergence dfGPdfNN Run 2/10, Epoch 104/1000, Training Loss (NLML): -949.2986\n",
      "convergence dfGPdfNN Run 2/10, Epoch 105/1000, Training Loss (NLML): -949.4075\n",
      "convergence dfGPdfNN Run 2/10, Epoch 106/1000, Training Loss (NLML): -949.6696\n",
      "convergence dfGPdfNN Run 2/10, Epoch 107/1000, Training Loss (NLML): -949.6508\n",
      "convergence dfGPdfNN Run 2/10, Epoch 108/1000, Training Loss (NLML): -949.7371\n",
      "convergence dfGPdfNN Run 2/10, Epoch 109/1000, Training Loss (NLML): -949.7993\n",
      "convergence dfGPdfNN Run 2/10, Epoch 110/1000, Training Loss (NLML): -949.9652\n",
      "convergence dfGPdfNN Run 2/10, Epoch 111/1000, Training Loss (NLML): -950.0520\n",
      "convergence dfGPdfNN Run 2/10, Epoch 112/1000, Training Loss (NLML): -950.0496\n",
      "convergence dfGPdfNN Run 2/10, Epoch 113/1000, Training Loss (NLML): -950.2177\n",
      "convergence dfGPdfNN Run 2/10, Epoch 114/1000, Training Loss (NLML): -950.3857\n",
      "convergence dfGPdfNN Run 2/10, Epoch 115/1000, Training Loss (NLML): -950.5573\n",
      "convergence dfGPdfNN Run 2/10, Epoch 116/1000, Training Loss (NLML): -950.7557\n",
      "convergence dfGPdfNN Run 2/10, Epoch 117/1000, Training Loss (NLML): -950.9478\n",
      "convergence dfGPdfNN Run 2/10, Epoch 118/1000, Training Loss (NLML): -951.0614\n",
      "convergence dfGPdfNN Run 2/10, Epoch 119/1000, Training Loss (NLML): -951.1467\n",
      "convergence dfGPdfNN Run 2/10, Epoch 120/1000, Training Loss (NLML): -951.2554\n",
      "convergence dfGPdfNN Run 2/10, Epoch 121/1000, Training Loss (NLML): -951.3710\n",
      "convergence dfGPdfNN Run 2/10, Epoch 122/1000, Training Loss (NLML): -951.3480\n",
      "convergence dfGPdfNN Run 2/10, Epoch 123/1000, Training Loss (NLML): -951.4015\n",
      "convergence dfGPdfNN Run 2/10, Epoch 124/1000, Training Loss (NLML): -951.4147\n",
      "convergence dfGPdfNN Run 2/10, Epoch 125/1000, Training Loss (NLML): -951.5065\n",
      "convergence dfGPdfNN Run 2/10, Epoch 126/1000, Training Loss (NLML): -951.5786\n",
      "convergence dfGPdfNN Run 2/10, Epoch 127/1000, Training Loss (NLML): -951.6858\n",
      "convergence dfGPdfNN Run 2/10, Epoch 128/1000, Training Loss (NLML): -951.7946\n",
      "convergence dfGPdfNN Run 2/10, Epoch 129/1000, Training Loss (NLML): -952.3678\n",
      "convergence dfGPdfNN Run 2/10, Epoch 130/1000, Training Loss (NLML): -952.4220\n",
      "convergence dfGPdfNN Run 2/10, Epoch 131/1000, Training Loss (NLML): -952.5657\n",
      "convergence dfGPdfNN Run 2/10, Epoch 132/1000, Training Loss (NLML): -952.5841\n",
      "convergence dfGPdfNN Run 2/10, Epoch 133/1000, Training Loss (NLML): -952.5986\n",
      "convergence dfGPdfNN Run 2/10, Epoch 134/1000, Training Loss (NLML): -952.5803\n",
      "convergence dfGPdfNN Run 2/10, Epoch 135/1000, Training Loss (NLML): -952.4835\n",
      "convergence dfGPdfNN Run 2/10, Epoch 136/1000, Training Loss (NLML): -952.7825\n",
      "convergence dfGPdfNN Run 2/10, Epoch 137/1000, Training Loss (NLML): -952.9329\n",
      "convergence dfGPdfNN Run 2/10, Epoch 138/1000, Training Loss (NLML): -953.0382\n",
      "convergence dfGPdfNN Run 2/10, Epoch 139/1000, Training Loss (NLML): -953.1587\n",
      "convergence dfGPdfNN Run 2/10, Epoch 140/1000, Training Loss (NLML): -953.3057\n",
      "convergence dfGPdfNN Run 2/10, Epoch 141/1000, Training Loss (NLML): -953.1177\n",
      "convergence dfGPdfNN Run 2/10, Epoch 142/1000, Training Loss (NLML): -953.4656\n",
      "convergence dfGPdfNN Run 2/10, Epoch 143/1000, Training Loss (NLML): -953.5441\n",
      "convergence dfGPdfNN Run 2/10, Epoch 144/1000, Training Loss (NLML): -953.6326\n",
      "convergence dfGPdfNN Run 2/10, Epoch 145/1000, Training Loss (NLML): -953.9126\n",
      "convergence dfGPdfNN Run 2/10, Epoch 146/1000, Training Loss (NLML): -954.0845\n",
      "convergence dfGPdfNN Run 2/10, Epoch 147/1000, Training Loss (NLML): -954.1047\n",
      "convergence dfGPdfNN Run 2/10, Epoch 148/1000, Training Loss (NLML): -954.1453\n",
      "convergence dfGPdfNN Run 2/10, Epoch 149/1000, Training Loss (NLML): -938.4696\n",
      "convergence dfGPdfNN Run 2/10, Epoch 150/1000, Training Loss (NLML): -934.1567\n",
      "convergence dfGPdfNN Run 2/10, Epoch 151/1000, Training Loss (NLML): -935.1003\n",
      "convergence dfGPdfNN Run 2/10, Epoch 152/1000, Training Loss (NLML): -945.3390\n",
      "convergence dfGPdfNN Run 2/10, Epoch 153/1000, Training Loss (NLML): -951.6500\n",
      "convergence dfGPdfNN Run 2/10, Epoch 154/1000, Training Loss (NLML): -951.9564\n",
      "convergence dfGPdfNN Run 2/10, Epoch 155/1000, Training Loss (NLML): -951.7491\n",
      "convergence dfGPdfNN Run 2/10, Epoch 156/1000, Training Loss (NLML): -951.8667\n",
      "convergence dfGPdfNN Run 2/10, Epoch 157/1000, Training Loss (NLML): -952.0035\n",
      "convergence dfGPdfNN Run 2/10, Epoch 158/1000, Training Loss (NLML): -952.0995\n",
      "convergence dfGPdfNN Run 2/10, Epoch 159/1000, Training Loss (NLML): -951.0109\n",
      "convergence dfGPdfNN Run 2/10, Epoch 160/1000, Training Loss (NLML): -950.0646\n",
      "convergence dfGPdfNN Run 2/10, Epoch 161/1000, Training Loss (NLML): -951.8894\n",
      "convergence dfGPdfNN Run 2/10, Epoch 162/1000, Training Loss (NLML): -952.4316\n",
      "convergence dfGPdfNN Run 2/10, Epoch 163/1000, Training Loss (NLML): -952.1536\n",
      "convergence dfGPdfNN Run 2/10, Epoch 164/1000, Training Loss (NLML): -952.1614\n",
      "convergence dfGPdfNN Run 2/10, Epoch 165/1000, Training Loss (NLML): -952.1945\n",
      "convergence dfGPdfNN Run 2/10, Epoch 166/1000, Training Loss (NLML): -952.2982\n",
      "convergence dfGPdfNN Run 2/10, Epoch 167/1000, Training Loss (NLML): -952.3792\n",
      "convergence dfGPdfNN Run 2/10, Epoch 168/1000, Training Loss (NLML): -952.4281\n",
      "convergence dfGPdfNN Run 2/10, Epoch 169/1000, Training Loss (NLML): -952.5286\n",
      "convergence dfGPdfNN Run 2/10, Epoch 170/1000, Training Loss (NLML): -952.2020\n",
      "convergence dfGPdfNN Run 2/10, Epoch 171/1000, Training Loss (NLML): -952.6147\n",
      "convergence dfGPdfNN Run 2/10, Epoch 172/1000, Training Loss (NLML): -952.6622\n",
      "convergence dfGPdfNN Run 2/10, Epoch 173/1000, Training Loss (NLML): -952.6234\n",
      "convergence dfGPdfNN Run 2/10, Epoch 174/1000, Training Loss (NLML): -952.5167\n",
      "convergence dfGPdfNN Run 2/10, Epoch 175/1000, Training Loss (NLML): -952.5421\n",
      "convergence dfGPdfNN Run 2/10, Epoch 176/1000, Training Loss (NLML): -952.5757\n",
      "convergence dfGPdfNN Run 2/10, Epoch 177/1000, Training Loss (NLML): -952.6343\n",
      "convergence dfGPdfNN Run 2/10, Epoch 178/1000, Training Loss (NLML): -952.7463\n",
      "convergence dfGPdfNN Run 2/10, Epoch 179/1000, Training Loss (NLML): -952.9113\n",
      "convergence dfGPdfNN Run 2/10, Epoch 180/1000, Training Loss (NLML): -953.0247\n",
      "convergence dfGPdfNN Run 2/10, Epoch 181/1000, Training Loss (NLML): -953.1422\n",
      "convergence dfGPdfNN Run 2/10, Epoch 182/1000, Training Loss (NLML): -953.2468\n",
      "convergence dfGPdfNN Run 2/10, Epoch 183/1000, Training Loss (NLML): -953.2513\n",
      "convergence dfGPdfNN Run 2/10, Epoch 184/1000, Training Loss (NLML): -953.3241\n",
      "convergence dfGPdfNN Run 2/10, Epoch 185/1000, Training Loss (NLML): -953.4377\n",
      "convergence dfGPdfNN Run 2/10, Epoch 186/1000, Training Loss (NLML): -953.5171\n",
      "convergence dfGPdfNN Run 2/10, Epoch 187/1000, Training Loss (NLML): -953.5768\n",
      "convergence dfGPdfNN Run 2/10, Epoch 188/1000, Training Loss (NLML): -953.7278\n",
      "convergence dfGPdfNN Run 2/10, Epoch 189/1000, Training Loss (NLML): -953.7753\n",
      "convergence dfGPdfNN Run 2/10, Epoch 190/1000, Training Loss (NLML): -953.8853\n",
      "convergence dfGPdfNN Run 2/10, Epoch 191/1000, Training Loss (NLML): -954.0483\n",
      "convergence dfGPdfNN Run 2/10, Epoch 192/1000, Training Loss (NLML): -954.0840\n",
      "convergence dfGPdfNN Run 2/10, Epoch 193/1000, Training Loss (NLML): -954.1671\n",
      "convergence dfGPdfNN Run 2/10, Epoch 194/1000, Training Loss (NLML): -954.2560\n",
      "convergence dfGPdfNN Run 2/10, Epoch 195/1000, Training Loss (NLML): -954.3564\n",
      "convergence dfGPdfNN Run 2/10, Epoch 196/1000, Training Loss (NLML): -954.4069\n",
      "convergence dfGPdfNN Run 2/10, Epoch 197/1000, Training Loss (NLML): -954.4590\n",
      "convergence dfGPdfNN Run 2/10, Epoch 198/1000, Training Loss (NLML): -954.4596\n",
      "convergence dfGPdfNN Run 2/10, Epoch 199/1000, Training Loss (NLML): -954.3790\n",
      "convergence dfGPdfNN Run 2/10, Epoch 200/1000, Training Loss (NLML): -954.2498\n",
      "convergence dfGPdfNN Run 2/10, Epoch 201/1000, Training Loss (NLML): -954.4115\n",
      "convergence dfGPdfNN Run 2/10, Epoch 202/1000, Training Loss (NLML): -954.4680\n",
      "convergence dfGPdfNN Run 2/10, Epoch 203/1000, Training Loss (NLML): -953.3136\n",
      "convergence dfGPdfNN Run 2/10, Epoch 204/1000, Training Loss (NLML): -949.0040\n",
      "convergence dfGPdfNN Run 2/10, Epoch 205/1000, Training Loss (NLML): -950.4375\n",
      "convergence dfGPdfNN Run 2/10, Epoch 206/1000, Training Loss (NLML): -952.8220\n",
      "convergence dfGPdfNN Run 2/10, Epoch 207/1000, Training Loss (NLML): -953.5277\n",
      "convergence dfGPdfNN Run 2/10, Epoch 208/1000, Training Loss (NLML): -954.6201\n",
      "convergence dfGPdfNN Run 2/10, Epoch 209/1000, Training Loss (NLML): -954.7830\n",
      "convergence dfGPdfNN Run 2/10, Epoch 210/1000, Training Loss (NLML): -954.9269\n",
      "convergence dfGPdfNN Run 2/10, Epoch 211/1000, Training Loss (NLML): -954.8516\n",
      "convergence dfGPdfNN Run 2/10, Epoch 212/1000, Training Loss (NLML): -954.7261\n",
      "convergence dfGPdfNN Run 2/10, Epoch 213/1000, Training Loss (NLML): -954.7272\n",
      "convergence dfGPdfNN Run 2/10, Epoch 214/1000, Training Loss (NLML): -954.7609\n",
      "convergence dfGPdfNN Run 2/10, Epoch 215/1000, Training Loss (NLML): -954.8383\n",
      "convergence dfGPdfNN Run 2/10, Epoch 216/1000, Training Loss (NLML): -954.9709\n",
      "convergence dfGPdfNN Run 2/10, Epoch 217/1000, Training Loss (NLML): -955.1329\n",
      "convergence dfGPdfNN Run 2/10, Epoch 218/1000, Training Loss (NLML): -955.3032\n",
      "convergence dfGPdfNN Run 2/10, Epoch 219/1000, Training Loss (NLML): -955.3533\n",
      "convergence dfGPdfNN Run 2/10, Epoch 220/1000, Training Loss (NLML): -955.3832\n",
      "convergence dfGPdfNN Run 2/10, Epoch 221/1000, Training Loss (NLML): -955.3413\n",
      "convergence dfGPdfNN Run 2/10, Epoch 222/1000, Training Loss (NLML): -955.2300\n",
      "convergence dfGPdfNN Run 2/10, Epoch 223/1000, Training Loss (NLML): -955.3522\n",
      "convergence dfGPdfNN Run 2/10, Epoch 224/1000, Training Loss (NLML): -955.4636\n",
      "convergence dfGPdfNN Run 2/10, Epoch 225/1000, Training Loss (NLML): -955.5538\n",
      "convergence dfGPdfNN Run 2/10, Epoch 226/1000, Training Loss (NLML): -955.6119\n",
      "convergence dfGPdfNN Run 2/10, Epoch 227/1000, Training Loss (NLML): -938.8821\n",
      "convergence dfGPdfNN Run 2/10, Epoch 228/1000, Training Loss (NLML): -934.3165\n",
      "convergence dfGPdfNN Run 2/10, Epoch 229/1000, Training Loss (NLML): -955.1128\n",
      "convergence dfGPdfNN Run 2/10, Epoch 230/1000, Training Loss (NLML): -954.8199\n",
      "convergence dfGPdfNN Run 2/10, Epoch 231/1000, Training Loss (NLML): -954.6681\n",
      "convergence dfGPdfNN Run 2/10, Epoch 232/1000, Training Loss (NLML): -954.6116\n",
      "convergence dfGPdfNN Run 2/10, Epoch 233/1000, Training Loss (NLML): -954.5104\n",
      "convergence dfGPdfNN Run 2/10, Epoch 234/1000, Training Loss (NLML): -954.4236\n",
      "convergence dfGPdfNN Run 2/10, Epoch 235/1000, Training Loss (NLML): -954.3989\n",
      "convergence dfGPdfNN Run 2/10, Epoch 236/1000, Training Loss (NLML): -954.3401\n",
      "convergence dfGPdfNN Run 2/10, Epoch 237/1000, Training Loss (NLML): -954.2825\n",
      "convergence dfGPdfNN Run 2/10, Epoch 238/1000, Training Loss (NLML): -954.2333\n",
      "convergence dfGPdfNN Run 2/10, Epoch 239/1000, Training Loss (NLML): -954.2277\n",
      "convergence dfGPdfNN Run 2/10, Epoch 240/1000, Training Loss (NLML): -954.2198\n",
      "convergence dfGPdfNN Run 2/10, Epoch 241/1000, Training Loss (NLML): -954.2888\n",
      "convergence dfGPdfNN Run 2/10, Epoch 242/1000, Training Loss (NLML): -954.3280\n",
      "convergence dfGPdfNN Run 2/10, Epoch 243/1000, Training Loss (NLML): -954.3650\n",
      "convergence dfGPdfNN Run 2/10, Epoch 244/1000, Training Loss (NLML): -954.3749\n",
      "convergence dfGPdfNN Run 2/10, Epoch 245/1000, Training Loss (NLML): -954.4336\n",
      "convergence dfGPdfNN Run 2/10, Epoch 246/1000, Training Loss (NLML): -954.5104\n",
      "convergence dfGPdfNN Run 2/10, Epoch 247/1000, Training Loss (NLML): -953.9905\n",
      "convergence dfGPdfNN Run 2/10, Epoch 248/1000, Training Loss (NLML): -953.6650\n",
      "convergence dfGPdfNN Run 2/10, Epoch 249/1000, Training Loss (NLML): -953.9517\n",
      "convergence dfGPdfNN Run 2/10, Epoch 250/1000, Training Loss (NLML): -954.3372\n",
      "convergence dfGPdfNN Run 2/10, Epoch 251/1000, Training Loss (NLML): -954.4749\n",
      "convergence dfGPdfNN Run 2/10, Epoch 252/1000, Training Loss (NLML): -954.7255\n",
      "convergence dfGPdfNN Run 2/10, Epoch 253/1000, Training Loss (NLML): -954.8556\n",
      "convergence dfGPdfNN Run 2/10, Epoch 254/1000, Training Loss (NLML): -954.9696\n",
      "convergence dfGPdfNN Run 2/10, Epoch 255/1000, Training Loss (NLML): -955.0011\n",
      "convergence dfGPdfNN Run 2/10, Epoch 256/1000, Training Loss (NLML): -955.0908\n",
      "convergence dfGPdfNN Run 2/10, Epoch 257/1000, Training Loss (NLML): -955.1049\n",
      "convergence dfGPdfNN Run 2/10, Epoch 258/1000, Training Loss (NLML): -955.1158\n",
      "convergence dfGPdfNN Run 2/10, Epoch 259/1000, Training Loss (NLML): -955.1453\n",
      "convergence dfGPdfNN Run 2/10, Epoch 260/1000, Training Loss (NLML): -955.2008\n",
      "convergence dfGPdfNN Run 2/10, Epoch 261/1000, Training Loss (NLML): -955.2644\n",
      "convergence dfGPdfNN Run 2/10, Epoch 262/1000, Training Loss (NLML): -955.2933\n",
      "convergence dfGPdfNN Run 2/10, Epoch 263/1000, Training Loss (NLML): -955.3608\n",
      "convergence dfGPdfNN Run 2/10, Epoch 264/1000, Training Loss (NLML): -955.4368\n",
      "convergence dfGPdfNN Run 2/10, Epoch 265/1000, Training Loss (NLML): -955.5079\n",
      "convergence dfGPdfNN Run 2/10, Epoch 266/1000, Training Loss (NLML): -955.5620\n",
      "convergence dfGPdfNN Run 2/10, Epoch 267/1000, Training Loss (NLML): -955.6140\n",
      "convergence dfGPdfNN Run 2/10, Epoch 268/1000, Training Loss (NLML): -955.6647\n",
      "convergence dfGPdfNN Run 2/10, Epoch 269/1000, Training Loss (NLML): -955.7214\n",
      "convergence dfGPdfNN Run 2/10, Epoch 270/1000, Training Loss (NLML): -955.7795\n",
      "convergence dfGPdfNN Run 2/10, Epoch 271/1000, Training Loss (NLML): -955.8547\n",
      "convergence dfGPdfNN Run 2/10, Epoch 272/1000, Training Loss (NLML): -955.9163\n",
      "convergence dfGPdfNN Run 2/10, Epoch 273/1000, Training Loss (NLML): -956.0723\n",
      "convergence dfGPdfNN Run 2/10, Epoch 274/1000, Training Loss (NLML): -956.1361\n",
      "convergence dfGPdfNN Run 2/10, Epoch 275/1000, Training Loss (NLML): -956.1959\n",
      "convergence dfGPdfNN Run 2/10, Epoch 276/1000, Training Loss (NLML): -956.2517\n",
      "convergence dfGPdfNN Run 2/10, Epoch 277/1000, Training Loss (NLML): -956.3420\n",
      "convergence dfGPdfNN Run 2/10, Epoch 278/1000, Training Loss (NLML): -956.3163\n",
      "convergence dfGPdfNN Run 2/10, Epoch 279/1000, Training Loss (NLML): -956.3507\n",
      "convergence dfGPdfNN Run 2/10, Epoch 280/1000, Training Loss (NLML): -956.4590\n",
      "convergence dfGPdfNN Run 2/10, Epoch 281/1000, Training Loss (NLML): -956.5841\n",
      "convergence dfGPdfNN Run 2/10, Epoch 282/1000, Training Loss (NLML): -956.6237\n",
      "convergence dfGPdfNN Run 2/10, Epoch 283/1000, Training Loss (NLML): -956.6195\n",
      "convergence dfGPdfNN Run 2/10, Epoch 284/1000, Training Loss (NLML): -956.7686\n",
      "convergence dfGPdfNN Run 2/10, Epoch 285/1000, Training Loss (NLML): -956.8326\n",
      "convergence dfGPdfNN Run 2/10, Epoch 286/1000, Training Loss (NLML): -956.8977\n",
      "convergence dfGPdfNN Run 2/10, Epoch 287/1000, Training Loss (NLML): -956.9556\n",
      "convergence dfGPdfNN Run 2/10, Epoch 288/1000, Training Loss (NLML): -957.0397\n",
      "convergence dfGPdfNN Run 2/10, Epoch 289/1000, Training Loss (NLML): -957.0814\n",
      "convergence dfGPdfNN Run 2/10, Epoch 290/1000, Training Loss (NLML): -957.1311\n",
      "convergence dfGPdfNN Run 2/10, Epoch 291/1000, Training Loss (NLML): -957.1786\n",
      "convergence dfGPdfNN Run 2/10, Epoch 292/1000, Training Loss (NLML): -957.2865\n",
      "convergence dfGPdfNN Run 2/10, Epoch 293/1000, Training Loss (NLML): -957.3677\n",
      "convergence dfGPdfNN Run 2/10, Epoch 294/1000, Training Loss (NLML): -957.4432\n",
      "convergence dfGPdfNN Run 2/10, Epoch 295/1000, Training Loss (NLML): -957.4430\n",
      "convergence dfGPdfNN Run 2/10, Epoch 296/1000, Training Loss (NLML): -957.5061\n",
      "convergence dfGPdfNN Run 2/10, Epoch 297/1000, Training Loss (NLML): -957.5634\n",
      "convergence dfGPdfNN Run 2/10, Epoch 298/1000, Training Loss (NLML): -957.6178\n",
      "convergence dfGPdfNN Run 2/10, Epoch 299/1000, Training Loss (NLML): -957.6661\n",
      "convergence dfGPdfNN Run 2/10, Epoch 300/1000, Training Loss (NLML): -957.7861\n",
      "convergence dfGPdfNN Run 2/10, Epoch 301/1000, Training Loss (NLML): -957.8199\n",
      "convergence dfGPdfNN Run 2/10, Epoch 302/1000, Training Loss (NLML): -957.8658\n",
      "convergence dfGPdfNN Run 2/10, Epoch 303/1000, Training Loss (NLML): -957.8875\n",
      "convergence dfGPdfNN Run 2/10, Epoch 304/1000, Training Loss (NLML): -957.3560\n",
      "convergence dfGPdfNN Run 2/10, Epoch 305/1000, Training Loss (NLML): -957.8506\n",
      "convergence dfGPdfNN Run 2/10, Epoch 306/1000, Training Loss (NLML): -957.8634\n",
      "convergence dfGPdfNN Run 2/10, Epoch 307/1000, Training Loss (NLML): -957.9592\n",
      "convergence dfGPdfNN Run 2/10, Epoch 308/1000, Training Loss (NLML): -957.9838\n",
      "convergence dfGPdfNN Run 2/10, Epoch 309/1000, Training Loss (NLML): -958.0063\n",
      "convergence dfGPdfNN Run 2/10, Epoch 310/1000, Training Loss (NLML): -958.0200\n",
      "convergence dfGPdfNN Run 2/10, Epoch 311/1000, Training Loss (NLML): -958.0314\n",
      "convergence dfGPdfNN Run 2/10, Epoch 312/1000, Training Loss (NLML): -958.0574\n",
      "convergence dfGPdfNN Run 2/10, Epoch 313/1000, Training Loss (NLML): -958.0712\n",
      "convergence dfGPdfNN Run 2/10, Epoch 314/1000, Training Loss (NLML): -958.0802\n",
      "convergence dfGPdfNN Run 2/10, Epoch 315/1000, Training Loss (NLML): -958.1067\n",
      "convergence dfGPdfNN Run 2/10, Epoch 316/1000, Training Loss (NLML): -958.1179\n",
      "convergence dfGPdfNN Run 2/10, Epoch 317/1000, Training Loss (NLML): -958.1279\n",
      "convergence dfGPdfNN Run 2/10, Epoch 318/1000, Training Loss (NLML): -958.1140\n",
      "convergence dfGPdfNN Run 2/10, Epoch 319/1000, Training Loss (NLML): -958.1262\n",
      "convergence dfGPdfNN Run 2/10, Epoch 320/1000, Training Loss (NLML): -958.1349\n",
      "convergence dfGPdfNN Run 2/10, Epoch 321/1000, Training Loss (NLML): -958.1375\n",
      "convergence dfGPdfNN Run 2/10, Epoch 322/1000, Training Loss (NLML): -958.1464\n",
      "convergence dfGPdfNN Run 2/10, Epoch 323/1000, Training Loss (NLML): -958.1598\n",
      "convergence dfGPdfNN Run 2/10, Epoch 324/1000, Training Loss (NLML): -958.1721\n",
      "convergence dfGPdfNN Run 2/10, Epoch 325/1000, Training Loss (NLML): -958.1862\n",
      "convergence dfGPdfNN Run 2/10, Epoch 326/1000, Training Loss (NLML): -958.1976\n",
      "convergence dfGPdfNN Run 2/10, Epoch 327/1000, Training Loss (NLML): -958.2106\n",
      "convergence dfGPdfNN Run 2/10, Epoch 328/1000, Training Loss (NLML): -958.2272\n",
      "convergence dfGPdfNN Run 2/10, Epoch 329/1000, Training Loss (NLML): -958.2544\n",
      "convergence dfGPdfNN Run 2/10, Epoch 330/1000, Training Loss (NLML): -958.2571\n",
      "convergence dfGPdfNN Run 2/10, Epoch 331/1000, Training Loss (NLML): -958.2606\n",
      "convergence dfGPdfNN Run 2/10, Epoch 332/1000, Training Loss (NLML): -958.2714\n",
      "convergence dfGPdfNN Run 2/10, Epoch 333/1000, Training Loss (NLML): -958.2821\n",
      "convergence dfGPdfNN Run 2/10, Epoch 334/1000, Training Loss (NLML): -958.2926\n",
      "convergence dfGPdfNN Run 2/10, Epoch 335/1000, Training Loss (NLML): -958.3059\n",
      "convergence dfGPdfNN Run 2/10, Epoch 336/1000, Training Loss (NLML): -958.3191\n",
      "convergence dfGPdfNN Run 2/10, Epoch 337/1000, Training Loss (NLML): -958.3314\n",
      "convergence dfGPdfNN Run 2/10, Epoch 338/1000, Training Loss (NLML): -958.3416\n",
      "convergence dfGPdfNN Run 2/10, Epoch 339/1000, Training Loss (NLML): -958.3531\n",
      "convergence dfGPdfNN Run 2/10, Epoch 340/1000, Training Loss (NLML): -958.3646\n",
      "convergence dfGPdfNN Run 2/10, Epoch 341/1000, Training Loss (NLML): -958.3755\n",
      "convergence dfGPdfNN Run 2/10, Epoch 342/1000, Training Loss (NLML): -958.3997\n",
      "convergence dfGPdfNN Run 2/10, Epoch 343/1000, Training Loss (NLML): -958.4093\n",
      "convergence dfGPdfNN Run 2/10, Epoch 344/1000, Training Loss (NLML): -958.4197\n",
      "convergence dfGPdfNN Run 2/10, Epoch 345/1000, Training Loss (NLML): -958.4293\n",
      "convergence dfGPdfNN Run 2/10, Epoch 346/1000, Training Loss (NLML): -958.4390\n",
      "convergence dfGPdfNN Run 2/10, Epoch 347/1000, Training Loss (NLML): -958.4492\n",
      "convergence dfGPdfNN Run 2/10, Epoch 348/1000, Training Loss (NLML): -958.4464\n",
      "convergence dfGPdfNN Run 2/10, Epoch 349/1000, Training Loss (NLML): -958.4541\n",
      "convergence dfGPdfNN Run 2/10, Epoch 350/1000, Training Loss (NLML): -958.4658\n",
      "convergence dfGPdfNN Run 2/10, Epoch 351/1000, Training Loss (NLML): -958.4753\n",
      "convergence dfGPdfNN Run 2/10, Epoch 352/1000, Training Loss (NLML): -958.5000\n",
      "convergence dfGPdfNN Run 2/10, Epoch 353/1000, Training Loss (NLML): -958.5085\n",
      "convergence dfGPdfNN Run 2/10, Epoch 354/1000, Training Loss (NLML): -958.5198\n",
      "convergence dfGPdfNN Run 2/10, Epoch 355/1000, Training Loss (NLML): -958.5283\n",
      "convergence dfGPdfNN Run 2/10, Epoch 356/1000, Training Loss (NLML): -958.5383\n",
      "convergence dfGPdfNN Run 2/10, Epoch 357/1000, Training Loss (NLML): -958.5397\n",
      "convergence dfGPdfNN Run 2/10, Epoch 358/1000, Training Loss (NLML): -958.5455\n",
      "convergence dfGPdfNN Run 2/10, Epoch 359/1000, Training Loss (NLML): -958.5520\n",
      "convergence dfGPdfNN Run 2/10, Epoch 360/1000, Training Loss (NLML): -958.5615\n",
      "convergence dfGPdfNN Run 2/10, Epoch 361/1000, Training Loss (NLML): -958.5692\n",
      "convergence dfGPdfNN Run 2/10, Epoch 362/1000, Training Loss (NLML): -958.5934\n",
      "convergence dfGPdfNN Run 2/10, Epoch 363/1000, Training Loss (NLML): -958.6014\n",
      "convergence dfGPdfNN Run 2/10, Epoch 364/1000, Training Loss (NLML): -958.6111\n",
      "convergence dfGPdfNN Run 2/10, Epoch 365/1000, Training Loss (NLML): -958.6190\n",
      "convergence dfGPdfNN Run 2/10, Epoch 366/1000, Training Loss (NLML): -958.6187\n",
      "convergence dfGPdfNN Run 2/10, Epoch 367/1000, Training Loss (NLML): -958.6243\n",
      "convergence dfGPdfNN Run 2/10, Epoch 368/1000, Training Loss (NLML): -958.6331\n",
      "convergence dfGPdfNN Run 2/10, Epoch 369/1000, Training Loss (NLML): -958.6421\n",
      "convergence dfGPdfNN Run 2/10, Epoch 370/1000, Training Loss (NLML): -958.6531\n",
      "convergence dfGPdfNN Run 2/10, Epoch 371/1000, Training Loss (NLML): -958.6642\n",
      "convergence dfGPdfNN Run 2/10, Epoch 372/1000, Training Loss (NLML): -958.6798\n",
      "convergence dfGPdfNN Run 2/10, Epoch 373/1000, Training Loss (NLML): -958.6886\n",
      "convergence dfGPdfNN Run 2/10, Epoch 374/1000, Training Loss (NLML): -958.6973\n",
      "convergence dfGPdfNN Run 2/10, Epoch 375/1000, Training Loss (NLML): -958.6929\n",
      "convergence dfGPdfNN Run 2/10, Epoch 376/1000, Training Loss (NLML): -958.7006\n",
      "convergence dfGPdfNN Run 2/10, Epoch 377/1000, Training Loss (NLML): -958.7096\n",
      "convergence dfGPdfNN Run 2/10, Epoch 378/1000, Training Loss (NLML): -958.7184\n",
      "convergence dfGPdfNN Run 2/10, Epoch 379/1000, Training Loss (NLML): -958.7399\n",
      "convergence dfGPdfNN Run 2/10, Epoch 380/1000, Training Loss (NLML): -958.7480\n",
      "convergence dfGPdfNN Run 2/10, Epoch 381/1000, Training Loss (NLML): -958.7432\n",
      "convergence dfGPdfNN Run 2/10, Epoch 382/1000, Training Loss (NLML): -958.7520\n",
      "convergence dfGPdfNN Run 2/10, Epoch 383/1000, Training Loss (NLML): -958.7596\n",
      "convergence dfGPdfNN Run 2/10, Epoch 384/1000, Training Loss (NLML): -958.7738\n",
      "convergence dfGPdfNN Run 2/10, Epoch 385/1000, Training Loss (NLML): -958.7888\n",
      "convergence dfGPdfNN Run 2/10, Epoch 386/1000, Training Loss (NLML): -958.7852\n",
      "convergence dfGPdfNN Run 2/10, Epoch 387/1000, Training Loss (NLML): -958.7933\n",
      "convergence dfGPdfNN Run 2/10, Epoch 388/1000, Training Loss (NLML): -958.8073\n",
      "convergence dfGPdfNN Run 2/10, Epoch 389/1000, Training Loss (NLML): -958.8134\n",
      "convergence dfGPdfNN Run 2/10, Epoch 390/1000, Training Loss (NLML): -958.8230\n",
      "convergence dfGPdfNN Run 2/10, Epoch 391/1000, Training Loss (NLML): -958.8304\n",
      "convergence dfGPdfNN Run 2/10, Epoch 392/1000, Training Loss (NLML): -958.8380\n",
      "convergence dfGPdfNN Run 2/10, Epoch 393/1000, Training Loss (NLML): -958.8462\n",
      "convergence dfGPdfNN Run 2/10, Epoch 394/1000, Training Loss (NLML): -958.8474\n",
      "convergence dfGPdfNN Run 2/10, Epoch 395/1000, Training Loss (NLML): -958.8622\n",
      "convergence dfGPdfNN Run 2/10, Epoch 396/1000, Training Loss (NLML): -958.8759\n",
      "convergence dfGPdfNN Run 2/10, Epoch 397/1000, Training Loss (NLML): -958.8723\n",
      "convergence dfGPdfNN Run 2/10, Epoch 398/1000, Training Loss (NLML): -958.8853\n",
      "convergence dfGPdfNN Run 2/10, Epoch 399/1000, Training Loss (NLML): -958.8914\n",
      "convergence dfGPdfNN Run 2/10, Epoch 400/1000, Training Loss (NLML): -958.9010\n",
      "convergence dfGPdfNN Run 2/10, Epoch 401/1000, Training Loss (NLML): -958.9153\n",
      "convergence dfGPdfNN Run 2/10, Epoch 402/1000, Training Loss (NLML): -958.9108\n",
      "convergence dfGPdfNN Run 2/10, Epoch 403/1000, Training Loss (NLML): -958.9172\n",
      "convergence dfGPdfNN Run 2/10, Epoch 404/1000, Training Loss (NLML): -958.9371\n",
      "convergence dfGPdfNN Run 2/10, Epoch 405/1000, Training Loss (NLML): -958.9375\n",
      "convergence dfGPdfNN Run 2/10, Epoch 406/1000, Training Loss (NLML): -958.9456\n",
      "convergence dfGPdfNN Run 2/10, Epoch 407/1000, Training Loss (NLML): -958.9523\n",
      "convergence dfGPdfNN Run 2/10, Epoch 408/1000, Training Loss (NLML): -958.9542\n",
      "convergence dfGPdfNN Run 2/10, Epoch 409/1000, Training Loss (NLML): -958.9673\n",
      "convergence dfGPdfNN Run 2/10, Epoch 410/1000, Training Loss (NLML): -958.9810\n",
      "convergence dfGPdfNN Run 2/10, Epoch 411/1000, Training Loss (NLML): -958.9819\n",
      "convergence dfGPdfNN Run 2/10, Epoch 412/1000, Training Loss (NLML): -958.9844\n",
      "convergence dfGPdfNN Run 2/10, Epoch 413/1000, Training Loss (NLML): -958.9957\n",
      "convergence dfGPdfNN Run 2/10, Epoch 414/1000, Training Loss (NLML): -959.0027\n",
      "convergence dfGPdfNN Run 2/10, Epoch 415/1000, Training Loss (NLML): -959.0166\n",
      "convergence dfGPdfNN Run 2/10, Epoch 416/1000, Training Loss (NLML): -959.0109\n",
      "convergence dfGPdfNN Run 2/10, Epoch 417/1000, Training Loss (NLML): -959.0181\n",
      "convergence dfGPdfNN Run 2/10, Epoch 418/1000, Training Loss (NLML): -959.0365\n",
      "convergence dfGPdfNN Run 2/10, Epoch 419/1000, Training Loss (NLML): -959.0446\n",
      "convergence dfGPdfNN Run 2/10, Epoch 420/1000, Training Loss (NLML): -959.0382\n",
      "convergence dfGPdfNN Run 2/10, Epoch 421/1000, Training Loss (NLML): -959.0468\n",
      "convergence dfGPdfNN Run 2/10, Epoch 422/1000, Training Loss (NLML): -959.0568\n",
      "convergence dfGPdfNN Run 2/10, Epoch 423/1000, Training Loss (NLML): -959.0713\n",
      "convergence dfGPdfNN Run 2/10, Epoch 424/1000, Training Loss (NLML): -959.0781\n",
      "convergence dfGPdfNN Run 2/10, Epoch 425/1000, Training Loss (NLML): -959.0775\n",
      "convergence dfGPdfNN Run 2/10, Epoch 426/1000, Training Loss (NLML): -959.0801\n",
      "convergence dfGPdfNN Run 2/10, Epoch 427/1000, Training Loss (NLML): -959.0928\n",
      "convergence dfGPdfNN Run 2/10, Epoch 428/1000, Training Loss (NLML): -959.1063\n",
      "convergence dfGPdfNN Run 2/10, Epoch 429/1000, Training Loss (NLML): -959.1128\n",
      "convergence dfGPdfNN Run 2/10, Epoch 430/1000, Training Loss (NLML): -959.1129\n",
      "convergence dfGPdfNN Run 2/10, Epoch 431/1000, Training Loss (NLML): -959.1135\n",
      "convergence dfGPdfNN Run 2/10, Epoch 432/1000, Training Loss (NLML): -959.1324\n",
      "convergence dfGPdfNN Run 2/10, Epoch 433/1000, Training Loss (NLML): -959.1381\n",
      "convergence dfGPdfNN Run 2/10, Epoch 434/1000, Training Loss (NLML): -959.1322\n",
      "convergence dfGPdfNN Run 2/10, Epoch 435/1000, Training Loss (NLML): -959.1438\n",
      "convergence dfGPdfNN Run 2/10, Epoch 436/1000, Training Loss (NLML): -959.1591\n",
      "convergence dfGPdfNN Run 2/10, Epoch 437/1000, Training Loss (NLML): -959.1582\n",
      "convergence dfGPdfNN Run 2/10, Epoch 438/1000, Training Loss (NLML): -959.1583\n",
      "convergence dfGPdfNN Run 2/10, Epoch 439/1000, Training Loss (NLML): -959.1782\n",
      "convergence dfGPdfNN Run 2/10, Epoch 440/1000, Training Loss (NLML): -959.1769\n",
      "convergence dfGPdfNN Run 2/10, Epoch 441/1000, Training Loss (NLML): -959.1792\n",
      "convergence dfGPdfNN Run 2/10, Epoch 442/1000, Training Loss (NLML): -959.1890\n",
      "convergence dfGPdfNN Run 2/10, Epoch 443/1000, Training Loss (NLML): -959.1948\n",
      "convergence dfGPdfNN Run 2/10, Epoch 444/1000, Training Loss (NLML): -959.2086\n",
      "convergence dfGPdfNN Run 2/10, Epoch 445/1000, Training Loss (NLML): -959.2037\n",
      "convergence dfGPdfNN Run 2/10, Epoch 446/1000, Training Loss (NLML): -959.2095\n",
      "convergence dfGPdfNN Run 2/10, Epoch 447/1000, Training Loss (NLML): -959.2203\n",
      "convergence dfGPdfNN Run 2/10, Epoch 448/1000, Training Loss (NLML): -959.2321\n",
      "convergence dfGPdfNN Run 2/10, Epoch 449/1000, Training Loss (NLML): -959.2405\n",
      "convergence dfGPdfNN Run 2/10, Epoch 450/1000, Training Loss (NLML): -959.2338\n",
      "convergence dfGPdfNN Run 2/10, Epoch 451/1000, Training Loss (NLML): -959.2400\n",
      "convergence dfGPdfNN Run 2/10, Epoch 452/1000, Training Loss (NLML): -959.2467\n",
      "convergence dfGPdfNN Run 2/10, Epoch 453/1000, Training Loss (NLML): -959.2557\n",
      "convergence dfGPdfNN Run 2/10, Epoch 454/1000, Training Loss (NLML): -959.2688\n",
      "convergence dfGPdfNN Run 2/10, Epoch 455/1000, Training Loss (NLML): -959.2754\n",
      "convergence dfGPdfNN Run 2/10, Epoch 456/1000, Training Loss (NLML): -959.2812\n",
      "convergence dfGPdfNN Run 2/10, Epoch 457/1000, Training Loss (NLML): -959.2776\n",
      "convergence dfGPdfNN Run 2/10, Epoch 458/1000, Training Loss (NLML): -959.2826\n",
      "convergence dfGPdfNN Run 2/10, Epoch 459/1000, Training Loss (NLML): -959.2885\n",
      "convergence dfGPdfNN Run 2/10, Epoch 460/1000, Training Loss (NLML): -959.3063\n",
      "convergence dfGPdfNN Run 2/10, Epoch 461/1000, Training Loss (NLML): -959.3119\n",
      "convergence dfGPdfNN Run 2/10, Epoch 462/1000, Training Loss (NLML): -959.3113\n",
      "convergence dfGPdfNN Run 2/10, Epoch 463/1000, Training Loss (NLML): -959.3125\n",
      "convergence dfGPdfNN Run 2/10, Epoch 464/1000, Training Loss (NLML): -959.3226\n",
      "convergence dfGPdfNN Run 2/10, Epoch 465/1000, Training Loss (NLML): -959.3336\n",
      "convergence dfGPdfNN Run 2/10, Epoch 466/1000, Training Loss (NLML): -959.3411\n",
      "convergence dfGPdfNN Run 2/10, Epoch 467/1000, Training Loss (NLML): -959.3372\n",
      "convergence dfGPdfNN Run 2/10, Epoch 468/1000, Training Loss (NLML): -959.3413\n",
      "convergence dfGPdfNN Run 2/10, Epoch 469/1000, Training Loss (NLML): -959.3500\n",
      "convergence dfGPdfNN Run 2/10, Epoch 470/1000, Training Loss (NLML): -959.3640\n",
      "convergence dfGPdfNN Run 2/10, Epoch 471/1000, Training Loss (NLML): -959.3689\n",
      "convergence dfGPdfNN Run 2/10, Epoch 472/1000, Training Loss (NLML): -959.3645\n",
      "convergence dfGPdfNN Run 2/10, Epoch 473/1000, Training Loss (NLML): -959.3689\n",
      "convergence dfGPdfNN Run 2/10, Epoch 474/1000, Training Loss (NLML): -959.3857\n",
      "convergence dfGPdfNN Run 2/10, Epoch 475/1000, Training Loss (NLML): -959.3854\n",
      "convergence dfGPdfNN Run 2/10, Epoch 476/1000, Training Loss (NLML): -959.3981\n",
      "convergence dfGPdfNN Run 2/10, Epoch 477/1000, Training Loss (NLML): -959.3912\n",
      "convergence dfGPdfNN Run 2/10, Epoch 478/1000, Training Loss (NLML): -959.4374\n",
      "convergence dfGPdfNN Run 2/10, Epoch 479/1000, Training Loss (NLML): -959.4144\n",
      "convergence dfGPdfNN Run 2/10, Epoch 480/1000, Training Loss (NLML): -959.4205\n",
      "convergence dfGPdfNN Run 2/10, Epoch 481/1000, Training Loss (NLML): -959.4175\n",
      "convergence dfGPdfNN Run 2/10, Epoch 482/1000, Training Loss (NLML): -959.4240\n",
      "convergence dfGPdfNN Run 2/10, Epoch 483/1000, Training Loss (NLML): -959.4288\n",
      "convergence dfGPdfNN Run 2/10, Epoch 484/1000, Training Loss (NLML): -959.4349\n",
      "convergence dfGPdfNN Run 2/10, Epoch 485/1000, Training Loss (NLML): -959.5157\n",
      "convergence dfGPdfNN Run 2/10, Epoch 486/1000, Training Loss (NLML): -959.4456\n",
      "convergence dfGPdfNN Run 2/10, Epoch 487/1000, Training Loss (NLML): -959.4515\n",
      "convergence dfGPdfNN Run 2/10, Epoch 488/1000, Training Loss (NLML): -959.4553\n",
      "convergence dfGPdfNN Run 2/10, Epoch 489/1000, Training Loss (NLML): -959.4668\n",
      "convergence dfGPdfNN Run 2/10, Epoch 490/1000, Training Loss (NLML): -959.4734\n",
      "convergence dfGPdfNN Run 2/10, Epoch 491/1000, Training Loss (NLML): -959.5112\n",
      "convergence dfGPdfNN Run 2/10, Epoch 492/1000, Training Loss (NLML): -959.5122\n",
      "convergence dfGPdfNN Run 2/10, Epoch 493/1000, Training Loss (NLML): -959.5232\n",
      "convergence dfGPdfNN Run 2/10, Epoch 494/1000, Training Loss (NLML): -959.4944\n",
      "convergence dfGPdfNN Run 2/10, Epoch 495/1000, Training Loss (NLML): -959.4979\n",
      "convergence dfGPdfNN Run 2/10, Epoch 496/1000, Training Loss (NLML): -959.4967\n",
      "convergence dfGPdfNN Run 2/10, Epoch 497/1000, Training Loss (NLML): -959.4968\n",
      "convergence dfGPdfNN Run 2/10, Epoch 498/1000, Training Loss (NLML): -959.5543\n",
      "convergence dfGPdfNN Run 2/10, Epoch 499/1000, Training Loss (NLML): -959.5592\n",
      "convergence dfGPdfNN Run 2/10, Epoch 500/1000, Training Loss (NLML): -959.5540\n",
      "convergence dfGPdfNN Run 2/10, Epoch 501/1000, Training Loss (NLML): -959.5188\n",
      "convergence dfGPdfNN Run 2/10, Epoch 502/1000, Training Loss (NLML): -959.5282\n",
      "convergence dfGPdfNN Run 2/10, Epoch 503/1000, Training Loss (NLML): -959.5399\n",
      "convergence dfGPdfNN Run 2/10, Epoch 504/1000, Training Loss (NLML): -959.5850\n",
      "convergence dfGPdfNN Run 2/10, Epoch 505/1000, Training Loss (NLML): -959.5798\n",
      "convergence dfGPdfNN Run 2/10, Epoch 506/1000, Training Loss (NLML): -959.5840\n",
      "convergence dfGPdfNN Run 2/10, Epoch 507/1000, Training Loss (NLML): -959.5471\n",
      "convergence dfGPdfNN Run 2/10, Epoch 508/1000, Training Loss (NLML): -959.5582\n",
      "convergence dfGPdfNN Run 2/10, Epoch 509/1000, Training Loss (NLML): -959.6094\n",
      "convergence dfGPdfNN Run 2/10, Epoch 510/1000, Training Loss (NLML): -959.6139\n",
      "convergence dfGPdfNN Run 2/10, Epoch 511/1000, Training Loss (NLML): -959.6090\n",
      "convergence dfGPdfNN Run 2/10, Epoch 512/1000, Training Loss (NLML): -959.5729\n",
      "convergence dfGPdfNN Run 2/10, Epoch 513/1000, Training Loss (NLML): -959.5775\n",
      "convergence dfGPdfNN Run 2/10, Epoch 514/1000, Training Loss (NLML): -959.6324\n",
      "convergence dfGPdfNN Run 2/10, Epoch 515/1000, Training Loss (NLML): -959.6707\n",
      "convergence dfGPdfNN Run 2/10, Epoch 516/1000, Training Loss (NLML): -959.6354\n",
      "convergence dfGPdfNN Run 2/10, Epoch 517/1000, Training Loss (NLML): -959.5961\n",
      "convergence dfGPdfNN Run 2/10, Epoch 518/1000, Training Loss (NLML): -959.6057\n",
      "convergence dfGPdfNN Run 2/10, Epoch 519/1000, Training Loss (NLML): -959.6572\n",
      "convergence dfGPdfNN Run 2/10, Epoch 520/1000, Training Loss (NLML): -959.6613\n",
      "convergence dfGPdfNN Run 2/10, Epoch 521/1000, Training Loss (NLML): -959.6619\n",
      "convergence dfGPdfNN Run 2/10, Epoch 522/1000, Training Loss (NLML): -959.6606\n",
      "convergence dfGPdfNN Run 2/10, Epoch 523/1000, Training Loss (NLML): -959.6664\n",
      "convergence dfGPdfNN Run 2/10, Epoch 524/1000, Training Loss (NLML): -959.6892\n",
      "convergence dfGPdfNN Run 2/10, Epoch 525/1000, Training Loss (NLML): -959.6530\n",
      "convergence dfGPdfNN Run 2/10, Epoch 526/1000, Training Loss (NLML): -959.6570\n",
      "convergence dfGPdfNN Run 2/10, Epoch 527/1000, Training Loss (NLML): -959.7025\n",
      "convergence dfGPdfNN Run 2/10, Epoch 528/1000, Training Loss (NLML): -959.6998\n",
      "convergence dfGPdfNN Run 2/10, Epoch 529/1000, Training Loss (NLML): -959.7030\n",
      "convergence dfGPdfNN Run 2/10, Epoch 530/1000, Training Loss (NLML): -959.7087\n",
      "convergence dfGPdfNN Run 2/10, Epoch 531/1000, Training Loss (NLML): -959.6733\n",
      "convergence dfGPdfNN Run 2/10, Epoch 532/1000, Training Loss (NLML): -959.6844\n",
      "convergence dfGPdfNN Run 2/10, Epoch 533/1000, Training Loss (NLML): -959.7607\n",
      "convergence dfGPdfNN Run 2/10, Epoch 534/1000, Training Loss (NLML): -959.7330\n",
      "convergence dfGPdfNN Run 2/10, Epoch 535/1000, Training Loss (NLML): -959.6968\n",
      "convergence dfGPdfNN Run 2/10, Epoch 536/1000, Training Loss (NLML): -959.7362\n",
      "convergence dfGPdfNN Run 2/10, Epoch 537/1000, Training Loss (NLML): -959.7395\n",
      "convergence dfGPdfNN Run 2/10, Epoch 538/1000, Training Loss (NLML): -959.7449\n",
      "convergence dfGPdfNN Run 2/10, Epoch 539/1000, Training Loss (NLML): -959.7068\n",
      "convergence dfGPdfNN Run 2/10, Epoch 540/1000, Training Loss (NLML): -959.7515\n",
      "convergence dfGPdfNN Run 2/10, Epoch 541/1000, Training Loss (NLML): -959.7965\n",
      "convergence dfGPdfNN Run 2/10, Epoch 542/1000, Training Loss (NLML): -959.7703\n",
      "convergence dfGPdfNN Run 2/10, Epoch 543/1000, Training Loss (NLML): -959.7328\n",
      "convergence dfGPdfNN Run 2/10, Epoch 544/1000, Training Loss (NLML): -959.7292\n",
      "convergence dfGPdfNN Run 2/10, Epoch 545/1000, Training Loss (NLML): -959.7733\n",
      "convergence dfGPdfNN Run 2/10, Epoch 546/1000, Training Loss (NLML): -959.7795\n",
      "convergence dfGPdfNN Run 2/10, Epoch 547/1000, Training Loss (NLML): -959.8152\n",
      "convergence dfGPdfNN Run 2/10, Epoch 548/1000, Training Loss (NLML): -959.7872\n",
      "convergence dfGPdfNN Run 2/10, Epoch 549/1000, Training Loss (NLML): -959.7909\n",
      "convergence dfGPdfNN Run 2/10, Epoch 550/1000, Training Loss (NLML): -959.7535\n",
      "convergence dfGPdfNN Run 2/10, Epoch 551/1000, Training Loss (NLML): -959.7910\n",
      "convergence dfGPdfNN Run 2/10, Epoch 552/1000, Training Loss (NLML): -959.7965\n",
      "convergence dfGPdfNN Run 2/10, Epoch 553/1000, Training Loss (NLML): -959.8350\n",
      "convergence dfGPdfNN Run 2/10, Epoch 554/1000, Training Loss (NLML): -959.8429\n",
      "convergence dfGPdfNN Run 2/10, Epoch 555/1000, Training Loss (NLML): -959.8167\n",
      "convergence dfGPdfNN Run 2/10, Epoch 556/1000, Training Loss (NLML): -959.8153\n",
      "convergence dfGPdfNN Run 2/10, Epoch 557/1000, Training Loss (NLML): -959.8179\n",
      "convergence dfGPdfNN Run 2/10, Epoch 558/1000, Training Loss (NLML): -959.8274\n",
      "convergence dfGPdfNN Run 2/10, Epoch 559/1000, Training Loss (NLML): -959.8318\n",
      "convergence dfGPdfNN Run 2/10, Epoch 560/1000, Training Loss (NLML): -959.8767\n",
      "convergence dfGPdfNN Run 2/10, Epoch 561/1000, Training Loss (NLML): -959.8438\n",
      "convergence dfGPdfNN Run 2/10, Epoch 562/1000, Training Loss (NLML): -959.8475\n",
      "convergence dfGPdfNN Run 2/10, Epoch 563/1000, Training Loss (NLML): -959.8481\n",
      "convergence dfGPdfNN Run 2/10, Epoch 564/1000, Training Loss (NLML): -959.8560\n",
      "convergence dfGPdfNN Run 2/10, Epoch 565/1000, Training Loss (NLML): -959.8639\n",
      "convergence dfGPdfNN Run 2/10, Epoch 566/1000, Training Loss (NLML): -959.8644\n",
      "convergence dfGPdfNN Run 2/10, Epoch 567/1000, Training Loss (NLML): -959.8662\n",
      "convergence dfGPdfNN Run 2/10, Epoch 568/1000, Training Loss (NLML): -959.8722\n",
      "convergence dfGPdfNN Run 2/10, Epoch 569/1000, Training Loss (NLML): -959.9076\n",
      "convergence dfGPdfNN Run 2/10, Epoch 570/1000, Training Loss (NLML): -959.8771\n",
      "convergence dfGPdfNN Run 2/10, Epoch 571/1000, Training Loss (NLML): -959.8866\n",
      "convergence dfGPdfNN Run 2/10, Epoch 572/1000, Training Loss (NLML): -959.8950\n",
      "convergence dfGPdfNN Run 2/10, Epoch 573/1000, Training Loss (NLML): -959.8887\n",
      "convergence dfGPdfNN Run 2/10, Epoch 574/1000, Training Loss (NLML): -959.8929\n",
      "convergence dfGPdfNN Run 2/10, Epoch 575/1000, Training Loss (NLML): -959.8955\n",
      "convergence dfGPdfNN Run 2/10, Epoch 576/1000, Training Loss (NLML): -959.9098\n",
      "convergence dfGPdfNN Run 2/10, Epoch 577/1000, Training Loss (NLML): -959.9452\n",
      "convergence dfGPdfNN Run 2/10, Epoch 578/1000, Training Loss (NLML): -959.9081\n",
      "convergence dfGPdfNN Run 2/10, Epoch 579/1000, Training Loss (NLML): -959.9115\n",
      "convergence dfGPdfNN Run 2/10, Epoch 580/1000, Training Loss (NLML): -959.9171\n",
      "convergence dfGPdfNN Run 2/10, Epoch 581/1000, Training Loss (NLML): -959.9214\n",
      "convergence dfGPdfNN Run 2/10, Epoch 582/1000, Training Loss (NLML): -959.9595\n",
      "convergence dfGPdfNN Run 2/10, Epoch 583/1000, Training Loss (NLML): -959.9362\n",
      "convergence dfGPdfNN Run 2/10, Epoch 584/1000, Training Loss (NLML): -959.9406\n",
      "convergence dfGPdfNN Run 2/10, Epoch 585/1000, Training Loss (NLML): -959.9369\n",
      "convergence dfGPdfNN Run 2/10, Epoch 586/1000, Training Loss (NLML): -959.9393\n",
      "convergence dfGPdfNN Run 2/10, Epoch 587/1000, Training Loss (NLML): -959.9796\n",
      "convergence dfGPdfNN Run 2/10, Epoch 588/1000, Training Loss (NLML): -959.9886\n",
      "convergence dfGPdfNN Run 2/10, Epoch 589/1000, Training Loss (NLML): -959.9495\n",
      "convergence dfGPdfNN Run 2/10, Epoch 590/1000, Training Loss (NLML): -959.9570\n",
      "convergence dfGPdfNN Run 2/10, Epoch 591/1000, Training Loss (NLML): -959.9655\n",
      "convergence dfGPdfNN Run 2/10, Epoch 592/1000, Training Loss (NLML): -959.9656\n",
      "convergence dfGPdfNN Run 2/10, Epoch 593/1000, Training Loss (NLML): -960.0012\n",
      "convergence dfGPdfNN Run 2/10, Epoch 594/1000, Training Loss (NLML): -960.0048\n",
      "convergence dfGPdfNN Run 2/10, Epoch 595/1000, Training Loss (NLML): -959.9756\n",
      "convergence dfGPdfNN Run 2/10, Epoch 596/1000, Training Loss (NLML): -959.9845\n",
      "convergence dfGPdfNN Run 2/10, Epoch 597/1000, Training Loss (NLML): -959.9800\n",
      "convergence dfGPdfNN Run 2/10, Epoch 598/1000, Training Loss (NLML): -960.0260\n",
      "convergence dfGPdfNN Run 2/10, Epoch 599/1000, Training Loss (NLML): -960.0205\n",
      "convergence dfGPdfNN Run 2/10, Epoch 600/1000, Training Loss (NLML): -959.9961\n",
      "convergence dfGPdfNN Run 2/10, Epoch 601/1000, Training Loss (NLML): -959.9991\n",
      "convergence dfGPdfNN Run 2/10, Epoch 602/1000, Training Loss (NLML): -960.0070\n",
      "convergence dfGPdfNN Run 2/10, Epoch 603/1000, Training Loss (NLML): -960.0374\n",
      "convergence dfGPdfNN Run 2/10, Epoch 604/1000, Training Loss (NLML): -960.0382\n",
      "convergence dfGPdfNN Run 2/10, Epoch 605/1000, Training Loss (NLML): -960.0083\n",
      "convergence dfGPdfNN Run 2/10, Epoch 606/1000, Training Loss (NLML): -960.0128\n",
      "convergence dfGPdfNN Run 2/10, Epoch 607/1000, Training Loss (NLML): -960.0576\n",
      "convergence dfGPdfNN Run 2/10, Epoch 608/1000, Training Loss (NLML): -960.0568\n",
      "convergence dfGPdfNN Run 2/10, Epoch 609/1000, Training Loss (NLML): -960.0234\n",
      "convergence dfGPdfNN Run 2/10, Epoch 610/1000, Training Loss (NLML): -960.0272\n",
      "convergence dfGPdfNN Run 2/10, Epoch 611/1000, Training Loss (NLML): -960.0641\n",
      "convergence dfGPdfNN Run 2/10, Epoch 612/1000, Training Loss (NLML): -960.0669\n",
      "convergence dfGPdfNN Run 2/10, Epoch 613/1000, Training Loss (NLML): -960.0376\n",
      "convergence dfGPdfNN Run 2/10, Epoch 614/1000, Training Loss (NLML): -960.0781\n",
      "convergence dfGPdfNN Run 2/10, Epoch 615/1000, Training Loss (NLML): -960.0815\n",
      "convergence dfGPdfNN Run 2/10, Epoch 616/1000, Training Loss (NLML): -960.0474\n",
      "convergence dfGPdfNN Run 2/10, Epoch 617/1000, Training Loss (NLML): -960.0505\n",
      "convergence dfGPdfNN Run 2/10, Epoch 618/1000, Training Loss (NLML): -960.0914\n",
      "convergence dfGPdfNN Run 2/10, Epoch 619/1000, Training Loss (NLML): -960.0955\n",
      "convergence dfGPdfNN Run 2/10, Epoch 620/1000, Training Loss (NLML): -960.0995\n",
      "convergence dfGPdfNN Run 2/10, Epoch 621/1000, Training Loss (NLML): -960.0995\n",
      "convergence dfGPdfNN Run 2/10, Epoch 622/1000, Training Loss (NLML): -960.0682\n",
      "convergence dfGPdfNN Run 2/10, Epoch 623/1000, Training Loss (NLML): -960.0775\n",
      "convergence dfGPdfNN Run 2/10, Epoch 624/1000, Training Loss (NLML): -960.1179\n",
      "convergence dfGPdfNN Run 2/10, Epoch 625/1000, Training Loss (NLML): -960.1063\n",
      "convergence dfGPdfNN Run 2/10, Epoch 626/1000, Training Loss (NLML): -960.1094\n",
      "convergence dfGPdfNN Run 2/10, Epoch 627/1000, Training Loss (NLML): -960.1129\n",
      "convergence dfGPdfNN Run 2/10, Epoch 628/1000, Training Loss (NLML): -960.1318\n",
      "convergence dfGPdfNN Run 2/10, Epoch 629/1000, Training Loss (NLML): -960.0957\n",
      "convergence dfGPdfNN Run 2/10, Epoch 630/1000, Training Loss (NLML): -960.1339\n",
      "convergence dfGPdfNN Run 2/10, Epoch 631/1000, Training Loss (NLML): -960.0983\n",
      "convergence dfGPdfNN Run 2/10, Epoch 632/1000, Training Loss (NLML): -960.1440\n",
      "convergence dfGPdfNN Run 2/10, Epoch 633/1000, Training Loss (NLML): -960.1442\n",
      "convergence dfGPdfNN Run 2/10, Epoch 634/1000, Training Loss (NLML): -960.1423\n",
      "convergence dfGPdfNN Run 2/10, Epoch 635/1000, Training Loss (NLML): -960.1451\n",
      "convergence dfGPdfNN Run 2/10, Epoch 636/1000, Training Loss (NLML): -960.1190\n",
      "convergence dfGPdfNN Run 2/10, Epoch 637/1000, Training Loss (NLML): -960.1255\n",
      "convergence dfGPdfNN Run 2/10, Epoch 638/1000, Training Loss (NLML): -960.1631\n",
      "convergence dfGPdfNN Run 2/10, Epoch 639/1000, Training Loss (NLML): -960.1603\n",
      "convergence dfGPdfNN Run 2/10, Epoch 640/1000, Training Loss (NLML): -960.1633\n",
      "convergence dfGPdfNN Run 2/10, Epoch 641/1000, Training Loss (NLML): -960.1650\n",
      "convergence dfGPdfNN Run 2/10, Epoch 642/1000, Training Loss (NLML): -960.1699\n",
      "convergence dfGPdfNN Run 2/10, Epoch 643/1000, Training Loss (NLML): -960.1727\n",
      "convergence dfGPdfNN Run 2/10, Epoch 644/1000, Training Loss (NLML): -960.1838\n",
      "convergence dfGPdfNN Run 2/10, Epoch 645/1000, Training Loss (NLML): -960.1501\n",
      "convergence dfGPdfNN Run 2/10, Epoch 646/1000, Training Loss (NLML): -960.1495\n",
      "convergence dfGPdfNN Run 2/10, Epoch 647/1000, Training Loss (NLML): -960.1863\n",
      "convergence dfGPdfNN Run 2/10, Epoch 648/1000, Training Loss (NLML): -960.1880\n",
      "convergence dfGPdfNN Run 2/10, Epoch 649/1000, Training Loss (NLML): -960.1852\n",
      "convergence dfGPdfNN Run 2/10, Epoch 650/1000, Training Loss (NLML): -960.1936\n",
      "convergence dfGPdfNN Run 2/10, Epoch 651/1000, Training Loss (NLML): -960.1986\n",
      "convergence dfGPdfNN Run 2/10, Epoch 652/1000, Training Loss (NLML): -960.1962\n",
      "convergence dfGPdfNN Run 2/10, Epoch 653/1000, Training Loss (NLML): -960.2041\n",
      "convergence dfGPdfNN Run 2/10, Epoch 654/1000, Training Loss (NLML): -960.2083\n",
      "convergence dfGPdfNN Run 2/10, Epoch 655/1000, Training Loss (NLML): -960.2141\n",
      "convergence dfGPdfNN Run 2/10, Epoch 656/1000, Training Loss (NLML): -960.2180\n",
      "convergence dfGPdfNN Run 2/10, Epoch 657/1000, Training Loss (NLML): -960.2205\n",
      "convergence dfGPdfNN Run 2/10, Epoch 658/1000, Training Loss (NLML): -960.2235\n",
      "convergence dfGPdfNN Run 2/10, Epoch 659/1000, Training Loss (NLML): -960.1874\n",
      "convergence dfGPdfNN Run 2/10, Epoch 660/1000, Training Loss (NLML): -960.2290\n",
      "convergence dfGPdfNN Run 2/10, Epoch 661/1000, Training Loss (NLML): -960.2338\n",
      "convergence dfGPdfNN Run 2/10, Epoch 662/1000, Training Loss (NLML): -960.2358\n",
      "convergence dfGPdfNN Run 2/10, Epoch 663/1000, Training Loss (NLML): -960.2351\n",
      "convergence dfGPdfNN Run 2/10, Epoch 664/1000, Training Loss (NLML): -960.2421\n",
      "convergence dfGPdfNN Run 2/10, Epoch 665/1000, Training Loss (NLML): -960.2445\n",
      "convergence dfGPdfNN Run 2/10, Epoch 666/1000, Training Loss (NLML): -960.2472\n",
      "convergence dfGPdfNN Run 2/10, Epoch 667/1000, Training Loss (NLML): -960.2487\n",
      "convergence dfGPdfNN Run 2/10, Epoch 668/1000, Training Loss (NLML): -960.2509\n",
      "convergence dfGPdfNN Run 2/10, Epoch 669/1000, Training Loss (NLML): -960.2609\n",
      "convergence dfGPdfNN Run 2/10, Epoch 670/1000, Training Loss (NLML): -960.2605\n",
      "convergence dfGPdfNN Run 2/10, Epoch 671/1000, Training Loss (NLML): -960.2582\n",
      "convergence dfGPdfNN Run 2/10, Epoch 672/1000, Training Loss (NLML): -960.2622\n",
      "convergence dfGPdfNN Run 2/10, Epoch 673/1000, Training Loss (NLML): -960.2666\n",
      "convergence dfGPdfNN Run 2/10, Epoch 674/1000, Training Loss (NLML): -960.2731\n",
      "convergence dfGPdfNN Run 2/10, Epoch 675/1000, Training Loss (NLML): -960.2795\n",
      "convergence dfGPdfNN Run 2/10, Epoch 676/1000, Training Loss (NLML): -960.2754\n",
      "convergence dfGPdfNN Run 2/10, Epoch 677/1000, Training Loss (NLML): -960.2765\n",
      "convergence dfGPdfNN Run 2/10, Epoch 678/1000, Training Loss (NLML): -960.2843\n",
      "convergence dfGPdfNN Run 2/10, Epoch 679/1000, Training Loss (NLML): -960.2809\n",
      "convergence dfGPdfNN Run 2/10, Epoch 680/1000, Training Loss (NLML): -960.2842\n",
      "convergence dfGPdfNN Run 2/10, Epoch 681/1000, Training Loss (NLML): -960.2872\n",
      "convergence dfGPdfNN Run 2/10, Epoch 682/1000, Training Loss (NLML): -960.2864\n",
      "convergence dfGPdfNN Run 2/10, Epoch 683/1000, Training Loss (NLML): -960.2947\n",
      "convergence dfGPdfNN Run 2/10, Epoch 684/1000, Training Loss (NLML): -960.3019\n",
      "convergence dfGPdfNN Run 2/10, Epoch 685/1000, Training Loss (NLML): -960.3083\n",
      "convergence dfGPdfNN Run 2/10, Epoch 686/1000, Training Loss (NLML): -960.3073\n",
      "convergence dfGPdfNN Run 2/10, Epoch 687/1000, Training Loss (NLML): -960.3071\n",
      "convergence dfGPdfNN Run 2/10, Epoch 688/1000, Training Loss (NLML): -960.3098\n",
      "convergence dfGPdfNN Run 2/10, Epoch 689/1000, Training Loss (NLML): -960.3135\n",
      "convergence dfGPdfNN Run 2/10, Epoch 690/1000, Training Loss (NLML): -960.3229\n",
      "convergence dfGPdfNN Run 2/10, Epoch 691/1000, Training Loss (NLML): -960.3243\n",
      "convergence dfGPdfNN Run 2/10, Epoch 692/1000, Training Loss (NLML): -960.3223\n",
      "convergence dfGPdfNN Run 2/10, Epoch 693/1000, Training Loss (NLML): -960.3245\n",
      "convergence dfGPdfNN Run 2/10, Epoch 694/1000, Training Loss (NLML): -960.3260\n",
      "convergence dfGPdfNN Run 2/10, Epoch 695/1000, Training Loss (NLML): -960.3308\n",
      "convergence dfGPdfNN Run 2/10, Epoch 696/1000, Training Loss (NLML): -960.3350\n",
      "convergence dfGPdfNN Run 2/10, Epoch 697/1000, Training Loss (NLML): -960.3373\n",
      "convergence dfGPdfNN Run 2/10, Epoch 698/1000, Training Loss (NLML): -960.3451\n",
      "convergence dfGPdfNN Run 2/10, Epoch 699/1000, Training Loss (NLML): -960.3417\n",
      "convergence dfGPdfNN Run 2/10, Epoch 700/1000, Training Loss (NLML): -960.3439\n",
      "convergence dfGPdfNN Run 2/10, Epoch 701/1000, Training Loss (NLML): -960.3462\n",
      "convergence dfGPdfNN Run 2/10, Epoch 702/1000, Training Loss (NLML): -960.3535\n",
      "convergence dfGPdfNN Run 2/10, Epoch 703/1000, Training Loss (NLML): -960.3560\n",
      "convergence dfGPdfNN Run 2/10, Epoch 704/1000, Training Loss (NLML): -960.3582\n",
      "convergence dfGPdfNN Run 2/10, Epoch 705/1000, Training Loss (NLML): -960.3593\n",
      "convergence dfGPdfNN Run 2/10, Epoch 706/1000, Training Loss (NLML): -960.3654\n",
      "convergence dfGPdfNN Run 2/10, Epoch 707/1000, Training Loss (NLML): -960.3641\n",
      "convergence dfGPdfNN Run 2/10, Epoch 708/1000, Training Loss (NLML): -960.3680\n",
      "convergence dfGPdfNN Run 2/10, Epoch 709/1000, Training Loss (NLML): -960.3708\n",
      "convergence dfGPdfNN Run 2/10, Epoch 710/1000, Training Loss (NLML): -960.3733\n",
      "convergence dfGPdfNN Run 2/10, Epoch 711/1000, Training Loss (NLML): -960.3760\n",
      "convergence dfGPdfNN Run 2/10, Epoch 712/1000, Training Loss (NLML): -960.3796\n",
      "convergence dfGPdfNN Run 2/10, Epoch 713/1000, Training Loss (NLML): -960.3820\n",
      "convergence dfGPdfNN Run 2/10, Epoch 714/1000, Training Loss (NLML): -960.3763\n",
      "convergence dfGPdfNN Run 2/10, Epoch 715/1000, Training Loss (NLML): -960.3810\n",
      "convergence dfGPdfNN Run 2/10, Epoch 716/1000, Training Loss (NLML): -960.3893\n",
      "convergence dfGPdfNN Run 2/10, Epoch 717/1000, Training Loss (NLML): -960.3909\n",
      "convergence dfGPdfNN Run 2/10, Epoch 718/1000, Training Loss (NLML): -960.3997\n",
      "convergence dfGPdfNN Run 2/10, Epoch 719/1000, Training Loss (NLML): -960.3983\n",
      "convergence dfGPdfNN Run 2/10, Epoch 720/1000, Training Loss (NLML): -960.4028\n",
      "convergence dfGPdfNN Run 2/10, Epoch 721/1000, Training Loss (NLML): -960.4021\n",
      "convergence dfGPdfNN Run 2/10, Epoch 722/1000, Training Loss (NLML): -960.4052\n",
      "convergence dfGPdfNN Run 2/10, Epoch 723/1000, Training Loss (NLML): -960.4059\n",
      "convergence dfGPdfNN Run 2/10, Epoch 724/1000, Training Loss (NLML): -960.4138\n",
      "convergence dfGPdfNN Run 2/10, Epoch 725/1000, Training Loss (NLML): -960.4156\n",
      "convergence dfGPdfNN Run 2/10, Epoch 726/1000, Training Loss (NLML): -960.4183\n",
      "convergence dfGPdfNN Run 2/10, Epoch 727/1000, Training Loss (NLML): -960.4209\n",
      "convergence dfGPdfNN Run 2/10, Epoch 728/1000, Training Loss (NLML): -960.4192\n",
      "convergence dfGPdfNN Run 2/10, Epoch 729/1000, Training Loss (NLML): -960.4259\n",
      "convergence dfGPdfNN Run 2/10, Epoch 730/1000, Training Loss (NLML): -960.4247\n",
      "convergence dfGPdfNN Run 2/10, Epoch 731/1000, Training Loss (NLML): -960.4314\n",
      "convergence dfGPdfNN Run 2/10, Epoch 732/1000, Training Loss (NLML): -960.4327\n",
      "convergence dfGPdfNN Run 2/10, Epoch 733/1000, Training Loss (NLML): -960.4330\n",
      "convergence dfGPdfNN Run 2/10, Epoch 734/1000, Training Loss (NLML): -960.4404\n",
      "convergence dfGPdfNN Run 2/10, Epoch 735/1000, Training Loss (NLML): -960.4430\n",
      "convergence dfGPdfNN Run 2/10, Epoch 736/1000, Training Loss (NLML): -960.4414\n",
      "convergence dfGPdfNN Run 2/10, Epoch 737/1000, Training Loss (NLML): -960.4447\n",
      "convergence dfGPdfNN Run 2/10, Epoch 738/1000, Training Loss (NLML): -960.4495\n",
      "convergence dfGPdfNN Run 2/10, Epoch 739/1000, Training Loss (NLML): -960.4524\n",
      "convergence dfGPdfNN Run 2/10, Epoch 740/1000, Training Loss (NLML): -960.4543\n",
      "convergence dfGPdfNN Run 2/10, Epoch 741/1000, Training Loss (NLML): -960.4543\n",
      "convergence dfGPdfNN Run 2/10, Epoch 742/1000, Training Loss (NLML): -960.4579\n",
      "convergence dfGPdfNN Run 2/10, Epoch 743/1000, Training Loss (NLML): -960.4631\n",
      "convergence dfGPdfNN Run 2/10, Epoch 744/1000, Training Loss (NLML): -960.4650\n",
      "convergence dfGPdfNN Run 2/10, Epoch 745/1000, Training Loss (NLML): -960.4631\n",
      "convergence dfGPdfNN Run 2/10, Epoch 746/1000, Training Loss (NLML): -960.4618\n",
      "convergence dfGPdfNN Run 2/10, Epoch 747/1000, Training Loss (NLML): -960.4635\n",
      "convergence dfGPdfNN Run 2/10, Epoch 748/1000, Training Loss (NLML): -960.4669\n",
      "convergence dfGPdfNN Run 2/10, Epoch 749/1000, Training Loss (NLML): -960.4786\n",
      "convergence dfGPdfNN Run 2/10, Epoch 750/1000, Training Loss (NLML): -960.4799\n",
      "convergence dfGPdfNN Run 2/10, Epoch 751/1000, Training Loss (NLML): -960.4797\n",
      "convergence dfGPdfNN Run 2/10, Epoch 752/1000, Training Loss (NLML): -960.4827\n",
      "convergence dfGPdfNN Run 2/10, Epoch 753/1000, Training Loss (NLML): -960.4886\n",
      "convergence dfGPdfNN Run 2/10, Epoch 754/1000, Training Loss (NLML): -960.4901\n",
      "convergence dfGPdfNN Run 2/10, Epoch 755/1000, Training Loss (NLML): -960.4916\n",
      "convergence dfGPdfNN Run 2/10, Epoch 756/1000, Training Loss (NLML): -960.4921\n",
      "convergence dfGPdfNN Run 2/10, Epoch 757/1000, Training Loss (NLML): -960.4948\n",
      "convergence dfGPdfNN Run 2/10, Epoch 758/1000, Training Loss (NLML): -960.4951\n",
      "convergence dfGPdfNN Run 2/10, Epoch 759/1000, Training Loss (NLML): -960.4994\n",
      "convergence dfGPdfNN Run 2/10, Epoch 760/1000, Training Loss (NLML): -960.5013\n",
      "convergence dfGPdfNN Run 2/10, Epoch 761/1000, Training Loss (NLML): -960.5107\n",
      "convergence dfGPdfNN Run 2/10, Epoch 762/1000, Training Loss (NLML): -960.5109\n",
      "convergence dfGPdfNN Run 2/10, Epoch 763/1000, Training Loss (NLML): -960.5084\n",
      "convergence dfGPdfNN Run 2/10, Epoch 764/1000, Training Loss (NLML): -960.5125\n",
      "convergence dfGPdfNN Run 2/10, Epoch 765/1000, Training Loss (NLML): -960.5182\n",
      "convergence dfGPdfNN Run 2/10, Epoch 766/1000, Training Loss (NLML): -960.5190\n",
      "convergence dfGPdfNN Run 2/10, Epoch 767/1000, Training Loss (NLML): -960.5226\n",
      "convergence dfGPdfNN Run 2/10, Epoch 768/1000, Training Loss (NLML): -960.5242\n",
      "convergence dfGPdfNN Run 2/10, Epoch 769/1000, Training Loss (NLML): -960.5236\n",
      "convergence dfGPdfNN Run 2/10, Epoch 770/1000, Training Loss (NLML): -960.5275\n",
      "convergence dfGPdfNN Run 2/10, Epoch 771/1000, Training Loss (NLML): -960.5326\n",
      "convergence dfGPdfNN Run 2/10, Epoch 772/1000, Training Loss (NLML): -960.5342\n",
      "convergence dfGPdfNN Run 2/10, Epoch 773/1000, Training Loss (NLML): -960.5376\n",
      "convergence dfGPdfNN Run 2/10, Epoch 774/1000, Training Loss (NLML): -960.5359\n",
      "convergence dfGPdfNN Run 2/10, Epoch 775/1000, Training Loss (NLML): -960.5387\n",
      "convergence dfGPdfNN Run 2/10, Epoch 776/1000, Training Loss (NLML): -960.5404\n",
      "convergence dfGPdfNN Run 2/10, Epoch 777/1000, Training Loss (NLML): -960.5464\n",
      "convergence dfGPdfNN Run 2/10, Epoch 778/1000, Training Loss (NLML): -960.5481\n",
      "convergence dfGPdfNN Run 2/10, Epoch 779/1000, Training Loss (NLML): -960.5518\n",
      "convergence dfGPdfNN Run 2/10, Epoch 780/1000, Training Loss (NLML): -960.5504\n",
      "convergence dfGPdfNN Run 2/10, Epoch 781/1000, Training Loss (NLML): -960.5519\n",
      "convergence dfGPdfNN Run 2/10, Epoch 782/1000, Training Loss (NLML): -960.5544\n",
      "convergence dfGPdfNN Run 2/10, Epoch 783/1000, Training Loss (NLML): -960.5574\n",
      "convergence dfGPdfNN Run 2/10, Epoch 784/1000, Training Loss (NLML): -960.5588\n",
      "convergence dfGPdfNN Run 2/10, Epoch 785/1000, Training Loss (NLML): -960.5605\n",
      "convergence dfGPdfNN Run 2/10, Epoch 786/1000, Training Loss (NLML): -960.5620\n",
      "convergence dfGPdfNN Run 2/10, Epoch 787/1000, Training Loss (NLML): -960.5623\n",
      "convergence dfGPdfNN Run 2/10, Epoch 788/1000, Training Loss (NLML): -960.5597\n",
      "convergence dfGPdfNN Run 2/10, Epoch 789/1000, Training Loss (NLML): -960.5850\n",
      "convergence dfGPdfNN Run 2/10, Epoch 790/1000, Training Loss (NLML): -960.5897\n",
      "convergence dfGPdfNN Run 2/10, Epoch 791/1000, Training Loss (NLML): -960.5787\n",
      "convergence dfGPdfNN Run 2/10, Epoch 792/1000, Training Loss (NLML): -960.5789\n",
      "convergence dfGPdfNN Run 2/10, Epoch 793/1000, Training Loss (NLML): -960.5778\n",
      "convergence dfGPdfNN Run 2/10, Epoch 794/1000, Training Loss (NLML): -960.5791\n",
      "convergence dfGPdfNN Run 2/10, Epoch 795/1000, Training Loss (NLML): -960.5817\n",
      "convergence dfGPdfNN Run 2/10, Epoch 796/1000, Training Loss (NLML): -960.5850\n",
      "convergence dfGPdfNN Run 2/10, Epoch 797/1000, Training Loss (NLML): -960.5919\n",
      "convergence dfGPdfNN Run 2/10, Epoch 798/1000, Training Loss (NLML): -960.5957\n",
      "convergence dfGPdfNN Run 2/10, Epoch 799/1000, Training Loss (NLML): -960.5906\n",
      "convergence dfGPdfNN Run 2/10, Epoch 800/1000, Training Loss (NLML): -960.6086\n",
      "convergence dfGPdfNN Run 2/10, Epoch 801/1000, Training Loss (NLML): -960.5947\n",
      "convergence dfGPdfNN Run 2/10, Epoch 802/1000, Training Loss (NLML): -960.5996\n",
      "convergence dfGPdfNN Run 2/10, Epoch 803/1000, Training Loss (NLML): -960.6053\n",
      "convergence dfGPdfNN Run 2/10, Epoch 804/1000, Training Loss (NLML): -960.6077\n",
      "convergence dfGPdfNN Run 2/10, Epoch 805/1000, Training Loss (NLML): -960.6096\n",
      "convergence dfGPdfNN Run 2/10, Epoch 806/1000, Training Loss (NLML): -960.6057\n",
      "convergence dfGPdfNN Run 2/10, Epoch 807/1000, Training Loss (NLML): -960.6077\n",
      "convergence dfGPdfNN Run 2/10, Epoch 808/1000, Training Loss (NLML): -960.6104\n",
      "convergence dfGPdfNN Run 2/10, Epoch 809/1000, Training Loss (NLML): -960.6169\n",
      "convergence dfGPdfNN Run 2/10, Epoch 810/1000, Training Loss (NLML): -960.6378\n",
      "convergence dfGPdfNN Run 2/10, Epoch 811/1000, Training Loss (NLML): -960.6422\n",
      "convergence dfGPdfNN Run 2/10, Epoch 812/1000, Training Loss (NLML): -960.6195\n",
      "convergence dfGPdfNN Run 2/10, Epoch 813/1000, Training Loss (NLML): -960.6211\n",
      "convergence dfGPdfNN Run 2/10, Epoch 814/1000, Training Loss (NLML): -960.6250\n",
      "convergence dfGPdfNN Run 2/10, Epoch 815/1000, Training Loss (NLML): -960.6320\n",
      "convergence dfGPdfNN Run 2/10, Epoch 816/1000, Training Loss (NLML): -960.6349\n",
      "convergence dfGPdfNN Run 2/10, Epoch 817/1000, Training Loss (NLML): -960.6473\n",
      "convergence dfGPdfNN Run 2/10, Epoch 818/1000, Training Loss (NLML): -960.6501\n",
      "convergence dfGPdfNN Run 2/10, Epoch 819/1000, Training Loss (NLML): -960.6547\n",
      "convergence dfGPdfNN Run 2/10, Epoch 820/1000, Training Loss (NLML): -960.6405\n",
      "convergence dfGPdfNN Run 2/10, Epoch 821/1000, Training Loss (NLML): -960.6416\n",
      "convergence dfGPdfNN Run 2/10, Epoch 822/1000, Training Loss (NLML): -960.6471\n",
      "convergence dfGPdfNN Run 2/10, Epoch 823/1000, Training Loss (NLML): -960.6440\n",
      "convergence dfGPdfNN Run 2/10, Epoch 824/1000, Training Loss (NLML): -960.6654\n",
      "convergence dfGPdfNN Run 2/10, Epoch 825/1000, Training Loss (NLML): -960.6676\n",
      "convergence dfGPdfNN Run 2/10, Epoch 826/1000, Training Loss (NLML): -960.6698\n",
      "convergence dfGPdfNN Run 2/10, Epoch 827/1000, Training Loss (NLML): -960.6736\n",
      "convergence dfGPdfNN Run 2/10, Epoch 828/1000, Training Loss (NLML): -960.6533\n",
      "convergence dfGPdfNN Run 2/10, Epoch 829/1000, Training Loss (NLML): -960.6593\n",
      "convergence dfGPdfNN Run 2/10, Epoch 830/1000, Training Loss (NLML): -960.6593\n",
      "convergence dfGPdfNN Run 2/10, Epoch 831/1000, Training Loss (NLML): -960.6644\n",
      "convergence dfGPdfNN Run 2/10, Epoch 832/1000, Training Loss (NLML): -960.6835\n",
      "convergence dfGPdfNN Run 2/10, Epoch 833/1000, Training Loss (NLML): -960.6940\n",
      "convergence dfGPdfNN Run 2/10, Epoch 834/1000, Training Loss (NLML): -960.6851\n",
      "convergence dfGPdfNN Run 2/10, Epoch 835/1000, Training Loss (NLML): -960.6731\n",
      "convergence dfGPdfNN Run 2/10, Epoch 836/1000, Training Loss (NLML): -960.6747\n",
      "convergence dfGPdfNN Run 2/10, Epoch 837/1000, Training Loss (NLML): -960.6766\n",
      "convergence dfGPdfNN Run 2/10, Epoch 838/1000, Training Loss (NLML): -960.7014\n",
      "convergence dfGPdfNN Run 2/10, Epoch 839/1000, Training Loss (NLML): -960.7004\n",
      "convergence dfGPdfNN Run 2/10, Epoch 840/1000, Training Loss (NLML): -960.6974\n",
      "convergence dfGPdfNN Run 2/10, Epoch 841/1000, Training Loss (NLML): -960.6998\n",
      "convergence dfGPdfNN Run 2/10, Epoch 842/1000, Training Loss (NLML): -960.7030\n",
      "convergence dfGPdfNN Run 2/10, Epoch 843/1000, Training Loss (NLML): -960.7104\n",
      "convergence dfGPdfNN Run 2/10, Epoch 844/1000, Training Loss (NLML): -960.6896\n",
      "convergence dfGPdfNN Run 2/10, Epoch 845/1000, Training Loss (NLML): -960.6904\n",
      "convergence dfGPdfNN Run 2/10, Epoch 846/1000, Training Loss (NLML): -960.7123\n",
      "convergence dfGPdfNN Run 2/10, Epoch 847/1000, Training Loss (NLML): -960.7150\n",
      "convergence dfGPdfNN Run 2/10, Epoch 848/1000, Training Loss (NLML): -960.7151\n",
      "convergence dfGPdfNN Run 2/10, Epoch 849/1000, Training Loss (NLML): -960.7162\n",
      "convergence dfGPdfNN Run 2/10, Epoch 850/1000, Training Loss (NLML): -960.7178\n",
      "convergence dfGPdfNN Run 2/10, Epoch 851/1000, Training Loss (NLML): -960.7235\n",
      "convergence dfGPdfNN Run 2/10, Epoch 852/1000, Training Loss (NLML): -960.7273\n",
      "convergence dfGPdfNN Run 2/10, Epoch 853/1000, Training Loss (NLML): -960.7238\n",
      "convergence dfGPdfNN Run 2/10, Epoch 854/1000, Training Loss (NLML): -960.7087\n",
      "convergence dfGPdfNN Run 2/10, Epoch 855/1000, Training Loss (NLML): -960.7313\n",
      "convergence dfGPdfNN Run 2/10, Epoch 856/1000, Training Loss (NLML): -960.7328\n",
      "convergence dfGPdfNN Run 2/10, Epoch 857/1000, Training Loss (NLML): -960.7336\n",
      "convergence dfGPdfNN Run 2/10, Epoch 858/1000, Training Loss (NLML): -960.7507\n",
      "convergence dfGPdfNN Run 2/10, Epoch 859/1000, Training Loss (NLML): -960.7382\n",
      "convergence dfGPdfNN Run 2/10, Epoch 860/1000, Training Loss (NLML): -960.7373\n",
      "convergence dfGPdfNN Run 2/10, Epoch 861/1000, Training Loss (NLML): -960.7428\n",
      "convergence dfGPdfNN Run 2/10, Epoch 862/1000, Training Loss (NLML): -960.7449\n",
      "convergence dfGPdfNN Run 2/10, Epoch 863/1000, Training Loss (NLML): -960.7466\n",
      "convergence dfGPdfNN Run 2/10, Epoch 864/1000, Training Loss (NLML): -960.7485\n",
      "convergence dfGPdfNN Run 2/10, Epoch 865/1000, Training Loss (NLML): -960.7522\n",
      "convergence dfGPdfNN Run 2/10, Epoch 866/1000, Training Loss (NLML): -960.7671\n",
      "convergence dfGPdfNN Run 2/10, Epoch 867/1000, Training Loss (NLML): -960.7506\n",
      "convergence dfGPdfNN Run 2/10, Epoch 868/1000, Training Loss (NLML): -960.7574\n",
      "convergence dfGPdfNN Run 2/10, Epoch 869/1000, Training Loss (NLML): -960.7579\n",
      "convergence dfGPdfNN Run 2/10, Epoch 870/1000, Training Loss (NLML): -960.7607\n",
      "convergence dfGPdfNN Run 2/10, Epoch 871/1000, Training Loss (NLML): -960.7651\n",
      "convergence dfGPdfNN Run 2/10, Epoch 872/1000, Training Loss (NLML): -960.7629\n",
      "convergence dfGPdfNN Run 2/10, Epoch 873/1000, Training Loss (NLML): -960.7816\n",
      "convergence dfGPdfNN Run 2/10, Epoch 874/1000, Training Loss (NLML): -960.7687\n",
      "convergence dfGPdfNN Run 2/10, Epoch 875/1000, Training Loss (NLML): -960.7709\n",
      "convergence dfGPdfNN Run 2/10, Epoch 876/1000, Training Loss (NLML): -960.7693\n",
      "convergence dfGPdfNN Run 2/10, Epoch 877/1000, Training Loss (NLML): -960.7766\n",
      "convergence dfGPdfNN Run 2/10, Epoch 878/1000, Training Loss (NLML): -960.7957\n",
      "convergence dfGPdfNN Run 2/10, Epoch 879/1000, Training Loss (NLML): -960.7727\n",
      "convergence dfGPdfNN Run 2/10, Epoch 880/1000, Training Loss (NLML): -960.7692\n",
      "convergence dfGPdfNN Run 2/10, Epoch 881/1000, Training Loss (NLML): -960.7780\n",
      "convergence dfGPdfNN Run 2/10, Epoch 882/1000, Training Loss (NLML): -960.7863\n",
      "convergence dfGPdfNN Run 2/10, Epoch 883/1000, Training Loss (NLML): -960.8048\n",
      "convergence dfGPdfNN Run 2/10, Epoch 884/1000, Training Loss (NLML): -960.7876\n",
      "convergence dfGPdfNN Run 2/10, Epoch 885/1000, Training Loss (NLML): -960.7670\n",
      "convergence dfGPdfNN Run 2/10, Epoch 886/1000, Training Loss (NLML): -960.7878\n",
      "convergence dfGPdfNN Run 2/10, Epoch 887/1000, Training Loss (NLML): -960.8094\n",
      "convergence dfGPdfNN Run 2/10, Epoch 888/1000, Training Loss (NLML): -960.7925\n",
      "convergence dfGPdfNN Run 2/10, Epoch 889/1000, Training Loss (NLML): -960.7922\n",
      "convergence dfGPdfNN Run 2/10, Epoch 890/1000, Training Loss (NLML): -960.7928\n",
      "convergence dfGPdfNN Run 2/10, Epoch 891/1000, Training Loss (NLML): -960.7766\n",
      "convergence dfGPdfNN Run 2/10, Epoch 892/1000, Training Loss (NLML): -960.8126\n",
      "convergence dfGPdfNN Run 2/10, Epoch 893/1000, Training Loss (NLML): -960.8158\n",
      "convergence dfGPdfNN Run 2/10, Epoch 894/1000, Training Loss (NLML): -960.7816\n",
      "convergence dfGPdfNN Run 2/10, Epoch 895/1000, Training Loss (NLML): -960.7844\n",
      "convergence dfGPdfNN Run 2/10, Epoch 896/1000, Training Loss (NLML): -960.8217\n",
      "convergence dfGPdfNN Run 2/10, Epoch 897/1000, Training Loss (NLML): -960.8077\n",
      "convergence dfGPdfNN Run 2/10, Epoch 898/1000, Training Loss (NLML): -960.7926\n",
      "convergence dfGPdfNN Run 2/10, Epoch 899/1000, Training Loss (NLML): -960.8232\n",
      "convergence dfGPdfNN Run 2/10, Epoch 900/1000, Training Loss (NLML): -960.7947\n",
      "convergence dfGPdfNN Run 2/10, Epoch 901/1000, Training Loss (NLML): -960.8274\n",
      "convergence dfGPdfNN Run 2/10, Epoch 902/1000, Training Loss (NLML): -960.8010\n",
      "convergence dfGPdfNN Run 2/10, Epoch 903/1000, Training Loss (NLML): -960.8052\n",
      "convergence dfGPdfNN Run 2/10, Epoch 904/1000, Training Loss (NLML): -960.8337\n",
      "convergence dfGPdfNN Run 2/10, Epoch 905/1000, Training Loss (NLML): -960.8324\n",
      "convergence dfGPdfNN Run 2/10, Epoch 906/1000, Training Loss (NLML): -960.8348\n",
      "convergence dfGPdfNN Run 2/10, Epoch 907/1000, Training Loss (NLML): -960.8073\n",
      "convergence dfGPdfNN Run 2/10, Epoch 908/1000, Training Loss (NLML): -960.8112\n",
      "convergence dfGPdfNN Run 2/10, Epoch 909/1000, Training Loss (NLML): -960.8452\n",
      "convergence dfGPdfNN Run 2/10, Epoch 910/1000, Training Loss (NLML): -960.8488\n",
      "convergence dfGPdfNN Run 2/10, Epoch 911/1000, Training Loss (NLML): -960.8312\n",
      "convergence dfGPdfNN Run 2/10, Epoch 912/1000, Training Loss (NLML): -960.8346\n",
      "convergence dfGPdfNN Run 2/10, Epoch 913/1000, Training Loss (NLML): -960.8342\n",
      "convergence dfGPdfNN Run 2/10, Epoch 914/1000, Training Loss (NLML): -960.8489\n",
      "convergence dfGPdfNN Run 2/10, Epoch 915/1000, Training Loss (NLML): -960.8433\n",
      "convergence dfGPdfNN Run 2/10, Epoch 916/1000, Training Loss (NLML): -960.8446\n",
      "convergence dfGPdfNN Run 2/10, Epoch 917/1000, Training Loss (NLML): -960.8475\n",
      "convergence dfGPdfNN Run 2/10, Epoch 918/1000, Training Loss (NLML): -960.8560\n",
      "convergence dfGPdfNN Run 2/10, Epoch 919/1000, Training Loss (NLML): -960.8285\n",
      "convergence dfGPdfNN Run 2/10, Epoch 920/1000, Training Loss (NLML): -960.8591\n",
      "convergence dfGPdfNN Run 2/10, Epoch 921/1000, Training Loss (NLML): -960.8652\n",
      "convergence dfGPdfNN Run 2/10, Epoch 922/1000, Training Loss (NLML): -960.8391\n",
      "convergence dfGPdfNN Run 2/10, Epoch 923/1000, Training Loss (NLML): -960.8590\n",
      "convergence dfGPdfNN Run 2/10, Epoch 924/1000, Training Loss (NLML): -960.8685\n",
      "convergence dfGPdfNN Run 2/10, Epoch 925/1000, Training Loss (NLML): -960.8687\n",
      "convergence dfGPdfNN Run 2/10, Epoch 926/1000, Training Loss (NLML): -960.8700\n",
      "convergence dfGPdfNN Run 2/10, Epoch 927/1000, Training Loss (NLML): -960.8453\n",
      "convergence dfGPdfNN Run 2/10, Epoch 928/1000, Training Loss (NLML): -960.8514\n",
      "convergence dfGPdfNN Run 2/10, Epoch 929/1000, Training Loss (NLML): -960.8833\n",
      "convergence dfGPdfNN Run 2/10, Epoch 930/1000, Training Loss (NLML): -960.8811\n",
      "convergence dfGPdfNN Run 2/10, Epoch 931/1000, Training Loss (NLML): -960.8762\n",
      "convergence dfGPdfNN Run 2/10, Epoch 932/1000, Training Loss (NLML): -960.8785\n",
      "convergence dfGPdfNN Run 2/10, Epoch 933/1000, Training Loss (NLML): -960.8792\n",
      "convergence dfGPdfNN Run 2/10, Epoch 934/1000, Training Loss (NLML): -960.8506\n",
      "convergence dfGPdfNN Run 2/10, Epoch 935/1000, Training Loss (NLML): -960.8712\n",
      "convergence dfGPdfNN Run 2/10, Epoch 936/1000, Training Loss (NLML): -960.8909\n",
      "convergence dfGPdfNN Run 2/10, Epoch 937/1000, Training Loss (NLML): -960.8928\n",
      "convergence dfGPdfNN Run 2/10, Epoch 938/1000, Training Loss (NLML): -960.8865\n",
      "convergence dfGPdfNN Run 2/10, Epoch 939/1000, Training Loss (NLML): -960.8879\n",
      "convergence dfGPdfNN Run 2/10, Epoch 940/1000, Training Loss (NLML): -960.8706\n",
      "convergence dfGPdfNN Run 2/10, Epoch 941/1000, Training Loss (NLML): -960.8740\n",
      "convergence dfGPdfNN Run 2/10, Epoch 942/1000, Training Loss (NLML): -960.8748\n",
      "convergence dfGPdfNN Run 2/10, Epoch 943/1000, Training Loss (NLML): -960.8889\n",
      "convergence dfGPdfNN Run 2/10, Epoch 944/1000, Training Loss (NLML): -960.8925\n",
      "convergence dfGPdfNN Run 2/10, Epoch 945/1000, Training Loss (NLML): -960.8959\n",
      "convergence dfGPdfNN Run 2/10, Epoch 946/1000, Training Loss (NLML): -960.8981\n",
      "convergence dfGPdfNN Run 2/10, Epoch 947/1000, Training Loss (NLML): -960.8859\n",
      "convergence dfGPdfNN Run 2/10, Epoch 948/1000, Training Loss (NLML): -960.8718\n",
      "convergence dfGPdfNN Run 2/10, Epoch 949/1000, Training Loss (NLML): -960.9016\n",
      "convergence dfGPdfNN Run 2/10, Epoch 950/1000, Training Loss (NLML): -960.9022\n",
      "convergence dfGPdfNN Run 2/10, Epoch 951/1000, Training Loss (NLML): -960.9058\n",
      "convergence dfGPdfNN Run 2/10, Epoch 952/1000, Training Loss (NLML): -960.9092\n",
      "convergence dfGPdfNN Run 2/10, Epoch 953/1000, Training Loss (NLML): -960.8956\n",
      "convergence dfGPdfNN Run 2/10, Epoch 954/1000, Training Loss (NLML): -960.8969\n",
      "convergence dfGPdfNN Run 2/10, Epoch 955/1000, Training Loss (NLML): -960.9116\n",
      "convergence dfGPdfNN Run 2/10, Epoch 956/1000, Training Loss (NLML): -960.9050\n",
      "convergence dfGPdfNN Run 2/10, Epoch 957/1000, Training Loss (NLML): -960.9039\n",
      "convergence dfGPdfNN Run 2/10, Epoch 958/1000, Training Loss (NLML): -960.9092\n",
      "convergence dfGPdfNN Run 2/10, Epoch 959/1000, Training Loss (NLML): -960.8953\n",
      "convergence dfGPdfNN Run 2/10, Epoch 960/1000, Training Loss (NLML): -960.8960\n",
      "convergence dfGPdfNN Run 2/10, Epoch 961/1000, Training Loss (NLML): -960.9120\n",
      "convergence dfGPdfNN Run 2/10, Epoch 962/1000, Training Loss (NLML): -960.9125\n",
      "convergence dfGPdfNN Run 2/10, Epoch 963/1000, Training Loss (NLML): -960.9164\n",
      "convergence dfGPdfNN Run 2/10, Epoch 964/1000, Training Loss (NLML): -960.9180\n",
      "convergence dfGPdfNN Run 2/10, Epoch 965/1000, Training Loss (NLML): -960.9189\n",
      "convergence dfGPdfNN Run 2/10, Epoch 966/1000, Training Loss (NLML): -960.9076\n",
      "convergence dfGPdfNN Run 2/10, Epoch 967/1000, Training Loss (NLML): -960.9152\n",
      "convergence dfGPdfNN Run 2/10, Epoch 968/1000, Training Loss (NLML): -960.9128\n",
      "convergence dfGPdfNN Run 2/10, Epoch 969/1000, Training Loss (NLML): -960.9268\n",
      "convergence dfGPdfNN Run 2/10, Epoch 970/1000, Training Loss (NLML): -960.9261\n",
      "convergence dfGPdfNN Run 2/10, Epoch 971/1000, Training Loss (NLML): -960.9258\n",
      "convergence dfGPdfNN Run 2/10, Epoch 972/1000, Training Loss (NLML): -960.9285\n",
      "convergence dfGPdfNN Run 2/10, Epoch 973/1000, Training Loss (NLML): -960.9324\n",
      "convergence dfGPdfNN Run 2/10, Epoch 974/1000, Training Loss (NLML): -960.9321\n",
      "convergence dfGPdfNN Run 2/10, Epoch 975/1000, Training Loss (NLML): -960.9360\n",
      "convergence dfGPdfNN Run 2/10, Epoch 976/1000, Training Loss (NLML): -960.9355\n",
      "convergence dfGPdfNN Run 2/10, Epoch 977/1000, Training Loss (NLML): -960.9382\n",
      "convergence dfGPdfNN Run 2/10, Epoch 978/1000, Training Loss (NLML): -960.9395\n",
      "convergence dfGPdfNN Run 2/10, Epoch 979/1000, Training Loss (NLML): -960.9370\n",
      "convergence dfGPdfNN Run 2/10, Epoch 980/1000, Training Loss (NLML): -960.9375\n",
      "convergence dfGPdfNN Run 2/10, Epoch 981/1000, Training Loss (NLML): -960.9410\n",
      "convergence dfGPdfNN Run 2/10, Epoch 982/1000, Training Loss (NLML): -960.9337\n",
      "convergence dfGPdfNN Run 2/10, Epoch 983/1000, Training Loss (NLML): -960.9337\n",
      "convergence dfGPdfNN Run 2/10, Epoch 984/1000, Training Loss (NLML): -960.9348\n",
      "convergence dfGPdfNN Run 2/10, Epoch 985/1000, Training Loss (NLML): -960.9296\n",
      "convergence dfGPdfNN Run 2/10, Epoch 986/1000, Training Loss (NLML): -960.9254\n",
      "convergence dfGPdfNN Run 2/10, Epoch 987/1000, Training Loss (NLML): -960.9263\n",
      "convergence dfGPdfNN Run 2/10, Epoch 988/1000, Training Loss (NLML): -960.9309\n",
      "convergence dfGPdfNN Run 2/10, Epoch 989/1000, Training Loss (NLML): -960.9333\n",
      "convergence dfGPdfNN Run 2/10, Epoch 990/1000, Training Loss (NLML): -960.9353\n",
      "convergence dfGPdfNN Run 2/10, Epoch 991/1000, Training Loss (NLML): -960.9302\n",
      "convergence dfGPdfNN Run 2/10, Epoch 992/1000, Training Loss (NLML): -960.9396\n",
      "convergence dfGPdfNN Run 2/10, Epoch 993/1000, Training Loss (NLML): -960.9390\n",
      "convergence dfGPdfNN Run 2/10, Epoch 994/1000, Training Loss (NLML): -960.9377\n",
      "convergence dfGPdfNN Run 2/10, Epoch 995/1000, Training Loss (NLML): -960.9395\n",
      "convergence dfGPdfNN Run 2/10, Epoch 996/1000, Training Loss (NLML): -960.9373\n",
      "convergence dfGPdfNN Run 2/10, Epoch 997/1000, Training Loss (NLML): -960.9320\n",
      "convergence dfGPdfNN Run 2/10, Epoch 998/1000, Training Loss (NLML): -960.9431\n",
      "convergence dfGPdfNN Run 2/10, Epoch 999/1000, Training Loss (NLML): -960.9371\n",
      "convergence dfGPdfNN Run 2/10, Epoch 1000/1000, Training Loss (NLML): -960.9424\n",
      "\n",
      "--- Training Run 3/10 ---\n",
      "\n",
      "Start Training\n",
      "convergence dfGPdfNN Run 3/10, Epoch 1/1000, Training Loss (NLML): -862.1488\n",
      "convergence dfGPdfNN Run 3/10, Epoch 2/1000, Training Loss (NLML): -875.7240\n",
      "convergence dfGPdfNN Run 3/10, Epoch 3/1000, Training Loss (NLML): -882.3160\n",
      "convergence dfGPdfNN Run 3/10, Epoch 4/1000, Training Loss (NLML): -885.6136\n",
      "convergence dfGPdfNN Run 3/10, Epoch 5/1000, Training Loss (NLML): -888.4983\n",
      "convergence dfGPdfNN Run 3/10, Epoch 6/1000, Training Loss (NLML): -891.2635\n",
      "convergence dfGPdfNN Run 3/10, Epoch 7/1000, Training Loss (NLML): -894.0785\n",
      "convergence dfGPdfNN Run 3/10, Epoch 8/1000, Training Loss (NLML): -896.0087\n",
      "convergence dfGPdfNN Run 3/10, Epoch 9/1000, Training Loss (NLML): -898.1326\n",
      "convergence dfGPdfNN Run 3/10, Epoch 10/1000, Training Loss (NLML): -900.1550\n",
      "convergence dfGPdfNN Run 3/10, Epoch 11/1000, Training Loss (NLML): -901.8853\n",
      "convergence dfGPdfNN Run 3/10, Epoch 12/1000, Training Loss (NLML): -903.6483\n",
      "convergence dfGPdfNN Run 3/10, Epoch 13/1000, Training Loss (NLML): -905.3789\n",
      "convergence dfGPdfNN Run 3/10, Epoch 14/1000, Training Loss (NLML): -907.1729\n",
      "convergence dfGPdfNN Run 3/10, Epoch 15/1000, Training Loss (NLML): -908.7870\n",
      "convergence dfGPdfNN Run 3/10, Epoch 16/1000, Training Loss (NLML): -910.3483\n",
      "convergence dfGPdfNN Run 3/10, Epoch 17/1000, Training Loss (NLML): -911.8600\n",
      "convergence dfGPdfNN Run 3/10, Epoch 18/1000, Training Loss (NLML): -913.1165\n",
      "convergence dfGPdfNN Run 3/10, Epoch 19/1000, Training Loss (NLML): -914.3374\n",
      "convergence dfGPdfNN Run 3/10, Epoch 20/1000, Training Loss (NLML): -915.7462\n",
      "convergence dfGPdfNN Run 3/10, Epoch 21/1000, Training Loss (NLML): -917.0553\n",
      "convergence dfGPdfNN Run 3/10, Epoch 22/1000, Training Loss (NLML): -918.1902\n",
      "convergence dfGPdfNN Run 3/10, Epoch 23/1000, Training Loss (NLML): -919.1698\n",
      "convergence dfGPdfNN Run 3/10, Epoch 24/1000, Training Loss (NLML): -920.3804\n",
      "convergence dfGPdfNN Run 3/10, Epoch 25/1000, Training Loss (NLML): -921.3977\n",
      "convergence dfGPdfNN Run 3/10, Epoch 26/1000, Training Loss (NLML): -922.4009\n",
      "convergence dfGPdfNN Run 3/10, Epoch 27/1000, Training Loss (NLML): -923.1918\n",
      "convergence dfGPdfNN Run 3/10, Epoch 28/1000, Training Loss (NLML): -923.5127\n",
      "convergence dfGPdfNN Run 3/10, Epoch 29/1000, Training Loss (NLML): -924.9617\n",
      "convergence dfGPdfNN Run 3/10, Epoch 30/1000, Training Loss (NLML): -925.9597\n",
      "convergence dfGPdfNN Run 3/10, Epoch 31/1000, Training Loss (NLML): -926.7715\n",
      "convergence dfGPdfNN Run 3/10, Epoch 32/1000, Training Loss (NLML): -927.5536\n",
      "convergence dfGPdfNN Run 3/10, Epoch 33/1000, Training Loss (NLML): -927.9907\n",
      "convergence dfGPdfNN Run 3/10, Epoch 34/1000, Training Loss (NLML): -928.9550\n",
      "convergence dfGPdfNN Run 3/10, Epoch 35/1000, Training Loss (NLML): -929.6284\n",
      "convergence dfGPdfNN Run 3/10, Epoch 36/1000, Training Loss (NLML): -930.3464\n",
      "convergence dfGPdfNN Run 3/10, Epoch 37/1000, Training Loss (NLML): -931.0604\n",
      "convergence dfGPdfNN Run 3/10, Epoch 38/1000, Training Loss (NLML): -931.6627\n",
      "convergence dfGPdfNN Run 3/10, Epoch 39/1000, Training Loss (NLML): -932.2719\n",
      "convergence dfGPdfNN Run 3/10, Epoch 40/1000, Training Loss (NLML): -932.8600\n",
      "convergence dfGPdfNN Run 3/10, Epoch 41/1000, Training Loss (NLML): -933.2101\n",
      "convergence dfGPdfNN Run 3/10, Epoch 42/1000, Training Loss (NLML): -933.7808\n",
      "convergence dfGPdfNN Run 3/10, Epoch 43/1000, Training Loss (NLML): -934.5194\n",
      "convergence dfGPdfNN Run 3/10, Epoch 44/1000, Training Loss (NLML): -934.9761\n",
      "convergence dfGPdfNN Run 3/10, Epoch 45/1000, Training Loss (NLML): -935.4098\n",
      "convergence dfGPdfNN Run 3/10, Epoch 46/1000, Training Loss (NLML): -936.0702\n",
      "convergence dfGPdfNN Run 3/10, Epoch 47/1000, Training Loss (NLML): -936.5494\n",
      "convergence dfGPdfNN Run 3/10, Epoch 48/1000, Training Loss (NLML): -936.8783\n",
      "convergence dfGPdfNN Run 3/10, Epoch 49/1000, Training Loss (NLML): -937.2791\n",
      "convergence dfGPdfNN Run 3/10, Epoch 50/1000, Training Loss (NLML): -937.7358\n",
      "convergence dfGPdfNN Run 3/10, Epoch 51/1000, Training Loss (NLML): -937.3933\n",
      "convergence dfGPdfNN Run 3/10, Epoch 52/1000, Training Loss (NLML): -938.6793\n",
      "convergence dfGPdfNN Run 3/10, Epoch 53/1000, Training Loss (NLML): -938.9773\n",
      "convergence dfGPdfNN Run 3/10, Epoch 54/1000, Training Loss (NLML): -939.1362\n",
      "convergence dfGPdfNN Run 3/10, Epoch 55/1000, Training Loss (NLML): -939.9271\n",
      "convergence dfGPdfNN Run 3/10, Epoch 56/1000, Training Loss (NLML): -940.3271\n",
      "convergence dfGPdfNN Run 3/10, Epoch 57/1000, Training Loss (NLML): -940.6824\n",
      "convergence dfGPdfNN Run 3/10, Epoch 58/1000, Training Loss (NLML): -940.9139\n",
      "convergence dfGPdfNN Run 3/10, Epoch 59/1000, Training Loss (NLML): -941.2456\n",
      "convergence dfGPdfNN Run 3/10, Epoch 60/1000, Training Loss (NLML): -941.2520\n",
      "convergence dfGPdfNN Run 3/10, Epoch 61/1000, Training Loss (NLML): -941.8751\n",
      "convergence dfGPdfNN Run 3/10, Epoch 62/1000, Training Loss (NLML): -942.3672\n",
      "convergence dfGPdfNN Run 3/10, Epoch 63/1000, Training Loss (NLML): -942.1173\n",
      "convergence dfGPdfNN Run 3/10, Epoch 64/1000, Training Loss (NLML): -942.9634\n",
      "convergence dfGPdfNN Run 3/10, Epoch 65/1000, Training Loss (NLML): -943.1914\n",
      "convergence dfGPdfNN Run 3/10, Epoch 66/1000, Training Loss (NLML): -943.5547\n",
      "convergence dfGPdfNN Run 3/10, Epoch 67/1000, Training Loss (NLML): -943.6008\n",
      "convergence dfGPdfNN Run 3/10, Epoch 68/1000, Training Loss (NLML): -943.7786\n",
      "convergence dfGPdfNN Run 3/10, Epoch 69/1000, Training Loss (NLML): -944.4091\n",
      "convergence dfGPdfNN Run 3/10, Epoch 70/1000, Training Loss (NLML): -944.0654\n",
      "convergence dfGPdfNN Run 3/10, Epoch 71/1000, Training Loss (NLML): -944.7344\n",
      "convergence dfGPdfNN Run 3/10, Epoch 72/1000, Training Loss (NLML): -944.9376\n",
      "convergence dfGPdfNN Run 3/10, Epoch 73/1000, Training Loss (NLML): -945.2551\n",
      "convergence dfGPdfNN Run 3/10, Epoch 74/1000, Training Loss (NLML): -945.5725\n",
      "convergence dfGPdfNN Run 3/10, Epoch 75/1000, Training Loss (NLML): -946.0028\n",
      "convergence dfGPdfNN Run 3/10, Epoch 76/1000, Training Loss (NLML): -945.9460\n",
      "convergence dfGPdfNN Run 3/10, Epoch 77/1000, Training Loss (NLML): -943.5714\n",
      "convergence dfGPdfNN Run 3/10, Epoch 78/1000, Training Loss (NLML): -945.9829\n",
      "convergence dfGPdfNN Run 3/10, Epoch 79/1000, Training Loss (NLML): -945.4730\n",
      "convergence dfGPdfNN Run 3/10, Epoch 80/1000, Training Loss (NLML): -946.7584\n",
      "convergence dfGPdfNN Run 3/10, Epoch 81/1000, Training Loss (NLML): -946.9357\n",
      "convergence dfGPdfNN Run 3/10, Epoch 82/1000, Training Loss (NLML): -947.1116\n",
      "convergence dfGPdfNN Run 3/10, Epoch 83/1000, Training Loss (NLML): -947.2727\n",
      "convergence dfGPdfNN Run 3/10, Epoch 84/1000, Training Loss (NLML): -947.6954\n",
      "convergence dfGPdfNN Run 3/10, Epoch 85/1000, Training Loss (NLML): -947.8717\n",
      "convergence dfGPdfNN Run 3/10, Epoch 86/1000, Training Loss (NLML): -948.0789\n",
      "convergence dfGPdfNN Run 3/10, Epoch 87/1000, Training Loss (NLML): -947.0686\n",
      "convergence dfGPdfNN Run 3/10, Epoch 88/1000, Training Loss (NLML): -948.3057\n",
      "convergence dfGPdfNN Run 3/10, Epoch 89/1000, Training Loss (NLML): -948.6558\n",
      "convergence dfGPdfNN Run 3/10, Epoch 90/1000, Training Loss (NLML): -947.8103\n",
      "convergence dfGPdfNN Run 3/10, Epoch 91/1000, Training Loss (NLML): -949.0024\n",
      "convergence dfGPdfNN Run 3/10, Epoch 92/1000, Training Loss (NLML): -948.9150\n",
      "convergence dfGPdfNN Run 3/10, Epoch 93/1000, Training Loss (NLML): -949.3092\n",
      "convergence dfGPdfNN Run 3/10, Epoch 94/1000, Training Loss (NLML): -943.7252\n",
      "convergence dfGPdfNN Run 3/10, Epoch 95/1000, Training Loss (NLML): -949.6289\n",
      "convergence dfGPdfNN Run 3/10, Epoch 96/1000, Training Loss (NLML): -949.6085\n",
      "convergence dfGPdfNN Run 3/10, Epoch 97/1000, Training Loss (NLML): -949.5721\n",
      "convergence dfGPdfNN Run 3/10, Epoch 98/1000, Training Loss (NLML): -949.7584\n",
      "convergence dfGPdfNN Run 3/10, Epoch 99/1000, Training Loss (NLML): -949.9359\n",
      "convergence dfGPdfNN Run 3/10, Epoch 100/1000, Training Loss (NLML): -950.0370\n",
      "convergence dfGPdfNN Run 3/10, Epoch 101/1000, Training Loss (NLML): -949.6040\n",
      "convergence dfGPdfNN Run 3/10, Epoch 102/1000, Training Loss (NLML): -949.4521\n",
      "convergence dfGPdfNN Run 3/10, Epoch 103/1000, Training Loss (NLML): -949.6877\n",
      "convergence dfGPdfNN Run 3/10, Epoch 104/1000, Training Loss (NLML): -950.3147\n",
      "convergence dfGPdfNN Run 3/10, Epoch 105/1000, Training Loss (NLML): -950.4655\n",
      "convergence dfGPdfNN Run 3/10, Epoch 106/1000, Training Loss (NLML): -950.5851\n",
      "convergence dfGPdfNN Run 3/10, Epoch 107/1000, Training Loss (NLML): -950.7599\n",
      "convergence dfGPdfNN Run 3/10, Epoch 108/1000, Training Loss (NLML): -950.4789\n",
      "convergence dfGPdfNN Run 3/10, Epoch 109/1000, Training Loss (NLML): -950.5601\n",
      "convergence dfGPdfNN Run 3/10, Epoch 110/1000, Training Loss (NLML): -951.1090\n",
      "convergence dfGPdfNN Run 3/10, Epoch 111/1000, Training Loss (NLML): -950.4209\n",
      "convergence dfGPdfNN Run 3/10, Epoch 112/1000, Training Loss (NLML): -951.2168\n",
      "convergence dfGPdfNN Run 3/10, Epoch 113/1000, Training Loss (NLML): -951.3761\n",
      "convergence dfGPdfNN Run 3/10, Epoch 114/1000, Training Loss (NLML): -951.4969\n",
      "convergence dfGPdfNN Run 3/10, Epoch 115/1000, Training Loss (NLML): -951.6798\n",
      "convergence dfGPdfNN Run 3/10, Epoch 116/1000, Training Loss (NLML): -951.2542\n",
      "convergence dfGPdfNN Run 3/10, Epoch 117/1000, Training Loss (NLML): -947.3206\n",
      "convergence dfGPdfNN Run 3/10, Epoch 118/1000, Training Loss (NLML): -951.6111\n",
      "convergence dfGPdfNN Run 3/10, Epoch 119/1000, Training Loss (NLML): -952.0695\n",
      "convergence dfGPdfNN Run 3/10, Epoch 120/1000, Training Loss (NLML): -952.3500\n",
      "convergence dfGPdfNN Run 3/10, Epoch 121/1000, Training Loss (NLML): -951.2341\n",
      "convergence dfGPdfNN Run 3/10, Epoch 122/1000, Training Loss (NLML): -951.7855\n",
      "convergence dfGPdfNN Run 3/10, Epoch 123/1000, Training Loss (NLML): -952.7742\n",
      "convergence dfGPdfNN Run 3/10, Epoch 124/1000, Training Loss (NLML): -952.4769\n",
      "convergence dfGPdfNN Run 3/10, Epoch 125/1000, Training Loss (NLML): -952.1654\n",
      "convergence dfGPdfNN Run 3/10, Epoch 126/1000, Training Loss (NLML): -951.3602\n",
      "convergence dfGPdfNN Run 3/10, Epoch 127/1000, Training Loss (NLML): -952.0878\n",
      "convergence dfGPdfNN Run 3/10, Epoch 128/1000, Training Loss (NLML): -952.1210\n",
      "convergence dfGPdfNN Run 3/10, Epoch 129/1000, Training Loss (NLML): -952.8850\n",
      "convergence dfGPdfNN Run 3/10, Epoch 130/1000, Training Loss (NLML): -952.9512\n",
      "convergence dfGPdfNN Run 3/10, Epoch 131/1000, Training Loss (NLML): -952.9750\n",
      "convergence dfGPdfNN Run 3/10, Epoch 132/1000, Training Loss (NLML): -953.1869\n",
      "convergence dfGPdfNN Run 3/10, Epoch 133/1000, Training Loss (NLML): -953.0184\n",
      "convergence dfGPdfNN Run 3/10, Epoch 134/1000, Training Loss (NLML): -952.6907\n",
      "convergence dfGPdfNN Run 3/10, Epoch 135/1000, Training Loss (NLML): -951.7029\n",
      "convergence dfGPdfNN Run 3/10, Epoch 136/1000, Training Loss (NLML): -951.7831\n",
      "convergence dfGPdfNN Run 3/10, Epoch 137/1000, Training Loss (NLML): -953.0813\n",
      "convergence dfGPdfNN Run 3/10, Epoch 138/1000, Training Loss (NLML): -953.0818\n",
      "convergence dfGPdfNN Run 3/10, Epoch 139/1000, Training Loss (NLML): -952.6069\n",
      "convergence dfGPdfNN Run 3/10, Epoch 140/1000, Training Loss (NLML): -952.8323\n",
      "convergence dfGPdfNN Run 3/10, Epoch 141/1000, Training Loss (NLML): -953.1631\n",
      "convergence dfGPdfNN Run 3/10, Epoch 142/1000, Training Loss (NLML): -950.2722\n",
      "convergence dfGPdfNN Run 3/10, Epoch 143/1000, Training Loss (NLML): -953.1754\n",
      "convergence dfGPdfNN Run 3/10, Epoch 144/1000, Training Loss (NLML): -953.2831\n",
      "convergence dfGPdfNN Run 3/10, Epoch 145/1000, Training Loss (NLML): -953.5039\n",
      "convergence dfGPdfNN Run 3/10, Epoch 146/1000, Training Loss (NLML): -953.4917\n",
      "convergence dfGPdfNN Run 3/10, Epoch 147/1000, Training Loss (NLML): -953.5406\n",
      "convergence dfGPdfNN Run 3/10, Epoch 148/1000, Training Loss (NLML): -953.8207\n",
      "convergence dfGPdfNN Run 3/10, Epoch 149/1000, Training Loss (NLML): -953.8260\n",
      "convergence dfGPdfNN Run 3/10, Epoch 150/1000, Training Loss (NLML): -950.8331\n",
      "convergence dfGPdfNN Run 3/10, Epoch 151/1000, Training Loss (NLML): -953.6515\n",
      "convergence dfGPdfNN Run 3/10, Epoch 152/1000, Training Loss (NLML): -953.7815\n",
      "convergence dfGPdfNN Run 3/10, Epoch 153/1000, Training Loss (NLML): -953.7491\n",
      "convergence dfGPdfNN Run 3/10, Epoch 154/1000, Training Loss (NLML): -953.7318\n",
      "convergence dfGPdfNN Run 3/10, Epoch 155/1000, Training Loss (NLML): -953.9240\n",
      "convergence dfGPdfNN Run 3/10, Epoch 156/1000, Training Loss (NLML): -954.1317\n",
      "convergence dfGPdfNN Run 3/10, Epoch 157/1000, Training Loss (NLML): -954.2185\n",
      "convergence dfGPdfNN Run 3/10, Epoch 158/1000, Training Loss (NLML): -954.2472\n",
      "convergence dfGPdfNN Run 3/10, Epoch 159/1000, Training Loss (NLML): -954.1592\n",
      "convergence dfGPdfNN Run 3/10, Epoch 160/1000, Training Loss (NLML): -954.5420\n",
      "convergence dfGPdfNN Run 3/10, Epoch 161/1000, Training Loss (NLML): -954.5183\n",
      "convergence dfGPdfNN Run 3/10, Epoch 162/1000, Training Loss (NLML): -954.5089\n",
      "convergence dfGPdfNN Run 3/10, Epoch 163/1000, Training Loss (NLML): -954.5095\n",
      "convergence dfGPdfNN Run 3/10, Epoch 164/1000, Training Loss (NLML): -954.4666\n",
      "convergence dfGPdfNN Run 3/10, Epoch 165/1000, Training Loss (NLML): -954.3955\n",
      "convergence dfGPdfNN Run 3/10, Epoch 166/1000, Training Loss (NLML): -953.8506\n",
      "convergence dfGPdfNN Run 3/10, Epoch 167/1000, Training Loss (NLML): -952.0356\n",
      "convergence dfGPdfNN Run 3/10, Epoch 168/1000, Training Loss (NLML): -954.3069\n",
      "convergence dfGPdfNN Run 3/10, Epoch 169/1000, Training Loss (NLML): -954.2358\n",
      "convergence dfGPdfNN Run 3/10, Epoch 170/1000, Training Loss (NLML): -954.3728\n",
      "convergence dfGPdfNN Run 3/10, Epoch 171/1000, Training Loss (NLML): -954.4771\n",
      "convergence dfGPdfNN Run 3/10, Epoch 172/1000, Training Loss (NLML): -954.4951\n",
      "convergence dfGPdfNN Run 3/10, Epoch 173/1000, Training Loss (NLML): -954.4301\n",
      "convergence dfGPdfNN Run 3/10, Epoch 174/1000, Training Loss (NLML): -954.2971\n",
      "convergence dfGPdfNN Run 3/10, Epoch 175/1000, Training Loss (NLML): -954.2517\n",
      "convergence dfGPdfNN Run 3/10, Epoch 176/1000, Training Loss (NLML): -954.2513\n",
      "convergence dfGPdfNN Run 3/10, Epoch 177/1000, Training Loss (NLML): -954.2623\n",
      "convergence dfGPdfNN Run 3/10, Epoch 178/1000, Training Loss (NLML): -954.5474\n",
      "convergence dfGPdfNN Run 3/10, Epoch 179/1000, Training Loss (NLML): -954.6447\n",
      "convergence dfGPdfNN Run 3/10, Epoch 180/1000, Training Loss (NLML): -954.7396\n",
      "convergence dfGPdfNN Run 3/10, Epoch 181/1000, Training Loss (NLML): -954.6473\n",
      "convergence dfGPdfNN Run 3/10, Epoch 182/1000, Training Loss (NLML): -954.5198\n",
      "convergence dfGPdfNN Run 3/10, Epoch 183/1000, Training Loss (NLML): -954.6082\n",
      "convergence dfGPdfNN Run 3/10, Epoch 184/1000, Training Loss (NLML): -954.2898\n",
      "convergence dfGPdfNN Run 3/10, Epoch 185/1000, Training Loss (NLML): -954.1307\n",
      "convergence dfGPdfNN Run 3/10, Epoch 186/1000, Training Loss (NLML): -953.8308\n",
      "convergence dfGPdfNN Run 3/10, Epoch 187/1000, Training Loss (NLML): -954.2990\n",
      "convergence dfGPdfNN Run 3/10, Epoch 188/1000, Training Loss (NLML): -954.4215\n",
      "convergence dfGPdfNN Run 3/10, Epoch 189/1000, Training Loss (NLML): -954.4783\n",
      "convergence dfGPdfNN Run 3/10, Epoch 190/1000, Training Loss (NLML): -954.5798\n",
      "convergence dfGPdfNN Run 3/10, Epoch 191/1000, Training Loss (NLML): -954.6792\n",
      "convergence dfGPdfNN Run 3/10, Epoch 192/1000, Training Loss (NLML): -954.3210\n",
      "convergence dfGPdfNN Run 3/10, Epoch 193/1000, Training Loss (NLML): -954.9576\n",
      "convergence dfGPdfNN Run 3/10, Epoch 194/1000, Training Loss (NLML): -955.0562\n",
      "convergence dfGPdfNN Run 3/10, Epoch 195/1000, Training Loss (NLML): -955.1069\n",
      "convergence dfGPdfNN Run 3/10, Epoch 196/1000, Training Loss (NLML): -955.1553\n",
      "convergence dfGPdfNN Run 3/10, Epoch 197/1000, Training Loss (NLML): -955.1455\n",
      "convergence dfGPdfNN Run 3/10, Epoch 198/1000, Training Loss (NLML): -955.2961\n",
      "convergence dfGPdfNN Run 3/10, Epoch 199/1000, Training Loss (NLML): -955.4453\n",
      "convergence dfGPdfNN Run 3/10, Epoch 200/1000, Training Loss (NLML): -955.3357\n",
      "convergence dfGPdfNN Run 3/10, Epoch 201/1000, Training Loss (NLML): -955.0853\n",
      "convergence dfGPdfNN Run 3/10, Epoch 202/1000, Training Loss (NLML): -954.9969\n",
      "convergence dfGPdfNN Run 3/10, Epoch 203/1000, Training Loss (NLML): -954.9963\n",
      "convergence dfGPdfNN Run 3/10, Epoch 204/1000, Training Loss (NLML): -955.1462\n",
      "convergence dfGPdfNN Run 3/10, Epoch 205/1000, Training Loss (NLML): -955.1581\n",
      "convergence dfGPdfNN Run 3/10, Epoch 206/1000, Training Loss (NLML): -955.2266\n",
      "convergence dfGPdfNN Run 3/10, Epoch 207/1000, Training Loss (NLML): -955.2262\n",
      "convergence dfGPdfNN Run 3/10, Epoch 208/1000, Training Loss (NLML): -955.2006\n",
      "convergence dfGPdfNN Run 3/10, Epoch 209/1000, Training Loss (NLML): -955.1886\n",
      "convergence dfGPdfNN Run 3/10, Epoch 210/1000, Training Loss (NLML): -955.2122\n",
      "convergence dfGPdfNN Run 3/10, Epoch 211/1000, Training Loss (NLML): -955.2821\n",
      "convergence dfGPdfNN Run 3/10, Epoch 212/1000, Training Loss (NLML): -955.3226\n",
      "convergence dfGPdfNN Run 3/10, Epoch 213/1000, Training Loss (NLML): -955.3622\n",
      "convergence dfGPdfNN Run 3/10, Epoch 214/1000, Training Loss (NLML): -954.6223\n",
      "convergence dfGPdfNN Run 3/10, Epoch 215/1000, Training Loss (NLML): -955.3849\n",
      "convergence dfGPdfNN Run 3/10, Epoch 216/1000, Training Loss (NLML): -955.4181\n",
      "convergence dfGPdfNN Run 3/10, Epoch 217/1000, Training Loss (NLML): -955.4536\n",
      "convergence dfGPdfNN Run 3/10, Epoch 218/1000, Training Loss (NLML): -955.6212\n",
      "convergence dfGPdfNN Run 3/10, Epoch 219/1000, Training Loss (NLML): -955.5275\n",
      "convergence dfGPdfNN Run 3/10, Epoch 220/1000, Training Loss (NLML): -955.6102\n",
      "convergence dfGPdfNN Run 3/10, Epoch 221/1000, Training Loss (NLML): -955.6555\n",
      "convergence dfGPdfNN Run 3/10, Epoch 222/1000, Training Loss (NLML): -955.7472\n",
      "convergence dfGPdfNN Run 3/10, Epoch 223/1000, Training Loss (NLML): -955.7660\n",
      "convergence dfGPdfNN Run 3/10, Epoch 224/1000, Training Loss (NLML): -955.8093\n",
      "convergence dfGPdfNN Run 3/10, Epoch 225/1000, Training Loss (NLML): -955.8425\n",
      "convergence dfGPdfNN Run 3/10, Epoch 226/1000, Training Loss (NLML): -955.8737\n",
      "convergence dfGPdfNN Run 3/10, Epoch 227/1000, Training Loss (NLML): -955.9758\n",
      "convergence dfGPdfNN Run 3/10, Epoch 228/1000, Training Loss (NLML): -955.9030\n",
      "convergence dfGPdfNN Run 3/10, Epoch 229/1000, Training Loss (NLML): -955.9872\n",
      "convergence dfGPdfNN Run 3/10, Epoch 230/1000, Training Loss (NLML): -955.9409\n",
      "convergence dfGPdfNN Run 3/10, Epoch 231/1000, Training Loss (NLML): -956.0750\n",
      "convergence dfGPdfNN Run 3/10, Epoch 232/1000, Training Loss (NLML): -956.0302\n",
      "convergence dfGPdfNN Run 3/10, Epoch 233/1000, Training Loss (NLML): -955.8485\n",
      "convergence dfGPdfNN Run 3/10, Epoch 234/1000, Training Loss (NLML): -955.6946\n",
      "convergence dfGPdfNN Run 3/10, Epoch 235/1000, Training Loss (NLML): -955.6533\n",
      "convergence dfGPdfNN Run 3/10, Epoch 236/1000, Training Loss (NLML): -955.6614\n",
      "convergence dfGPdfNN Run 3/10, Epoch 237/1000, Training Loss (NLML): -955.6562\n",
      "convergence dfGPdfNN Run 3/10, Epoch 238/1000, Training Loss (NLML): -955.7284\n",
      "convergence dfGPdfNN Run 3/10, Epoch 239/1000, Training Loss (NLML): -955.8073\n",
      "convergence dfGPdfNN Run 3/10, Epoch 240/1000, Training Loss (NLML): -955.7795\n",
      "convergence dfGPdfNN Run 3/10, Epoch 241/1000, Training Loss (NLML): -955.7529\n",
      "convergence dfGPdfNN Run 3/10, Epoch 242/1000, Training Loss (NLML): -955.7053\n",
      "convergence dfGPdfNN Run 3/10, Epoch 243/1000, Training Loss (NLML): -955.6958\n",
      "convergence dfGPdfNN Run 3/10, Epoch 244/1000, Training Loss (NLML): -955.6105\n",
      "convergence dfGPdfNN Run 3/10, Epoch 245/1000, Training Loss (NLML): -955.8787\n",
      "convergence dfGPdfNN Run 3/10, Epoch 246/1000, Training Loss (NLML): -955.9944\n",
      "convergence dfGPdfNN Run 3/10, Epoch 247/1000, Training Loss (NLML): -956.0260\n",
      "convergence dfGPdfNN Run 3/10, Epoch 248/1000, Training Loss (NLML): -955.9736\n",
      "convergence dfGPdfNN Run 3/10, Epoch 249/1000, Training Loss (NLML): -956.0066\n",
      "convergence dfGPdfNN Run 3/10, Epoch 250/1000, Training Loss (NLML): -956.0031\n",
      "convergence dfGPdfNN Run 3/10, Epoch 251/1000, Training Loss (NLML): -956.0339\n",
      "convergence dfGPdfNN Run 3/10, Epoch 252/1000, Training Loss (NLML): -956.0658\n",
      "convergence dfGPdfNN Run 3/10, Epoch 253/1000, Training Loss (NLML): -956.0750\n",
      "convergence dfGPdfNN Run 3/10, Epoch 254/1000, Training Loss (NLML): -956.0426\n",
      "convergence dfGPdfNN Run 3/10, Epoch 255/1000, Training Loss (NLML): -956.0592\n",
      "convergence dfGPdfNN Run 3/10, Epoch 256/1000, Training Loss (NLML): -956.0496\n",
      "convergence dfGPdfNN Run 3/10, Epoch 257/1000, Training Loss (NLML): -956.0753\n",
      "convergence dfGPdfNN Run 3/10, Epoch 258/1000, Training Loss (NLML): -956.0917\n",
      "convergence dfGPdfNN Run 3/10, Epoch 259/1000, Training Loss (NLML): -956.1669\n",
      "convergence dfGPdfNN Run 3/10, Epoch 260/1000, Training Loss (NLML): -956.2454\n",
      "convergence dfGPdfNN Run 3/10, Epoch 261/1000, Training Loss (NLML): -956.2844\n",
      "convergence dfGPdfNN Run 3/10, Epoch 262/1000, Training Loss (NLML): -956.3314\n",
      "convergence dfGPdfNN Run 3/10, Epoch 263/1000, Training Loss (NLML): -956.3668\n",
      "convergence dfGPdfNN Run 3/10, Epoch 264/1000, Training Loss (NLML): -956.3834\n",
      "convergence dfGPdfNN Run 3/10, Epoch 265/1000, Training Loss (NLML): -956.4131\n",
      "convergence dfGPdfNN Run 3/10, Epoch 266/1000, Training Loss (NLML): -956.4686\n",
      "convergence dfGPdfNN Run 3/10, Epoch 267/1000, Training Loss (NLML): -956.4313\n",
      "convergence dfGPdfNN Run 3/10, Epoch 268/1000, Training Loss (NLML): -956.4160\n",
      "convergence dfGPdfNN Run 3/10, Epoch 269/1000, Training Loss (NLML): -956.5463\n",
      "convergence dfGPdfNN Run 3/10, Epoch 270/1000, Training Loss (NLML): -956.5873\n",
      "convergence dfGPdfNN Run 3/10, Epoch 271/1000, Training Loss (NLML): -956.6075\n",
      "convergence dfGPdfNN Run 3/10, Epoch 272/1000, Training Loss (NLML): -956.6017\n",
      "convergence dfGPdfNN Run 3/10, Epoch 273/1000, Training Loss (NLML): -956.6536\n",
      "convergence dfGPdfNN Run 3/10, Epoch 274/1000, Training Loss (NLML): -956.6759\n",
      "convergence dfGPdfNN Run 3/10, Epoch 275/1000, Training Loss (NLML): -956.7223\n",
      "convergence dfGPdfNN Run 3/10, Epoch 276/1000, Training Loss (NLML): -956.7577\n",
      "convergence dfGPdfNN Run 3/10, Epoch 277/1000, Training Loss (NLML): -956.8018\n",
      "convergence dfGPdfNN Run 3/10, Epoch 278/1000, Training Loss (NLML): -956.7366\n",
      "convergence dfGPdfNN Run 3/10, Epoch 279/1000, Training Loss (NLML): -956.7550\n",
      "convergence dfGPdfNN Run 3/10, Epoch 280/1000, Training Loss (NLML): -956.6602\n",
      "convergence dfGPdfNN Run 3/10, Epoch 281/1000, Training Loss (NLML): -956.6848\n",
      "convergence dfGPdfNN Run 3/10, Epoch 282/1000, Training Loss (NLML): -956.7357\n",
      "convergence dfGPdfNN Run 3/10, Epoch 283/1000, Training Loss (NLML): -956.7628\n",
      "convergence dfGPdfNN Run 3/10, Epoch 284/1000, Training Loss (NLML): -956.7710\n",
      "convergence dfGPdfNN Run 3/10, Epoch 285/1000, Training Loss (NLML): -956.7987\n",
      "convergence dfGPdfNN Run 3/10, Epoch 286/1000, Training Loss (NLML): -956.8184\n",
      "convergence dfGPdfNN Run 3/10, Epoch 287/1000, Training Loss (NLML): -956.8317\n",
      "convergence dfGPdfNN Run 3/10, Epoch 288/1000, Training Loss (NLML): -956.8453\n",
      "convergence dfGPdfNN Run 3/10, Epoch 289/1000, Training Loss (NLML): -956.8499\n",
      "convergence dfGPdfNN Run 3/10, Epoch 290/1000, Training Loss (NLML): -956.8760\n",
      "convergence dfGPdfNN Run 3/10, Epoch 291/1000, Training Loss (NLML): -956.9417\n",
      "convergence dfGPdfNN Run 3/10, Epoch 292/1000, Training Loss (NLML): -956.9611\n",
      "convergence dfGPdfNN Run 3/10, Epoch 293/1000, Training Loss (NLML): -956.9834\n",
      "convergence dfGPdfNN Run 3/10, Epoch 294/1000, Training Loss (NLML): -956.9757\n",
      "convergence dfGPdfNN Run 3/10, Epoch 295/1000, Training Loss (NLML): -956.9360\n",
      "convergence dfGPdfNN Run 3/10, Epoch 296/1000, Training Loss (NLML): -956.9659\n",
      "convergence dfGPdfNN Run 3/10, Epoch 297/1000, Training Loss (NLML): -956.9741\n",
      "convergence dfGPdfNN Run 3/10, Epoch 298/1000, Training Loss (NLML): -956.9642\n",
      "convergence dfGPdfNN Run 3/10, Epoch 299/1000, Training Loss (NLML): -956.9875\n",
      "convergence dfGPdfNN Run 3/10, Epoch 300/1000, Training Loss (NLML): -956.9973\n",
      "convergence dfGPdfNN Run 3/10, Epoch 301/1000, Training Loss (NLML): -956.9813\n",
      "convergence dfGPdfNN Run 3/10, Epoch 302/1000, Training Loss (NLML): -957.0361\n",
      "convergence dfGPdfNN Run 3/10, Epoch 303/1000, Training Loss (NLML): -957.0804\n",
      "convergence dfGPdfNN Run 3/10, Epoch 304/1000, Training Loss (NLML): -957.1075\n",
      "convergence dfGPdfNN Run 3/10, Epoch 305/1000, Training Loss (NLML): -957.1315\n",
      "convergence dfGPdfNN Run 3/10, Epoch 306/1000, Training Loss (NLML): -957.1177\n",
      "convergence dfGPdfNN Run 3/10, Epoch 307/1000, Training Loss (NLML): -957.1261\n",
      "convergence dfGPdfNN Run 3/10, Epoch 308/1000, Training Loss (NLML): -957.1873\n",
      "convergence dfGPdfNN Run 3/10, Epoch 309/1000, Training Loss (NLML): -957.2079\n",
      "convergence dfGPdfNN Run 3/10, Epoch 310/1000, Training Loss (NLML): -957.2551\n",
      "convergence dfGPdfNN Run 3/10, Epoch 311/1000, Training Loss (NLML): -957.3062\n",
      "convergence dfGPdfNN Run 3/10, Epoch 312/1000, Training Loss (NLML): -957.2625\n",
      "convergence dfGPdfNN Run 3/10, Epoch 313/1000, Training Loss (NLML): -957.3060\n",
      "convergence dfGPdfNN Run 3/10, Epoch 314/1000, Training Loss (NLML): -957.3346\n",
      "convergence dfGPdfNN Run 3/10, Epoch 315/1000, Training Loss (NLML): -957.3156\n",
      "convergence dfGPdfNN Run 3/10, Epoch 316/1000, Training Loss (NLML): -957.3300\n",
      "convergence dfGPdfNN Run 3/10, Epoch 317/1000, Training Loss (NLML): -957.3448\n",
      "convergence dfGPdfNN Run 3/10, Epoch 318/1000, Training Loss (NLML): -957.3621\n",
      "convergence dfGPdfNN Run 3/10, Epoch 319/1000, Training Loss (NLML): -957.3949\n",
      "convergence dfGPdfNN Run 3/10, Epoch 320/1000, Training Loss (NLML): -957.3958\n",
      "convergence dfGPdfNN Run 3/10, Epoch 321/1000, Training Loss (NLML): -957.4039\n",
      "convergence dfGPdfNN Run 3/10, Epoch 322/1000, Training Loss (NLML): -957.4093\n",
      "convergence dfGPdfNN Run 3/10, Epoch 323/1000, Training Loss (NLML): -957.3973\n",
      "convergence dfGPdfNN Run 3/10, Epoch 324/1000, Training Loss (NLML): -957.4095\n",
      "convergence dfGPdfNN Run 3/10, Epoch 325/1000, Training Loss (NLML): -957.3928\n",
      "convergence dfGPdfNN Run 3/10, Epoch 326/1000, Training Loss (NLML): -957.4111\n",
      "convergence dfGPdfNN Run 3/10, Epoch 327/1000, Training Loss (NLML): -957.4720\n",
      "convergence dfGPdfNN Run 3/10, Epoch 328/1000, Training Loss (NLML): -957.5353\n",
      "convergence dfGPdfNN Run 3/10, Epoch 329/1000, Training Loss (NLML): -957.5494\n",
      "convergence dfGPdfNN Run 3/10, Epoch 330/1000, Training Loss (NLML): -957.6005\n",
      "convergence dfGPdfNN Run 3/10, Epoch 331/1000, Training Loss (NLML): -957.5912\n",
      "convergence dfGPdfNN Run 3/10, Epoch 332/1000, Training Loss (NLML): -957.6851\n",
      "convergence dfGPdfNN Run 3/10, Epoch 333/1000, Training Loss (NLML): -957.6837\n",
      "convergence dfGPdfNN Run 3/10, Epoch 334/1000, Training Loss (NLML): -957.7095\n",
      "convergence dfGPdfNN Run 3/10, Epoch 335/1000, Training Loss (NLML): -957.7537\n",
      "convergence dfGPdfNN Run 3/10, Epoch 336/1000, Training Loss (NLML): -957.6721\n",
      "convergence dfGPdfNN Run 3/10, Epoch 337/1000, Training Loss (NLML): -957.7484\n",
      "convergence dfGPdfNN Run 3/10, Epoch 338/1000, Training Loss (NLML): -957.7495\n",
      "convergence dfGPdfNN Run 3/10, Epoch 339/1000, Training Loss (NLML): -957.6686\n",
      "convergence dfGPdfNN Run 3/10, Epoch 340/1000, Training Loss (NLML): -957.6702\n",
      "convergence dfGPdfNN Run 3/10, Epoch 341/1000, Training Loss (NLML): -957.6786\n",
      "convergence dfGPdfNN Run 3/10, Epoch 342/1000, Training Loss (NLML): -957.6810\n",
      "convergence dfGPdfNN Run 3/10, Epoch 343/1000, Training Loss (NLML): -957.6740\n",
      "convergence dfGPdfNN Run 3/10, Epoch 344/1000, Training Loss (NLML): -957.6842\n",
      "convergence dfGPdfNN Run 3/10, Epoch 345/1000, Training Loss (NLML): -957.6472\n",
      "convergence dfGPdfNN Run 3/10, Epoch 346/1000, Training Loss (NLML): -957.6007\n",
      "convergence dfGPdfNN Run 3/10, Epoch 347/1000, Training Loss (NLML): -957.6992\n",
      "convergence dfGPdfNN Run 3/10, Epoch 348/1000, Training Loss (NLML): -957.6407\n",
      "convergence dfGPdfNN Run 3/10, Epoch 349/1000, Training Loss (NLML): -957.6545\n",
      "convergence dfGPdfNN Run 3/10, Epoch 350/1000, Training Loss (NLML): -957.6570\n",
      "convergence dfGPdfNN Run 3/10, Epoch 351/1000, Training Loss (NLML): -957.6769\n",
      "convergence dfGPdfNN Run 3/10, Epoch 352/1000, Training Loss (NLML): -957.7065\n",
      "convergence dfGPdfNN Run 3/10, Epoch 353/1000, Training Loss (NLML): -957.7405\n",
      "convergence dfGPdfNN Run 3/10, Epoch 354/1000, Training Loss (NLML): -957.8361\n",
      "convergence dfGPdfNN Run 3/10, Epoch 355/1000, Training Loss (NLML): -957.8417\n",
      "convergence dfGPdfNN Run 3/10, Epoch 356/1000, Training Loss (NLML): -957.8640\n",
      "convergence dfGPdfNN Run 3/10, Epoch 357/1000, Training Loss (NLML): -957.8778\n",
      "convergence dfGPdfNN Run 3/10, Epoch 358/1000, Training Loss (NLML): -957.8950\n",
      "convergence dfGPdfNN Run 3/10, Epoch 359/1000, Training Loss (NLML): -957.8292\n",
      "convergence dfGPdfNN Run 3/10, Epoch 360/1000, Training Loss (NLML): -957.7992\n",
      "convergence dfGPdfNN Run 3/10, Epoch 361/1000, Training Loss (NLML): -951.5458\n",
      "convergence dfGPdfNN Run 3/10, Epoch 362/1000, Training Loss (NLML): -957.7136\n",
      "convergence dfGPdfNN Run 3/10, Epoch 363/1000, Training Loss (NLML): -957.6984\n",
      "convergence dfGPdfNN Run 3/10, Epoch 364/1000, Training Loss (NLML): -957.6688\n",
      "convergence dfGPdfNN Run 3/10, Epoch 365/1000, Training Loss (NLML): -957.6826\n",
      "convergence dfGPdfNN Run 3/10, Epoch 366/1000, Training Loss (NLML): -957.6888\n",
      "convergence dfGPdfNN Run 3/10, Epoch 367/1000, Training Loss (NLML): -957.7130\n",
      "convergence dfGPdfNN Run 3/10, Epoch 368/1000, Training Loss (NLML): -957.6975\n",
      "convergence dfGPdfNN Run 3/10, Epoch 369/1000, Training Loss (NLML): -957.7587\n",
      "convergence dfGPdfNN Run 3/10, Epoch 370/1000, Training Loss (NLML): -957.8225\n",
      "convergence dfGPdfNN Run 3/10, Epoch 371/1000, Training Loss (NLML): -957.4102\n",
      "convergence dfGPdfNN Run 3/10, Epoch 372/1000, Training Loss (NLML): -957.3112\n",
      "convergence dfGPdfNN Run 3/10, Epoch 373/1000, Training Loss (NLML): -957.6036\n",
      "convergence dfGPdfNN Run 3/10, Epoch 374/1000, Training Loss (NLML): -957.5497\n",
      "convergence dfGPdfNN Run 3/10, Epoch 375/1000, Training Loss (NLML): -957.6599\n",
      "convergence dfGPdfNN Run 3/10, Epoch 376/1000, Training Loss (NLML): -957.6865\n",
      "convergence dfGPdfNN Run 3/10, Epoch 377/1000, Training Loss (NLML): -957.7551\n",
      "convergence dfGPdfNN Run 3/10, Epoch 378/1000, Training Loss (NLML): -957.6165\n",
      "convergence dfGPdfNN Run 3/10, Epoch 379/1000, Training Loss (NLML): -956.2737\n",
      "convergence dfGPdfNN Run 3/10, Epoch 380/1000, Training Loss (NLML): -957.1881\n",
      "convergence dfGPdfNN Run 3/10, Epoch 381/1000, Training Loss (NLML): -957.3746\n",
      "convergence dfGPdfNN Run 3/10, Epoch 382/1000, Training Loss (NLML): -957.6918\n",
      "convergence dfGPdfNN Run 3/10, Epoch 383/1000, Training Loss (NLML): -957.3279\n",
      "convergence dfGPdfNN Run 3/10, Epoch 384/1000, Training Loss (NLML): -957.9563\n",
      "convergence dfGPdfNN Run 3/10, Epoch 385/1000, Training Loss (NLML): -957.8054\n",
      "convergence dfGPdfNN Run 3/10, Epoch 386/1000, Training Loss (NLML): -957.8174\n",
      "convergence dfGPdfNN Run 3/10, Epoch 387/1000, Training Loss (NLML): -956.8274\n",
      "convergence dfGPdfNN Run 3/10, Epoch 388/1000, Training Loss (NLML): -957.7538\n",
      "convergence dfGPdfNN Run 3/10, Epoch 389/1000, Training Loss (NLML): -955.3022\n",
      "convergence dfGPdfNN Run 3/10, Epoch 390/1000, Training Loss (NLML): -958.0382\n",
      "convergence dfGPdfNN Run 3/10, Epoch 391/1000, Training Loss (NLML): -957.9829\n",
      "convergence dfGPdfNN Run 3/10, Epoch 392/1000, Training Loss (NLML): -957.7993\n",
      "convergence dfGPdfNN Run 3/10, Epoch 393/1000, Training Loss (NLML): -957.9845\n",
      "convergence dfGPdfNN Run 3/10, Epoch 394/1000, Training Loss (NLML): -957.9496\n",
      "convergence dfGPdfNN Run 3/10, Epoch 395/1000, Training Loss (NLML): -958.0581\n",
      "convergence dfGPdfNN Run 3/10, Epoch 396/1000, Training Loss (NLML): -958.0110\n",
      "convergence dfGPdfNN Run 3/10, Epoch 397/1000, Training Loss (NLML): -958.0220\n",
      "convergence dfGPdfNN Run 3/10, Epoch 398/1000, Training Loss (NLML): -958.0522\n",
      "convergence dfGPdfNN Run 3/10, Epoch 399/1000, Training Loss (NLML): -958.1394\n",
      "convergence dfGPdfNN Run 3/10, Epoch 400/1000, Training Loss (NLML): -958.1327\n",
      "convergence dfGPdfNN Run 3/10, Epoch 401/1000, Training Loss (NLML): -958.1610\n",
      "convergence dfGPdfNN Run 3/10, Epoch 402/1000, Training Loss (NLML): -958.1038\n",
      "convergence dfGPdfNN Run 3/10, Epoch 403/1000, Training Loss (NLML): -958.1394\n",
      "convergence dfGPdfNN Run 3/10, Epoch 404/1000, Training Loss (NLML): -951.3452\n",
      "convergence dfGPdfNN Run 3/10, Epoch 405/1000, Training Loss (NLML): -957.1007\n",
      "convergence dfGPdfNN Run 3/10, Epoch 406/1000, Training Loss (NLML): -958.0033\n",
      "convergence dfGPdfNN Run 3/10, Epoch 407/1000, Training Loss (NLML): -957.4066\n",
      "convergence dfGPdfNN Run 3/10, Epoch 408/1000, Training Loss (NLML): -957.6941\n",
      "convergence dfGPdfNN Run 3/10, Epoch 409/1000, Training Loss (NLML): -957.9098\n",
      "convergence dfGPdfNN Run 3/10, Epoch 410/1000, Training Loss (NLML): -957.4297\n",
      "convergence dfGPdfNN Run 3/10, Epoch 411/1000, Training Loss (NLML): -957.8580\n",
      "convergence dfGPdfNN Run 3/10, Epoch 412/1000, Training Loss (NLML): -957.5896\n",
      "convergence dfGPdfNN Run 3/10, Epoch 413/1000, Training Loss (NLML): -957.3014\n",
      "convergence dfGPdfNN Run 3/10, Epoch 414/1000, Training Loss (NLML): -957.3799\n",
      "convergence dfGPdfNN Run 3/10, Epoch 415/1000, Training Loss (NLML): -957.4869\n",
      "convergence dfGPdfNN Run 3/10, Epoch 416/1000, Training Loss (NLML): -957.4886\n",
      "convergence dfGPdfNN Run 3/10, Epoch 417/1000, Training Loss (NLML): -957.7599\n",
      "convergence dfGPdfNN Run 3/10, Epoch 418/1000, Training Loss (NLML): -957.8043\n",
      "convergence dfGPdfNN Run 3/10, Epoch 419/1000, Training Loss (NLML): -957.7461\n",
      "convergence dfGPdfNN Run 3/10, Epoch 420/1000, Training Loss (NLML): -957.8025\n",
      "convergence dfGPdfNN Run 3/10, Epoch 421/1000, Training Loss (NLML): -957.8739\n",
      "convergence dfGPdfNN Run 3/10, Epoch 422/1000, Training Loss (NLML): -957.9432\n",
      "convergence dfGPdfNN Run 3/10, Epoch 423/1000, Training Loss (NLML): -958.0470\n",
      "convergence dfGPdfNN Run 3/10, Epoch 424/1000, Training Loss (NLML): -958.0872\n",
      "convergence dfGPdfNN Run 3/10, Epoch 425/1000, Training Loss (NLML): -958.1122\n",
      "convergence dfGPdfNN Run 3/10, Epoch 426/1000, Training Loss (NLML): -958.2325\n",
      "convergence dfGPdfNN Run 3/10, Epoch 427/1000, Training Loss (NLML): -958.3650\n",
      "convergence dfGPdfNN Run 3/10, Epoch 428/1000, Training Loss (NLML): -958.4230\n",
      "convergence dfGPdfNN Run 3/10, Epoch 429/1000, Training Loss (NLML): -958.4843\n",
      "convergence dfGPdfNN Run 3/10, Epoch 430/1000, Training Loss (NLML): -958.4747\n",
      "convergence dfGPdfNN Run 3/10, Epoch 431/1000, Training Loss (NLML): -958.2946\n",
      "convergence dfGPdfNN Run 3/10, Epoch 432/1000, Training Loss (NLML): -956.9827\n",
      "convergence dfGPdfNN Run 3/10, Epoch 433/1000, Training Loss (NLML): -954.5803\n",
      "convergence dfGPdfNN Run 3/10, Epoch 434/1000, Training Loss (NLML): -957.8860\n",
      "convergence dfGPdfNN Run 3/10, Epoch 435/1000, Training Loss (NLML): -958.0693\n",
      "convergence dfGPdfNN Run 3/10, Epoch 436/1000, Training Loss (NLML): -958.0378\n",
      "convergence dfGPdfNN Run 3/10, Epoch 437/1000, Training Loss (NLML): -958.0444\n",
      "convergence dfGPdfNN Run 3/10, Epoch 438/1000, Training Loss (NLML): -958.0299\n",
      "convergence dfGPdfNN Run 3/10, Epoch 439/1000, Training Loss (NLML): -958.1935\n",
      "convergence dfGPdfNN Run 3/10, Epoch 440/1000, Training Loss (NLML): -958.2948\n",
      "convergence dfGPdfNN Run 3/10, Epoch 441/1000, Training Loss (NLML): -958.2703\n",
      "convergence dfGPdfNN Run 3/10, Epoch 442/1000, Training Loss (NLML): -958.2070\n",
      "convergence dfGPdfNN Run 3/10, Epoch 443/1000, Training Loss (NLML): -958.1593\n",
      "convergence dfGPdfNN Run 3/10, Epoch 444/1000, Training Loss (NLML): -958.1323\n",
      "convergence dfGPdfNN Run 3/10, Epoch 445/1000, Training Loss (NLML): -958.1486\n",
      "convergence dfGPdfNN Run 3/10, Epoch 446/1000, Training Loss (NLML): -958.1224\n",
      "convergence dfGPdfNN Run 3/10, Epoch 447/1000, Training Loss (NLML): -958.1781\n",
      "convergence dfGPdfNN Run 3/10, Epoch 448/1000, Training Loss (NLML): -958.1954\n",
      "convergence dfGPdfNN Run 3/10, Epoch 449/1000, Training Loss (NLML): -958.3285\n",
      "convergence dfGPdfNN Run 3/10, Epoch 450/1000, Training Loss (NLML): -958.4644\n",
      "convergence dfGPdfNN Run 3/10, Epoch 451/1000, Training Loss (NLML): -958.4420\n",
      "convergence dfGPdfNN Run 3/10, Epoch 452/1000, Training Loss (NLML): -958.0505\n",
      "convergence dfGPdfNN Run 3/10, Epoch 453/1000, Training Loss (NLML): -958.0115\n",
      "convergence dfGPdfNN Run 3/10, Epoch 454/1000, Training Loss (NLML): -958.3735\n",
      "convergence dfGPdfNN Run 3/10, Epoch 455/1000, Training Loss (NLML): -958.5946\n",
      "convergence dfGPdfNN Run 3/10, Epoch 456/1000, Training Loss (NLML): -958.6984\n",
      "convergence dfGPdfNN Run 3/10, Epoch 457/1000, Training Loss (NLML): -958.7087\n",
      "convergence dfGPdfNN Run 3/10, Epoch 458/1000, Training Loss (NLML): -958.6979\n",
      "convergence dfGPdfNN Run 3/10, Epoch 459/1000, Training Loss (NLML): -958.7086\n",
      "convergence dfGPdfNN Run 3/10, Epoch 460/1000, Training Loss (NLML): -958.6874\n",
      "convergence dfGPdfNN Run 3/10, Epoch 461/1000, Training Loss (NLML): -958.5977\n",
      "convergence dfGPdfNN Run 3/10, Epoch 462/1000, Training Loss (NLML): -958.5897\n",
      "convergence dfGPdfNN Run 3/10, Epoch 463/1000, Training Loss (NLML): -958.5835\n",
      "convergence dfGPdfNN Run 3/10, Epoch 464/1000, Training Loss (NLML): -958.5695\n",
      "convergence dfGPdfNN Run 3/10, Epoch 465/1000, Training Loss (NLML): -958.6927\n",
      "convergence dfGPdfNN Run 3/10, Epoch 466/1000, Training Loss (NLML): -958.7900\n",
      "convergence dfGPdfNN Run 3/10, Epoch 467/1000, Training Loss (NLML): -958.8159\n",
      "convergence dfGPdfNN Run 3/10, Epoch 468/1000, Training Loss (NLML): -958.8230\n",
      "convergence dfGPdfNN Run 3/10, Epoch 469/1000, Training Loss (NLML): -958.8198\n",
      "convergence dfGPdfNN Run 3/10, Epoch 470/1000, Training Loss (NLML): -958.7383\n",
      "convergence dfGPdfNN Run 3/10, Epoch 471/1000, Training Loss (NLML): -958.6801\n",
      "convergence dfGPdfNN Run 3/10, Epoch 472/1000, Training Loss (NLML): -958.6410\n",
      "convergence dfGPdfNN Run 3/10, Epoch 473/1000, Training Loss (NLML): -958.7034\n",
      "convergence dfGPdfNN Run 3/10, Epoch 474/1000, Training Loss (NLML): -958.7864\n",
      "convergence dfGPdfNN Run 3/10, Epoch 475/1000, Training Loss (NLML): -958.8215\n",
      "convergence dfGPdfNN Run 3/10, Epoch 476/1000, Training Loss (NLML): -958.7968\n",
      "convergence dfGPdfNN Run 3/10, Epoch 477/1000, Training Loss (NLML): -958.8575\n",
      "convergence dfGPdfNN Run 3/10, Epoch 478/1000, Training Loss (NLML): -958.8173\n",
      "convergence dfGPdfNN Run 3/10, Epoch 479/1000, Training Loss (NLML): -958.7987\n",
      "convergence dfGPdfNN Run 3/10, Epoch 480/1000, Training Loss (NLML): -958.8076\n",
      "convergence dfGPdfNN Run 3/10, Epoch 481/1000, Training Loss (NLML): -958.8101\n",
      "convergence dfGPdfNN Run 3/10, Epoch 482/1000, Training Loss (NLML): -958.8142\n",
      "convergence dfGPdfNN Run 3/10, Epoch 483/1000, Training Loss (NLML): -958.8165\n",
      "convergence dfGPdfNN Run 3/10, Epoch 484/1000, Training Loss (NLML): -958.8263\n",
      "convergence dfGPdfNN Run 3/10, Epoch 485/1000, Training Loss (NLML): -958.8368\n",
      "convergence dfGPdfNN Run 3/10, Epoch 486/1000, Training Loss (NLML): -958.8455\n",
      "convergence dfGPdfNN Run 3/10, Epoch 487/1000, Training Loss (NLML): -958.8872\n",
      "convergence dfGPdfNN Run 3/10, Epoch 488/1000, Training Loss (NLML): -958.8923\n",
      "convergence dfGPdfNN Run 3/10, Epoch 489/1000, Training Loss (NLML): -958.8970\n",
      "convergence dfGPdfNN Run 3/10, Epoch 490/1000, Training Loss (NLML): -958.8708\n",
      "convergence dfGPdfNN Run 3/10, Epoch 491/1000, Training Loss (NLML): -958.8831\n",
      "convergence dfGPdfNN Run 3/10, Epoch 492/1000, Training Loss (NLML): -958.8901\n",
      "convergence dfGPdfNN Run 3/10, Epoch 493/1000, Training Loss (NLML): -958.8959\n",
      "convergence dfGPdfNN Run 3/10, Epoch 494/1000, Training Loss (NLML): -958.9054\n",
      "convergence dfGPdfNN Run 3/10, Epoch 495/1000, Training Loss (NLML): -958.9095\n",
      "convergence dfGPdfNN Run 3/10, Epoch 496/1000, Training Loss (NLML): -958.9169\n",
      "convergence dfGPdfNN Run 3/10, Epoch 497/1000, Training Loss (NLML): -958.9255\n",
      "convergence dfGPdfNN Run 3/10, Epoch 498/1000, Training Loss (NLML): -958.9327\n",
      "convergence dfGPdfNN Run 3/10, Epoch 499/1000, Training Loss (NLML): -958.9700\n",
      "convergence dfGPdfNN Run 3/10, Epoch 500/1000, Training Loss (NLML): -959.0239\n",
      "convergence dfGPdfNN Run 3/10, Epoch 501/1000, Training Loss (NLML): -958.9827\n",
      "convergence dfGPdfNN Run 3/10, Epoch 502/1000, Training Loss (NLML): -958.9611\n",
      "convergence dfGPdfNN Run 3/10, Epoch 503/1000, Training Loss (NLML): -958.9670\n",
      "convergence dfGPdfNN Run 3/10, Epoch 504/1000, Training Loss (NLML): -958.9746\n",
      "convergence dfGPdfNN Run 3/10, Epoch 505/1000, Training Loss (NLML): -958.9797\n",
      "convergence dfGPdfNN Run 3/10, Epoch 506/1000, Training Loss (NLML): -958.9880\n",
      "convergence dfGPdfNN Run 3/10, Epoch 507/1000, Training Loss (NLML): -958.9913\n",
      "convergence dfGPdfNN Run 3/10, Epoch 508/1000, Training Loss (NLML): -958.9988\n",
      "convergence dfGPdfNN Run 3/10, Epoch 509/1000, Training Loss (NLML): -959.0054\n",
      "convergence dfGPdfNN Run 3/10, Epoch 510/1000, Training Loss (NLML): -959.0410\n",
      "convergence dfGPdfNN Run 3/10, Epoch 511/1000, Training Loss (NLML): -959.0935\n",
      "convergence dfGPdfNN Run 3/10, Epoch 512/1000, Training Loss (NLML): -959.0557\n",
      "convergence dfGPdfNN Run 3/10, Epoch 513/1000, Training Loss (NLML): -959.0326\n",
      "convergence dfGPdfNN Run 3/10, Epoch 514/1000, Training Loss (NLML): -959.0397\n",
      "convergence dfGPdfNN Run 3/10, Epoch 515/1000, Training Loss (NLML): -959.0469\n",
      "convergence dfGPdfNN Run 3/10, Epoch 516/1000, Training Loss (NLML): -959.0530\n",
      "convergence dfGPdfNN Run 3/10, Epoch 517/1000, Training Loss (NLML): -959.0588\n",
      "convergence dfGPdfNN Run 3/10, Epoch 518/1000, Training Loss (NLML): -959.0651\n",
      "convergence dfGPdfNN Run 3/10, Epoch 519/1000, Training Loss (NLML): -959.0712\n",
      "convergence dfGPdfNN Run 3/10, Epoch 520/1000, Training Loss (NLML): -959.1069\n",
      "convergence dfGPdfNN Run 3/10, Epoch 521/1000, Training Loss (NLML): -959.1128\n",
      "convergence dfGPdfNN Run 3/10, Epoch 522/1000, Training Loss (NLML): -959.1190\n",
      "convergence dfGPdfNN Run 3/10, Epoch 523/1000, Training Loss (NLML): -959.1267\n",
      "convergence dfGPdfNN Run 3/10, Epoch 524/1000, Training Loss (NLML): -959.1018\n",
      "convergence dfGPdfNN Run 3/10, Epoch 525/1000, Training Loss (NLML): -959.1125\n",
      "convergence dfGPdfNN Run 3/10, Epoch 526/1000, Training Loss (NLML): -959.1174\n",
      "convergence dfGPdfNN Run 3/10, Epoch 527/1000, Training Loss (NLML): -959.1261\n",
      "convergence dfGPdfNN Run 3/10, Epoch 528/1000, Training Loss (NLML): -959.1322\n",
      "convergence dfGPdfNN Run 3/10, Epoch 529/1000, Training Loss (NLML): -959.1660\n",
      "convergence dfGPdfNN Run 3/10, Epoch 530/1000, Training Loss (NLML): -959.1718\n",
      "convergence dfGPdfNN Run 3/10, Epoch 531/1000, Training Loss (NLML): -959.1793\n",
      "convergence dfGPdfNN Run 3/10, Epoch 532/1000, Training Loss (NLML): -959.1837\n",
      "convergence dfGPdfNN Run 3/10, Epoch 533/1000, Training Loss (NLML): -959.1620\n",
      "convergence dfGPdfNN Run 3/10, Epoch 534/1000, Training Loss (NLML): -959.1682\n",
      "convergence dfGPdfNN Run 3/10, Epoch 535/1000, Training Loss (NLML): -959.1730\n",
      "convergence dfGPdfNN Run 3/10, Epoch 536/1000, Training Loss (NLML): -959.1808\n",
      "convergence dfGPdfNN Run 3/10, Epoch 537/1000, Training Loss (NLML): -959.2163\n",
      "convergence dfGPdfNN Run 3/10, Epoch 538/1000, Training Loss (NLML): -959.2222\n",
      "convergence dfGPdfNN Run 3/10, Epoch 539/1000, Training Loss (NLML): -959.2281\n",
      "convergence dfGPdfNN Run 3/10, Epoch 540/1000, Training Loss (NLML): -959.2030\n",
      "convergence dfGPdfNN Run 3/10, Epoch 541/1000, Training Loss (NLML): -959.2112\n",
      "convergence dfGPdfNN Run 3/10, Epoch 542/1000, Training Loss (NLML): -959.2156\n",
      "convergence dfGPdfNN Run 3/10, Epoch 543/1000, Training Loss (NLML): -959.2487\n",
      "convergence dfGPdfNN Run 3/10, Epoch 544/1000, Training Loss (NLML): -959.2546\n",
      "convergence dfGPdfNN Run 3/10, Epoch 545/1000, Training Loss (NLML): -959.2617\n",
      "convergence dfGPdfNN Run 3/10, Epoch 546/1000, Training Loss (NLML): -959.2362\n",
      "convergence dfGPdfNN Run 3/10, Epoch 547/1000, Training Loss (NLML): -959.2407\n",
      "convergence dfGPdfNN Run 3/10, Epoch 548/1000, Training Loss (NLML): -959.2766\n",
      "convergence dfGPdfNN Run 3/10, Epoch 549/1000, Training Loss (NLML): -959.2533\n",
      "convergence dfGPdfNN Run 3/10, Epoch 550/1000, Training Loss (NLML): -959.2871\n",
      "convergence dfGPdfNN Run 3/10, Epoch 551/1000, Training Loss (NLML): -959.2939\n",
      "convergence dfGPdfNN Run 3/10, Epoch 552/1000, Training Loss (NLML): -959.2703\n",
      "convergence dfGPdfNN Run 3/10, Epoch 553/1000, Training Loss (NLML): -959.2772\n",
      "convergence dfGPdfNN Run 3/10, Epoch 554/1000, Training Loss (NLML): -959.3096\n",
      "convergence dfGPdfNN Run 3/10, Epoch 555/1000, Training Loss (NLML): -959.3152\n",
      "convergence dfGPdfNN Run 3/10, Epoch 556/1000, Training Loss (NLML): -959.3207\n",
      "convergence dfGPdfNN Run 3/10, Epoch 557/1000, Training Loss (NLML): -959.2983\n",
      "convergence dfGPdfNN Run 3/10, Epoch 558/1000, Training Loss (NLML): -959.3038\n",
      "convergence dfGPdfNN Run 3/10, Epoch 559/1000, Training Loss (NLML): -959.3402\n",
      "convergence dfGPdfNN Run 3/10, Epoch 560/1000, Training Loss (NLML): -959.3466\n",
      "convergence dfGPdfNN Run 3/10, Epoch 561/1000, Training Loss (NLML): -959.3528\n",
      "convergence dfGPdfNN Run 3/10, Epoch 562/1000, Training Loss (NLML): -959.3573\n",
      "convergence dfGPdfNN Run 3/10, Epoch 563/1000, Training Loss (NLML): -959.3345\n",
      "convergence dfGPdfNN Run 3/10, Epoch 564/1000, Training Loss (NLML): -959.3685\n",
      "convergence dfGPdfNN Run 3/10, Epoch 565/1000, Training Loss (NLML): -959.3734\n",
      "convergence dfGPdfNN Run 3/10, Epoch 566/1000, Training Loss (NLML): -959.3788\n",
      "convergence dfGPdfNN Run 3/10, Epoch 567/1000, Training Loss (NLML): -959.3851\n",
      "convergence dfGPdfNN Run 3/10, Epoch 568/1000, Training Loss (NLML): -959.3607\n",
      "convergence dfGPdfNN Run 3/10, Epoch 569/1000, Training Loss (NLML): -959.3955\n",
      "convergence dfGPdfNN Run 3/10, Epoch 570/1000, Training Loss (NLML): -959.3999\n",
      "convergence dfGPdfNN Run 3/10, Epoch 571/1000, Training Loss (NLML): -959.4059\n",
      "convergence dfGPdfNN Run 3/10, Epoch 572/1000, Training Loss (NLML): -959.4106\n",
      "convergence dfGPdfNN Run 3/10, Epoch 573/1000, Training Loss (NLML): -959.3879\n",
      "convergence dfGPdfNN Run 3/10, Epoch 574/1000, Training Loss (NLML): -959.4215\n",
      "convergence dfGPdfNN Run 3/10, Epoch 575/1000, Training Loss (NLML): -959.4255\n",
      "convergence dfGPdfNN Run 3/10, Epoch 576/1000, Training Loss (NLML): -959.4371\n",
      "convergence dfGPdfNN Run 3/10, Epoch 577/1000, Training Loss (NLML): -959.4427\n",
      "convergence dfGPdfNN Run 3/10, Epoch 578/1000, Training Loss (NLML): -959.4467\n",
      "convergence dfGPdfNN Run 3/10, Epoch 579/1000, Training Loss (NLML): -959.4525\n",
      "convergence dfGPdfNN Run 3/10, Epoch 580/1000, Training Loss (NLML): -959.4587\n",
      "convergence dfGPdfNN Run 3/10, Epoch 581/1000, Training Loss (NLML): -959.4631\n",
      "convergence dfGPdfNN Run 3/10, Epoch 582/1000, Training Loss (NLML): -959.4697\n",
      "convergence dfGPdfNN Run 3/10, Epoch 583/1000, Training Loss (NLML): -959.4736\n",
      "convergence dfGPdfNN Run 3/10, Epoch 584/1000, Training Loss (NLML): -959.4774\n",
      "convergence dfGPdfNN Run 3/10, Epoch 585/1000, Training Loss (NLML): -959.4838\n",
      "convergence dfGPdfNN Run 3/10, Epoch 586/1000, Training Loss (NLML): -959.4896\n",
      "convergence dfGPdfNN Run 3/10, Epoch 587/1000, Training Loss (NLML): -959.4935\n",
      "convergence dfGPdfNN Run 3/10, Epoch 588/1000, Training Loss (NLML): -959.4806\n",
      "convergence dfGPdfNN Run 3/10, Epoch 589/1000, Training Loss (NLML): -959.4866\n",
      "convergence dfGPdfNN Run 3/10, Epoch 590/1000, Training Loss (NLML): -959.5355\n",
      "convergence dfGPdfNN Run 3/10, Epoch 591/1000, Training Loss (NLML): -959.4962\n",
      "convergence dfGPdfNN Run 3/10, Epoch 592/1000, Training Loss (NLML): -959.4725\n",
      "convergence dfGPdfNN Run 3/10, Epoch 593/1000, Training Loss (NLML): -959.4832\n",
      "convergence dfGPdfNN Run 3/10, Epoch 594/1000, Training Loss (NLML): -959.5603\n",
      "convergence dfGPdfNN Run 3/10, Epoch 595/1000, Training Loss (NLML): -959.5662\n",
      "convergence dfGPdfNN Run 3/10, Epoch 596/1000, Training Loss (NLML): -959.5724\n",
      "convergence dfGPdfNN Run 3/10, Epoch 597/1000, Training Loss (NLML): -959.5325\n",
      "convergence dfGPdfNN Run 3/10, Epoch 598/1000, Training Loss (NLML): -959.5092\n",
      "convergence dfGPdfNN Run 3/10, Epoch 599/1000, Training Loss (NLML): -959.5121\n",
      "convergence dfGPdfNN Run 3/10, Epoch 600/1000, Training Loss (NLML): -959.5475\n",
      "convergence dfGPdfNN Run 3/10, Epoch 601/1000, Training Loss (NLML): -959.5964\n",
      "convergence dfGPdfNN Run 3/10, Epoch 602/1000, Training Loss (NLML): -959.6023\n",
      "convergence dfGPdfNN Run 3/10, Epoch 603/1000, Training Loss (NLML): -959.6072\n",
      "convergence dfGPdfNN Run 3/10, Epoch 604/1000, Training Loss (NLML): -959.5676\n",
      "convergence dfGPdfNN Run 3/10, Epoch 605/1000, Training Loss (NLML): -959.5436\n",
      "convergence dfGPdfNN Run 3/10, Epoch 606/1000, Training Loss (NLML): -959.5490\n",
      "convergence dfGPdfNN Run 3/10, Epoch 607/1000, Training Loss (NLML): -959.5802\n",
      "convergence dfGPdfNN Run 3/10, Epoch 608/1000, Training Loss (NLML): -959.6307\n",
      "convergence dfGPdfNN Run 3/10, Epoch 609/1000, Training Loss (NLML): -959.6345\n",
      "convergence dfGPdfNN Run 3/10, Epoch 610/1000, Training Loss (NLML): -959.6414\n",
      "convergence dfGPdfNN Run 3/10, Epoch 611/1000, Training Loss (NLML): -959.6016\n",
      "convergence dfGPdfNN Run 3/10, Epoch 612/1000, Training Loss (NLML): -959.5787\n",
      "convergence dfGPdfNN Run 3/10, Epoch 613/1000, Training Loss (NLML): -959.5868\n",
      "convergence dfGPdfNN Run 3/10, Epoch 614/1000, Training Loss (NLML): -959.6655\n",
      "convergence dfGPdfNN Run 3/10, Epoch 615/1000, Training Loss (NLML): -959.6707\n",
      "convergence dfGPdfNN Run 3/10, Epoch 616/1000, Training Loss (NLML): -959.6752\n",
      "convergence dfGPdfNN Run 3/10, Epoch 617/1000, Training Loss (NLML): -959.6364\n",
      "convergence dfGPdfNN Run 3/10, Epoch 618/1000, Training Loss (NLML): -959.6083\n",
      "convergence dfGPdfNN Run 3/10, Epoch 619/1000, Training Loss (NLML): -959.6832\n",
      "convergence dfGPdfNN Run 3/10, Epoch 620/1000, Training Loss (NLML): -959.6898\n",
      "convergence dfGPdfNN Run 3/10, Epoch 621/1000, Training Loss (NLML): -959.6478\n",
      "convergence dfGPdfNN Run 3/10, Epoch 622/1000, Training Loss (NLML): -959.6371\n",
      "convergence dfGPdfNN Run 3/10, Epoch 623/1000, Training Loss (NLML): -959.6807\n",
      "convergence dfGPdfNN Run 3/10, Epoch 624/1000, Training Loss (NLML): -959.6465\n",
      "convergence dfGPdfNN Run 3/10, Epoch 625/1000, Training Loss (NLML): -959.6484\n",
      "convergence dfGPdfNN Run 3/10, Epoch 626/1000, Training Loss (NLML): -959.6542\n",
      "convergence dfGPdfNN Run 3/10, Epoch 627/1000, Training Loss (NLML): -959.7003\n",
      "convergence dfGPdfNN Run 3/10, Epoch 628/1000, Training Loss (NLML): -959.7073\n",
      "convergence dfGPdfNN Run 3/10, Epoch 629/1000, Training Loss (NLML): -959.7115\n",
      "convergence dfGPdfNN Run 3/10, Epoch 630/1000, Training Loss (NLML): -959.6418\n",
      "convergence dfGPdfNN Run 3/10, Epoch 631/1000, Training Loss (NLML): -959.6791\n",
      "convergence dfGPdfNN Run 3/10, Epoch 632/1000, Training Loss (NLML): -959.7213\n",
      "convergence dfGPdfNN Run 3/10, Epoch 633/1000, Training Loss (NLML): -959.7274\n",
      "convergence dfGPdfNN Run 3/10, Epoch 634/1000, Training Loss (NLML): -959.6875\n",
      "convergence dfGPdfNN Run 3/10, Epoch 635/1000, Training Loss (NLML): -959.6958\n",
      "convergence dfGPdfNN Run 3/10, Epoch 636/1000, Training Loss (NLML): -959.7413\n",
      "convergence dfGPdfNN Run 3/10, Epoch 637/1000, Training Loss (NLML): -959.7021\n",
      "convergence dfGPdfNN Run 3/10, Epoch 638/1000, Training Loss (NLML): -959.7068\n",
      "convergence dfGPdfNN Run 3/10, Epoch 639/1000, Training Loss (NLML): -959.7546\n",
      "convergence dfGPdfNN Run 3/10, Epoch 640/1000, Training Loss (NLML): -959.7585\n",
      "convergence dfGPdfNN Run 3/10, Epoch 641/1000, Training Loss (NLML): -959.7640\n",
      "convergence dfGPdfNN Run 3/10, Epoch 642/1000, Training Loss (NLML): -959.6973\n",
      "convergence dfGPdfNN Run 3/10, Epoch 643/1000, Training Loss (NLML): -959.7301\n",
      "convergence dfGPdfNN Run 3/10, Epoch 644/1000, Training Loss (NLML): -959.7749\n",
      "convergence dfGPdfNN Run 3/10, Epoch 645/1000, Training Loss (NLML): -959.7803\n",
      "convergence dfGPdfNN Run 3/10, Epoch 646/1000, Training Loss (NLML): -959.7849\n",
      "convergence dfGPdfNN Run 3/10, Epoch 647/1000, Training Loss (NLML): -959.7461\n",
      "convergence dfGPdfNN Run 3/10, Epoch 648/1000, Training Loss (NLML): -959.7185\n",
      "convergence dfGPdfNN Run 3/10, Epoch 649/1000, Training Loss (NLML): -959.7931\n",
      "convergence dfGPdfNN Run 3/10, Epoch 650/1000, Training Loss (NLML): -959.8007\n",
      "convergence dfGPdfNN Run 3/10, Epoch 651/1000, Training Loss (NLML): -959.8020\n",
      "convergence dfGPdfNN Run 3/10, Epoch 652/1000, Training Loss (NLML): -959.7623\n",
      "convergence dfGPdfNN Run 3/10, Epoch 653/1000, Training Loss (NLML): -959.7683\n",
      "convergence dfGPdfNN Run 3/10, Epoch 654/1000, Training Loss (NLML): -959.7715\n",
      "convergence dfGPdfNN Run 3/10, Epoch 655/1000, Training Loss (NLML): -959.8192\n",
      "convergence dfGPdfNN Run 3/10, Epoch 656/1000, Training Loss (NLML): -959.8228\n",
      "convergence dfGPdfNN Run 3/10, Epoch 657/1000, Training Loss (NLML): -959.8260\n",
      "convergence dfGPdfNN Run 3/10, Epoch 658/1000, Training Loss (NLML): -959.7612\n",
      "convergence dfGPdfNN Run 3/10, Epoch 659/1000, Training Loss (NLML): -959.7906\n",
      "convergence dfGPdfNN Run 3/10, Epoch 660/1000, Training Loss (NLML): -959.8383\n",
      "convergence dfGPdfNN Run 3/10, Epoch 661/1000, Training Loss (NLML): -959.8433\n",
      "convergence dfGPdfNN Run 3/10, Epoch 662/1000, Training Loss (NLML): -959.8475\n",
      "convergence dfGPdfNN Run 3/10, Epoch 663/1000, Training Loss (NLML): -959.8512\n",
      "convergence dfGPdfNN Run 3/10, Epoch 664/1000, Training Loss (NLML): -959.7837\n",
      "convergence dfGPdfNN Run 3/10, Epoch 665/1000, Training Loss (NLML): -959.8243\n",
      "convergence dfGPdfNN Run 3/10, Epoch 666/1000, Training Loss (NLML): -959.8715\n",
      "convergence dfGPdfNN Run 3/10, Epoch 667/1000, Training Loss (NLML): -959.8741\n",
      "convergence dfGPdfNN Run 3/10, Epoch 668/1000, Training Loss (NLML): -959.8804\n",
      "convergence dfGPdfNN Run 3/10, Epoch 669/1000, Training Loss (NLML): -959.8838\n",
      "convergence dfGPdfNN Run 3/10, Epoch 670/1000, Training Loss (NLML): -959.8870\n",
      "convergence dfGPdfNN Run 3/10, Epoch 671/1000, Training Loss (NLML): -959.8480\n",
      "convergence dfGPdfNN Run 3/10, Epoch 672/1000, Training Loss (NLML): -959.8522\n",
      "convergence dfGPdfNN Run 3/10, Epoch 673/1000, Training Loss (NLML): -959.8955\n",
      "convergence dfGPdfNN Run 3/10, Epoch 674/1000, Training Loss (NLML): -959.9009\n",
      "convergence dfGPdfNN Run 3/10, Epoch 675/1000, Training Loss (NLML): -959.9030\n",
      "convergence dfGPdfNN Run 3/10, Epoch 676/1000, Training Loss (NLML): -959.9093\n",
      "convergence dfGPdfNN Run 3/10, Epoch 677/1000, Training Loss (NLML): -959.9128\n",
      "convergence dfGPdfNN Run 3/10, Epoch 678/1000, Training Loss (NLML): -959.9154\n",
      "convergence dfGPdfNN Run 3/10, Epoch 679/1000, Training Loss (NLML): -959.8508\n",
      "convergence dfGPdfNN Run 3/10, Epoch 680/1000, Training Loss (NLML): -959.9167\n",
      "convergence dfGPdfNN Run 3/10, Epoch 681/1000, Training Loss (NLML): -959.9220\n",
      "convergence dfGPdfNN Run 3/10, Epoch 682/1000, Training Loss (NLML): -959.9261\n",
      "convergence dfGPdfNN Run 3/10, Epoch 683/1000, Training Loss (NLML): -959.9292\n",
      "convergence dfGPdfNN Run 3/10, Epoch 684/1000, Training Loss (NLML): -959.9310\n",
      "convergence dfGPdfNN Run 3/10, Epoch 685/1000, Training Loss (NLML): -959.8665\n",
      "convergence dfGPdfNN Run 3/10, Epoch 686/1000, Training Loss (NLML): -959.9382\n",
      "convergence dfGPdfNN Run 3/10, Epoch 687/1000, Training Loss (NLML): -959.9427\n",
      "convergence dfGPdfNN Run 3/10, Epoch 688/1000, Training Loss (NLML): -959.9476\n",
      "convergence dfGPdfNN Run 3/10, Epoch 689/1000, Training Loss (NLML): -959.9497\n",
      "convergence dfGPdfNN Run 3/10, Epoch 690/1000, Training Loss (NLML): -959.9138\n",
      "convergence dfGPdfNN Run 3/10, Epoch 691/1000, Training Loss (NLML): -959.9573\n",
      "convergence dfGPdfNN Run 3/10, Epoch 692/1000, Training Loss (NLML): -959.9626\n",
      "convergence dfGPdfNN Run 3/10, Epoch 693/1000, Training Loss (NLML): -959.9232\n",
      "convergence dfGPdfNN Run 3/10, Epoch 694/1000, Training Loss (NLML): -959.9679\n",
      "convergence dfGPdfNN Run 3/10, Epoch 695/1000, Training Loss (NLML): -959.9719\n",
      "convergence dfGPdfNN Run 3/10, Epoch 696/1000, Training Loss (NLML): -959.9760\n",
      "convergence dfGPdfNN Run 3/10, Epoch 697/1000, Training Loss (NLML): -959.9790\n",
      "convergence dfGPdfNN Run 3/10, Epoch 698/1000, Training Loss (NLML): -959.9829\n",
      "convergence dfGPdfNN Run 3/10, Epoch 699/1000, Training Loss (NLML): -959.9238\n",
      "convergence dfGPdfNN Run 3/10, Epoch 700/1000, Training Loss (NLML): -959.9955\n",
      "convergence dfGPdfNN Run 3/10, Epoch 701/1000, Training Loss (NLML): -960.0002\n",
      "convergence dfGPdfNN Run 3/10, Epoch 702/1000, Training Loss (NLML): -960.0013\n",
      "convergence dfGPdfNN Run 3/10, Epoch 703/1000, Training Loss (NLML): -960.0073\n",
      "convergence dfGPdfNN Run 3/10, Epoch 704/1000, Training Loss (NLML): -960.0045\n",
      "convergence dfGPdfNN Run 3/10, Epoch 705/1000, Training Loss (NLML): -960.0077\n",
      "convergence dfGPdfNN Run 3/10, Epoch 706/1000, Training Loss (NLML): -960.0128\n",
      "convergence dfGPdfNN Run 3/10, Epoch 707/1000, Training Loss (NLML): -960.0151\n",
      "convergence dfGPdfNN Run 3/10, Epoch 708/1000, Training Loss (NLML): -960.0190\n",
      "convergence dfGPdfNN Run 3/10, Epoch 709/1000, Training Loss (NLML): -959.9541\n",
      "convergence dfGPdfNN Run 3/10, Epoch 710/1000, Training Loss (NLML): -960.0265\n",
      "convergence dfGPdfNN Run 3/10, Epoch 711/1000, Training Loss (NLML): -960.0292\n",
      "convergence dfGPdfNN Run 3/10, Epoch 712/1000, Training Loss (NLML): -960.0330\n",
      "convergence dfGPdfNN Run 3/10, Epoch 713/1000, Training Loss (NLML): -960.0367\n",
      "convergence dfGPdfNN Run 3/10, Epoch 714/1000, Training Loss (NLML): -960.0392\n",
      "convergence dfGPdfNN Run 3/10, Epoch 715/1000, Training Loss (NLML): -960.0491\n",
      "convergence dfGPdfNN Run 3/10, Epoch 716/1000, Training Loss (NLML): -960.0519\n",
      "convergence dfGPdfNN Run 3/10, Epoch 717/1000, Training Loss (NLML): -960.0557\n",
      "convergence dfGPdfNN Run 3/10, Epoch 718/1000, Training Loss (NLML): -960.0531\n",
      "convergence dfGPdfNN Run 3/10, Epoch 719/1000, Training Loss (NLML): -960.0548\n",
      "convergence dfGPdfNN Run 3/10, Epoch 720/1000, Training Loss (NLML): -960.0604\n",
      "convergence dfGPdfNN Run 3/10, Epoch 721/1000, Training Loss (NLML): -960.0621\n",
      "convergence dfGPdfNN Run 3/10, Epoch 722/1000, Training Loss (NLML): -960.0671\n",
      "convergence dfGPdfNN Run 3/10, Epoch 723/1000, Training Loss (NLML): -960.0306\n",
      "convergence dfGPdfNN Run 3/10, Epoch 724/1000, Training Loss (NLML): -960.0720\n",
      "convergence dfGPdfNN Run 3/10, Epoch 725/1000, Training Loss (NLML): -960.0773\n",
      "convergence dfGPdfNN Run 3/10, Epoch 726/1000, Training Loss (NLML): -960.0863\n",
      "convergence dfGPdfNN Run 3/10, Epoch 727/1000, Training Loss (NLML): -960.0909\n",
      "convergence dfGPdfNN Run 3/10, Epoch 728/1000, Training Loss (NLML): -960.0878\n",
      "convergence dfGPdfNN Run 3/10, Epoch 729/1000, Training Loss (NLML): -960.0892\n",
      "convergence dfGPdfNN Run 3/10, Epoch 730/1000, Training Loss (NLML): -960.0942\n",
      "convergence dfGPdfNN Run 3/10, Epoch 731/1000, Training Loss (NLML): -960.0968\n",
      "convergence dfGPdfNN Run 3/10, Epoch 732/1000, Training Loss (NLML): -960.0992\n",
      "convergence dfGPdfNN Run 3/10, Epoch 733/1000, Training Loss (NLML): -960.1034\n",
      "convergence dfGPdfNN Run 3/10, Epoch 734/1000, Training Loss (NLML): -960.1062\n",
      "convergence dfGPdfNN Run 3/10, Epoch 735/1000, Training Loss (NLML): -960.1110\n",
      "convergence dfGPdfNN Run 3/10, Epoch 736/1000, Training Loss (NLML): -960.1204\n",
      "convergence dfGPdfNN Run 3/10, Epoch 737/1000, Training Loss (NLML): -960.1221\n",
      "convergence dfGPdfNN Run 3/10, Epoch 738/1000, Training Loss (NLML): -960.1257\n",
      "convergence dfGPdfNN Run 3/10, Epoch 739/1000, Training Loss (NLML): -960.1289\n",
      "convergence dfGPdfNN Run 3/10, Epoch 740/1000, Training Loss (NLML): -960.1265\n",
      "convergence dfGPdfNN Run 3/10, Epoch 741/1000, Training Loss (NLML): -960.1306\n",
      "convergence dfGPdfNN Run 3/10, Epoch 742/1000, Training Loss (NLML): -960.1313\n",
      "convergence dfGPdfNN Run 3/10, Epoch 743/1000, Training Loss (NLML): -960.1366\n",
      "convergence dfGPdfNN Run 3/10, Epoch 744/1000, Training Loss (NLML): -960.1375\n",
      "convergence dfGPdfNN Run 3/10, Epoch 745/1000, Training Loss (NLML): -960.1412\n",
      "convergence dfGPdfNN Run 3/10, Epoch 746/1000, Training Loss (NLML): -960.1448\n",
      "convergence dfGPdfNN Run 3/10, Epoch 747/1000, Training Loss (NLML): -960.1478\n",
      "convergence dfGPdfNN Run 3/10, Epoch 748/1000, Training Loss (NLML): -960.1510\n",
      "convergence dfGPdfNN Run 3/10, Epoch 749/1000, Training Loss (NLML): -960.1549\n",
      "convergence dfGPdfNN Run 3/10, Epoch 750/1000, Training Loss (NLML): -960.1583\n",
      "convergence dfGPdfNN Run 3/10, Epoch 751/1000, Training Loss (NLML): -960.1611\n",
      "convergence dfGPdfNN Run 3/10, Epoch 752/1000, Training Loss (NLML): -960.1630\n",
      "convergence dfGPdfNN Run 3/10, Epoch 753/1000, Training Loss (NLML): -960.1674\n",
      "convergence dfGPdfNN Run 3/10, Epoch 754/1000, Training Loss (NLML): -960.1761\n",
      "convergence dfGPdfNN Run 3/10, Epoch 755/1000, Training Loss (NLML): -960.1790\n",
      "convergence dfGPdfNN Run 3/10, Epoch 756/1000, Training Loss (NLML): -960.1813\n",
      "convergence dfGPdfNN Run 3/10, Epoch 757/1000, Training Loss (NLML): -960.1798\n",
      "convergence dfGPdfNN Run 3/10, Epoch 758/1000, Training Loss (NLML): -960.1832\n",
      "convergence dfGPdfNN Run 3/10, Epoch 759/1000, Training Loss (NLML): -960.1852\n",
      "convergence dfGPdfNN Run 3/10, Epoch 760/1000, Training Loss (NLML): -960.1877\n",
      "convergence dfGPdfNN Run 3/10, Epoch 761/1000, Training Loss (NLML): -960.1917\n",
      "convergence dfGPdfNN Run 3/10, Epoch 762/1000, Training Loss (NLML): -960.1945\n",
      "convergence dfGPdfNN Run 3/10, Epoch 763/1000, Training Loss (NLML): -960.1913\n",
      "convergence dfGPdfNN Run 3/10, Epoch 764/1000, Training Loss (NLML): -960.1956\n",
      "convergence dfGPdfNN Run 3/10, Epoch 765/1000, Training Loss (NLML): -960.1989\n",
      "convergence dfGPdfNN Run 3/10, Epoch 766/1000, Training Loss (NLML): -960.2029\n",
      "convergence dfGPdfNN Run 3/10, Epoch 767/1000, Training Loss (NLML): -960.2052\n",
      "convergence dfGPdfNN Run 3/10, Epoch 768/1000, Training Loss (NLML): -960.2080\n",
      "convergence dfGPdfNN Run 3/10, Epoch 769/1000, Training Loss (NLML): -960.2118\n",
      "convergence dfGPdfNN Run 3/10, Epoch 770/1000, Training Loss (NLML): -960.2145\n",
      "convergence dfGPdfNN Run 3/10, Epoch 771/1000, Training Loss (NLML): -960.2170\n",
      "convergence dfGPdfNN Run 3/10, Epoch 772/1000, Training Loss (NLML): -960.2202\n",
      "convergence dfGPdfNN Run 3/10, Epoch 773/1000, Training Loss (NLML): -960.2218\n",
      "convergence dfGPdfNN Run 3/10, Epoch 774/1000, Training Loss (NLML): -960.2260\n",
      "convergence dfGPdfNN Run 3/10, Epoch 775/1000, Training Loss (NLML): -960.2284\n",
      "convergence dfGPdfNN Run 3/10, Epoch 776/1000, Training Loss (NLML): -960.2313\n",
      "convergence dfGPdfNN Run 3/10, Epoch 777/1000, Training Loss (NLML): -960.2347\n",
      "convergence dfGPdfNN Run 3/10, Epoch 778/1000, Training Loss (NLML): -960.2380\n",
      "convergence dfGPdfNN Run 3/10, Epoch 779/1000, Training Loss (NLML): -960.2408\n",
      "convergence dfGPdfNN Run 3/10, Epoch 780/1000, Training Loss (NLML): -960.2397\n",
      "convergence dfGPdfNN Run 3/10, Epoch 781/1000, Training Loss (NLML): -960.2418\n",
      "convergence dfGPdfNN Run 3/10, Epoch 782/1000, Training Loss (NLML): -960.2443\n",
      "convergence dfGPdfNN Run 3/10, Epoch 783/1000, Training Loss (NLML): -960.2473\n",
      "convergence dfGPdfNN Run 3/10, Epoch 784/1000, Training Loss (NLML): -960.2520\n",
      "convergence dfGPdfNN Run 3/10, Epoch 785/1000, Training Loss (NLML): -960.2543\n",
      "convergence dfGPdfNN Run 3/10, Epoch 786/1000, Training Loss (NLML): -960.2567\n",
      "convergence dfGPdfNN Run 3/10, Epoch 787/1000, Training Loss (NLML): -960.2592\n",
      "convergence dfGPdfNN Run 3/10, Epoch 788/1000, Training Loss (NLML): -960.2635\n",
      "convergence dfGPdfNN Run 3/10, Epoch 789/1000, Training Loss (NLML): -960.2654\n",
      "convergence dfGPdfNN Run 3/10, Epoch 790/1000, Training Loss (NLML): -960.2675\n",
      "convergence dfGPdfNN Run 3/10, Epoch 791/1000, Training Loss (NLML): -960.2711\n",
      "convergence dfGPdfNN Run 3/10, Epoch 792/1000, Training Loss (NLML): -960.2734\n",
      "convergence dfGPdfNN Run 3/10, Epoch 793/1000, Training Loss (NLML): -960.2745\n",
      "convergence dfGPdfNN Run 3/10, Epoch 794/1000, Training Loss (NLML): -960.2815\n",
      "convergence dfGPdfNN Run 3/10, Epoch 795/1000, Training Loss (NLML): -960.2853\n",
      "convergence dfGPdfNN Run 3/10, Epoch 796/1000, Training Loss (NLML): -960.2877\n",
      "convergence dfGPdfNN Run 3/10, Epoch 797/1000, Training Loss (NLML): -960.2898\n",
      "convergence dfGPdfNN Run 3/10, Epoch 798/1000, Training Loss (NLML): -960.3035\n",
      "convergence dfGPdfNN Run 3/10, Epoch 799/1000, Training Loss (NLML): -960.2965\n",
      "convergence dfGPdfNN Run 3/10, Epoch 800/1000, Training Loss (NLML): -960.2981\n",
      "convergence dfGPdfNN Run 3/10, Epoch 801/1000, Training Loss (NLML): -960.3014\n",
      "convergence dfGPdfNN Run 3/10, Epoch 802/1000, Training Loss (NLML): -960.2905\n",
      "convergence dfGPdfNN Run 3/10, Epoch 803/1000, Training Loss (NLML): -960.3048\n",
      "convergence dfGPdfNN Run 3/10, Epoch 804/1000, Training Loss (NLML): -960.2958\n",
      "convergence dfGPdfNN Run 3/10, Epoch 805/1000, Training Loss (NLML): -960.3002\n",
      "convergence dfGPdfNN Run 3/10, Epoch 806/1000, Training Loss (NLML): -960.3030\n",
      "convergence dfGPdfNN Run 3/10, Epoch 807/1000, Training Loss (NLML): -960.3175\n",
      "convergence dfGPdfNN Run 3/10, Epoch 808/1000, Training Loss (NLML): -960.3206\n",
      "convergence dfGPdfNN Run 3/10, Epoch 809/1000, Training Loss (NLML): -960.3242\n",
      "convergence dfGPdfNN Run 3/10, Epoch 810/1000, Training Loss (NLML): -960.3145\n",
      "convergence dfGPdfNN Run 3/10, Epoch 811/1000, Training Loss (NLML): -960.3167\n",
      "convergence dfGPdfNN Run 3/10, Epoch 812/1000, Training Loss (NLML): -960.3202\n",
      "convergence dfGPdfNN Run 3/10, Epoch 813/1000, Training Loss (NLML): -960.3239\n",
      "convergence dfGPdfNN Run 3/10, Epoch 814/1000, Training Loss (NLML): -960.3256\n",
      "convergence dfGPdfNN Run 3/10, Epoch 815/1000, Training Loss (NLML): -960.3408\n",
      "convergence dfGPdfNN Run 3/10, Epoch 816/1000, Training Loss (NLML): -960.3411\n",
      "convergence dfGPdfNN Run 3/10, Epoch 817/1000, Training Loss (NLML): -960.3461\n",
      "convergence dfGPdfNN Run 3/10, Epoch 818/1000, Training Loss (NLML): -960.3356\n",
      "convergence dfGPdfNN Run 3/10, Epoch 819/1000, Training Loss (NLML): -960.3397\n",
      "convergence dfGPdfNN Run 3/10, Epoch 820/1000, Training Loss (NLML): -960.3542\n",
      "convergence dfGPdfNN Run 3/10, Epoch 821/1000, Training Loss (NLML): -960.3562\n",
      "convergence dfGPdfNN Run 3/10, Epoch 822/1000, Training Loss (NLML): -960.3480\n",
      "convergence dfGPdfNN Run 3/10, Epoch 823/1000, Training Loss (NLML): -960.3511\n",
      "convergence dfGPdfNN Run 3/10, Epoch 824/1000, Training Loss (NLML): -960.3545\n",
      "convergence dfGPdfNN Run 3/10, Epoch 825/1000, Training Loss (NLML): -960.3920\n",
      "convergence dfGPdfNN Run 3/10, Epoch 826/1000, Training Loss (NLML): -960.3586\n",
      "convergence dfGPdfNN Run 3/10, Epoch 827/1000, Training Loss (NLML): -960.3621\n",
      "convergence dfGPdfNN Run 3/10, Epoch 828/1000, Training Loss (NLML): -960.3638\n",
      "convergence dfGPdfNN Run 3/10, Epoch 829/1000, Training Loss (NLML): -960.3783\n",
      "convergence dfGPdfNN Run 3/10, Epoch 830/1000, Training Loss (NLML): -960.3794\n",
      "convergence dfGPdfNN Run 3/10, Epoch 831/1000, Training Loss (NLML): -960.3833\n",
      "convergence dfGPdfNN Run 3/10, Epoch 832/1000, Training Loss (NLML): -960.3737\n",
      "convergence dfGPdfNN Run 3/10, Epoch 833/1000, Training Loss (NLML): -960.3774\n",
      "convergence dfGPdfNN Run 3/10, Epoch 834/1000, Training Loss (NLML): -960.3807\n",
      "convergence dfGPdfNN Run 3/10, Epoch 835/1000, Training Loss (NLML): -960.4199\n",
      "convergence dfGPdfNN Run 3/10, Epoch 836/1000, Training Loss (NLML): -960.3939\n",
      "convergence dfGPdfNN Run 3/10, Epoch 837/1000, Training Loss (NLML): -960.3853\n",
      "convergence dfGPdfNN Run 3/10, Epoch 838/1000, Training Loss (NLML): -960.3866\n",
      "convergence dfGPdfNN Run 3/10, Epoch 839/1000, Training Loss (NLML): -960.4253\n",
      "convergence dfGPdfNN Run 3/10, Epoch 840/1000, Training Loss (NLML): -960.4292\n",
      "convergence dfGPdfNN Run 3/10, Epoch 841/1000, Training Loss (NLML): -960.3943\n",
      "convergence dfGPdfNN Run 3/10, Epoch 842/1000, Training Loss (NLML): -960.3977\n",
      "convergence dfGPdfNN Run 3/10, Epoch 843/1000, Training Loss (NLML): -960.3986\n",
      "convergence dfGPdfNN Run 3/10, Epoch 844/1000, Training Loss (NLML): -960.4012\n",
      "convergence dfGPdfNN Run 3/10, Epoch 845/1000, Training Loss (NLML): -960.4014\n",
      "convergence dfGPdfNN Run 3/10, Epoch 846/1000, Training Loss (NLML): -960.4312\n",
      "convergence dfGPdfNN Run 3/10, Epoch 847/1000, Training Loss (NLML): -960.4091\n",
      "convergence dfGPdfNN Run 3/10, Epoch 848/1000, Training Loss (NLML): -960.4125\n",
      "convergence dfGPdfNN Run 3/10, Epoch 849/1000, Training Loss (NLML): -960.4117\n",
      "convergence dfGPdfNN Run 3/10, Epoch 850/1000, Training Loss (NLML): -960.4399\n",
      "convergence dfGPdfNN Run 3/10, Epoch 851/1000, Training Loss (NLML): -960.4425\n",
      "convergence dfGPdfNN Run 3/10, Epoch 852/1000, Training Loss (NLML): -960.4193\n",
      "convergence dfGPdfNN Run 3/10, Epoch 853/1000, Training Loss (NLML): -960.4174\n",
      "convergence dfGPdfNN Run 3/10, Epoch 854/1000, Training Loss (NLML): -960.4169\n",
      "convergence dfGPdfNN Run 3/10, Epoch 855/1000, Training Loss (NLML): -960.4480\n",
      "convergence dfGPdfNN Run 3/10, Epoch 856/1000, Training Loss (NLML): -960.4425\n",
      "convergence dfGPdfNN Run 3/10, Epoch 857/1000, Training Loss (NLML): -960.4446\n",
      "convergence dfGPdfNN Run 3/10, Epoch 858/1000, Training Loss (NLML): -960.4181\n",
      "convergence dfGPdfNN Run 3/10, Epoch 859/1000, Training Loss (NLML): -960.4199\n",
      "convergence dfGPdfNN Run 3/10, Epoch 860/1000, Training Loss (NLML): -960.4230\n",
      "convergence dfGPdfNN Run 3/10, Epoch 861/1000, Training Loss (NLML): -960.4517\n",
      "convergence dfGPdfNN Run 3/10, Epoch 862/1000, Training Loss (NLML): -960.4553\n",
      "convergence dfGPdfNN Run 3/10, Epoch 863/1000, Training Loss (NLML): -960.4580\n",
      "convergence dfGPdfNN Run 3/10, Epoch 864/1000, Training Loss (NLML): -960.4604\n",
      "convergence dfGPdfNN Run 3/10, Epoch 865/1000, Training Loss (NLML): -960.4596\n",
      "convergence dfGPdfNN Run 3/10, Epoch 866/1000, Training Loss (NLML): -960.4386\n",
      "convergence dfGPdfNN Run 3/10, Epoch 867/1000, Training Loss (NLML): -960.4414\n",
      "convergence dfGPdfNN Run 3/10, Epoch 868/1000, Training Loss (NLML): -960.4678\n",
      "convergence dfGPdfNN Run 3/10, Epoch 869/1000, Training Loss (NLML): -960.4718\n",
      "convergence dfGPdfNN Run 3/10, Epoch 870/1000, Training Loss (NLML): -960.4753\n",
      "convergence dfGPdfNN Run 3/10, Epoch 871/1000, Training Loss (NLML): -960.4777\n",
      "convergence dfGPdfNN Run 3/10, Epoch 872/1000, Training Loss (NLML): -960.4739\n",
      "convergence dfGPdfNN Run 3/10, Epoch 873/1000, Training Loss (NLML): -960.4493\n",
      "convergence dfGPdfNN Run 3/10, Epoch 874/1000, Training Loss (NLML): -960.4792\n",
      "convergence dfGPdfNN Run 3/10, Epoch 875/1000, Training Loss (NLML): -960.4819\n",
      "convergence dfGPdfNN Run 3/10, Epoch 876/1000, Training Loss (NLML): -960.4832\n",
      "convergence dfGPdfNN Run 3/10, Epoch 877/1000, Training Loss (NLML): -960.4868\n",
      "convergence dfGPdfNN Run 3/10, Epoch 878/1000, Training Loss (NLML): -960.4642\n",
      "convergence dfGPdfNN Run 3/10, Epoch 879/1000, Training Loss (NLML): -960.4926\n",
      "convergence dfGPdfNN Run 3/10, Epoch 880/1000, Training Loss (NLML): -960.4886\n",
      "convergence dfGPdfNN Run 3/10, Epoch 881/1000, Training Loss (NLML): -960.4911\n",
      "convergence dfGPdfNN Run 3/10, Epoch 882/1000, Training Loss (NLML): -960.4957\n",
      "convergence dfGPdfNN Run 3/10, Epoch 883/1000, Training Loss (NLML): -960.4971\n",
      "convergence dfGPdfNN Run 3/10, Epoch 884/1000, Training Loss (NLML): -960.4991\n",
      "convergence dfGPdfNN Run 3/10, Epoch 885/1000, Training Loss (NLML): -960.5037\n",
      "convergence dfGPdfNN Run 3/10, Epoch 886/1000, Training Loss (NLML): -960.4775\n",
      "convergence dfGPdfNN Run 3/10, Epoch 887/1000, Training Loss (NLML): -960.5061\n",
      "convergence dfGPdfNN Run 3/10, Epoch 888/1000, Training Loss (NLML): -960.5099\n",
      "convergence dfGPdfNN Run 3/10, Epoch 889/1000, Training Loss (NLML): -960.5105\n",
      "convergence dfGPdfNN Run 3/10, Epoch 890/1000, Training Loss (NLML): -960.5150\n",
      "convergence dfGPdfNN Run 3/10, Epoch 891/1000, Training Loss (NLML): -960.5139\n",
      "convergence dfGPdfNN Run 3/10, Epoch 892/1000, Training Loss (NLML): -960.5214\n",
      "convergence dfGPdfNN Run 3/10, Epoch 893/1000, Training Loss (NLML): -960.5215\n",
      "convergence dfGPdfNN Run 3/10, Epoch 894/1000, Training Loss (NLML): -960.5259\n",
      "convergence dfGPdfNN Run 3/10, Epoch 895/1000, Training Loss (NLML): -960.5254\n",
      "convergence dfGPdfNN Run 3/10, Epoch 896/1000, Training Loss (NLML): -960.5283\n",
      "convergence dfGPdfNN Run 3/10, Epoch 897/1000, Training Loss (NLML): -960.5306\n",
      "convergence dfGPdfNN Run 3/10, Epoch 898/1000, Training Loss (NLML): -960.5331\n",
      "convergence dfGPdfNN Run 3/10, Epoch 899/1000, Training Loss (NLML): -960.5325\n",
      "convergence dfGPdfNN Run 3/10, Epoch 900/1000, Training Loss (NLML): -960.5338\n",
      "convergence dfGPdfNN Run 3/10, Epoch 901/1000, Training Loss (NLML): -960.5358\n",
      "convergence dfGPdfNN Run 3/10, Epoch 902/1000, Training Loss (NLML): -960.5380\n",
      "convergence dfGPdfNN Run 3/10, Epoch 903/1000, Training Loss (NLML): -960.5139\n",
      "convergence dfGPdfNN Run 3/10, Epoch 904/1000, Training Loss (NLML): -960.5426\n",
      "convergence dfGPdfNN Run 3/10, Epoch 905/1000, Training Loss (NLML): -960.5468\n",
      "convergence dfGPdfNN Run 3/10, Epoch 906/1000, Training Loss (NLML): -960.5460\n",
      "convergence dfGPdfNN Run 3/10, Epoch 907/1000, Training Loss (NLML): -960.5521\n",
      "convergence dfGPdfNN Run 3/10, Epoch 908/1000, Training Loss (NLML): -960.5548\n",
      "convergence dfGPdfNN Run 3/10, Epoch 909/1000, Training Loss (NLML): -960.5555\n",
      "convergence dfGPdfNN Run 3/10, Epoch 910/1000, Training Loss (NLML): -960.5581\n",
      "convergence dfGPdfNN Run 3/10, Epoch 911/1000, Training Loss (NLML): -960.5336\n",
      "convergence dfGPdfNN Run 3/10, Epoch 912/1000, Training Loss (NLML): -960.5623\n",
      "convergence dfGPdfNN Run 3/10, Epoch 913/1000, Training Loss (NLML): -960.5658\n",
      "convergence dfGPdfNN Run 3/10, Epoch 914/1000, Training Loss (NLML): -960.5680\n",
      "convergence dfGPdfNN Run 3/10, Epoch 915/1000, Training Loss (NLML): -960.5669\n",
      "convergence dfGPdfNN Run 3/10, Epoch 916/1000, Training Loss (NLML): -960.5698\n",
      "convergence dfGPdfNN Run 3/10, Epoch 917/1000, Training Loss (NLML): -960.5720\n",
      "convergence dfGPdfNN Run 3/10, Epoch 918/1000, Training Loss (NLML): -960.5745\n",
      "convergence dfGPdfNN Run 3/10, Epoch 919/1000, Training Loss (NLML): -960.5764\n",
      "convergence dfGPdfNN Run 3/10, Epoch 920/1000, Training Loss (NLML): -960.5531\n",
      "convergence dfGPdfNN Run 3/10, Epoch 921/1000, Training Loss (NLML): -960.5847\n",
      "convergence dfGPdfNN Run 3/10, Epoch 922/1000, Training Loss (NLML): -960.5839\n",
      "convergence dfGPdfNN Run 3/10, Epoch 923/1000, Training Loss (NLML): -960.5885\n",
      "convergence dfGPdfNN Run 3/10, Epoch 924/1000, Training Loss (NLML): -960.5880\n",
      "convergence dfGPdfNN Run 3/10, Epoch 925/1000, Training Loss (NLML): -960.5897\n",
      "convergence dfGPdfNN Run 3/10, Epoch 926/1000, Training Loss (NLML): -960.5941\n",
      "convergence dfGPdfNN Run 3/10, Epoch 927/1000, Training Loss (NLML): -960.5947\n",
      "convergence dfGPdfNN Run 3/10, Epoch 928/1000, Training Loss (NLML): -960.5980\n",
      "convergence dfGPdfNN Run 3/10, Epoch 929/1000, Training Loss (NLML): -960.5991\n",
      "convergence dfGPdfNN Run 3/10, Epoch 930/1000, Training Loss (NLML): -960.6017\n",
      "convergence dfGPdfNN Run 3/10, Epoch 931/1000, Training Loss (NLML): -960.6039\n",
      "convergence dfGPdfNN Run 3/10, Epoch 932/1000, Training Loss (NLML): -960.6077\n",
      "convergence dfGPdfNN Run 3/10, Epoch 933/1000, Training Loss (NLML): -960.6085\n",
      "convergence dfGPdfNN Run 3/10, Epoch 934/1000, Training Loss (NLML): -960.6101\n",
      "convergence dfGPdfNN Run 3/10, Epoch 935/1000, Training Loss (NLML): -960.6141\n",
      "convergence dfGPdfNN Run 3/10, Epoch 936/1000, Training Loss (NLML): -960.6177\n",
      "convergence dfGPdfNN Run 3/10, Epoch 937/1000, Training Loss (NLML): -960.6178\n",
      "convergence dfGPdfNN Run 3/10, Epoch 938/1000, Training Loss (NLML): -960.6219\n",
      "convergence dfGPdfNN Run 3/10, Epoch 939/1000, Training Loss (NLML): -960.6248\n",
      "convergence dfGPdfNN Run 3/10, Epoch 940/1000, Training Loss (NLML): -960.6271\n",
      "convergence dfGPdfNN Run 3/10, Epoch 941/1000, Training Loss (NLML): -960.6290\n",
      "convergence dfGPdfNN Run 3/10, Epoch 942/1000, Training Loss (NLML): -960.6298\n",
      "convergence dfGPdfNN Run 3/10, Epoch 943/1000, Training Loss (NLML): -960.6317\n",
      "convergence dfGPdfNN Run 3/10, Epoch 944/1000, Training Loss (NLML): -960.6366\n",
      "convergence dfGPdfNN Run 3/10, Epoch 945/1000, Training Loss (NLML): -960.6360\n",
      "convergence dfGPdfNN Run 3/10, Epoch 946/1000, Training Loss (NLML): -960.6384\n",
      "convergence dfGPdfNN Run 3/10, Epoch 947/1000, Training Loss (NLML): -960.6433\n",
      "convergence dfGPdfNN Run 3/10, Epoch 948/1000, Training Loss (NLML): -960.6433\n",
      "convergence dfGPdfNN Run 3/10, Epoch 949/1000, Training Loss (NLML): -960.6459\n",
      "convergence dfGPdfNN Run 3/10, Epoch 950/1000, Training Loss (NLML): -960.6476\n",
      "convergence dfGPdfNN Run 3/10, Epoch 951/1000, Training Loss (NLML): -960.6498\n",
      "convergence dfGPdfNN Run 3/10, Epoch 952/1000, Training Loss (NLML): -960.6497\n",
      "convergence dfGPdfNN Run 3/10, Epoch 953/1000, Training Loss (NLML): -960.6532\n",
      "convergence dfGPdfNN Run 3/10, Epoch 954/1000, Training Loss (NLML): -960.6561\n",
      "convergence dfGPdfNN Run 3/10, Epoch 955/1000, Training Loss (NLML): -960.6570\n",
      "convergence dfGPdfNN Run 3/10, Epoch 956/1000, Training Loss (NLML): -960.6616\n",
      "convergence dfGPdfNN Run 3/10, Epoch 957/1000, Training Loss (NLML): -960.6632\n",
      "convergence dfGPdfNN Run 3/10, Epoch 958/1000, Training Loss (NLML): -960.6633\n",
      "convergence dfGPdfNN Run 3/10, Epoch 959/1000, Training Loss (NLML): -960.6658\n",
      "convergence dfGPdfNN Run 3/10, Epoch 960/1000, Training Loss (NLML): -960.6678\n",
      "convergence dfGPdfNN Run 3/10, Epoch 961/1000, Training Loss (NLML): -960.6702\n",
      "convergence dfGPdfNN Run 3/10, Epoch 962/1000, Training Loss (NLML): -960.6752\n",
      "convergence dfGPdfNN Run 3/10, Epoch 963/1000, Training Loss (NLML): -960.6729\n",
      "convergence dfGPdfNN Run 3/10, Epoch 964/1000, Training Loss (NLML): -960.6771\n",
      "convergence dfGPdfNN Run 3/10, Epoch 965/1000, Training Loss (NLML): -960.6779\n",
      "convergence dfGPdfNN Run 3/10, Epoch 966/1000, Training Loss (NLML): -960.6792\n",
      "convergence dfGPdfNN Run 3/10, Epoch 967/1000, Training Loss (NLML): -960.6819\n",
      "convergence dfGPdfNN Run 3/10, Epoch 968/1000, Training Loss (NLML): -960.6864\n",
      "convergence dfGPdfNN Run 3/10, Epoch 969/1000, Training Loss (NLML): -960.6863\n",
      "convergence dfGPdfNN Run 3/10, Epoch 970/1000, Training Loss (NLML): -960.6885\n",
      "convergence dfGPdfNN Run 3/10, Epoch 971/1000, Training Loss (NLML): -960.6881\n",
      "convergence dfGPdfNN Run 3/10, Epoch 972/1000, Training Loss (NLML): -960.6918\n",
      "convergence dfGPdfNN Run 3/10, Epoch 973/1000, Training Loss (NLML): -960.6930\n",
      "convergence dfGPdfNN Run 3/10, Epoch 974/1000, Training Loss (NLML): -960.6960\n",
      "convergence dfGPdfNN Run 3/10, Epoch 975/1000, Training Loss (NLML): -960.6978\n",
      "convergence dfGPdfNN Run 3/10, Epoch 976/1000, Training Loss (NLML): -960.7008\n",
      "convergence dfGPdfNN Run 3/10, Epoch 977/1000, Training Loss (NLML): -960.7036\n",
      "convergence dfGPdfNN Run 3/10, Epoch 978/1000, Training Loss (NLML): -960.7054\n",
      "convergence dfGPdfNN Run 3/10, Epoch 979/1000, Training Loss (NLML): -960.7086\n",
      "convergence dfGPdfNN Run 3/10, Epoch 980/1000, Training Loss (NLML): -960.7113\n",
      "convergence dfGPdfNN Run 3/10, Epoch 981/1000, Training Loss (NLML): -960.7101\n",
      "convergence dfGPdfNN Run 3/10, Epoch 982/1000, Training Loss (NLML): -960.7126\n",
      "convergence dfGPdfNN Run 3/10, Epoch 983/1000, Training Loss (NLML): -960.7144\n",
      "convergence dfGPdfNN Run 3/10, Epoch 984/1000, Training Loss (NLML): -960.7181\n",
      "convergence dfGPdfNN Run 3/10, Epoch 985/1000, Training Loss (NLML): -960.7198\n",
      "convergence dfGPdfNN Run 3/10, Epoch 986/1000, Training Loss (NLML): -960.7198\n",
      "convergence dfGPdfNN Run 3/10, Epoch 987/1000, Training Loss (NLML): -960.7228\n",
      "convergence dfGPdfNN Run 3/10, Epoch 988/1000, Training Loss (NLML): -960.7240\n",
      "convergence dfGPdfNN Run 3/10, Epoch 989/1000, Training Loss (NLML): -960.7285\n",
      "convergence dfGPdfNN Run 3/10, Epoch 990/1000, Training Loss (NLML): -960.7306\n",
      "convergence dfGPdfNN Run 3/10, Epoch 991/1000, Training Loss (NLML): -960.7299\n",
      "convergence dfGPdfNN Run 3/10, Epoch 992/1000, Training Loss (NLML): -960.7324\n",
      "convergence dfGPdfNN Run 3/10, Epoch 993/1000, Training Loss (NLML): -960.7347\n",
      "convergence dfGPdfNN Run 3/10, Epoch 994/1000, Training Loss (NLML): -960.7356\n",
      "convergence dfGPdfNN Run 3/10, Epoch 995/1000, Training Loss (NLML): -960.7360\n",
      "convergence dfGPdfNN Run 3/10, Epoch 996/1000, Training Loss (NLML): -960.7407\n",
      "convergence dfGPdfNN Run 3/10, Epoch 997/1000, Training Loss (NLML): -960.7418\n",
      "convergence dfGPdfNN Run 3/10, Epoch 998/1000, Training Loss (NLML): -960.7440\n",
      "convergence dfGPdfNN Run 3/10, Epoch 999/1000, Training Loss (NLML): -960.7485\n",
      "convergence dfGPdfNN Run 3/10, Epoch 1000/1000, Training Loss (NLML): -960.7482\n",
      "\n",
      "--- Training Run 4/10 ---\n",
      "\n",
      "Start Training\n",
      "convergence dfGPdfNN Run 4/10, Epoch 1/1000, Training Loss (NLML): -811.8303\n",
      "convergence dfGPdfNN Run 4/10, Epoch 2/1000, Training Loss (NLML): -822.9938\n",
      "convergence dfGPdfNN Run 4/10, Epoch 3/1000, Training Loss (NLML): -837.1634\n",
      "convergence dfGPdfNN Run 4/10, Epoch 4/1000, Training Loss (NLML): -844.3440\n",
      "convergence dfGPdfNN Run 4/10, Epoch 5/1000, Training Loss (NLML): -850.3025\n",
      "convergence dfGPdfNN Run 4/10, Epoch 6/1000, Training Loss (NLML): -854.8527\n",
      "convergence dfGPdfNN Run 4/10, Epoch 7/1000, Training Loss (NLML): -858.9056\n",
      "convergence dfGPdfNN Run 4/10, Epoch 8/1000, Training Loss (NLML): -862.5701\n",
      "convergence dfGPdfNN Run 4/10, Epoch 9/1000, Training Loss (NLML): -866.3110\n",
      "convergence dfGPdfNN Run 4/10, Epoch 10/1000, Training Loss (NLML): -869.7727\n",
      "convergence dfGPdfNN Run 4/10, Epoch 11/1000, Training Loss (NLML): -872.9858\n",
      "convergence dfGPdfNN Run 4/10, Epoch 12/1000, Training Loss (NLML): -875.6328\n",
      "convergence dfGPdfNN Run 4/10, Epoch 13/1000, Training Loss (NLML): -877.2495\n",
      "convergence dfGPdfNN Run 4/10, Epoch 14/1000, Training Loss (NLML): -881.2917\n",
      "convergence dfGPdfNN Run 4/10, Epoch 15/1000, Training Loss (NLML): -883.8160\n",
      "convergence dfGPdfNN Run 4/10, Epoch 16/1000, Training Loss (NLML): -886.0479\n",
      "convergence dfGPdfNN Run 4/10, Epoch 17/1000, Training Loss (NLML): -888.4878\n",
      "convergence dfGPdfNN Run 4/10, Epoch 18/1000, Training Loss (NLML): -890.7140\n",
      "convergence dfGPdfNN Run 4/10, Epoch 19/1000, Training Loss (NLML): -892.7094\n",
      "convergence dfGPdfNN Run 4/10, Epoch 20/1000, Training Loss (NLML): -894.8656\n",
      "convergence dfGPdfNN Run 4/10, Epoch 21/1000, Training Loss (NLML): -896.6597\n",
      "convergence dfGPdfNN Run 4/10, Epoch 22/1000, Training Loss (NLML): -898.3228\n",
      "convergence dfGPdfNN Run 4/10, Epoch 23/1000, Training Loss (NLML): -900.0111\n",
      "convergence dfGPdfNN Run 4/10, Epoch 24/1000, Training Loss (NLML): -901.5420\n",
      "convergence dfGPdfNN Run 4/10, Epoch 25/1000, Training Loss (NLML): -903.1541\n",
      "convergence dfGPdfNN Run 4/10, Epoch 26/1000, Training Loss (NLML): -904.6365\n",
      "convergence dfGPdfNN Run 4/10, Epoch 27/1000, Training Loss (NLML): -906.0446\n",
      "convergence dfGPdfNN Run 4/10, Epoch 28/1000, Training Loss (NLML): -907.1010\n",
      "convergence dfGPdfNN Run 4/10, Epoch 29/1000, Training Loss (NLML): -908.5927\n",
      "convergence dfGPdfNN Run 4/10, Epoch 30/1000, Training Loss (NLML): -909.6791\n",
      "convergence dfGPdfNN Run 4/10, Epoch 31/1000, Training Loss (NLML): -910.9086\n",
      "convergence dfGPdfNN Run 4/10, Epoch 32/1000, Training Loss (NLML): -912.0243\n",
      "convergence dfGPdfNN Run 4/10, Epoch 33/1000, Training Loss (NLML): -913.1228\n",
      "convergence dfGPdfNN Run 4/10, Epoch 34/1000, Training Loss (NLML): -914.2223\n",
      "convergence dfGPdfNN Run 4/10, Epoch 35/1000, Training Loss (NLML): -915.1504\n",
      "convergence dfGPdfNN Run 4/10, Epoch 36/1000, Training Loss (NLML): -916.0437\n",
      "convergence dfGPdfNN Run 4/10, Epoch 37/1000, Training Loss (NLML): -917.0448\n",
      "convergence dfGPdfNN Run 4/10, Epoch 38/1000, Training Loss (NLML): -918.0267\n",
      "convergence dfGPdfNN Run 4/10, Epoch 39/1000, Training Loss (NLML): -918.8810\n",
      "convergence dfGPdfNN Run 4/10, Epoch 40/1000, Training Loss (NLML): -919.7405\n",
      "convergence dfGPdfNN Run 4/10, Epoch 41/1000, Training Loss (NLML): -920.3763\n",
      "convergence dfGPdfNN Run 4/10, Epoch 42/1000, Training Loss (NLML): -921.1678\n",
      "convergence dfGPdfNN Run 4/10, Epoch 43/1000, Training Loss (NLML): -922.0577\n",
      "convergence dfGPdfNN Run 4/10, Epoch 44/1000, Training Loss (NLML): -922.7451\n",
      "convergence dfGPdfNN Run 4/10, Epoch 45/1000, Training Loss (NLML): -923.4324\n",
      "convergence dfGPdfNN Run 4/10, Epoch 46/1000, Training Loss (NLML): -923.3020\n",
      "convergence dfGPdfNN Run 4/10, Epoch 47/1000, Training Loss (NLML): -924.8895\n",
      "convergence dfGPdfNN Run 4/10, Epoch 48/1000, Training Loss (NLML): -925.4696\n",
      "convergence dfGPdfNN Run 4/10, Epoch 49/1000, Training Loss (NLML): -926.1232\n",
      "convergence dfGPdfNN Run 4/10, Epoch 50/1000, Training Loss (NLML): -926.6283\n",
      "convergence dfGPdfNN Run 4/10, Epoch 51/1000, Training Loss (NLML): -927.2404\n",
      "convergence dfGPdfNN Run 4/10, Epoch 52/1000, Training Loss (NLML): -927.9092\n",
      "convergence dfGPdfNN Run 4/10, Epoch 53/1000, Training Loss (NLML): -928.3727\n",
      "convergence dfGPdfNN Run 4/10, Epoch 54/1000, Training Loss (NLML): -929.0747\n",
      "convergence dfGPdfNN Run 4/10, Epoch 55/1000, Training Loss (NLML): -929.6144\n",
      "convergence dfGPdfNN Run 4/10, Epoch 56/1000, Training Loss (NLML): -930.1503\n",
      "convergence dfGPdfNN Run 4/10, Epoch 57/1000, Training Loss (NLML): -930.4282\n",
      "convergence dfGPdfNN Run 4/10, Epoch 58/1000, Training Loss (NLML): -931.0300\n",
      "convergence dfGPdfNN Run 4/10, Epoch 59/1000, Training Loss (NLML): -931.5048\n",
      "convergence dfGPdfNN Run 4/10, Epoch 60/1000, Training Loss (NLML): -932.1930\n",
      "convergence dfGPdfNN Run 4/10, Epoch 61/1000, Training Loss (NLML): -932.4858\n",
      "convergence dfGPdfNN Run 4/10, Epoch 62/1000, Training Loss (NLML): -932.9037\n",
      "convergence dfGPdfNN Run 4/10, Epoch 63/1000, Training Loss (NLML): -933.4979\n",
      "convergence dfGPdfNN Run 4/10, Epoch 64/1000, Training Loss (NLML): -933.8284\n",
      "convergence dfGPdfNN Run 4/10, Epoch 65/1000, Training Loss (NLML): -934.0266\n",
      "convergence dfGPdfNN Run 4/10, Epoch 66/1000, Training Loss (NLML): -934.7029\n",
      "convergence dfGPdfNN Run 4/10, Epoch 67/1000, Training Loss (NLML): -935.1796\n",
      "convergence dfGPdfNN Run 4/10, Epoch 68/1000, Training Loss (NLML): -935.7373\n",
      "convergence dfGPdfNN Run 4/10, Epoch 69/1000, Training Loss (NLML): -935.5475\n",
      "convergence dfGPdfNN Run 4/10, Epoch 70/1000, Training Loss (NLML): -935.5260\n",
      "convergence dfGPdfNN Run 4/10, Epoch 71/1000, Training Loss (NLML): -936.8728\n",
      "convergence dfGPdfNN Run 4/10, Epoch 72/1000, Training Loss (NLML): -937.0959\n",
      "convergence dfGPdfNN Run 4/10, Epoch 73/1000, Training Loss (NLML): -937.4742\n",
      "convergence dfGPdfNN Run 4/10, Epoch 74/1000, Training Loss (NLML): -937.8669\n",
      "convergence dfGPdfNN Run 4/10, Epoch 75/1000, Training Loss (NLML): -938.0839\n",
      "convergence dfGPdfNN Run 4/10, Epoch 76/1000, Training Loss (NLML): -938.6250\n",
      "convergence dfGPdfNN Run 4/10, Epoch 77/1000, Training Loss (NLML): -938.9347\n",
      "convergence dfGPdfNN Run 4/10, Epoch 78/1000, Training Loss (NLML): -939.2949\n",
      "convergence dfGPdfNN Run 4/10, Epoch 79/1000, Training Loss (NLML): -937.7091\n",
      "convergence dfGPdfNN Run 4/10, Epoch 80/1000, Training Loss (NLML): -939.9634\n",
      "convergence dfGPdfNN Run 4/10, Epoch 81/1000, Training Loss (NLML): -940.3339\n",
      "convergence dfGPdfNN Run 4/10, Epoch 82/1000, Training Loss (NLML): -940.6006\n",
      "convergence dfGPdfNN Run 4/10, Epoch 83/1000, Training Loss (NLML): -940.7725\n",
      "convergence dfGPdfNN Run 4/10, Epoch 84/1000, Training Loss (NLML): -941.0643\n",
      "convergence dfGPdfNN Run 4/10, Epoch 85/1000, Training Loss (NLML): -941.5674\n",
      "convergence dfGPdfNN Run 4/10, Epoch 86/1000, Training Loss (NLML): -941.1898\n",
      "convergence dfGPdfNN Run 4/10, Epoch 87/1000, Training Loss (NLML): -942.0966\n",
      "convergence dfGPdfNN Run 4/10, Epoch 88/1000, Training Loss (NLML): -942.5079\n",
      "convergence dfGPdfNN Run 4/10, Epoch 89/1000, Training Loss (NLML): -942.8043\n",
      "convergence dfGPdfNN Run 4/10, Epoch 90/1000, Training Loss (NLML): -942.5254\n",
      "convergence dfGPdfNN Run 4/10, Epoch 91/1000, Training Loss (NLML): -943.2687\n",
      "convergence dfGPdfNN Run 4/10, Epoch 92/1000, Training Loss (NLML): -942.9536\n",
      "convergence dfGPdfNN Run 4/10, Epoch 93/1000, Training Loss (NLML): -943.7313\n",
      "convergence dfGPdfNN Run 4/10, Epoch 94/1000, Training Loss (NLML): -944.0018\n",
      "convergence dfGPdfNN Run 4/10, Epoch 95/1000, Training Loss (NLML): -943.6136\n",
      "convergence dfGPdfNN Run 4/10, Epoch 96/1000, Training Loss (NLML): -943.9668\n",
      "convergence dfGPdfNN Run 4/10, Epoch 97/1000, Training Loss (NLML): -944.2930\n",
      "convergence dfGPdfNN Run 4/10, Epoch 98/1000, Training Loss (NLML): -945.2031\n",
      "convergence dfGPdfNN Run 4/10, Epoch 99/1000, Training Loss (NLML): -945.4313\n",
      "convergence dfGPdfNN Run 4/10, Epoch 100/1000, Training Loss (NLML): -944.9412\n",
      "convergence dfGPdfNN Run 4/10, Epoch 101/1000, Training Loss (NLML): -945.8781\n",
      "convergence dfGPdfNN Run 4/10, Epoch 102/1000, Training Loss (NLML): -946.1121\n",
      "convergence dfGPdfNN Run 4/10, Epoch 103/1000, Training Loss (NLML): -946.2729\n",
      "convergence dfGPdfNN Run 4/10, Epoch 104/1000, Training Loss (NLML): -946.5876\n",
      "convergence dfGPdfNN Run 4/10, Epoch 105/1000, Training Loss (NLML): -946.7527\n",
      "convergence dfGPdfNN Run 4/10, Epoch 106/1000, Training Loss (NLML): -946.9163\n",
      "convergence dfGPdfNN Run 4/10, Epoch 107/1000, Training Loss (NLML): -943.9717\n",
      "convergence dfGPdfNN Run 4/10, Epoch 108/1000, Training Loss (NLML): -947.1613\n",
      "convergence dfGPdfNN Run 4/10, Epoch 109/1000, Training Loss (NLML): -946.8120\n",
      "convergence dfGPdfNN Run 4/10, Epoch 110/1000, Training Loss (NLML): -947.3479\n",
      "convergence dfGPdfNN Run 4/10, Epoch 111/1000, Training Loss (NLML): -947.6077\n",
      "convergence dfGPdfNN Run 4/10, Epoch 112/1000, Training Loss (NLML): -947.8138\n",
      "convergence dfGPdfNN Run 4/10, Epoch 113/1000, Training Loss (NLML): -948.2982\n",
      "convergence dfGPdfNN Run 4/10, Epoch 114/1000, Training Loss (NLML): -948.4767\n",
      "convergence dfGPdfNN Run 4/10, Epoch 115/1000, Training Loss (NLML): -947.2401\n",
      "convergence dfGPdfNN Run 4/10, Epoch 116/1000, Training Loss (NLML): -948.4041\n",
      "convergence dfGPdfNN Run 4/10, Epoch 117/1000, Training Loss (NLML): -949.1064\n",
      "convergence dfGPdfNN Run 4/10, Epoch 118/1000, Training Loss (NLML): -949.2312\n",
      "convergence dfGPdfNN Run 4/10, Epoch 119/1000, Training Loss (NLML): -949.2971\n",
      "convergence dfGPdfNN Run 4/10, Epoch 120/1000, Training Loss (NLML): -948.9745\n",
      "convergence dfGPdfNN Run 4/10, Epoch 121/1000, Training Loss (NLML): -949.3009\n",
      "convergence dfGPdfNN Run 4/10, Epoch 122/1000, Training Loss (NLML): -949.0763\n",
      "convergence dfGPdfNN Run 4/10, Epoch 123/1000, Training Loss (NLML): -949.6499\n",
      "convergence dfGPdfNN Run 4/10, Epoch 124/1000, Training Loss (NLML): -949.0607\n",
      "convergence dfGPdfNN Run 4/10, Epoch 125/1000, Training Loss (NLML): -949.7397\n",
      "convergence dfGPdfNN Run 4/10, Epoch 126/1000, Training Loss (NLML): -949.8802\n",
      "convergence dfGPdfNN Run 4/10, Epoch 127/1000, Training Loss (NLML): -950.0011\n",
      "convergence dfGPdfNN Run 4/10, Epoch 128/1000, Training Loss (NLML): -950.1268\n",
      "convergence dfGPdfNN Run 4/10, Epoch 129/1000, Training Loss (NLML): -950.1497\n",
      "convergence dfGPdfNN Run 4/10, Epoch 130/1000, Training Loss (NLML): -946.6564\n",
      "convergence dfGPdfNN Run 4/10, Epoch 131/1000, Training Loss (NLML): -950.0089\n",
      "convergence dfGPdfNN Run 4/10, Epoch 132/1000, Training Loss (NLML): -950.0278\n",
      "convergence dfGPdfNN Run 4/10, Epoch 133/1000, Training Loss (NLML): -950.2621\n",
      "convergence dfGPdfNN Run 4/10, Epoch 134/1000, Training Loss (NLML): -950.2571\n",
      "convergence dfGPdfNN Run 4/10, Epoch 135/1000, Training Loss (NLML): -950.3485\n",
      "convergence dfGPdfNN Run 4/10, Epoch 136/1000, Training Loss (NLML): -950.4136\n",
      "convergence dfGPdfNN Run 4/10, Epoch 137/1000, Training Loss (NLML): -950.4233\n",
      "convergence dfGPdfNN Run 4/10, Epoch 138/1000, Training Loss (NLML): -950.3195\n",
      "convergence dfGPdfNN Run 4/10, Epoch 139/1000, Training Loss (NLML): -946.6348\n",
      "convergence dfGPdfNN Run 4/10, Epoch 140/1000, Training Loss (NLML): -948.5078\n",
      "convergence dfGPdfNN Run 4/10, Epoch 141/1000, Training Loss (NLML): -950.0879\n",
      "convergence dfGPdfNN Run 4/10, Epoch 142/1000, Training Loss (NLML): -950.2557\n",
      "convergence dfGPdfNN Run 4/10, Epoch 143/1000, Training Loss (NLML): -949.9177\n",
      "convergence dfGPdfNN Run 4/10, Epoch 144/1000, Training Loss (NLML): -950.4052\n",
      "convergence dfGPdfNN Run 4/10, Epoch 145/1000, Training Loss (NLML): -950.5068\n",
      "convergence dfGPdfNN Run 4/10, Epoch 146/1000, Training Loss (NLML): -950.5051\n",
      "convergence dfGPdfNN Run 4/10, Epoch 147/1000, Training Loss (NLML): -948.3989\n",
      "convergence dfGPdfNN Run 4/10, Epoch 148/1000, Training Loss (NLML): -950.8213\n",
      "convergence dfGPdfNN Run 4/10, Epoch 149/1000, Training Loss (NLML): -950.8197\n",
      "convergence dfGPdfNN Run 4/10, Epoch 150/1000, Training Loss (NLML): -950.9763\n",
      "convergence dfGPdfNN Run 4/10, Epoch 151/1000, Training Loss (NLML): -950.2195\n",
      "convergence dfGPdfNN Run 4/10, Epoch 152/1000, Training Loss (NLML): -949.0127\n",
      "convergence dfGPdfNN Run 4/10, Epoch 153/1000, Training Loss (NLML): -951.4403\n",
      "convergence dfGPdfNN Run 4/10, Epoch 154/1000, Training Loss (NLML): -951.0707\n",
      "convergence dfGPdfNN Run 4/10, Epoch 155/1000, Training Loss (NLML): -950.4314\n",
      "convergence dfGPdfNN Run 4/10, Epoch 156/1000, Training Loss (NLML): -950.6023\n",
      "convergence dfGPdfNN Run 4/10, Epoch 157/1000, Training Loss (NLML): -948.9901\n",
      "convergence dfGPdfNN Run 4/10, Epoch 158/1000, Training Loss (NLML): -951.1455\n",
      "convergence dfGPdfNN Run 4/10, Epoch 159/1000, Training Loss (NLML): -949.0957\n",
      "convergence dfGPdfNN Run 4/10, Epoch 160/1000, Training Loss (NLML): -951.4532\n",
      "convergence dfGPdfNN Run 4/10, Epoch 161/1000, Training Loss (NLML): -951.4700\n",
      "convergence dfGPdfNN Run 4/10, Epoch 162/1000, Training Loss (NLML): -951.6677\n",
      "convergence dfGPdfNN Run 4/10, Epoch 163/1000, Training Loss (NLML): -951.4659\n",
      "convergence dfGPdfNN Run 4/10, Epoch 164/1000, Training Loss (NLML): -951.4142\n",
      "convergence dfGPdfNN Run 4/10, Epoch 165/1000, Training Loss (NLML): -951.3424\n",
      "convergence dfGPdfNN Run 4/10, Epoch 166/1000, Training Loss (NLML): -951.4429\n",
      "convergence dfGPdfNN Run 4/10, Epoch 167/1000, Training Loss (NLML): -951.5320\n",
      "convergence dfGPdfNN Run 4/10, Epoch 168/1000, Training Loss (NLML): -951.7421\n",
      "convergence dfGPdfNN Run 4/10, Epoch 169/1000, Training Loss (NLML): -951.9098\n",
      "convergence dfGPdfNN Run 4/10, Epoch 170/1000, Training Loss (NLML): -952.1315\n",
      "convergence dfGPdfNN Run 4/10, Epoch 171/1000, Training Loss (NLML): -952.1433\n",
      "convergence dfGPdfNN Run 4/10, Epoch 172/1000, Training Loss (NLML): -952.2120\n",
      "convergence dfGPdfNN Run 4/10, Epoch 173/1000, Training Loss (NLML): -952.2455\n",
      "convergence dfGPdfNN Run 4/10, Epoch 174/1000, Training Loss (NLML): -951.5808\n",
      "convergence dfGPdfNN Run 4/10, Epoch 175/1000, Training Loss (NLML): -951.5527\n",
      "convergence dfGPdfNN Run 4/10, Epoch 176/1000, Training Loss (NLML): -951.0195\n",
      "convergence dfGPdfNN Run 4/10, Epoch 177/1000, Training Loss (NLML): -951.8140\n",
      "convergence dfGPdfNN Run 4/10, Epoch 178/1000, Training Loss (NLML): -951.9553\n",
      "convergence dfGPdfNN Run 4/10, Epoch 179/1000, Training Loss (NLML): -952.0137\n",
      "convergence dfGPdfNN Run 4/10, Epoch 180/1000, Training Loss (NLML): -952.0450\n",
      "convergence dfGPdfNN Run 4/10, Epoch 181/1000, Training Loss (NLML): -952.2048\n",
      "convergence dfGPdfNN Run 4/10, Epoch 182/1000, Training Loss (NLML): -952.1975\n",
      "convergence dfGPdfNN Run 4/10, Epoch 183/1000, Training Loss (NLML): -952.2488\n",
      "convergence dfGPdfNN Run 4/10, Epoch 184/1000, Training Loss (NLML): -952.1566\n",
      "convergence dfGPdfNN Run 4/10, Epoch 185/1000, Training Loss (NLML): -951.9203\n",
      "convergence dfGPdfNN Run 4/10, Epoch 186/1000, Training Loss (NLML): -952.1909\n",
      "convergence dfGPdfNN Run 4/10, Epoch 187/1000, Training Loss (NLML): -952.2097\n",
      "convergence dfGPdfNN Run 4/10, Epoch 188/1000, Training Loss (NLML): -952.2584\n",
      "convergence dfGPdfNN Run 4/10, Epoch 189/1000, Training Loss (NLML): -952.2081\n",
      "convergence dfGPdfNN Run 4/10, Epoch 190/1000, Training Loss (NLML): -952.1979\n",
      "convergence dfGPdfNN Run 4/10, Epoch 191/1000, Training Loss (NLML): -952.2191\n",
      "convergence dfGPdfNN Run 4/10, Epoch 192/1000, Training Loss (NLML): -952.2570\n",
      "convergence dfGPdfNN Run 4/10, Epoch 193/1000, Training Loss (NLML): -946.7457\n",
      "convergence dfGPdfNN Run 4/10, Epoch 194/1000, Training Loss (NLML): -952.1134\n",
      "convergence dfGPdfNN Run 4/10, Epoch 195/1000, Training Loss (NLML): -952.1053\n",
      "convergence dfGPdfNN Run 4/10, Epoch 196/1000, Training Loss (NLML): -952.2703\n",
      "convergence dfGPdfNN Run 4/10, Epoch 197/1000, Training Loss (NLML): -952.2378\n",
      "convergence dfGPdfNN Run 4/10, Epoch 198/1000, Training Loss (NLML): -952.1964\n",
      "convergence dfGPdfNN Run 4/10, Epoch 199/1000, Training Loss (NLML): -952.2366\n",
      "convergence dfGPdfNN Run 4/10, Epoch 200/1000, Training Loss (NLML): -952.2196\n",
      "convergence dfGPdfNN Run 4/10, Epoch 201/1000, Training Loss (NLML): -952.0801\n",
      "convergence dfGPdfNN Run 4/10, Epoch 202/1000, Training Loss (NLML): -952.1571\n",
      "convergence dfGPdfNN Run 4/10, Epoch 203/1000, Training Loss (NLML): -952.2872\n",
      "convergence dfGPdfNN Run 4/10, Epoch 204/1000, Training Loss (NLML): -952.1252\n",
      "convergence dfGPdfNN Run 4/10, Epoch 205/1000, Training Loss (NLML): -952.3184\n",
      "convergence dfGPdfNN Run 4/10, Epoch 206/1000, Training Loss (NLML): -952.6252\n",
      "convergence dfGPdfNN Run 4/10, Epoch 207/1000, Training Loss (NLML): -952.3732\n",
      "convergence dfGPdfNN Run 4/10, Epoch 208/1000, Training Loss (NLML): -952.4043\n",
      "convergence dfGPdfNN Run 4/10, Epoch 209/1000, Training Loss (NLML): -951.5668\n",
      "convergence dfGPdfNN Run 4/10, Epoch 210/1000, Training Loss (NLML): -952.7744\n",
      "convergence dfGPdfNN Run 4/10, Epoch 211/1000, Training Loss (NLML): -952.4648\n",
      "convergence dfGPdfNN Run 4/10, Epoch 212/1000, Training Loss (NLML): -951.7958\n",
      "convergence dfGPdfNN Run 4/10, Epoch 213/1000, Training Loss (NLML): -952.2146\n",
      "convergence dfGPdfNN Run 4/10, Epoch 214/1000, Training Loss (NLML): -952.3313\n",
      "convergence dfGPdfNN Run 4/10, Epoch 215/1000, Training Loss (NLML): -952.6740\n",
      "convergence dfGPdfNN Run 4/10, Epoch 216/1000, Training Loss (NLML): -952.6096\n",
      "convergence dfGPdfNN Run 4/10, Epoch 217/1000, Training Loss (NLML): -952.4144\n",
      "convergence dfGPdfNN Run 4/10, Epoch 218/1000, Training Loss (NLML): -952.4613\n",
      "convergence dfGPdfNN Run 4/10, Epoch 219/1000, Training Loss (NLML): -952.5693\n",
      "convergence dfGPdfNN Run 4/10, Epoch 220/1000, Training Loss (NLML): -952.6571\n",
      "convergence dfGPdfNN Run 4/10, Epoch 221/1000, Training Loss (NLML): -952.7469\n",
      "convergence dfGPdfNN Run 4/10, Epoch 222/1000, Training Loss (NLML): -952.8623\n",
      "convergence dfGPdfNN Run 4/10, Epoch 223/1000, Training Loss (NLML): -952.9082\n",
      "convergence dfGPdfNN Run 4/10, Epoch 224/1000, Training Loss (NLML): -952.7776\n",
      "convergence dfGPdfNN Run 4/10, Epoch 225/1000, Training Loss (NLML): -952.7860\n",
      "convergence dfGPdfNN Run 4/10, Epoch 226/1000, Training Loss (NLML): -953.1516\n",
      "convergence dfGPdfNN Run 4/10, Epoch 227/1000, Training Loss (NLML): -953.2683\n",
      "convergence dfGPdfNN Run 4/10, Epoch 228/1000, Training Loss (NLML): -952.8292\n",
      "convergence dfGPdfNN Run 4/10, Epoch 229/1000, Training Loss (NLML): -952.1499\n",
      "convergence dfGPdfNN Run 4/10, Epoch 230/1000, Training Loss (NLML): -952.7168\n",
      "convergence dfGPdfNN Run 4/10, Epoch 231/1000, Training Loss (NLML): -953.0321\n",
      "convergence dfGPdfNN Run 4/10, Epoch 232/1000, Training Loss (NLML): -952.5977\n",
      "convergence dfGPdfNN Run 4/10, Epoch 233/1000, Training Loss (NLML): -952.1125\n",
      "convergence dfGPdfNN Run 4/10, Epoch 234/1000, Training Loss (NLML): -951.8710\n",
      "convergence dfGPdfNN Run 4/10, Epoch 235/1000, Training Loss (NLML): -952.6138\n",
      "convergence dfGPdfNN Run 4/10, Epoch 236/1000, Training Loss (NLML): -952.7323\n",
      "convergence dfGPdfNN Run 4/10, Epoch 237/1000, Training Loss (NLML): -952.8232\n",
      "convergence dfGPdfNN Run 4/10, Epoch 238/1000, Training Loss (NLML): -952.4569\n",
      "convergence dfGPdfNN Run 4/10, Epoch 239/1000, Training Loss (NLML): -952.9673\n",
      "convergence dfGPdfNN Run 4/10, Epoch 240/1000, Training Loss (NLML): -952.9924\n",
      "convergence dfGPdfNN Run 4/10, Epoch 241/1000, Training Loss (NLML): -953.0906\n",
      "convergence dfGPdfNN Run 4/10, Epoch 242/1000, Training Loss (NLML): -953.1136\n",
      "convergence dfGPdfNN Run 4/10, Epoch 243/1000, Training Loss (NLML): -953.0245\n",
      "convergence dfGPdfNN Run 4/10, Epoch 244/1000, Training Loss (NLML): -947.2738\n",
      "convergence dfGPdfNN Run 4/10, Epoch 245/1000, Training Loss (NLML): -952.7725\n",
      "convergence dfGPdfNN Run 4/10, Epoch 246/1000, Training Loss (NLML): -952.4006\n",
      "convergence dfGPdfNN Run 4/10, Epoch 247/1000, Training Loss (NLML): -951.9788\n",
      "convergence dfGPdfNN Run 4/10, Epoch 248/1000, Training Loss (NLML): -952.8508\n",
      "convergence dfGPdfNN Run 4/10, Epoch 249/1000, Training Loss (NLML): -952.7777\n",
      "convergence dfGPdfNN Run 4/10, Epoch 250/1000, Training Loss (NLML): -953.1620\n",
      "convergence dfGPdfNN Run 4/10, Epoch 251/1000, Training Loss (NLML): -953.2308\n",
      "convergence dfGPdfNN Run 4/10, Epoch 252/1000, Training Loss (NLML): -953.0485\n",
      "convergence dfGPdfNN Run 4/10, Epoch 253/1000, Training Loss (NLML): -953.0375\n",
      "convergence dfGPdfNN Run 4/10, Epoch 254/1000, Training Loss (NLML): -952.7184\n",
      "convergence dfGPdfNN Run 4/10, Epoch 255/1000, Training Loss (NLML): -951.5707\n",
      "convergence dfGPdfNN Run 4/10, Epoch 256/1000, Training Loss (NLML): -953.6484\n",
      "convergence dfGPdfNN Run 4/10, Epoch 257/1000, Training Loss (NLML): -952.8201\n",
      "convergence dfGPdfNN Run 4/10, Epoch 258/1000, Training Loss (NLML): -952.7410\n",
      "convergence dfGPdfNN Run 4/10, Epoch 259/1000, Training Loss (NLML): -953.1290\n",
      "convergence dfGPdfNN Run 4/10, Epoch 260/1000, Training Loss (NLML): -953.2858\n",
      "convergence dfGPdfNN Run 4/10, Epoch 261/1000, Training Loss (NLML): -953.1300\n",
      "convergence dfGPdfNN Run 4/10, Epoch 262/1000, Training Loss (NLML): -953.0848\n",
      "convergence dfGPdfNN Run 4/10, Epoch 263/1000, Training Loss (NLML): -951.6229\n",
      "convergence dfGPdfNN Run 4/10, Epoch 264/1000, Training Loss (NLML): -951.8977\n",
      "convergence dfGPdfNN Run 4/10, Epoch 265/1000, Training Loss (NLML): -953.0243\n",
      "convergence dfGPdfNN Run 4/10, Epoch 266/1000, Training Loss (NLML): -952.7600\n",
      "convergence dfGPdfNN Run 4/10, Epoch 267/1000, Training Loss (NLML): -952.7074\n",
      "convergence dfGPdfNN Run 4/10, Epoch 268/1000, Training Loss (NLML): -952.8286\n",
      "convergence dfGPdfNN Run 4/10, Epoch 269/1000, Training Loss (NLML): -952.7262\n",
      "convergence dfGPdfNN Run 4/10, Epoch 270/1000, Training Loss (NLML): -952.4758\n",
      "convergence dfGPdfNN Run 4/10, Epoch 271/1000, Training Loss (NLML): -952.1213\n",
      "convergence dfGPdfNN Run 4/10, Epoch 272/1000, Training Loss (NLML): -952.2982\n",
      "convergence dfGPdfNN Run 4/10, Epoch 273/1000, Training Loss (NLML): -952.9602\n",
      "convergence dfGPdfNN Run 4/10, Epoch 274/1000, Training Loss (NLML): -953.1002\n",
      "convergence dfGPdfNN Run 4/10, Epoch 275/1000, Training Loss (NLML): -953.3767\n",
      "convergence dfGPdfNN Run 4/10, Epoch 276/1000, Training Loss (NLML): -953.0706\n",
      "convergence dfGPdfNN Run 4/10, Epoch 277/1000, Training Loss (NLML): -953.1316\n",
      "convergence dfGPdfNN Run 4/10, Epoch 278/1000, Training Loss (NLML): -952.9697\n",
      "convergence dfGPdfNN Run 4/10, Epoch 279/1000, Training Loss (NLML): -953.5162\n",
      "convergence dfGPdfNN Run 4/10, Epoch 280/1000, Training Loss (NLML): -951.6671\n",
      "convergence dfGPdfNN Run 4/10, Epoch 281/1000, Training Loss (NLML): -953.7386\n",
      "convergence dfGPdfNN Run 4/10, Epoch 282/1000, Training Loss (NLML): -954.2715\n",
      "convergence dfGPdfNN Run 4/10, Epoch 283/1000, Training Loss (NLML): -954.0223\n",
      "convergence dfGPdfNN Run 4/10, Epoch 284/1000, Training Loss (NLML): -953.8314\n",
      "convergence dfGPdfNN Run 4/10, Epoch 285/1000, Training Loss (NLML): -953.7545\n",
      "convergence dfGPdfNN Run 4/10, Epoch 286/1000, Training Loss (NLML): -953.8398\n",
      "convergence dfGPdfNN Run 4/10, Epoch 287/1000, Training Loss (NLML): -953.4191\n",
      "convergence dfGPdfNN Run 4/10, Epoch 288/1000, Training Loss (NLML): -954.4579\n",
      "convergence dfGPdfNN Run 4/10, Epoch 289/1000, Training Loss (NLML): -954.3405\n",
      "convergence dfGPdfNN Run 4/10, Epoch 290/1000, Training Loss (NLML): -954.3356\n",
      "convergence dfGPdfNN Run 4/10, Epoch 291/1000, Training Loss (NLML): -954.3555\n",
      "convergence dfGPdfNN Run 4/10, Epoch 292/1000, Training Loss (NLML): -954.4762\n",
      "convergence dfGPdfNN Run 4/10, Epoch 293/1000, Training Loss (NLML): -954.6273\n",
      "convergence dfGPdfNN Run 4/10, Epoch 294/1000, Training Loss (NLML): -954.5557\n",
      "convergence dfGPdfNN Run 4/10, Epoch 295/1000, Training Loss (NLML): -954.5183\n",
      "convergence dfGPdfNN Run 4/10, Epoch 296/1000, Training Loss (NLML): -954.3439\n",
      "convergence dfGPdfNN Run 4/10, Epoch 297/1000, Training Loss (NLML): -954.3405\n",
      "convergence dfGPdfNN Run 4/10, Epoch 298/1000, Training Loss (NLML): -954.4773\n",
      "convergence dfGPdfNN Run 4/10, Epoch 299/1000, Training Loss (NLML): -952.7699\n",
      "convergence dfGPdfNN Run 4/10, Epoch 300/1000, Training Loss (NLML): -954.7856\n",
      "convergence dfGPdfNN Run 4/10, Epoch 301/1000, Training Loss (NLML): -954.8376\n",
      "convergence dfGPdfNN Run 4/10, Epoch 302/1000, Training Loss (NLML): -954.8917\n",
      "convergence dfGPdfNN Run 4/10, Epoch 303/1000, Training Loss (NLML): -954.8773\n",
      "convergence dfGPdfNN Run 4/10, Epoch 304/1000, Training Loss (NLML): -954.9089\n",
      "convergence dfGPdfNN Run 4/10, Epoch 305/1000, Training Loss (NLML): -954.9351\n",
      "convergence dfGPdfNN Run 4/10, Epoch 306/1000, Training Loss (NLML): -954.9645\n",
      "convergence dfGPdfNN Run 4/10, Epoch 307/1000, Training Loss (NLML): -954.9744\n",
      "convergence dfGPdfNN Run 4/10, Epoch 308/1000, Training Loss (NLML): -955.0299\n",
      "convergence dfGPdfNN Run 4/10, Epoch 309/1000, Training Loss (NLML): -955.0558\n",
      "convergence dfGPdfNN Run 4/10, Epoch 310/1000, Training Loss (NLML): -954.9681\n",
      "convergence dfGPdfNN Run 4/10, Epoch 311/1000, Training Loss (NLML): -954.8158\n",
      "convergence dfGPdfNN Run 4/10, Epoch 312/1000, Training Loss (NLML): -954.7711\n",
      "convergence dfGPdfNN Run 4/10, Epoch 313/1000, Training Loss (NLML): -954.5173\n",
      "convergence dfGPdfNN Run 4/10, Epoch 314/1000, Training Loss (NLML): -954.8750\n",
      "convergence dfGPdfNN Run 4/10, Epoch 315/1000, Training Loss (NLML): -954.8656\n",
      "convergence dfGPdfNN Run 4/10, Epoch 316/1000, Training Loss (NLML): -953.4421\n",
      "convergence dfGPdfNN Run 4/10, Epoch 317/1000, Training Loss (NLML): -954.9021\n",
      "convergence dfGPdfNN Run 4/10, Epoch 318/1000, Training Loss (NLML): -954.9197\n",
      "convergence dfGPdfNN Run 4/10, Epoch 319/1000, Training Loss (NLML): -954.9622\n",
      "convergence dfGPdfNN Run 4/10, Epoch 320/1000, Training Loss (NLML): -954.9995\n",
      "convergence dfGPdfNN Run 4/10, Epoch 321/1000, Training Loss (NLML): -955.1750\n",
      "convergence dfGPdfNN Run 4/10, Epoch 322/1000, Training Loss (NLML): -955.3483\n",
      "convergence dfGPdfNN Run 4/10, Epoch 323/1000, Training Loss (NLML): -955.3824\n",
      "convergence dfGPdfNN Run 4/10, Epoch 324/1000, Training Loss (NLML): -955.3456\n",
      "convergence dfGPdfNN Run 4/10, Epoch 325/1000, Training Loss (NLML): -955.2725\n",
      "convergence dfGPdfNN Run 4/10, Epoch 326/1000, Training Loss (NLML): -955.2570\n",
      "convergence dfGPdfNN Run 4/10, Epoch 327/1000, Training Loss (NLML): -955.2849\n",
      "convergence dfGPdfNN Run 4/10, Epoch 328/1000, Training Loss (NLML): -955.3701\n",
      "convergence dfGPdfNN Run 4/10, Epoch 329/1000, Training Loss (NLML): -955.3163\n",
      "convergence dfGPdfNN Run 4/10, Epoch 330/1000, Training Loss (NLML): -955.1903\n",
      "convergence dfGPdfNN Run 4/10, Epoch 331/1000, Training Loss (NLML): -955.1344\n",
      "convergence dfGPdfNN Run 4/10, Epoch 332/1000, Training Loss (NLML): -955.1099\n",
      "convergence dfGPdfNN Run 4/10, Epoch 333/1000, Training Loss (NLML): -955.0918\n",
      "convergence dfGPdfNN Run 4/10, Epoch 334/1000, Training Loss (NLML): -955.1042\n",
      "convergence dfGPdfNN Run 4/10, Epoch 335/1000, Training Loss (NLML): -954.7035\n",
      "convergence dfGPdfNN Run 4/10, Epoch 336/1000, Training Loss (NLML): -955.1139\n",
      "convergence dfGPdfNN Run 4/10, Epoch 337/1000, Training Loss (NLML): -955.1964\n",
      "convergence dfGPdfNN Run 4/10, Epoch 338/1000, Training Loss (NLML): -955.2809\n",
      "convergence dfGPdfNN Run 4/10, Epoch 339/1000, Training Loss (NLML): -955.4644\n",
      "convergence dfGPdfNN Run 4/10, Epoch 340/1000, Training Loss (NLML): -955.6937\n",
      "convergence dfGPdfNN Run 4/10, Epoch 341/1000, Training Loss (NLML): -954.9232\n",
      "convergence dfGPdfNN Run 4/10, Epoch 342/1000, Training Loss (NLML): -955.6179\n",
      "convergence dfGPdfNN Run 4/10, Epoch 343/1000, Training Loss (NLML): -955.4167\n",
      "convergence dfGPdfNN Run 4/10, Epoch 344/1000, Training Loss (NLML): -955.2716\n",
      "convergence dfGPdfNN Run 4/10, Epoch 345/1000, Training Loss (NLML): -955.2958\n",
      "convergence dfGPdfNN Run 4/10, Epoch 346/1000, Training Loss (NLML): -955.3320\n",
      "convergence dfGPdfNN Run 4/10, Epoch 347/1000, Training Loss (NLML): -955.4368\n",
      "convergence dfGPdfNN Run 4/10, Epoch 348/1000, Training Loss (NLML): -955.6138\n",
      "convergence dfGPdfNN Run 4/10, Epoch 349/1000, Training Loss (NLML): -955.7350\n",
      "convergence dfGPdfNN Run 4/10, Epoch 350/1000, Training Loss (NLML): -955.7424\n",
      "convergence dfGPdfNN Run 4/10, Epoch 351/1000, Training Loss (NLML): -955.7955\n",
      "convergence dfGPdfNN Run 4/10, Epoch 352/1000, Training Loss (NLML): -955.8245\n",
      "convergence dfGPdfNN Run 4/10, Epoch 353/1000, Training Loss (NLML): -955.9102\n",
      "convergence dfGPdfNN Run 4/10, Epoch 354/1000, Training Loss (NLML): -955.9357\n",
      "convergence dfGPdfNN Run 4/10, Epoch 355/1000, Training Loss (NLML): -955.8744\n",
      "convergence dfGPdfNN Run 4/10, Epoch 356/1000, Training Loss (NLML): -955.8817\n",
      "convergence dfGPdfNN Run 4/10, Epoch 357/1000, Training Loss (NLML): -955.7910\n",
      "convergence dfGPdfNN Run 4/10, Epoch 358/1000, Training Loss (NLML): -955.7882\n",
      "convergence dfGPdfNN Run 4/10, Epoch 359/1000, Training Loss (NLML): -955.9489\n",
      "convergence dfGPdfNN Run 4/10, Epoch 360/1000, Training Loss (NLML): -955.9752\n",
      "convergence dfGPdfNN Run 4/10, Epoch 361/1000, Training Loss (NLML): -956.0881\n",
      "convergence dfGPdfNN Run 4/10, Epoch 362/1000, Training Loss (NLML): -956.1033\n",
      "convergence dfGPdfNN Run 4/10, Epoch 363/1000, Training Loss (NLML): -956.0940\n",
      "convergence dfGPdfNN Run 4/10, Epoch 364/1000, Training Loss (NLML): -956.1205\n",
      "convergence dfGPdfNN Run 4/10, Epoch 365/1000, Training Loss (NLML): -956.0914\n",
      "convergence dfGPdfNN Run 4/10, Epoch 366/1000, Training Loss (NLML): -956.0283\n",
      "convergence dfGPdfNN Run 4/10, Epoch 367/1000, Training Loss (NLML): -955.8434\n",
      "convergence dfGPdfNN Run 4/10, Epoch 368/1000, Training Loss (NLML): -955.7592\n",
      "convergence dfGPdfNN Run 4/10, Epoch 369/1000, Training Loss (NLML): -955.8059\n",
      "convergence dfGPdfNN Run 4/10, Epoch 370/1000, Training Loss (NLML): -955.8287\n",
      "convergence dfGPdfNN Run 4/10, Epoch 371/1000, Training Loss (NLML): -955.8575\n",
      "convergence dfGPdfNN Run 4/10, Epoch 372/1000, Training Loss (NLML): -955.8717\n",
      "convergence dfGPdfNN Run 4/10, Epoch 373/1000, Training Loss (NLML): -955.8873\n",
      "convergence dfGPdfNN Run 4/10, Epoch 374/1000, Training Loss (NLML): -955.7565\n",
      "convergence dfGPdfNN Run 4/10, Epoch 375/1000, Training Loss (NLML): -955.9451\n",
      "convergence dfGPdfNN Run 4/10, Epoch 376/1000, Training Loss (NLML): -955.4320\n",
      "convergence dfGPdfNN Run 4/10, Epoch 377/1000, Training Loss (NLML): -956.2065\n",
      "convergence dfGPdfNN Run 4/10, Epoch 378/1000, Training Loss (NLML): -956.2996\n",
      "convergence dfGPdfNN Run 4/10, Epoch 379/1000, Training Loss (NLML): -956.2322\n",
      "convergence dfGPdfNN Run 4/10, Epoch 380/1000, Training Loss (NLML): -956.2240\n",
      "convergence dfGPdfNN Run 4/10, Epoch 381/1000, Training Loss (NLML): -956.2632\n",
      "convergence dfGPdfNN Run 4/10, Epoch 382/1000, Training Loss (NLML): -956.1675\n",
      "convergence dfGPdfNN Run 4/10, Epoch 383/1000, Training Loss (NLML): -955.9596\n",
      "convergence dfGPdfNN Run 4/10, Epoch 384/1000, Training Loss (NLML): -955.9292\n",
      "convergence dfGPdfNN Run 4/10, Epoch 385/1000, Training Loss (NLML): -956.0167\n",
      "convergence dfGPdfNN Run 4/10, Epoch 386/1000, Training Loss (NLML): -955.9927\n",
      "convergence dfGPdfNN Run 4/10, Epoch 387/1000, Training Loss (NLML): -955.9004\n",
      "convergence dfGPdfNN Run 4/10, Epoch 388/1000, Training Loss (NLML): -955.8131\n",
      "convergence dfGPdfNN Run 4/10, Epoch 389/1000, Training Loss (NLML): -955.9740\n",
      "convergence dfGPdfNN Run 4/10, Epoch 390/1000, Training Loss (NLML): -955.9154\n",
      "convergence dfGPdfNN Run 4/10, Epoch 391/1000, Training Loss (NLML): -955.9198\n",
      "convergence dfGPdfNN Run 4/10, Epoch 392/1000, Training Loss (NLML): -955.9990\n",
      "convergence dfGPdfNN Run 4/10, Epoch 393/1000, Training Loss (NLML): -956.0006\n",
      "convergence dfGPdfNN Run 4/10, Epoch 394/1000, Training Loss (NLML): -956.0380\n",
      "convergence dfGPdfNN Run 4/10, Epoch 395/1000, Training Loss (NLML): -955.7980\n",
      "convergence dfGPdfNN Run 4/10, Epoch 396/1000, Training Loss (NLML): -956.1912\n",
      "convergence dfGPdfNN Run 4/10, Epoch 397/1000, Training Loss (NLML): -956.1860\n",
      "convergence dfGPdfNN Run 4/10, Epoch 398/1000, Training Loss (NLML): -956.4475\n",
      "convergence dfGPdfNN Run 4/10, Epoch 399/1000, Training Loss (NLML): -956.5521\n",
      "convergence dfGPdfNN Run 4/10, Epoch 400/1000, Training Loss (NLML): -954.8030\n",
      "convergence dfGPdfNN Run 4/10, Epoch 401/1000, Training Loss (NLML): -956.5250\n",
      "convergence dfGPdfNN Run 4/10, Epoch 402/1000, Training Loss (NLML): -956.5098\n",
      "convergence dfGPdfNN Run 4/10, Epoch 403/1000, Training Loss (NLML): -956.4553\n",
      "convergence dfGPdfNN Run 4/10, Epoch 404/1000, Training Loss (NLML): -956.4742\n",
      "convergence dfGPdfNN Run 4/10, Epoch 405/1000, Training Loss (NLML): -956.4760\n",
      "convergence dfGPdfNN Run 4/10, Epoch 406/1000, Training Loss (NLML): -956.5442\n",
      "convergence dfGPdfNN Run 4/10, Epoch 407/1000, Training Loss (NLML): -956.5537\n",
      "convergence dfGPdfNN Run 4/10, Epoch 408/1000, Training Loss (NLML): -956.5073\n",
      "convergence dfGPdfNN Run 4/10, Epoch 409/1000, Training Loss (NLML): -956.4227\n",
      "convergence dfGPdfNN Run 4/10, Epoch 410/1000, Training Loss (NLML): -956.2551\n",
      "convergence dfGPdfNN Run 4/10, Epoch 411/1000, Training Loss (NLML): -955.3494\n",
      "convergence dfGPdfNN Run 4/10, Epoch 412/1000, Training Loss (NLML): -956.2324\n",
      "convergence dfGPdfNN Run 4/10, Epoch 413/1000, Training Loss (NLML): -956.2524\n",
      "convergence dfGPdfNN Run 4/10, Epoch 414/1000, Training Loss (NLML): -956.2985\n",
      "convergence dfGPdfNN Run 4/10, Epoch 415/1000, Training Loss (NLML): -956.3444\n",
      "convergence dfGPdfNN Run 4/10, Epoch 416/1000, Training Loss (NLML): -956.5485\n",
      "convergence dfGPdfNN Run 4/10, Epoch 417/1000, Training Loss (NLML): -956.7697\n",
      "convergence dfGPdfNN Run 4/10, Epoch 418/1000, Training Loss (NLML): -956.8547\n",
      "convergence dfGPdfNN Run 4/10, Epoch 419/1000, Training Loss (NLML): -956.9114\n",
      "convergence dfGPdfNN Run 4/10, Epoch 420/1000, Training Loss (NLML): -956.8579\n",
      "convergence dfGPdfNN Run 4/10, Epoch 421/1000, Training Loss (NLML): -955.0405\n",
      "convergence dfGPdfNN Run 4/10, Epoch 422/1000, Training Loss (NLML): -956.5421\n",
      "convergence dfGPdfNN Run 4/10, Epoch 423/1000, Training Loss (NLML): -956.3947\n",
      "convergence dfGPdfNN Run 4/10, Epoch 424/1000, Training Loss (NLML): -956.4858\n",
      "convergence dfGPdfNN Run 4/10, Epoch 425/1000, Training Loss (NLML): -956.5746\n",
      "convergence dfGPdfNN Run 4/10, Epoch 426/1000, Training Loss (NLML): -956.7133\n",
      "convergence dfGPdfNN Run 4/10, Epoch 427/1000, Training Loss (NLML): -956.8276\n",
      "convergence dfGPdfNN Run 4/10, Epoch 428/1000, Training Loss (NLML): -956.5077\n",
      "convergence dfGPdfNN Run 4/10, Epoch 429/1000, Training Loss (NLML): -956.9580\n",
      "convergence dfGPdfNN Run 4/10, Epoch 430/1000, Training Loss (NLML): -956.9788\n",
      "convergence dfGPdfNN Run 4/10, Epoch 431/1000, Training Loss (NLML): -957.0033\n",
      "convergence dfGPdfNN Run 4/10, Epoch 432/1000, Training Loss (NLML): -957.0281\n",
      "convergence dfGPdfNN Run 4/10, Epoch 433/1000, Training Loss (NLML): -956.8865\n",
      "convergence dfGPdfNN Run 4/10, Epoch 434/1000, Training Loss (NLML): -956.8218\n",
      "convergence dfGPdfNN Run 4/10, Epoch 435/1000, Training Loss (NLML): -956.7699\n",
      "convergence dfGPdfNN Run 4/10, Epoch 436/1000, Training Loss (NLML): -956.7791\n",
      "convergence dfGPdfNN Run 4/10, Epoch 437/1000, Training Loss (NLML): -956.7042\n",
      "convergence dfGPdfNN Run 4/10, Epoch 438/1000, Training Loss (NLML): -956.8842\n",
      "convergence dfGPdfNN Run 4/10, Epoch 439/1000, Training Loss (NLML): -956.9941\n",
      "convergence dfGPdfNN Run 4/10, Epoch 440/1000, Training Loss (NLML): -957.0099\n",
      "convergence dfGPdfNN Run 4/10, Epoch 441/1000, Training Loss (NLML): -957.0895\n",
      "convergence dfGPdfNN Run 4/10, Epoch 442/1000, Training Loss (NLML): -957.1222\n",
      "convergence dfGPdfNN Run 4/10, Epoch 443/1000, Training Loss (NLML): -957.1470\n",
      "convergence dfGPdfNN Run 4/10, Epoch 444/1000, Training Loss (NLML): -957.1866\n",
      "convergence dfGPdfNN Run 4/10, Epoch 445/1000, Training Loss (NLML): -957.1913\n",
      "convergence dfGPdfNN Run 4/10, Epoch 446/1000, Training Loss (NLML): -957.2163\n",
      "convergence dfGPdfNN Run 4/10, Epoch 447/1000, Training Loss (NLML): -957.2236\n",
      "convergence dfGPdfNN Run 4/10, Epoch 448/1000, Training Loss (NLML): -957.1765\n",
      "convergence dfGPdfNN Run 4/10, Epoch 449/1000, Training Loss (NLML): -957.1572\n",
      "convergence dfGPdfNN Run 4/10, Epoch 450/1000, Training Loss (NLML): -957.2313\n",
      "convergence dfGPdfNN Run 4/10, Epoch 451/1000, Training Loss (NLML): -957.2340\n",
      "convergence dfGPdfNN Run 4/10, Epoch 452/1000, Training Loss (NLML): -957.2581\n",
      "convergence dfGPdfNN Run 4/10, Epoch 453/1000, Training Loss (NLML): -957.2747\n",
      "convergence dfGPdfNN Run 4/10, Epoch 454/1000, Training Loss (NLML): -957.3938\n",
      "convergence dfGPdfNN Run 4/10, Epoch 455/1000, Training Loss (NLML): -957.3939\n",
      "convergence dfGPdfNN Run 4/10, Epoch 456/1000, Training Loss (NLML): -957.4017\n",
      "convergence dfGPdfNN Run 4/10, Epoch 457/1000, Training Loss (NLML): -957.4323\n",
      "convergence dfGPdfNN Run 4/10, Epoch 458/1000, Training Loss (NLML): -957.1241\n",
      "convergence dfGPdfNN Run 4/10, Epoch 459/1000, Training Loss (NLML): -957.4124\n",
      "convergence dfGPdfNN Run 4/10, Epoch 460/1000, Training Loss (NLML): -957.4261\n",
      "convergence dfGPdfNN Run 4/10, Epoch 461/1000, Training Loss (NLML): -957.4517\n",
      "convergence dfGPdfNN Run 4/10, Epoch 462/1000, Training Loss (NLML): -957.3260\n",
      "convergence dfGPdfNN Run 4/10, Epoch 463/1000, Training Loss (NLML): -957.3516\n",
      "convergence dfGPdfNN Run 4/10, Epoch 464/1000, Training Loss (NLML): -957.3567\n",
      "convergence dfGPdfNN Run 4/10, Epoch 465/1000, Training Loss (NLML): -957.3624\n",
      "convergence dfGPdfNN Run 4/10, Epoch 466/1000, Training Loss (NLML): -957.4685\n",
      "convergence dfGPdfNN Run 4/10, Epoch 467/1000, Training Loss (NLML): -957.5085\n",
      "convergence dfGPdfNN Run 4/10, Epoch 468/1000, Training Loss (NLML): -957.5242\n",
      "convergence dfGPdfNN Run 4/10, Epoch 469/1000, Training Loss (NLML): -957.5344\n",
      "convergence dfGPdfNN Run 4/10, Epoch 470/1000, Training Loss (NLML): -957.5518\n",
      "convergence dfGPdfNN Run 4/10, Epoch 471/1000, Training Loss (NLML): -957.5735\n",
      "convergence dfGPdfNN Run 4/10, Epoch 472/1000, Training Loss (NLML): -957.5432\n",
      "convergence dfGPdfNN Run 4/10, Epoch 473/1000, Training Loss (NLML): -957.5538\n",
      "convergence dfGPdfNN Run 4/10, Epoch 474/1000, Training Loss (NLML): -957.5023\n",
      "convergence dfGPdfNN Run 4/10, Epoch 475/1000, Training Loss (NLML): -957.5121\n",
      "convergence dfGPdfNN Run 4/10, Epoch 476/1000, Training Loss (NLML): -957.5173\n",
      "convergence dfGPdfNN Run 4/10, Epoch 477/1000, Training Loss (NLML): -957.5529\n",
      "convergence dfGPdfNN Run 4/10, Epoch 478/1000, Training Loss (NLML): -957.5658\n",
      "convergence dfGPdfNN Run 4/10, Epoch 479/1000, Training Loss (NLML): -957.5942\n",
      "convergence dfGPdfNN Run 4/10, Epoch 480/1000, Training Loss (NLML): -957.5880\n",
      "convergence dfGPdfNN Run 4/10, Epoch 481/1000, Training Loss (NLML): -957.6050\n",
      "convergence dfGPdfNN Run 4/10, Epoch 482/1000, Training Loss (NLML): -957.6096\n",
      "convergence dfGPdfNN Run 4/10, Epoch 483/1000, Training Loss (NLML): -957.4865\n",
      "convergence dfGPdfNN Run 4/10, Epoch 484/1000, Training Loss (NLML): -957.4969\n",
      "convergence dfGPdfNN Run 4/10, Epoch 485/1000, Training Loss (NLML): -957.5361\n",
      "convergence dfGPdfNN Run 4/10, Epoch 486/1000, Training Loss (NLML): -957.4370\n",
      "convergence dfGPdfNN Run 4/10, Epoch 487/1000, Training Loss (NLML): -957.4528\n",
      "convergence dfGPdfNN Run 4/10, Epoch 488/1000, Training Loss (NLML): -957.5592\n",
      "convergence dfGPdfNN Run 4/10, Epoch 489/1000, Training Loss (NLML): -957.0073\n",
      "convergence dfGPdfNN Run 4/10, Epoch 490/1000, Training Loss (NLML): -957.6216\n",
      "convergence dfGPdfNN Run 4/10, Epoch 491/1000, Training Loss (NLML): -957.6033\n",
      "convergence dfGPdfNN Run 4/10, Epoch 492/1000, Training Loss (NLML): -956.5020\n",
      "convergence dfGPdfNN Run 4/10, Epoch 493/1000, Training Loss (NLML): -957.5798\n",
      "convergence dfGPdfNN Run 4/10, Epoch 494/1000, Training Loss (NLML): -957.5922\n",
      "convergence dfGPdfNN Run 4/10, Epoch 495/1000, Training Loss (NLML): -957.5935\n",
      "convergence dfGPdfNN Run 4/10, Epoch 496/1000, Training Loss (NLML): -957.5884\n",
      "convergence dfGPdfNN Run 4/10, Epoch 497/1000, Training Loss (NLML): -957.6056\n",
      "convergence dfGPdfNN Run 4/10, Epoch 498/1000, Training Loss (NLML): -957.6454\n",
      "convergence dfGPdfNN Run 4/10, Epoch 499/1000, Training Loss (NLML): -957.6547\n",
      "convergence dfGPdfNN Run 4/10, Epoch 500/1000, Training Loss (NLML): -957.6641\n",
      "convergence dfGPdfNN Run 4/10, Epoch 501/1000, Training Loss (NLML): -957.6765\n",
      "convergence dfGPdfNN Run 4/10, Epoch 502/1000, Training Loss (NLML): -955.7958\n",
      "convergence dfGPdfNN Run 4/10, Epoch 503/1000, Training Loss (NLML): -957.6903\n",
      "convergence dfGPdfNN Run 4/10, Epoch 504/1000, Training Loss (NLML): -957.6759\n",
      "convergence dfGPdfNN Run 4/10, Epoch 505/1000, Training Loss (NLML): -957.6683\n",
      "convergence dfGPdfNN Run 4/10, Epoch 506/1000, Training Loss (NLML): -957.6418\n",
      "convergence dfGPdfNN Run 4/10, Epoch 507/1000, Training Loss (NLML): -957.6561\n",
      "convergence dfGPdfNN Run 4/10, Epoch 508/1000, Training Loss (NLML): -957.6769\n",
      "convergence dfGPdfNN Run 4/10, Epoch 509/1000, Training Loss (NLML): -957.6772\n",
      "convergence dfGPdfNN Run 4/10, Epoch 510/1000, Training Loss (NLML): -957.6633\n",
      "convergence dfGPdfNN Run 4/10, Epoch 511/1000, Training Loss (NLML): -957.6764\n",
      "convergence dfGPdfNN Run 4/10, Epoch 512/1000, Training Loss (NLML): -957.6783\n",
      "convergence dfGPdfNN Run 4/10, Epoch 513/1000, Training Loss (NLML): -957.6738\n",
      "convergence dfGPdfNN Run 4/10, Epoch 514/1000, Training Loss (NLML): -957.6976\n",
      "convergence dfGPdfNN Run 4/10, Epoch 515/1000, Training Loss (NLML): -957.7360\n",
      "convergence dfGPdfNN Run 4/10, Epoch 516/1000, Training Loss (NLML): -957.7717\n",
      "convergence dfGPdfNN Run 4/10, Epoch 517/1000, Training Loss (NLML): -957.7827\n",
      "convergence dfGPdfNN Run 4/10, Epoch 518/1000, Training Loss (NLML): -956.9376\n",
      "convergence dfGPdfNN Run 4/10, Epoch 519/1000, Training Loss (NLML): -957.7957\n",
      "convergence dfGPdfNN Run 4/10, Epoch 520/1000, Training Loss (NLML): -957.8433\n",
      "convergence dfGPdfNN Run 4/10, Epoch 521/1000, Training Loss (NLML): -957.8743\n",
      "convergence dfGPdfNN Run 4/10, Epoch 522/1000, Training Loss (NLML): -957.8878\n",
      "convergence dfGPdfNN Run 4/10, Epoch 523/1000, Training Loss (NLML): -957.8795\n",
      "convergence dfGPdfNN Run 4/10, Epoch 524/1000, Training Loss (NLML): -957.8896\n",
      "convergence dfGPdfNN Run 4/10, Epoch 525/1000, Training Loss (NLML): -957.7823\n",
      "convergence dfGPdfNN Run 4/10, Epoch 526/1000, Training Loss (NLML): -957.7903\n",
      "convergence dfGPdfNN Run 4/10, Epoch 527/1000, Training Loss (NLML): -957.8260\n",
      "convergence dfGPdfNN Run 4/10, Epoch 528/1000, Training Loss (NLML): -957.4467\n",
      "convergence dfGPdfNN Run 4/10, Epoch 529/1000, Training Loss (NLML): -957.8500\n",
      "convergence dfGPdfNN Run 4/10, Epoch 530/1000, Training Loss (NLML): -957.8333\n",
      "convergence dfGPdfNN Run 4/10, Epoch 531/1000, Training Loss (NLML): -957.8491\n",
      "convergence dfGPdfNN Run 4/10, Epoch 532/1000, Training Loss (NLML): -957.8639\n",
      "convergence dfGPdfNN Run 4/10, Epoch 533/1000, Training Loss (NLML): -957.8724\n",
      "convergence dfGPdfNN Run 4/10, Epoch 534/1000, Training Loss (NLML): -957.8818\n",
      "convergence dfGPdfNN Run 4/10, Epoch 535/1000, Training Loss (NLML): -957.8483\n",
      "convergence dfGPdfNN Run 4/10, Epoch 536/1000, Training Loss (NLML): -957.8538\n",
      "convergence dfGPdfNN Run 4/10, Epoch 537/1000, Training Loss (NLML): -957.8617\n",
      "convergence dfGPdfNN Run 4/10, Epoch 538/1000, Training Loss (NLML): -957.8734\n",
      "convergence dfGPdfNN Run 4/10, Epoch 539/1000, Training Loss (NLML): -957.8827\n",
      "convergence dfGPdfNN Run 4/10, Epoch 540/1000, Training Loss (NLML): -957.8822\n",
      "convergence dfGPdfNN Run 4/10, Epoch 541/1000, Training Loss (NLML): -957.9194\n",
      "convergence dfGPdfNN Run 4/10, Epoch 542/1000, Training Loss (NLML): -957.9282\n",
      "convergence dfGPdfNN Run 4/10, Epoch 543/1000, Training Loss (NLML): -957.7931\n",
      "convergence dfGPdfNN Run 4/10, Epoch 544/1000, Training Loss (NLML): -957.9540\n",
      "convergence dfGPdfNN Run 4/10, Epoch 545/1000, Training Loss (NLML): -957.9006\n",
      "convergence dfGPdfNN Run 4/10, Epoch 546/1000, Training Loss (NLML): -957.9102\n",
      "convergence dfGPdfNN Run 4/10, Epoch 547/1000, Training Loss (NLML): -957.8903\n",
      "convergence dfGPdfNN Run 4/10, Epoch 548/1000, Training Loss (NLML): -957.8737\n",
      "convergence dfGPdfNN Run 4/10, Epoch 549/1000, Training Loss (NLML): -957.8883\n",
      "convergence dfGPdfNN Run 4/10, Epoch 550/1000, Training Loss (NLML): -957.8976\n",
      "convergence dfGPdfNN Run 4/10, Epoch 551/1000, Training Loss (NLML): -957.9171\n",
      "convergence dfGPdfNN Run 4/10, Epoch 552/1000, Training Loss (NLML): -957.9266\n",
      "convergence dfGPdfNN Run 4/10, Epoch 553/1000, Training Loss (NLML): -957.9312\n",
      "convergence dfGPdfNN Run 4/10, Epoch 554/1000, Training Loss (NLML): -957.9376\n",
      "convergence dfGPdfNN Run 4/10, Epoch 555/1000, Training Loss (NLML): -958.0276\n",
      "convergence dfGPdfNN Run 4/10, Epoch 556/1000, Training Loss (NLML): -958.0359\n",
      "convergence dfGPdfNN Run 4/10, Epoch 557/1000, Training Loss (NLML): -958.1123\n",
      "convergence dfGPdfNN Run 4/10, Epoch 558/1000, Training Loss (NLML): -958.1295\n",
      "convergence dfGPdfNN Run 4/10, Epoch 559/1000, Training Loss (NLML): -958.1317\n",
      "convergence dfGPdfNN Run 4/10, Epoch 560/1000, Training Loss (NLML): -958.1223\n",
      "convergence dfGPdfNN Run 4/10, Epoch 561/1000, Training Loss (NLML): -958.1390\n",
      "convergence dfGPdfNN Run 4/10, Epoch 562/1000, Training Loss (NLML): -958.1445\n",
      "convergence dfGPdfNN Run 4/10, Epoch 563/1000, Training Loss (NLML): -958.1490\n",
      "convergence dfGPdfNN Run 4/10, Epoch 564/1000, Training Loss (NLML): -958.1558\n",
      "convergence dfGPdfNN Run 4/10, Epoch 565/1000, Training Loss (NLML): -958.1641\n",
      "convergence dfGPdfNN Run 4/10, Epoch 566/1000, Training Loss (NLML): -957.8820\n",
      "convergence dfGPdfNN Run 4/10, Epoch 567/1000, Training Loss (NLML): -958.1819\n",
      "convergence dfGPdfNN Run 4/10, Epoch 568/1000, Training Loss (NLML): -958.1912\n",
      "convergence dfGPdfNN Run 4/10, Epoch 569/1000, Training Loss (NLML): -958.2029\n",
      "convergence dfGPdfNN Run 4/10, Epoch 570/1000, Training Loss (NLML): -958.2111\n",
      "convergence dfGPdfNN Run 4/10, Epoch 571/1000, Training Loss (NLML): -958.1472\n",
      "convergence dfGPdfNN Run 4/10, Epoch 572/1000, Training Loss (NLML): -956.9908\n",
      "convergence dfGPdfNN Run 4/10, Epoch 573/1000, Training Loss (NLML): -958.2325\n",
      "convergence dfGPdfNN Run 4/10, Epoch 574/1000, Training Loss (NLML): -958.2382\n",
      "convergence dfGPdfNN Run 4/10, Epoch 575/1000, Training Loss (NLML): -958.2416\n",
      "convergence dfGPdfNN Run 4/10, Epoch 576/1000, Training Loss (NLML): -958.2141\n",
      "convergence dfGPdfNN Run 4/10, Epoch 577/1000, Training Loss (NLML): -958.1989\n",
      "convergence dfGPdfNN Run 4/10, Epoch 578/1000, Training Loss (NLML): -958.2026\n",
      "convergence dfGPdfNN Run 4/10, Epoch 579/1000, Training Loss (NLML): -958.2222\n",
      "convergence dfGPdfNN Run 4/10, Epoch 580/1000, Training Loss (NLML): -958.2355\n",
      "convergence dfGPdfNN Run 4/10, Epoch 581/1000, Training Loss (NLML): -958.1904\n",
      "convergence dfGPdfNN Run 4/10, Epoch 582/1000, Training Loss (NLML): -958.2504\n",
      "convergence dfGPdfNN Run 4/10, Epoch 583/1000, Training Loss (NLML): -958.2003\n",
      "convergence dfGPdfNN Run 4/10, Epoch 584/1000, Training Loss (NLML): -958.2433\n",
      "convergence dfGPdfNN Run 4/10, Epoch 585/1000, Training Loss (NLML): -958.2635\n",
      "convergence dfGPdfNN Run 4/10, Epoch 586/1000, Training Loss (NLML): -958.2799\n",
      "convergence dfGPdfNN Run 4/10, Epoch 587/1000, Training Loss (NLML): -958.2915\n",
      "convergence dfGPdfNN Run 4/10, Epoch 588/1000, Training Loss (NLML): -958.1525\n",
      "convergence dfGPdfNN Run 4/10, Epoch 589/1000, Training Loss (NLML): -958.2169\n",
      "convergence dfGPdfNN Run 4/10, Epoch 590/1000, Training Loss (NLML): -958.2251\n",
      "convergence dfGPdfNN Run 4/10, Epoch 591/1000, Training Loss (NLML): -958.2126\n",
      "convergence dfGPdfNN Run 4/10, Epoch 592/1000, Training Loss (NLML): -958.2234\n",
      "convergence dfGPdfNN Run 4/10, Epoch 593/1000, Training Loss (NLML): -958.3052\n",
      "convergence dfGPdfNN Run 4/10, Epoch 594/1000, Training Loss (NLML): -958.3623\n",
      "convergence dfGPdfNN Run 4/10, Epoch 595/1000, Training Loss (NLML): -958.3018\n",
      "convergence dfGPdfNN Run 4/10, Epoch 596/1000, Training Loss (NLML): -958.3103\n",
      "convergence dfGPdfNN Run 4/10, Epoch 597/1000, Training Loss (NLML): -958.3130\n",
      "convergence dfGPdfNN Run 4/10, Epoch 598/1000, Training Loss (NLML): -958.3254\n",
      "convergence dfGPdfNN Run 4/10, Epoch 599/1000, Training Loss (NLML): -958.2881\n",
      "convergence dfGPdfNN Run 4/10, Epoch 600/1000, Training Loss (NLML): -958.2987\n",
      "convergence dfGPdfNN Run 4/10, Epoch 601/1000, Training Loss (NLML): -958.3073\n",
      "convergence dfGPdfNN Run 4/10, Epoch 602/1000, Training Loss (NLML): -958.3134\n",
      "convergence dfGPdfNN Run 4/10, Epoch 603/1000, Training Loss (NLML): -958.3264\n",
      "convergence dfGPdfNN Run 4/10, Epoch 604/1000, Training Loss (NLML): -958.3887\n",
      "convergence dfGPdfNN Run 4/10, Epoch 605/1000, Training Loss (NLML): -958.3601\n",
      "convergence dfGPdfNN Run 4/10, Epoch 606/1000, Training Loss (NLML): -958.3463\n",
      "convergence dfGPdfNN Run 4/10, Epoch 607/1000, Training Loss (NLML): -958.3521\n",
      "convergence dfGPdfNN Run 4/10, Epoch 608/1000, Training Loss (NLML): -958.3606\n",
      "convergence dfGPdfNN Run 4/10, Epoch 609/1000, Training Loss (NLML): -958.3649\n",
      "convergence dfGPdfNN Run 4/10, Epoch 610/1000, Training Loss (NLML): -958.3756\n",
      "convergence dfGPdfNN Run 4/10, Epoch 611/1000, Training Loss (NLML): -958.3811\n",
      "convergence dfGPdfNN Run 4/10, Epoch 612/1000, Training Loss (NLML): -958.3885\n",
      "convergence dfGPdfNN Run 4/10, Epoch 613/1000, Training Loss (NLML): -958.3076\n",
      "convergence dfGPdfNN Run 4/10, Epoch 614/1000, Training Loss (NLML): -958.3140\n",
      "convergence dfGPdfNN Run 4/10, Epoch 615/1000, Training Loss (NLML): -958.2538\n",
      "convergence dfGPdfNN Run 4/10, Epoch 616/1000, Training Loss (NLML): -958.2627\n",
      "convergence dfGPdfNN Run 4/10, Epoch 617/1000, Training Loss (NLML): -958.2694\n",
      "convergence dfGPdfNN Run 4/10, Epoch 618/1000, Training Loss (NLML): -958.2799\n",
      "convergence dfGPdfNN Run 4/10, Epoch 619/1000, Training Loss (NLML): -958.3064\n",
      "convergence dfGPdfNN Run 4/10, Epoch 620/1000, Training Loss (NLML): -958.3143\n",
      "convergence dfGPdfNN Run 4/10, Epoch 621/1000, Training Loss (NLML): -958.3218\n",
      "convergence dfGPdfNN Run 4/10, Epoch 622/1000, Training Loss (NLML): -958.3086\n",
      "convergence dfGPdfNN Run 4/10, Epoch 623/1000, Training Loss (NLML): -958.2439\n",
      "convergence dfGPdfNN Run 4/10, Epoch 624/1000, Training Loss (NLML): -958.3218\n",
      "convergence dfGPdfNN Run 4/10, Epoch 625/1000, Training Loss (NLML): -958.3287\n",
      "convergence dfGPdfNN Run 4/10, Epoch 626/1000, Training Loss (NLML): -958.3364\n",
      "convergence dfGPdfNN Run 4/10, Epoch 627/1000, Training Loss (NLML): -958.3445\n",
      "convergence dfGPdfNN Run 4/10, Epoch 628/1000, Training Loss (NLML): -958.3523\n",
      "convergence dfGPdfNN Run 4/10, Epoch 629/1000, Training Loss (NLML): -958.3578\n",
      "convergence dfGPdfNN Run 4/10, Epoch 630/1000, Training Loss (NLML): -958.3860\n",
      "convergence dfGPdfNN Run 4/10, Epoch 631/1000, Training Loss (NLML): -958.3918\n",
      "convergence dfGPdfNN Run 4/10, Epoch 632/1000, Training Loss (NLML): -958.4006\n",
      "convergence dfGPdfNN Run 4/10, Epoch 633/1000, Training Loss (NLML): -958.3865\n",
      "convergence dfGPdfNN Run 4/10, Epoch 634/1000, Training Loss (NLML): -958.3948\n",
      "convergence dfGPdfNN Run 4/10, Epoch 635/1000, Training Loss (NLML): -958.4015\n",
      "convergence dfGPdfNN Run 4/10, Epoch 636/1000, Training Loss (NLML): -958.4071\n",
      "convergence dfGPdfNN Run 4/10, Epoch 637/1000, Training Loss (NLML): -958.4153\n",
      "convergence dfGPdfNN Run 4/10, Epoch 638/1000, Training Loss (NLML): -958.4214\n",
      "convergence dfGPdfNN Run 4/10, Epoch 639/1000, Training Loss (NLML): -958.4487\n",
      "convergence dfGPdfNN Run 4/10, Epoch 640/1000, Training Loss (NLML): -958.4547\n",
      "convergence dfGPdfNN Run 4/10, Epoch 641/1000, Training Loss (NLML): -958.4614\n",
      "convergence dfGPdfNN Run 4/10, Epoch 642/1000, Training Loss (NLML): -958.4475\n",
      "convergence dfGPdfNN Run 4/10, Epoch 643/1000, Training Loss (NLML): -958.4557\n",
      "convergence dfGPdfNN Run 4/10, Epoch 644/1000, Training Loss (NLML): -958.4619\n",
      "convergence dfGPdfNN Run 4/10, Epoch 645/1000, Training Loss (NLML): -958.4695\n",
      "convergence dfGPdfNN Run 4/10, Epoch 646/1000, Training Loss (NLML): -958.4771\n",
      "convergence dfGPdfNN Run 4/10, Epoch 647/1000, Training Loss (NLML): -958.5039\n",
      "convergence dfGPdfNN Run 4/10, Epoch 648/1000, Training Loss (NLML): -958.5099\n",
      "convergence dfGPdfNN Run 4/10, Epoch 649/1000, Training Loss (NLML): -958.5155\n",
      "convergence dfGPdfNN Run 4/10, Epoch 650/1000, Training Loss (NLML): -958.5038\n",
      "convergence dfGPdfNN Run 4/10, Epoch 651/1000, Training Loss (NLML): -958.5098\n",
      "convergence dfGPdfNN Run 4/10, Epoch 652/1000, Training Loss (NLML): -958.5157\n",
      "convergence dfGPdfNN Run 4/10, Epoch 653/1000, Training Loss (NLML): -958.5238\n",
      "convergence dfGPdfNN Run 4/10, Epoch 654/1000, Training Loss (NLML): -958.5487\n",
      "convergence dfGPdfNN Run 4/10, Epoch 655/1000, Training Loss (NLML): -958.5551\n",
      "convergence dfGPdfNN Run 4/10, Epoch 656/1000, Training Loss (NLML): -958.5428\n",
      "convergence dfGPdfNN Run 4/10, Epoch 657/1000, Training Loss (NLML): -958.5481\n",
      "convergence dfGPdfNN Run 4/10, Epoch 658/1000, Training Loss (NLML): -958.5549\n",
      "convergence dfGPdfNN Run 4/10, Epoch 659/1000, Training Loss (NLML): -958.5811\n",
      "convergence dfGPdfNN Run 4/10, Epoch 660/1000, Training Loss (NLML): -958.5685\n",
      "convergence dfGPdfNN Run 4/10, Epoch 661/1000, Training Loss (NLML): -958.5941\n",
      "convergence dfGPdfNN Run 4/10, Epoch 662/1000, Training Loss (NLML): -958.5822\n",
      "convergence dfGPdfNN Run 4/10, Epoch 663/1000, Training Loss (NLML): -958.6062\n",
      "convergence dfGPdfNN Run 4/10, Epoch 664/1000, Training Loss (NLML): -958.5950\n",
      "convergence dfGPdfNN Run 4/10, Epoch 665/1000, Training Loss (NLML): -958.6013\n",
      "convergence dfGPdfNN Run 4/10, Epoch 666/1000, Training Loss (NLML): -958.6266\n",
      "convergence dfGPdfNN Run 4/10, Epoch 667/1000, Training Loss (NLML): -958.6333\n",
      "convergence dfGPdfNN Run 4/10, Epoch 668/1000, Training Loss (NLML): -958.6198\n",
      "convergence dfGPdfNN Run 4/10, Epoch 669/1000, Training Loss (NLML): -958.6263\n",
      "convergence dfGPdfNN Run 4/10, Epoch 670/1000, Training Loss (NLML): -958.6511\n",
      "convergence dfGPdfNN Run 4/10, Epoch 671/1000, Training Loss (NLML): -958.6584\n",
      "convergence dfGPdfNN Run 4/10, Epoch 672/1000, Training Loss (NLML): -958.6638\n",
      "convergence dfGPdfNN Run 4/10, Epoch 673/1000, Training Loss (NLML): -958.6519\n",
      "convergence dfGPdfNN Run 4/10, Epoch 674/1000, Training Loss (NLML): -958.6582\n",
      "convergence dfGPdfNN Run 4/10, Epoch 675/1000, Training Loss (NLML): -958.6626\n",
      "convergence dfGPdfNN Run 4/10, Epoch 676/1000, Training Loss (NLML): -958.6887\n",
      "convergence dfGPdfNN Run 4/10, Epoch 677/1000, Training Loss (NLML): -958.6948\n",
      "convergence dfGPdfNN Run 4/10, Epoch 678/1000, Training Loss (NLML): -958.7001\n",
      "convergence dfGPdfNN Run 4/10, Epoch 679/1000, Training Loss (NLML): -958.7063\n",
      "convergence dfGPdfNN Run 4/10, Epoch 680/1000, Training Loss (NLML): -958.6926\n",
      "convergence dfGPdfNN Run 4/10, Epoch 681/1000, Training Loss (NLML): -958.7103\n",
      "convergence dfGPdfNN Run 4/10, Epoch 682/1000, Training Loss (NLML): -958.7152\n",
      "convergence dfGPdfNN Run 4/10, Epoch 683/1000, Training Loss (NLML): -958.7407\n",
      "convergence dfGPdfNN Run 4/10, Epoch 684/1000, Training Loss (NLML): -958.7375\n",
      "convergence dfGPdfNN Run 4/10, Epoch 685/1000, Training Loss (NLML): -958.7419\n",
      "convergence dfGPdfNN Run 4/10, Epoch 686/1000, Training Loss (NLML): -958.7498\n",
      "convergence dfGPdfNN Run 4/10, Epoch 687/1000, Training Loss (NLML): -958.7358\n",
      "convergence dfGPdfNN Run 4/10, Epoch 688/1000, Training Loss (NLML): -958.7422\n",
      "convergence dfGPdfNN Run 4/10, Epoch 689/1000, Training Loss (NLML): -958.7666\n",
      "convergence dfGPdfNN Run 4/10, Epoch 690/1000, Training Loss (NLML): -958.7729\n",
      "convergence dfGPdfNN Run 4/10, Epoch 691/1000, Training Loss (NLML): -958.7786\n",
      "convergence dfGPdfNN Run 4/10, Epoch 692/1000, Training Loss (NLML): -958.7821\n",
      "convergence dfGPdfNN Run 4/10, Epoch 693/1000, Training Loss (NLML): -958.7881\n",
      "convergence dfGPdfNN Run 4/10, Epoch 694/1000, Training Loss (NLML): -958.7754\n",
      "convergence dfGPdfNN Run 4/10, Epoch 695/1000, Training Loss (NLML): -958.7994\n",
      "convergence dfGPdfNN Run 4/10, Epoch 696/1000, Training Loss (NLML): -958.8059\n",
      "convergence dfGPdfNN Run 4/10, Epoch 697/1000, Training Loss (NLML): -958.8125\n",
      "convergence dfGPdfNN Run 4/10, Epoch 698/1000, Training Loss (NLML): -958.8190\n",
      "convergence dfGPdfNN Run 4/10, Epoch 699/1000, Training Loss (NLML): -958.8245\n",
      "convergence dfGPdfNN Run 4/10, Epoch 700/1000, Training Loss (NLML): -958.8296\n",
      "convergence dfGPdfNN Run 4/10, Epoch 701/1000, Training Loss (NLML): -958.8165\n",
      "convergence dfGPdfNN Run 4/10, Epoch 702/1000, Training Loss (NLML): -958.8407\n",
      "convergence dfGPdfNN Run 4/10, Epoch 703/1000, Training Loss (NLML): -958.8467\n",
      "convergence dfGPdfNN Run 4/10, Epoch 704/1000, Training Loss (NLML): -958.8521\n",
      "convergence dfGPdfNN Run 4/10, Epoch 705/1000, Training Loss (NLML): -958.8588\n",
      "convergence dfGPdfNN Run 4/10, Epoch 706/1000, Training Loss (NLML): -958.8132\n",
      "convergence dfGPdfNN Run 4/10, Epoch 707/1000, Training Loss (NLML): -958.8369\n",
      "convergence dfGPdfNN Run 4/10, Epoch 708/1000, Training Loss (NLML): -958.8741\n",
      "convergence dfGPdfNN Run 4/10, Epoch 709/1000, Training Loss (NLML): -958.8784\n",
      "convergence dfGPdfNN Run 4/10, Epoch 710/1000, Training Loss (NLML): -958.8849\n",
      "convergence dfGPdfNN Run 4/10, Epoch 711/1000, Training Loss (NLML): -958.8608\n",
      "convergence dfGPdfNN Run 4/10, Epoch 712/1000, Training Loss (NLML): -958.8663\n",
      "convergence dfGPdfNN Run 4/10, Epoch 713/1000, Training Loss (NLML): -958.8726\n",
      "convergence dfGPdfNN Run 4/10, Epoch 714/1000, Training Loss (NLML): -958.8772\n",
      "convergence dfGPdfNN Run 4/10, Epoch 715/1000, Training Loss (NLML): -958.8824\n",
      "convergence dfGPdfNN Run 4/10, Epoch 716/1000, Training Loss (NLML): -958.8879\n",
      "convergence dfGPdfNN Run 4/10, Epoch 717/1000, Training Loss (NLML): -958.8939\n",
      "convergence dfGPdfNN Run 4/10, Epoch 718/1000, Training Loss (NLML): -958.8995\n",
      "convergence dfGPdfNN Run 4/10, Epoch 719/1000, Training Loss (NLML): -958.9337\n",
      "convergence dfGPdfNN Run 4/10, Epoch 720/1000, Training Loss (NLML): -957.3345\n",
      "convergence dfGPdfNN Run 4/10, Epoch 721/1000, Training Loss (NLML): -958.9185\n",
      "convergence dfGPdfNN Run 4/10, Epoch 722/1000, Training Loss (NLML): -958.9143\n",
      "convergence dfGPdfNN Run 4/10, Epoch 723/1000, Training Loss (NLML): -958.9077\n",
      "convergence dfGPdfNN Run 4/10, Epoch 724/1000, Training Loss (NLML): -958.9021\n",
      "convergence dfGPdfNN Run 4/10, Epoch 725/1000, Training Loss (NLML): -958.9475\n",
      "convergence dfGPdfNN Run 4/10, Epoch 726/1000, Training Loss (NLML): -958.9424\n",
      "convergence dfGPdfNN Run 4/10, Epoch 727/1000, Training Loss (NLML): -958.9659\n",
      "convergence dfGPdfNN Run 4/10, Epoch 728/1000, Training Loss (NLML): -958.9646\n",
      "convergence dfGPdfNN Run 4/10, Epoch 729/1000, Training Loss (NLML): -958.9692\n",
      "convergence dfGPdfNN Run 4/10, Epoch 730/1000, Training Loss (NLML): -958.9199\n",
      "convergence dfGPdfNN Run 4/10, Epoch 731/1000, Training Loss (NLML): -958.9846\n",
      "convergence dfGPdfNN Run 4/10, Epoch 732/1000, Training Loss (NLML): -959.0466\n",
      "convergence dfGPdfNN Run 4/10, Epoch 733/1000, Training Loss (NLML): -959.0535\n",
      "convergence dfGPdfNN Run 4/10, Epoch 734/1000, Training Loss (NLML): -959.0706\n",
      "convergence dfGPdfNN Run 4/10, Epoch 735/1000, Training Loss (NLML): -959.0687\n",
      "convergence dfGPdfNN Run 4/10, Epoch 736/1000, Training Loss (NLML): -958.9872\n",
      "convergence dfGPdfNN Run 4/10, Epoch 737/1000, Training Loss (NLML): -958.9869\n",
      "convergence dfGPdfNN Run 4/10, Epoch 738/1000, Training Loss (NLML): -959.1877\n",
      "convergence dfGPdfNN Run 4/10, Epoch 739/1000, Training Loss (NLML): -959.1696\n",
      "convergence dfGPdfNN Run 4/10, Epoch 740/1000, Training Loss (NLML): -959.1753\n",
      "convergence dfGPdfNN Run 4/10, Epoch 741/1000, Training Loss (NLML): -959.1809\n",
      "convergence dfGPdfNN Run 4/10, Epoch 742/1000, Training Loss (NLML): -959.1842\n",
      "convergence dfGPdfNN Run 4/10, Epoch 743/1000, Training Loss (NLML): -959.1908\n",
      "convergence dfGPdfNN Run 4/10, Epoch 744/1000, Training Loss (NLML): -959.2355\n",
      "convergence dfGPdfNN Run 4/10, Epoch 745/1000, Training Loss (NLML): -959.2383\n",
      "convergence dfGPdfNN Run 4/10, Epoch 746/1000, Training Loss (NLML): -959.2740\n",
      "convergence dfGPdfNN Run 4/10, Epoch 747/1000, Training Loss (NLML): -959.3022\n",
      "convergence dfGPdfNN Run 4/10, Epoch 748/1000, Training Loss (NLML): -959.3080\n",
      "convergence dfGPdfNN Run 4/10, Epoch 749/1000, Training Loss (NLML): -959.3142\n",
      "convergence dfGPdfNN Run 4/10, Epoch 750/1000, Training Loss (NLML): -959.3184\n",
      "convergence dfGPdfNN Run 4/10, Epoch 751/1000, Training Loss (NLML): -959.3291\n",
      "convergence dfGPdfNN Run 4/10, Epoch 752/1000, Training Loss (NLML): -959.3358\n",
      "convergence dfGPdfNN Run 4/10, Epoch 753/1000, Training Loss (NLML): -959.3416\n",
      "convergence dfGPdfNN Run 4/10, Epoch 754/1000, Training Loss (NLML): -959.3345\n",
      "convergence dfGPdfNN Run 4/10, Epoch 755/1000, Training Loss (NLML): -959.3647\n",
      "convergence dfGPdfNN Run 4/10, Epoch 756/1000, Training Loss (NLML): -959.3927\n",
      "convergence dfGPdfNN Run 4/10, Epoch 757/1000, Training Loss (NLML): -959.3969\n",
      "convergence dfGPdfNN Run 4/10, Epoch 758/1000, Training Loss (NLML): -959.4016\n",
      "convergence dfGPdfNN Run 4/10, Epoch 759/1000, Training Loss (NLML): -959.4066\n",
      "convergence dfGPdfNN Run 4/10, Epoch 760/1000, Training Loss (NLML): -959.4072\n",
      "convergence dfGPdfNN Run 4/10, Epoch 761/1000, Training Loss (NLML): -959.4113\n",
      "convergence dfGPdfNN Run 4/10, Epoch 762/1000, Training Loss (NLML): -959.4161\n",
      "convergence dfGPdfNN Run 4/10, Epoch 763/1000, Training Loss (NLML): -959.4293\n",
      "convergence dfGPdfNN Run 4/10, Epoch 764/1000, Training Loss (NLML): -959.4346\n",
      "convergence dfGPdfNN Run 4/10, Epoch 765/1000, Training Loss (NLML): -959.4275\n",
      "convergence dfGPdfNN Run 4/10, Epoch 766/1000, Training Loss (NLML): -959.4082\n",
      "convergence dfGPdfNN Run 4/10, Epoch 767/1000, Training Loss (NLML): -959.4116\n",
      "convergence dfGPdfNN Run 4/10, Epoch 768/1000, Training Loss (NLML): -959.4174\n",
      "convergence dfGPdfNN Run 4/10, Epoch 769/1000, Training Loss (NLML): -959.4204\n",
      "convergence dfGPdfNN Run 4/10, Epoch 770/1000, Training Loss (NLML): -959.4248\n",
      "convergence dfGPdfNN Run 4/10, Epoch 771/1000, Training Loss (NLML): -959.4290\n",
      "convergence dfGPdfNN Run 4/10, Epoch 772/1000, Training Loss (NLML): -959.4330\n",
      "convergence dfGPdfNN Run 4/10, Epoch 773/1000, Training Loss (NLML): -959.4371\n",
      "convergence dfGPdfNN Run 4/10, Epoch 774/1000, Training Loss (NLML): -959.4622\n",
      "convergence dfGPdfNN Run 4/10, Epoch 775/1000, Training Loss (NLML): -959.4435\n",
      "convergence dfGPdfNN Run 4/10, Epoch 776/1000, Training Loss (NLML): -959.4225\n",
      "convergence dfGPdfNN Run 4/10, Epoch 777/1000, Training Loss (NLML): -959.4275\n",
      "convergence dfGPdfNN Run 4/10, Epoch 778/1000, Training Loss (NLML): -959.4313\n",
      "convergence dfGPdfNN Run 4/10, Epoch 779/1000, Training Loss (NLML): -959.4681\n",
      "convergence dfGPdfNN Run 4/10, Epoch 780/1000, Training Loss (NLML): -959.4739\n",
      "convergence dfGPdfNN Run 4/10, Epoch 781/1000, Training Loss (NLML): -959.4791\n",
      "convergence dfGPdfNN Run 4/10, Epoch 782/1000, Training Loss (NLML): -959.4493\n",
      "convergence dfGPdfNN Run 4/10, Epoch 783/1000, Training Loss (NLML): -959.4282\n",
      "convergence dfGPdfNN Run 4/10, Epoch 784/1000, Training Loss (NLML): -959.4286\n",
      "convergence dfGPdfNN Run 4/10, Epoch 785/1000, Training Loss (NLML): -959.4320\n",
      "convergence dfGPdfNN Run 4/10, Epoch 786/1000, Training Loss (NLML): -959.4351\n",
      "convergence dfGPdfNN Run 4/10, Epoch 787/1000, Training Loss (NLML): -959.4406\n",
      "convergence dfGPdfNN Run 4/10, Epoch 788/1000, Training Loss (NLML): -959.4501\n",
      "convergence dfGPdfNN Run 4/10, Epoch 789/1000, Training Loss (NLML): -959.4164\n",
      "convergence dfGPdfNN Run 4/10, Epoch 790/1000, Training Loss (NLML): -959.4459\n",
      "convergence dfGPdfNN Run 4/10, Epoch 791/1000, Training Loss (NLML): -959.4463\n",
      "convergence dfGPdfNN Run 4/10, Epoch 792/1000, Training Loss (NLML): -959.4528\n",
      "convergence dfGPdfNN Run 4/10, Epoch 793/1000, Training Loss (NLML): -959.4835\n",
      "convergence dfGPdfNN Run 4/10, Epoch 794/1000, Training Loss (NLML): -959.4844\n",
      "convergence dfGPdfNN Run 4/10, Epoch 795/1000, Training Loss (NLML): -959.4905\n",
      "convergence dfGPdfNN Run 4/10, Epoch 796/1000, Training Loss (NLML): -959.4938\n",
      "convergence dfGPdfNN Run 4/10, Epoch 797/1000, Training Loss (NLML): -959.5024\n",
      "convergence dfGPdfNN Run 4/10, Epoch 798/1000, Training Loss (NLML): -959.5187\n",
      "convergence dfGPdfNN Run 4/10, Epoch 799/1000, Training Loss (NLML): -959.5093\n",
      "convergence dfGPdfNN Run 4/10, Epoch 800/1000, Training Loss (NLML): -959.4889\n",
      "convergence dfGPdfNN Run 4/10, Epoch 801/1000, Training Loss (NLML): -959.4973\n",
      "convergence dfGPdfNN Run 4/10, Epoch 802/1000, Training Loss (NLML): -959.4919\n",
      "convergence dfGPdfNN Run 4/10, Epoch 803/1000, Training Loss (NLML): -959.4995\n",
      "convergence dfGPdfNN Run 4/10, Epoch 804/1000, Training Loss (NLML): -959.5020\n",
      "convergence dfGPdfNN Run 4/10, Epoch 805/1000, Training Loss (NLML): -959.5103\n",
      "convergence dfGPdfNN Run 4/10, Epoch 806/1000, Training Loss (NLML): -959.5164\n",
      "convergence dfGPdfNN Run 4/10, Epoch 807/1000, Training Loss (NLML): -959.5216\n",
      "convergence dfGPdfNN Run 4/10, Epoch 808/1000, Training Loss (NLML): -959.5225\n",
      "convergence dfGPdfNN Run 4/10, Epoch 809/1000, Training Loss (NLML): -959.5234\n",
      "convergence dfGPdfNN Run 4/10, Epoch 810/1000, Training Loss (NLML): -959.5481\n",
      "convergence dfGPdfNN Run 4/10, Epoch 811/1000, Training Loss (NLML): -959.5502\n",
      "convergence dfGPdfNN Run 4/10, Epoch 812/1000, Training Loss (NLML): -959.5535\n",
      "convergence dfGPdfNN Run 4/10, Epoch 813/1000, Training Loss (NLML): -959.5579\n",
      "convergence dfGPdfNN Run 4/10, Epoch 814/1000, Training Loss (NLML): -959.5641\n",
      "convergence dfGPdfNN Run 4/10, Epoch 815/1000, Training Loss (NLML): -959.5476\n",
      "convergence dfGPdfNN Run 4/10, Epoch 816/1000, Training Loss (NLML): -959.5552\n",
      "convergence dfGPdfNN Run 4/10, Epoch 817/1000, Training Loss (NLML): -959.5602\n",
      "convergence dfGPdfNN Run 4/10, Epoch 818/1000, Training Loss (NLML): -959.5627\n",
      "convergence dfGPdfNN Run 4/10, Epoch 819/1000, Training Loss (NLML): -959.5665\n",
      "convergence dfGPdfNN Run 4/10, Epoch 820/1000, Training Loss (NLML): -959.5710\n",
      "convergence dfGPdfNN Run 4/10, Epoch 821/1000, Training Loss (NLML): -959.5735\n",
      "convergence dfGPdfNN Run 4/10, Epoch 822/1000, Training Loss (NLML): -959.5790\n",
      "convergence dfGPdfNN Run 4/10, Epoch 823/1000, Training Loss (NLML): -959.5813\n",
      "convergence dfGPdfNN Run 4/10, Epoch 824/1000, Training Loss (NLML): -959.5862\n",
      "convergence dfGPdfNN Run 4/10, Epoch 825/1000, Training Loss (NLML): -959.5916\n",
      "convergence dfGPdfNN Run 4/10, Epoch 826/1000, Training Loss (NLML): -959.5938\n",
      "convergence dfGPdfNN Run 4/10, Epoch 827/1000, Training Loss (NLML): -959.5989\n",
      "convergence dfGPdfNN Run 4/10, Epoch 828/1000, Training Loss (NLML): -959.5934\n",
      "convergence dfGPdfNN Run 4/10, Epoch 829/1000, Training Loss (NLML): -959.5977\n",
      "convergence dfGPdfNN Run 4/10, Epoch 830/1000, Training Loss (NLML): -959.6012\n",
      "convergence dfGPdfNN Run 4/10, Epoch 831/1000, Training Loss (NLML): -959.6028\n",
      "convergence dfGPdfNN Run 4/10, Epoch 832/1000, Training Loss (NLML): -959.6077\n",
      "convergence dfGPdfNN Run 4/10, Epoch 833/1000, Training Loss (NLML): -959.6205\n",
      "convergence dfGPdfNN Run 4/10, Epoch 834/1000, Training Loss (NLML): -959.6228\n",
      "convergence dfGPdfNN Run 4/10, Epoch 835/1000, Training Loss (NLML): -959.6255\n",
      "convergence dfGPdfNN Run 4/10, Epoch 836/1000, Training Loss (NLML): -959.6307\n",
      "convergence dfGPdfNN Run 4/10, Epoch 837/1000, Training Loss (NLML): -959.6328\n",
      "convergence dfGPdfNN Run 4/10, Epoch 838/1000, Training Loss (NLML): -959.6376\n",
      "convergence dfGPdfNN Run 4/10, Epoch 839/1000, Training Loss (NLML): -959.6406\n",
      "convergence dfGPdfNN Run 4/10, Epoch 840/1000, Training Loss (NLML): -959.6438\n",
      "convergence dfGPdfNN Run 4/10, Epoch 841/1000, Training Loss (NLML): -959.6482\n",
      "convergence dfGPdfNN Run 4/10, Epoch 842/1000, Training Loss (NLML): -959.6497\n",
      "convergence dfGPdfNN Run 4/10, Epoch 843/1000, Training Loss (NLML): -959.6544\n",
      "convergence dfGPdfNN Run 4/10, Epoch 844/1000, Training Loss (NLML): -959.6572\n",
      "convergence dfGPdfNN Run 4/10, Epoch 845/1000, Training Loss (NLML): -959.6606\n",
      "convergence dfGPdfNN Run 4/10, Epoch 846/1000, Training Loss (NLML): -959.6632\n",
      "convergence dfGPdfNN Run 4/10, Epoch 847/1000, Training Loss (NLML): -959.6669\n",
      "convergence dfGPdfNN Run 4/10, Epoch 848/1000, Training Loss (NLML): -959.6720\n",
      "convergence dfGPdfNN Run 4/10, Epoch 849/1000, Training Loss (NLML): -959.6749\n",
      "convergence dfGPdfNN Run 4/10, Epoch 850/1000, Training Loss (NLML): -959.6782\n",
      "convergence dfGPdfNN Run 4/10, Epoch 851/1000, Training Loss (NLML): -959.6808\n",
      "convergence dfGPdfNN Run 4/10, Epoch 852/1000, Training Loss (NLML): -959.6859\n",
      "convergence dfGPdfNN Run 4/10, Epoch 853/1000, Training Loss (NLML): -959.6882\n",
      "convergence dfGPdfNN Run 4/10, Epoch 854/1000, Training Loss (NLML): -959.6921\n",
      "convergence dfGPdfNN Run 4/10, Epoch 855/1000, Training Loss (NLML): -959.6947\n",
      "convergence dfGPdfNN Run 4/10, Epoch 856/1000, Training Loss (NLML): -959.6989\n",
      "convergence dfGPdfNN Run 4/10, Epoch 857/1000, Training Loss (NLML): -959.7028\n",
      "convergence dfGPdfNN Run 4/10, Epoch 858/1000, Training Loss (NLML): -959.7043\n",
      "convergence dfGPdfNN Run 4/10, Epoch 859/1000, Training Loss (NLML): -959.7079\n",
      "convergence dfGPdfNN Run 4/10, Epoch 860/1000, Training Loss (NLML): -959.7125\n",
      "convergence dfGPdfNN Run 4/10, Epoch 861/1000, Training Loss (NLML): -959.7151\n",
      "convergence dfGPdfNN Run 4/10, Epoch 862/1000, Training Loss (NLML): -959.7192\n",
      "convergence dfGPdfNN Run 4/10, Epoch 863/1000, Training Loss (NLML): -959.7218\n",
      "convergence dfGPdfNN Run 4/10, Epoch 864/1000, Training Loss (NLML): -959.7251\n",
      "convergence dfGPdfNN Run 4/10, Epoch 865/1000, Training Loss (NLML): -959.7274\n",
      "convergence dfGPdfNN Run 4/10, Epoch 866/1000, Training Loss (NLML): -959.7316\n",
      "convergence dfGPdfNN Run 4/10, Epoch 867/1000, Training Loss (NLML): -959.7344\n",
      "convergence dfGPdfNN Run 4/10, Epoch 868/1000, Training Loss (NLML): -959.7371\n",
      "convergence dfGPdfNN Run 4/10, Epoch 869/1000, Training Loss (NLML): -959.7408\n",
      "convergence dfGPdfNN Run 4/10, Epoch 870/1000, Training Loss (NLML): -959.7554\n",
      "convergence dfGPdfNN Run 4/10, Epoch 871/1000, Training Loss (NLML): -959.7482\n",
      "convergence dfGPdfNN Run 4/10, Epoch 872/1000, Training Loss (NLML): -959.7501\n",
      "convergence dfGPdfNN Run 4/10, Epoch 873/1000, Training Loss (NLML): -959.7535\n",
      "convergence dfGPdfNN Run 4/10, Epoch 874/1000, Training Loss (NLML): -959.7562\n",
      "convergence dfGPdfNN Run 4/10, Epoch 875/1000, Training Loss (NLML): -959.7616\n",
      "convergence dfGPdfNN Run 4/10, Epoch 876/1000, Training Loss (NLML): -959.7653\n",
      "convergence dfGPdfNN Run 4/10, Epoch 877/1000, Training Loss (NLML): -959.7670\n",
      "convergence dfGPdfNN Run 4/10, Epoch 878/1000, Training Loss (NLML): -959.7699\n",
      "convergence dfGPdfNN Run 4/10, Epoch 879/1000, Training Loss (NLML): -959.7729\n",
      "convergence dfGPdfNN Run 4/10, Epoch 880/1000, Training Loss (NLML): -959.7766\n",
      "convergence dfGPdfNN Run 4/10, Epoch 881/1000, Training Loss (NLML): -959.7786\n",
      "convergence dfGPdfNN Run 4/10, Epoch 882/1000, Training Loss (NLML): -959.7825\n",
      "convergence dfGPdfNN Run 4/10, Epoch 883/1000, Training Loss (NLML): -959.7852\n",
      "convergence dfGPdfNN Run 4/10, Epoch 884/1000, Training Loss (NLML): -959.7891\n",
      "convergence dfGPdfNN Run 4/10, Epoch 885/1000, Training Loss (NLML): -959.7914\n",
      "convergence dfGPdfNN Run 4/10, Epoch 886/1000, Training Loss (NLML): -959.7948\n",
      "convergence dfGPdfNN Run 4/10, Epoch 887/1000, Training Loss (NLML): -959.7985\n",
      "convergence dfGPdfNN Run 4/10, Epoch 888/1000, Training Loss (NLML): -959.8020\n",
      "convergence dfGPdfNN Run 4/10, Epoch 889/1000, Training Loss (NLML): -959.8149\n",
      "convergence dfGPdfNN Run 4/10, Epoch 890/1000, Training Loss (NLML): -959.8087\n",
      "convergence dfGPdfNN Run 4/10, Epoch 891/1000, Training Loss (NLML): -959.8123\n",
      "convergence dfGPdfNN Run 4/10, Epoch 892/1000, Training Loss (NLML): -959.8147\n",
      "convergence dfGPdfNN Run 4/10, Epoch 893/1000, Training Loss (NLML): -959.8165\n",
      "convergence dfGPdfNN Run 4/10, Epoch 894/1000, Training Loss (NLML): -959.8198\n",
      "convergence dfGPdfNN Run 4/10, Epoch 895/1000, Training Loss (NLML): -959.8220\n",
      "convergence dfGPdfNN Run 4/10, Epoch 896/1000, Training Loss (NLML): -959.8251\n",
      "convergence dfGPdfNN Run 4/10, Epoch 897/1000, Training Loss (NLML): -959.8286\n",
      "convergence dfGPdfNN Run 4/10, Epoch 898/1000, Training Loss (NLML): -959.8414\n",
      "convergence dfGPdfNN Run 4/10, Epoch 899/1000, Training Loss (NLML): -959.8461\n",
      "convergence dfGPdfNN Run 4/10, Epoch 900/1000, Training Loss (NLML): -959.8389\n",
      "convergence dfGPdfNN Run 4/10, Epoch 901/1000, Training Loss (NLML): -959.8409\n",
      "convergence dfGPdfNN Run 4/10, Epoch 902/1000, Training Loss (NLML): -959.8448\n",
      "convergence dfGPdfNN Run 4/10, Epoch 903/1000, Training Loss (NLML): -959.8481\n",
      "convergence dfGPdfNN Run 4/10, Epoch 904/1000, Training Loss (NLML): -959.8499\n",
      "convergence dfGPdfNN Run 4/10, Epoch 905/1000, Training Loss (NLML): -959.8544\n",
      "convergence dfGPdfNN Run 4/10, Epoch 906/1000, Training Loss (NLML): -959.8563\n",
      "convergence dfGPdfNN Run 4/10, Epoch 907/1000, Training Loss (NLML): -959.8684\n",
      "convergence dfGPdfNN Run 4/10, Epoch 908/1000, Training Loss (NLML): -959.8718\n",
      "convergence dfGPdfNN Run 4/10, Epoch 909/1000, Training Loss (NLML): -959.8650\n",
      "convergence dfGPdfNN Run 4/10, Epoch 910/1000, Training Loss (NLML): -959.8671\n",
      "convergence dfGPdfNN Run 4/10, Epoch 911/1000, Training Loss (NLML): -959.8699\n",
      "convergence dfGPdfNN Run 4/10, Epoch 912/1000, Training Loss (NLML): -959.8746\n",
      "convergence dfGPdfNN Run 4/10, Epoch 913/1000, Training Loss (NLML): -959.8779\n",
      "convergence dfGPdfNN Run 4/10, Epoch 914/1000, Training Loss (NLML): -959.8914\n",
      "convergence dfGPdfNN Run 4/10, Epoch 915/1000, Training Loss (NLML): -959.8837\n",
      "convergence dfGPdfNN Run 4/10, Epoch 916/1000, Training Loss (NLML): -959.8860\n",
      "convergence dfGPdfNN Run 4/10, Epoch 917/1000, Training Loss (NLML): -959.9000\n",
      "convergence dfGPdfNN Run 4/10, Epoch 918/1000, Training Loss (NLML): -959.8928\n",
      "convergence dfGPdfNN Run 4/10, Epoch 919/1000, Training Loss (NLML): -959.8955\n",
      "convergence dfGPdfNN Run 4/10, Epoch 920/1000, Training Loss (NLML): -959.8972\n",
      "convergence dfGPdfNN Run 4/10, Epoch 921/1000, Training Loss (NLML): -959.9119\n",
      "convergence dfGPdfNN Run 4/10, Epoch 922/1000, Training Loss (NLML): -959.9042\n",
      "convergence dfGPdfNN Run 4/10, Epoch 923/1000, Training Loss (NLML): -959.9058\n",
      "convergence dfGPdfNN Run 4/10, Epoch 924/1000, Training Loss (NLML): -959.9200\n",
      "convergence dfGPdfNN Run 4/10, Epoch 925/1000, Training Loss (NLML): -959.9125\n",
      "convergence dfGPdfNN Run 4/10, Epoch 926/1000, Training Loss (NLML): -959.9147\n",
      "convergence dfGPdfNN Run 4/10, Epoch 927/1000, Training Loss (NLML): -959.9279\n",
      "convergence dfGPdfNN Run 4/10, Epoch 928/1000, Training Loss (NLML): -959.9196\n",
      "convergence dfGPdfNN Run 4/10, Epoch 929/1000, Training Loss (NLML): -959.9244\n",
      "convergence dfGPdfNN Run 4/10, Epoch 930/1000, Training Loss (NLML): -959.9373\n",
      "convergence dfGPdfNN Run 4/10, Epoch 931/1000, Training Loss (NLML): -959.9298\n",
      "convergence dfGPdfNN Run 4/10, Epoch 932/1000, Training Loss (NLML): -959.9314\n",
      "convergence dfGPdfNN Run 4/10, Epoch 933/1000, Training Loss (NLML): -959.9457\n",
      "convergence dfGPdfNN Run 4/10, Epoch 934/1000, Training Loss (NLML): -959.9478\n",
      "convergence dfGPdfNN Run 4/10, Epoch 935/1000, Training Loss (NLML): -959.9397\n",
      "convergence dfGPdfNN Run 4/10, Epoch 936/1000, Training Loss (NLML): -959.9421\n",
      "convergence dfGPdfNN Run 4/10, Epoch 937/1000, Training Loss (NLML): -959.9478\n",
      "convergence dfGPdfNN Run 4/10, Epoch 938/1000, Training Loss (NLML): -959.9596\n",
      "convergence dfGPdfNN Run 4/10, Epoch 939/1000, Training Loss (NLML): -959.9630\n",
      "convergence dfGPdfNN Run 4/10, Epoch 940/1000, Training Loss (NLML): -959.9546\n",
      "convergence dfGPdfNN Run 4/10, Epoch 941/1000, Training Loss (NLML): -959.9570\n",
      "convergence dfGPdfNN Run 4/10, Epoch 942/1000, Training Loss (NLML): -959.9607\n",
      "convergence dfGPdfNN Run 4/10, Epoch 943/1000, Training Loss (NLML): -959.9747\n",
      "convergence dfGPdfNN Run 4/10, Epoch 944/1000, Training Loss (NLML): -959.9763\n",
      "convergence dfGPdfNN Run 4/10, Epoch 945/1000, Training Loss (NLML): -959.9791\n",
      "convergence dfGPdfNN Run 4/10, Epoch 946/1000, Training Loss (NLML): -959.9705\n",
      "convergence dfGPdfNN Run 4/10, Epoch 947/1000, Training Loss (NLML): -959.9738\n",
      "convergence dfGPdfNN Run 4/10, Epoch 948/1000, Training Loss (NLML): -959.9774\n",
      "convergence dfGPdfNN Run 4/10, Epoch 949/1000, Training Loss (NLML): -959.9906\n",
      "convergence dfGPdfNN Run 4/10, Epoch 950/1000, Training Loss (NLML): -959.9929\n",
      "convergence dfGPdfNN Run 4/10, Epoch 951/1000, Training Loss (NLML): -959.9943\n",
      "convergence dfGPdfNN Run 4/10, Epoch 952/1000, Training Loss (NLML): -959.9896\n",
      "convergence dfGPdfNN Run 4/10, Epoch 953/1000, Training Loss (NLML): -959.9915\n",
      "convergence dfGPdfNN Run 4/10, Epoch 954/1000, Training Loss (NLML): -959.9949\n",
      "convergence dfGPdfNN Run 4/10, Epoch 955/1000, Training Loss (NLML): -960.0057\n",
      "convergence dfGPdfNN Run 4/10, Epoch 956/1000, Training Loss (NLML): -960.0092\n",
      "convergence dfGPdfNN Run 4/10, Epoch 957/1000, Training Loss (NLML): -960.0127\n",
      "convergence dfGPdfNN Run 4/10, Epoch 958/1000, Training Loss (NLML): -960.0142\n",
      "convergence dfGPdfNN Run 4/10, Epoch 959/1000, Training Loss (NLML): -960.0071\n",
      "convergence dfGPdfNN Run 4/10, Epoch 960/1000, Training Loss (NLML): -960.0098\n",
      "convergence dfGPdfNN Run 4/10, Epoch 961/1000, Training Loss (NLML): -960.0132\n",
      "convergence dfGPdfNN Run 4/10, Epoch 962/1000, Training Loss (NLML): -960.0260\n",
      "convergence dfGPdfNN Run 4/10, Epoch 963/1000, Training Loss (NLML): -960.0270\n",
      "convergence dfGPdfNN Run 4/10, Epoch 964/1000, Training Loss (NLML): -960.0311\n",
      "convergence dfGPdfNN Run 4/10, Epoch 965/1000, Training Loss (NLML): -960.0341\n",
      "convergence dfGPdfNN Run 4/10, Epoch 966/1000, Training Loss (NLML): -960.0238\n",
      "convergence dfGPdfNN Run 4/10, Epoch 967/1000, Training Loss (NLML): -960.0299\n",
      "convergence dfGPdfNN Run 4/10, Epoch 968/1000, Training Loss (NLML): -960.0426\n",
      "convergence dfGPdfNN Run 4/10, Epoch 969/1000, Training Loss (NLML): -960.0436\n",
      "convergence dfGPdfNN Run 4/10, Epoch 970/1000, Training Loss (NLML): -960.0466\n",
      "convergence dfGPdfNN Run 4/10, Epoch 971/1000, Training Loss (NLML): -960.0392\n",
      "convergence dfGPdfNN Run 4/10, Epoch 972/1000, Training Loss (NLML): -960.0431\n",
      "convergence dfGPdfNN Run 4/10, Epoch 973/1000, Training Loss (NLML): -960.0548\n",
      "convergence dfGPdfNN Run 4/10, Epoch 974/1000, Training Loss (NLML): -960.0582\n",
      "convergence dfGPdfNN Run 4/10, Epoch 975/1000, Training Loss (NLML): -960.0604\n",
      "convergence dfGPdfNN Run 4/10, Epoch 976/1000, Training Loss (NLML): -960.0619\n",
      "convergence dfGPdfNN Run 4/10, Epoch 977/1000, Training Loss (NLML): -960.0552\n",
      "convergence dfGPdfNN Run 4/10, Epoch 978/1000, Training Loss (NLML): -960.0576\n",
      "convergence dfGPdfNN Run 4/10, Epoch 979/1000, Training Loss (NLML): -960.0717\n",
      "convergence dfGPdfNN Run 4/10, Epoch 980/1000, Training Loss (NLML): -960.0717\n",
      "convergence dfGPdfNN Run 4/10, Epoch 981/1000, Training Loss (NLML): -960.0747\n",
      "convergence dfGPdfNN Run 4/10, Epoch 982/1000, Training Loss (NLML): -960.0780\n",
      "convergence dfGPdfNN Run 4/10, Epoch 983/1000, Training Loss (NLML): -960.0797\n",
      "convergence dfGPdfNN Run 4/10, Epoch 984/1000, Training Loss (NLML): -960.0739\n",
      "convergence dfGPdfNN Run 4/10, Epoch 985/1000, Training Loss (NLML): -960.0764\n",
      "convergence dfGPdfNN Run 4/10, Epoch 986/1000, Training Loss (NLML): -960.0898\n",
      "convergence dfGPdfNN Run 4/10, Epoch 987/1000, Training Loss (NLML): -960.0898\n",
      "convergence dfGPdfNN Run 4/10, Epoch 988/1000, Training Loss (NLML): -960.0936\n",
      "convergence dfGPdfNN Run 4/10, Epoch 989/1000, Training Loss (NLML): -960.0945\n",
      "convergence dfGPdfNN Run 4/10, Epoch 990/1000, Training Loss (NLML): -960.0996\n",
      "convergence dfGPdfNN Run 4/10, Epoch 991/1000, Training Loss (NLML): -960.1011\n",
      "convergence dfGPdfNN Run 4/10, Epoch 992/1000, Training Loss (NLML): -960.0934\n",
      "convergence dfGPdfNN Run 4/10, Epoch 993/1000, Training Loss (NLML): -960.0957\n",
      "convergence dfGPdfNN Run 4/10, Epoch 994/1000, Training Loss (NLML): -960.1088\n",
      "convergence dfGPdfNN Run 4/10, Epoch 995/1000, Training Loss (NLML): -960.1105\n",
      "convergence dfGPdfNN Run 4/10, Epoch 996/1000, Training Loss (NLML): -960.1128\n",
      "convergence dfGPdfNN Run 4/10, Epoch 997/1000, Training Loss (NLML): -960.1179\n",
      "convergence dfGPdfNN Run 4/10, Epoch 998/1000, Training Loss (NLML): -960.1199\n",
      "convergence dfGPdfNN Run 4/10, Epoch 999/1000, Training Loss (NLML): -960.1221\n",
      "convergence dfGPdfNN Run 4/10, Epoch 1000/1000, Training Loss (NLML): -960.1249\n",
      "\n",
      "--- Training Run 5/10 ---\n",
      "\n",
      "Start Training\n",
      "convergence dfGPdfNN Run 5/10, Epoch 1/1000, Training Loss (NLML): -896.1194\n",
      "convergence dfGPdfNN Run 5/10, Epoch 2/1000, Training Loss (NLML): -907.4653\n",
      "convergence dfGPdfNN Run 5/10, Epoch 3/1000, Training Loss (NLML): -910.5035\n",
      "convergence dfGPdfNN Run 5/10, Epoch 4/1000, Training Loss (NLML): -910.7133\n",
      "convergence dfGPdfNN Run 5/10, Epoch 5/1000, Training Loss (NLML): -912.4310\n",
      "convergence dfGPdfNN Run 5/10, Epoch 6/1000, Training Loss (NLML): -913.9253\n",
      "convergence dfGPdfNN Run 5/10, Epoch 7/1000, Training Loss (NLML): -914.5103\n",
      "convergence dfGPdfNN Run 5/10, Epoch 8/1000, Training Loss (NLML): -916.2389\n",
      "convergence dfGPdfNN Run 5/10, Epoch 9/1000, Training Loss (NLML): -918.1936\n",
      "convergence dfGPdfNN Run 5/10, Epoch 10/1000, Training Loss (NLML): -919.6748\n",
      "convergence dfGPdfNN Run 5/10, Epoch 11/1000, Training Loss (NLML): -921.0027\n",
      "convergence dfGPdfNN Run 5/10, Epoch 12/1000, Training Loss (NLML): -922.1072\n",
      "convergence dfGPdfNN Run 5/10, Epoch 13/1000, Training Loss (NLML): -923.0941\n",
      "convergence dfGPdfNN Run 5/10, Epoch 14/1000, Training Loss (NLML): -924.0535\n",
      "convergence dfGPdfNN Run 5/10, Epoch 15/1000, Training Loss (NLML): -925.0524\n",
      "convergence dfGPdfNN Run 5/10, Epoch 16/1000, Training Loss (NLML): -926.0107\n",
      "convergence dfGPdfNN Run 5/10, Epoch 17/1000, Training Loss (NLML): -926.9684\n",
      "convergence dfGPdfNN Run 5/10, Epoch 18/1000, Training Loss (NLML): -927.8149\n",
      "convergence dfGPdfNN Run 5/10, Epoch 19/1000, Training Loss (NLML): -928.3574\n",
      "convergence dfGPdfNN Run 5/10, Epoch 20/1000, Training Loss (NLML): -929.5773\n",
      "convergence dfGPdfNN Run 5/10, Epoch 21/1000, Training Loss (NLML): -930.4033\n",
      "convergence dfGPdfNN Run 5/10, Epoch 22/1000, Training Loss (NLML): -931.2063\n",
      "convergence dfGPdfNN Run 5/10, Epoch 23/1000, Training Loss (NLML): -931.9612\n",
      "convergence dfGPdfNN Run 5/10, Epoch 24/1000, Training Loss (NLML): -932.6714\n",
      "convergence dfGPdfNN Run 5/10, Epoch 25/1000, Training Loss (NLML): -933.3492\n",
      "convergence dfGPdfNN Run 5/10, Epoch 26/1000, Training Loss (NLML): -934.0059\n",
      "convergence dfGPdfNN Run 5/10, Epoch 27/1000, Training Loss (NLML): -934.6360\n",
      "convergence dfGPdfNN Run 5/10, Epoch 28/1000, Training Loss (NLML): -935.2482\n",
      "convergence dfGPdfNN Run 5/10, Epoch 29/1000, Training Loss (NLML): -935.8285\n",
      "convergence dfGPdfNN Run 5/10, Epoch 30/1000, Training Loss (NLML): -936.3923\n",
      "convergence dfGPdfNN Run 5/10, Epoch 31/1000, Training Loss (NLML): -936.9379\n",
      "convergence dfGPdfNN Run 5/10, Epoch 32/1000, Training Loss (NLML): -937.4814\n",
      "convergence dfGPdfNN Run 5/10, Epoch 33/1000, Training Loss (NLML): -937.7180\n",
      "convergence dfGPdfNN Run 5/10, Epoch 34/1000, Training Loss (NLML): -938.2628\n",
      "convergence dfGPdfNN Run 5/10, Epoch 35/1000, Training Loss (NLML): -939.0376\n",
      "convergence dfGPdfNN Run 5/10, Epoch 36/1000, Training Loss (NLML): -939.5325\n",
      "convergence dfGPdfNN Run 5/10, Epoch 37/1000, Training Loss (NLML): -940.0157\n",
      "convergence dfGPdfNN Run 5/10, Epoch 38/1000, Training Loss (NLML): -940.4784\n",
      "convergence dfGPdfNN Run 5/10, Epoch 39/1000, Training Loss (NLML): -940.9219\n",
      "convergence dfGPdfNN Run 5/10, Epoch 40/1000, Training Loss (NLML): -941.3639\n",
      "convergence dfGPdfNN Run 5/10, Epoch 41/1000, Training Loss (NLML): -941.7263\n",
      "convergence dfGPdfNN Run 5/10, Epoch 42/1000, Training Loss (NLML): -942.1208\n",
      "convergence dfGPdfNN Run 5/10, Epoch 43/1000, Training Loss (NLML): -942.5653\n",
      "convergence dfGPdfNN Run 5/10, Epoch 44/1000, Training Loss (NLML): -942.9247\n",
      "convergence dfGPdfNN Run 5/10, Epoch 45/1000, Training Loss (NLML): -943.2994\n",
      "convergence dfGPdfNN Run 5/10, Epoch 46/1000, Training Loss (NLML): -943.6251\n",
      "convergence dfGPdfNN Run 5/10, Epoch 47/1000, Training Loss (NLML): -943.9504\n",
      "convergence dfGPdfNN Run 5/10, Epoch 48/1000, Training Loss (NLML): -944.3472\n",
      "convergence dfGPdfNN Run 5/10, Epoch 49/1000, Training Loss (NLML): -944.6833\n",
      "convergence dfGPdfNN Run 5/10, Epoch 50/1000, Training Loss (NLML): -945.0117\n",
      "convergence dfGPdfNN Run 5/10, Epoch 51/1000, Training Loss (NLML): -945.3419\n",
      "convergence dfGPdfNN Run 5/10, Epoch 52/1000, Training Loss (NLML): -945.6523\n",
      "convergence dfGPdfNN Run 5/10, Epoch 53/1000, Training Loss (NLML): -945.9524\n",
      "convergence dfGPdfNN Run 5/10, Epoch 54/1000, Training Loss (NLML): -944.2607\n",
      "convergence dfGPdfNN Run 5/10, Epoch 55/1000, Training Loss (NLML): -946.5247\n",
      "convergence dfGPdfNN Run 5/10, Epoch 56/1000, Training Loss (NLML): -946.7891\n",
      "convergence dfGPdfNN Run 5/10, Epoch 57/1000, Training Loss (NLML): -947.0499\n",
      "convergence dfGPdfNN Run 5/10, Epoch 58/1000, Training Loss (NLML): -947.2921\n",
      "convergence dfGPdfNN Run 5/10, Epoch 59/1000, Training Loss (NLML): -947.5155\n",
      "convergence dfGPdfNN Run 5/10, Epoch 60/1000, Training Loss (NLML): -947.7739\n",
      "convergence dfGPdfNN Run 5/10, Epoch 61/1000, Training Loss (NLML): -948.0110\n",
      "convergence dfGPdfNN Run 5/10, Epoch 62/1000, Training Loss (NLML): -948.2939\n",
      "convergence dfGPdfNN Run 5/10, Epoch 63/1000, Training Loss (NLML): -948.5210\n",
      "convergence dfGPdfNN Run 5/10, Epoch 64/1000, Training Loss (NLML): -948.6049\n",
      "convergence dfGPdfNN Run 5/10, Epoch 65/1000, Training Loss (NLML): -948.8469\n",
      "convergence dfGPdfNN Run 5/10, Epoch 66/1000, Training Loss (NLML): -949.1223\n",
      "convergence dfGPdfNN Run 5/10, Epoch 67/1000, Training Loss (NLML): -949.3472\n",
      "convergence dfGPdfNN Run 5/10, Epoch 68/1000, Training Loss (NLML): -949.5100\n",
      "convergence dfGPdfNN Run 5/10, Epoch 69/1000, Training Loss (NLML): -949.7361\n",
      "convergence dfGPdfNN Run 5/10, Epoch 70/1000, Training Loss (NLML): -949.9701\n",
      "convergence dfGPdfNN Run 5/10, Epoch 71/1000, Training Loss (NLML): -950.1764\n",
      "convergence dfGPdfNN Run 5/10, Epoch 72/1000, Training Loss (NLML): -950.4067\n",
      "convergence dfGPdfNN Run 5/10, Epoch 73/1000, Training Loss (NLML): -950.6279\n",
      "convergence dfGPdfNN Run 5/10, Epoch 74/1000, Training Loss (NLML): -950.8643\n",
      "convergence dfGPdfNN Run 5/10, Epoch 75/1000, Training Loss (NLML): -951.0792\n",
      "convergence dfGPdfNN Run 5/10, Epoch 76/1000, Training Loss (NLML): -951.3206\n",
      "convergence dfGPdfNN Run 5/10, Epoch 77/1000, Training Loss (NLML): -951.4382\n",
      "convergence dfGPdfNN Run 5/10, Epoch 78/1000, Training Loss (NLML): -951.5848\n",
      "convergence dfGPdfNN Run 5/10, Epoch 79/1000, Training Loss (NLML): -951.8333\n",
      "convergence dfGPdfNN Run 5/10, Epoch 80/1000, Training Loss (NLML): -952.0480\n",
      "convergence dfGPdfNN Run 5/10, Epoch 81/1000, Training Loss (NLML): -952.2310\n",
      "convergence dfGPdfNN Run 5/10, Epoch 82/1000, Training Loss (NLML): -952.3269\n",
      "convergence dfGPdfNN Run 5/10, Epoch 83/1000, Training Loss (NLML): -952.4736\n",
      "convergence dfGPdfNN Run 5/10, Epoch 84/1000, Training Loss (NLML): -952.5975\n",
      "convergence dfGPdfNN Run 5/10, Epoch 85/1000, Training Loss (NLML): -952.8486\n",
      "convergence dfGPdfNN Run 5/10, Epoch 86/1000, Training Loss (NLML): -953.0935\n",
      "convergence dfGPdfNN Run 5/10, Epoch 87/1000, Training Loss (NLML): -953.2643\n",
      "convergence dfGPdfNN Run 5/10, Epoch 88/1000, Training Loss (NLML): -953.3534\n",
      "convergence dfGPdfNN Run 5/10, Epoch 89/1000, Training Loss (NLML): -953.4056\n",
      "convergence dfGPdfNN Run 5/10, Epoch 90/1000, Training Loss (NLML): -953.3477\n",
      "convergence dfGPdfNN Run 5/10, Epoch 91/1000, Training Loss (NLML): -953.5901\n",
      "convergence dfGPdfNN Run 5/10, Epoch 92/1000, Training Loss (NLML): -953.7435\n",
      "convergence dfGPdfNN Run 5/10, Epoch 93/1000, Training Loss (NLML): -953.8817\n",
      "convergence dfGPdfNN Run 5/10, Epoch 94/1000, Training Loss (NLML): -953.8671\n",
      "convergence dfGPdfNN Run 5/10, Epoch 95/1000, Training Loss (NLML): -953.9641\n",
      "convergence dfGPdfNN Run 5/10, Epoch 96/1000, Training Loss (NLML): -954.0474\n",
      "convergence dfGPdfNN Run 5/10, Epoch 97/1000, Training Loss (NLML): -954.2189\n",
      "convergence dfGPdfNN Run 5/10, Epoch 98/1000, Training Loss (NLML): -954.3962\n",
      "convergence dfGPdfNN Run 5/10, Epoch 99/1000, Training Loss (NLML): -954.7307\n",
      "convergence dfGPdfNN Run 5/10, Epoch 100/1000, Training Loss (NLML): -954.9735\n",
      "convergence dfGPdfNN Run 5/10, Epoch 101/1000, Training Loss (NLML): -955.1318\n",
      "convergence dfGPdfNN Run 5/10, Epoch 102/1000, Training Loss (NLML): -955.2322\n",
      "convergence dfGPdfNN Run 5/10, Epoch 103/1000, Training Loss (NLML): -955.1989\n",
      "convergence dfGPdfNN Run 5/10, Epoch 104/1000, Training Loss (NLML): -955.2740\n",
      "convergence dfGPdfNN Run 5/10, Epoch 105/1000, Training Loss (NLML): -955.5824\n",
      "convergence dfGPdfNN Run 5/10, Epoch 106/1000, Training Loss (NLML): -955.7581\n",
      "convergence dfGPdfNN Run 5/10, Epoch 107/1000, Training Loss (NLML): -955.8267\n",
      "convergence dfGPdfNN Run 5/10, Epoch 108/1000, Training Loss (NLML): -955.7673\n",
      "convergence dfGPdfNN Run 5/10, Epoch 109/1000, Training Loss (NLML): -955.8021\n",
      "convergence dfGPdfNN Run 5/10, Epoch 110/1000, Training Loss (NLML): -956.0759\n",
      "convergence dfGPdfNN Run 5/10, Epoch 111/1000, Training Loss (NLML): -956.0830\n",
      "convergence dfGPdfNN Run 5/10, Epoch 112/1000, Training Loss (NLML): -955.9377\n",
      "convergence dfGPdfNN Run 5/10, Epoch 113/1000, Training Loss (NLML): -956.1935\n",
      "convergence dfGPdfNN Run 5/10, Epoch 114/1000, Training Loss (NLML): -956.2366\n",
      "convergence dfGPdfNN Run 5/10, Epoch 115/1000, Training Loss (NLML): -956.2487\n",
      "convergence dfGPdfNN Run 5/10, Epoch 116/1000, Training Loss (NLML): -956.3490\n",
      "convergence dfGPdfNN Run 5/10, Epoch 117/1000, Training Loss (NLML): -956.3721\n",
      "convergence dfGPdfNN Run 5/10, Epoch 118/1000, Training Loss (NLML): -956.3162\n",
      "convergence dfGPdfNN Run 5/10, Epoch 119/1000, Training Loss (NLML): -956.4417\n",
      "convergence dfGPdfNN Run 5/10, Epoch 120/1000, Training Loss (NLML): -956.3099\n",
      "convergence dfGPdfNN Run 5/10, Epoch 121/1000, Training Loss (NLML): -956.1532\n",
      "convergence dfGPdfNN Run 5/10, Epoch 122/1000, Training Loss (NLML): -956.1688\n",
      "convergence dfGPdfNN Run 5/10, Epoch 123/1000, Training Loss (NLML): -956.2787\n",
      "convergence dfGPdfNN Run 5/10, Epoch 124/1000, Training Loss (NLML): -956.3773\n",
      "convergence dfGPdfNN Run 5/10, Epoch 125/1000, Training Loss (NLML): -956.4684\n",
      "convergence dfGPdfNN Run 5/10, Epoch 126/1000, Training Loss (NLML): -956.5574\n",
      "convergence dfGPdfNN Run 5/10, Epoch 127/1000, Training Loss (NLML): -956.6663\n",
      "convergence dfGPdfNN Run 5/10, Epoch 128/1000, Training Loss (NLML): -956.6853\n",
      "convergence dfGPdfNN Run 5/10, Epoch 129/1000, Training Loss (NLML): -956.5492\n",
      "convergence dfGPdfNN Run 5/10, Epoch 130/1000, Training Loss (NLML): -956.4489\n",
      "convergence dfGPdfNN Run 5/10, Epoch 131/1000, Training Loss (NLML): -956.2738\n",
      "convergence dfGPdfNN Run 5/10, Epoch 132/1000, Training Loss (NLML): -956.3865\n",
      "convergence dfGPdfNN Run 5/10, Epoch 133/1000, Training Loss (NLML): -956.4081\n",
      "convergence dfGPdfNN Run 5/10, Epoch 134/1000, Training Loss (NLML): -956.4841\n",
      "convergence dfGPdfNN Run 5/10, Epoch 135/1000, Training Loss (NLML): -956.5394\n",
      "convergence dfGPdfNN Run 5/10, Epoch 136/1000, Training Loss (NLML): -956.4454\n",
      "convergence dfGPdfNN Run 5/10, Epoch 137/1000, Training Loss (NLML): -956.5620\n",
      "convergence dfGPdfNN Run 5/10, Epoch 138/1000, Training Loss (NLML): -956.3861\n",
      "convergence dfGPdfNN Run 5/10, Epoch 139/1000, Training Loss (NLML): -956.5242\n",
      "convergence dfGPdfNN Run 5/10, Epoch 140/1000, Training Loss (NLML): -956.5841\n",
      "convergence dfGPdfNN Run 5/10, Epoch 141/1000, Training Loss (NLML): -956.6527\n",
      "convergence dfGPdfNN Run 5/10, Epoch 142/1000, Training Loss (NLML): -956.6897\n",
      "convergence dfGPdfNN Run 5/10, Epoch 143/1000, Training Loss (NLML): -956.6273\n",
      "convergence dfGPdfNN Run 5/10, Epoch 144/1000, Training Loss (NLML): -956.6200\n",
      "convergence dfGPdfNN Run 5/10, Epoch 145/1000, Training Loss (NLML): -956.7869\n",
      "convergence dfGPdfNN Run 5/10, Epoch 146/1000, Training Loss (NLML): -956.7795\n",
      "convergence dfGPdfNN Run 5/10, Epoch 147/1000, Training Loss (NLML): -956.6290\n",
      "convergence dfGPdfNN Run 5/10, Epoch 148/1000, Training Loss (NLML): -956.7031\n",
      "convergence dfGPdfNN Run 5/10, Epoch 149/1000, Training Loss (NLML): -956.7386\n",
      "convergence dfGPdfNN Run 5/10, Epoch 150/1000, Training Loss (NLML): -956.7134\n",
      "convergence dfGPdfNN Run 5/10, Epoch 151/1000, Training Loss (NLML): -956.7712\n",
      "convergence dfGPdfNN Run 5/10, Epoch 152/1000, Training Loss (NLML): -956.7158\n",
      "convergence dfGPdfNN Run 5/10, Epoch 153/1000, Training Loss (NLML): -956.7357\n",
      "convergence dfGPdfNN Run 5/10, Epoch 154/1000, Training Loss (NLML): -956.7574\n",
      "convergence dfGPdfNN Run 5/10, Epoch 155/1000, Training Loss (NLML): -956.8022\n",
      "convergence dfGPdfNN Run 5/10, Epoch 156/1000, Training Loss (NLML): -956.8304\n",
      "convergence dfGPdfNN Run 5/10, Epoch 157/1000, Training Loss (NLML): -956.8971\n",
      "convergence dfGPdfNN Run 5/10, Epoch 158/1000, Training Loss (NLML): -956.9111\n",
      "convergence dfGPdfNN Run 5/10, Epoch 159/1000, Training Loss (NLML): -956.9266\n",
      "convergence dfGPdfNN Run 5/10, Epoch 160/1000, Training Loss (NLML): -956.9669\n",
      "convergence dfGPdfNN Run 5/10, Epoch 161/1000, Training Loss (NLML): -956.9840\n",
      "convergence dfGPdfNN Run 5/10, Epoch 162/1000, Training Loss (NLML): -957.0176\n",
      "convergence dfGPdfNN Run 5/10, Epoch 163/1000, Training Loss (NLML): -956.9716\n",
      "convergence dfGPdfNN Run 5/10, Epoch 164/1000, Training Loss (NLML): -956.9988\n",
      "convergence dfGPdfNN Run 5/10, Epoch 165/1000, Training Loss (NLML): -956.9977\n",
      "convergence dfGPdfNN Run 5/10, Epoch 166/1000, Training Loss (NLML): -957.0372\n",
      "convergence dfGPdfNN Run 5/10, Epoch 167/1000, Training Loss (NLML): -957.0555\n",
      "convergence dfGPdfNN Run 5/10, Epoch 168/1000, Training Loss (NLML): -957.0742\n",
      "convergence dfGPdfNN Run 5/10, Epoch 169/1000, Training Loss (NLML): -957.0935\n",
      "convergence dfGPdfNN Run 5/10, Epoch 170/1000, Training Loss (NLML): -957.1147\n",
      "convergence dfGPdfNN Run 5/10, Epoch 171/1000, Training Loss (NLML): -957.1576\n",
      "convergence dfGPdfNN Run 5/10, Epoch 172/1000, Training Loss (NLML): -957.1886\n",
      "convergence dfGPdfNN Run 5/10, Epoch 173/1000, Training Loss (NLML): -957.2463\n",
      "convergence dfGPdfNN Run 5/10, Epoch 174/1000, Training Loss (NLML): -957.2758\n",
      "convergence dfGPdfNN Run 5/10, Epoch 175/1000, Training Loss (NLML): -957.2854\n",
      "convergence dfGPdfNN Run 5/10, Epoch 176/1000, Training Loss (NLML): -957.3821\n",
      "convergence dfGPdfNN Run 5/10, Epoch 177/1000, Training Loss (NLML): -957.3074\n",
      "convergence dfGPdfNN Run 5/10, Epoch 178/1000, Training Loss (NLML): -957.3304\n",
      "convergence dfGPdfNN Run 5/10, Epoch 179/1000, Training Loss (NLML): -957.3477\n",
      "convergence dfGPdfNN Run 5/10, Epoch 180/1000, Training Loss (NLML): -957.4611\n",
      "convergence dfGPdfNN Run 5/10, Epoch 181/1000, Training Loss (NLML): -957.3914\n",
      "convergence dfGPdfNN Run 5/10, Epoch 182/1000, Training Loss (NLML): -957.4324\n",
      "convergence dfGPdfNN Run 5/10, Epoch 183/1000, Training Loss (NLML): -957.4436\n",
      "convergence dfGPdfNN Run 5/10, Epoch 184/1000, Training Loss (NLML): -957.4640\n",
      "convergence dfGPdfNN Run 5/10, Epoch 185/1000, Training Loss (NLML): -957.4855\n",
      "convergence dfGPdfNN Run 5/10, Epoch 186/1000, Training Loss (NLML): -957.4928\n",
      "convergence dfGPdfNN Run 5/10, Epoch 187/1000, Training Loss (NLML): -957.5098\n",
      "convergence dfGPdfNN Run 5/10, Epoch 188/1000, Training Loss (NLML): -957.5138\n",
      "convergence dfGPdfNN Run 5/10, Epoch 189/1000, Training Loss (NLML): -957.6051\n",
      "convergence dfGPdfNN Run 5/10, Epoch 190/1000, Training Loss (NLML): -957.6365\n",
      "convergence dfGPdfNN Run 5/10, Epoch 191/1000, Training Loss (NLML): -957.5872\n",
      "convergence dfGPdfNN Run 5/10, Epoch 192/1000, Training Loss (NLML): -957.5848\n",
      "convergence dfGPdfNN Run 5/10, Epoch 193/1000, Training Loss (NLML): -957.5885\n",
      "convergence dfGPdfNN Run 5/10, Epoch 194/1000, Training Loss (NLML): -957.5944\n",
      "convergence dfGPdfNN Run 5/10, Epoch 195/1000, Training Loss (NLML): -957.5958\n",
      "convergence dfGPdfNN Run 5/10, Epoch 196/1000, Training Loss (NLML): -957.6892\n",
      "convergence dfGPdfNN Run 5/10, Epoch 197/1000, Training Loss (NLML): -957.7346\n",
      "convergence dfGPdfNN Run 5/10, Epoch 198/1000, Training Loss (NLML): -957.7581\n",
      "convergence dfGPdfNN Run 5/10, Epoch 199/1000, Training Loss (NLML): -957.6920\n",
      "convergence dfGPdfNN Run 5/10, Epoch 200/1000, Training Loss (NLML): -957.6968\n",
      "convergence dfGPdfNN Run 5/10, Epoch 201/1000, Training Loss (NLML): -957.7189\n",
      "convergence dfGPdfNN Run 5/10, Epoch 202/1000, Training Loss (NLML): -957.7219\n",
      "convergence dfGPdfNN Run 5/10, Epoch 203/1000, Training Loss (NLML): -957.7983\n",
      "convergence dfGPdfNN Run 5/10, Epoch 204/1000, Training Loss (NLML): -957.7977\n",
      "convergence dfGPdfNN Run 5/10, Epoch 205/1000, Training Loss (NLML): -957.8237\n",
      "convergence dfGPdfNN Run 5/10, Epoch 206/1000, Training Loss (NLML): -957.7756\n",
      "convergence dfGPdfNN Run 5/10, Epoch 207/1000, Training Loss (NLML): -957.7699\n",
      "convergence dfGPdfNN Run 5/10, Epoch 208/1000, Training Loss (NLML): -957.7742\n",
      "convergence dfGPdfNN Run 5/10, Epoch 209/1000, Training Loss (NLML): -957.7828\n",
      "convergence dfGPdfNN Run 5/10, Epoch 210/1000, Training Loss (NLML): -957.8616\n",
      "convergence dfGPdfNN Run 5/10, Epoch 211/1000, Training Loss (NLML): -957.8906\n",
      "convergence dfGPdfNN Run 5/10, Epoch 212/1000, Training Loss (NLML): -957.9102\n",
      "convergence dfGPdfNN Run 5/10, Epoch 213/1000, Training Loss (NLML): -957.9213\n",
      "convergence dfGPdfNN Run 5/10, Epoch 214/1000, Training Loss (NLML): -957.8528\n",
      "convergence dfGPdfNN Run 5/10, Epoch 215/1000, Training Loss (NLML): -957.9297\n",
      "convergence dfGPdfNN Run 5/10, Epoch 216/1000, Training Loss (NLML): -957.9580\n",
      "convergence dfGPdfNN Run 5/10, Epoch 217/1000, Training Loss (NLML): -957.9764\n",
      "convergence dfGPdfNN Run 5/10, Epoch 218/1000, Training Loss (NLML): -957.9983\n",
      "convergence dfGPdfNN Run 5/10, Epoch 219/1000, Training Loss (NLML): -958.0228\n",
      "convergence dfGPdfNN Run 5/10, Epoch 220/1000, Training Loss (NLML): -958.0358\n",
      "convergence dfGPdfNN Run 5/10, Epoch 221/1000, Training Loss (NLML): -958.0476\n",
      "convergence dfGPdfNN Run 5/10, Epoch 222/1000, Training Loss (NLML): -958.0663\n",
      "convergence dfGPdfNN Run 5/10, Epoch 223/1000, Training Loss (NLML): -958.1042\n",
      "convergence dfGPdfNN Run 5/10, Epoch 224/1000, Training Loss (NLML): -958.1292\n",
      "convergence dfGPdfNN Run 5/10, Epoch 225/1000, Training Loss (NLML): -958.1343\n",
      "convergence dfGPdfNN Run 5/10, Epoch 226/1000, Training Loss (NLML): -958.1213\n",
      "convergence dfGPdfNN Run 5/10, Epoch 227/1000, Training Loss (NLML): -958.1257\n",
      "convergence dfGPdfNN Run 5/10, Epoch 228/1000, Training Loss (NLML): -958.1295\n",
      "convergence dfGPdfNN Run 5/10, Epoch 229/1000, Training Loss (NLML): -958.1312\n",
      "convergence dfGPdfNN Run 5/10, Epoch 230/1000, Training Loss (NLML): -958.1477\n",
      "convergence dfGPdfNN Run 5/10, Epoch 231/1000, Training Loss (NLML): -958.1914\n",
      "convergence dfGPdfNN Run 5/10, Epoch 232/1000, Training Loss (NLML): -958.2035\n",
      "convergence dfGPdfNN Run 5/10, Epoch 233/1000, Training Loss (NLML): -958.2145\n",
      "convergence dfGPdfNN Run 5/10, Epoch 234/1000, Training Loss (NLML): -958.2217\n",
      "convergence dfGPdfNN Run 5/10, Epoch 235/1000, Training Loss (NLML): -958.2322\n",
      "convergence dfGPdfNN Run 5/10, Epoch 236/1000, Training Loss (NLML): -958.2471\n",
      "convergence dfGPdfNN Run 5/10, Epoch 237/1000, Training Loss (NLML): -958.2679\n",
      "convergence dfGPdfNN Run 5/10, Epoch 238/1000, Training Loss (NLML): -958.2842\n",
      "convergence dfGPdfNN Run 5/10, Epoch 239/1000, Training Loss (NLML): -958.2767\n",
      "convergence dfGPdfNN Run 5/10, Epoch 240/1000, Training Loss (NLML): -958.3118\n",
      "convergence dfGPdfNN Run 5/10, Epoch 241/1000, Training Loss (NLML): -958.3640\n",
      "convergence dfGPdfNN Run 5/10, Epoch 242/1000, Training Loss (NLML): -958.3635\n",
      "convergence dfGPdfNN Run 5/10, Epoch 243/1000, Training Loss (NLML): -958.3760\n",
      "convergence dfGPdfNN Run 5/10, Epoch 244/1000, Training Loss (NLML): -958.3793\n",
      "convergence dfGPdfNN Run 5/10, Epoch 245/1000, Training Loss (NLML): -958.4263\n",
      "convergence dfGPdfNN Run 5/10, Epoch 246/1000, Training Loss (NLML): -958.3976\n",
      "convergence dfGPdfNN Run 5/10, Epoch 247/1000, Training Loss (NLML): -958.4332\n",
      "convergence dfGPdfNN Run 5/10, Epoch 248/1000, Training Loss (NLML): -958.4828\n",
      "convergence dfGPdfNN Run 5/10, Epoch 249/1000, Training Loss (NLML): -958.5034\n",
      "convergence dfGPdfNN Run 5/10, Epoch 250/1000, Training Loss (NLML): -958.4733\n",
      "convergence dfGPdfNN Run 5/10, Epoch 251/1000, Training Loss (NLML): -958.5023\n",
      "convergence dfGPdfNN Run 5/10, Epoch 252/1000, Training Loss (NLML): -958.5347\n",
      "convergence dfGPdfNN Run 5/10, Epoch 253/1000, Training Loss (NLML): -958.5808\n",
      "convergence dfGPdfNN Run 5/10, Epoch 254/1000, Training Loss (NLML): -958.6030\n",
      "convergence dfGPdfNN Run 5/10, Epoch 255/1000, Training Loss (NLML): -958.6215\n",
      "convergence dfGPdfNN Run 5/10, Epoch 256/1000, Training Loss (NLML): -958.6395\n",
      "convergence dfGPdfNN Run 5/10, Epoch 257/1000, Training Loss (NLML): -958.6705\n",
      "convergence dfGPdfNN Run 5/10, Epoch 258/1000, Training Loss (NLML): -958.7015\n",
      "convergence dfGPdfNN Run 5/10, Epoch 259/1000, Training Loss (NLML): -958.7185\n",
      "convergence dfGPdfNN Run 5/10, Epoch 260/1000, Training Loss (NLML): -958.7627\n",
      "convergence dfGPdfNN Run 5/10, Epoch 261/1000, Training Loss (NLML): -958.7844\n",
      "convergence dfGPdfNN Run 5/10, Epoch 262/1000, Training Loss (NLML): -958.8085\n",
      "convergence dfGPdfNN Run 5/10, Epoch 263/1000, Training Loss (NLML): -958.7804\n",
      "convergence dfGPdfNN Run 5/10, Epoch 264/1000, Training Loss (NLML): -958.7809\n",
      "convergence dfGPdfNN Run 5/10, Epoch 265/1000, Training Loss (NLML): -958.7728\n",
      "convergence dfGPdfNN Run 5/10, Epoch 266/1000, Training Loss (NLML): -958.7760\n",
      "convergence dfGPdfNN Run 5/10, Epoch 267/1000, Training Loss (NLML): -958.7582\n",
      "convergence dfGPdfNN Run 5/10, Epoch 268/1000, Training Loss (NLML): -958.7715\n",
      "convergence dfGPdfNN Run 5/10, Epoch 269/1000, Training Loss (NLML): -958.8147\n",
      "convergence dfGPdfNN Run 5/10, Epoch 270/1000, Training Loss (NLML): -958.8099\n",
      "convergence dfGPdfNN Run 5/10, Epoch 271/1000, Training Loss (NLML): -958.8242\n",
      "convergence dfGPdfNN Run 5/10, Epoch 272/1000, Training Loss (NLML): -958.8636\n",
      "convergence dfGPdfNN Run 5/10, Epoch 273/1000, Training Loss (NLML): -958.8397\n",
      "convergence dfGPdfNN Run 5/10, Epoch 274/1000, Training Loss (NLML): -958.8488\n",
      "convergence dfGPdfNN Run 5/10, Epoch 275/1000, Training Loss (NLML): -958.8665\n",
      "convergence dfGPdfNN Run 5/10, Epoch 276/1000, Training Loss (NLML): -958.8413\n",
      "convergence dfGPdfNN Run 5/10, Epoch 277/1000, Training Loss (NLML): -958.8406\n",
      "convergence dfGPdfNN Run 5/10, Epoch 278/1000, Training Loss (NLML): -957.8207\n",
      "convergence dfGPdfNN Run 5/10, Epoch 279/1000, Training Loss (NLML): -958.6885\n",
      "convergence dfGPdfNN Run 5/10, Epoch 280/1000, Training Loss (NLML): -958.6262\n",
      "convergence dfGPdfNN Run 5/10, Epoch 281/1000, Training Loss (NLML): -958.5660\n",
      "convergence dfGPdfNN Run 5/10, Epoch 282/1000, Training Loss (NLML): -958.5237\n",
      "convergence dfGPdfNN Run 5/10, Epoch 283/1000, Training Loss (NLML): -958.3265\n",
      "convergence dfGPdfNN Run 5/10, Epoch 284/1000, Training Loss (NLML): -958.5168\n",
      "convergence dfGPdfNN Run 5/10, Epoch 285/1000, Training Loss (NLML): -958.6204\n",
      "convergence dfGPdfNN Run 5/10, Epoch 286/1000, Training Loss (NLML): -958.5359\n",
      "convergence dfGPdfNN Run 5/10, Epoch 287/1000, Training Loss (NLML): -958.5193\n",
      "convergence dfGPdfNN Run 5/10, Epoch 288/1000, Training Loss (NLML): -958.7605\n",
      "convergence dfGPdfNN Run 5/10, Epoch 289/1000, Training Loss (NLML): -958.7712\n",
      "convergence dfGPdfNN Run 5/10, Epoch 290/1000, Training Loss (NLML): -958.7607\n",
      "convergence dfGPdfNN Run 5/10, Epoch 291/1000, Training Loss (NLML): -958.7034\n",
      "convergence dfGPdfNN Run 5/10, Epoch 292/1000, Training Loss (NLML): -958.6405\n",
      "convergence dfGPdfNN Run 5/10, Epoch 293/1000, Training Loss (NLML): -958.5770\n",
      "convergence dfGPdfNN Run 5/10, Epoch 294/1000, Training Loss (NLML): -958.5367\n",
      "convergence dfGPdfNN Run 5/10, Epoch 295/1000, Training Loss (NLML): -958.5500\n",
      "convergence dfGPdfNN Run 5/10, Epoch 296/1000, Training Loss (NLML): -958.6578\n",
      "convergence dfGPdfNN Run 5/10, Epoch 297/1000, Training Loss (NLML): -958.7798\n",
      "convergence dfGPdfNN Run 5/10, Epoch 298/1000, Training Loss (NLML): -958.8361\n",
      "convergence dfGPdfNN Run 5/10, Epoch 299/1000, Training Loss (NLML): -958.7524\n",
      "convergence dfGPdfNN Run 5/10, Epoch 300/1000, Training Loss (NLML): -958.6957\n",
      "convergence dfGPdfNN Run 5/10, Epoch 301/1000, Training Loss (NLML): -958.7184\n",
      "convergence dfGPdfNN Run 5/10, Epoch 302/1000, Training Loss (NLML): -958.7090\n",
      "convergence dfGPdfNN Run 5/10, Epoch 303/1000, Training Loss (NLML): -958.7067\n",
      "convergence dfGPdfNN Run 5/10, Epoch 304/1000, Training Loss (NLML): -958.7429\n",
      "convergence dfGPdfNN Run 5/10, Epoch 305/1000, Training Loss (NLML): -958.8357\n",
      "convergence dfGPdfNN Run 5/10, Epoch 306/1000, Training Loss (NLML): -958.9480\n",
      "convergence dfGPdfNN Run 5/10, Epoch 307/1000, Training Loss (NLML): -959.0048\n",
      "convergence dfGPdfNN Run 5/10, Epoch 308/1000, Training Loss (NLML): -959.0300\n",
      "convergence dfGPdfNN Run 5/10, Epoch 309/1000, Training Loss (NLML): -959.0917\n",
      "convergence dfGPdfNN Run 5/10, Epoch 310/1000, Training Loss (NLML): -959.0210\n",
      "convergence dfGPdfNN Run 5/10, Epoch 311/1000, Training Loss (NLML): -959.0382\n",
      "convergence dfGPdfNN Run 5/10, Epoch 312/1000, Training Loss (NLML): -959.0258\n",
      "convergence dfGPdfNN Run 5/10, Epoch 313/1000, Training Loss (NLML): -959.0438\n",
      "convergence dfGPdfNN Run 5/10, Epoch 314/1000, Training Loss (NLML): -958.9595\n",
      "convergence dfGPdfNN Run 5/10, Epoch 315/1000, Training Loss (NLML): -958.9532\n",
      "convergence dfGPdfNN Run 5/10, Epoch 316/1000, Training Loss (NLML): -958.9135\n",
      "convergence dfGPdfNN Run 5/10, Epoch 317/1000, Training Loss (NLML): -958.9082\n",
      "convergence dfGPdfNN Run 5/10, Epoch 318/1000, Training Loss (NLML): -958.9360\n",
      "convergence dfGPdfNN Run 5/10, Epoch 319/1000, Training Loss (NLML): -958.9872\n",
      "convergence dfGPdfNN Run 5/10, Epoch 320/1000, Training Loss (NLML): -959.0331\n",
      "convergence dfGPdfNN Run 5/10, Epoch 321/1000, Training Loss (NLML): -959.0704\n",
      "convergence dfGPdfNN Run 5/10, Epoch 322/1000, Training Loss (NLML): -959.0916\n",
      "convergence dfGPdfNN Run 5/10, Epoch 323/1000, Training Loss (NLML): -959.0883\n",
      "convergence dfGPdfNN Run 5/10, Epoch 324/1000, Training Loss (NLML): -959.1067\n",
      "convergence dfGPdfNN Run 5/10, Epoch 325/1000, Training Loss (NLML): -959.1212\n",
      "convergence dfGPdfNN Run 5/10, Epoch 326/1000, Training Loss (NLML): -959.1210\n",
      "convergence dfGPdfNN Run 5/10, Epoch 327/1000, Training Loss (NLML): -959.1506\n",
      "convergence dfGPdfNN Run 5/10, Epoch 328/1000, Training Loss (NLML): -959.1577\n",
      "convergence dfGPdfNN Run 5/10, Epoch 329/1000, Training Loss (NLML): -959.1887\n",
      "convergence dfGPdfNN Run 5/10, Epoch 330/1000, Training Loss (NLML): -959.1881\n",
      "convergence dfGPdfNN Run 5/10, Epoch 331/1000, Training Loss (NLML): -959.2150\n",
      "convergence dfGPdfNN Run 5/10, Epoch 332/1000, Training Loss (NLML): -959.2162\n",
      "convergence dfGPdfNN Run 5/10, Epoch 333/1000, Training Loss (NLML): -959.2122\n",
      "convergence dfGPdfNN Run 5/10, Epoch 334/1000, Training Loss (NLML): -959.2170\n",
      "convergence dfGPdfNN Run 5/10, Epoch 335/1000, Training Loss (NLML): -959.1869\n",
      "convergence dfGPdfNN Run 5/10, Epoch 336/1000, Training Loss (NLML): -959.1602\n",
      "convergence dfGPdfNN Run 5/10, Epoch 337/1000, Training Loss (NLML): -959.1741\n",
      "convergence dfGPdfNN Run 5/10, Epoch 338/1000, Training Loss (NLML): -959.1891\n",
      "convergence dfGPdfNN Run 5/10, Epoch 339/1000, Training Loss (NLML): -959.1583\n",
      "convergence dfGPdfNN Run 5/10, Epoch 340/1000, Training Loss (NLML): -959.1703\n",
      "convergence dfGPdfNN Run 5/10, Epoch 341/1000, Training Loss (NLML): -959.1895\n",
      "convergence dfGPdfNN Run 5/10, Epoch 342/1000, Training Loss (NLML): -959.1593\n",
      "convergence dfGPdfNN Run 5/10, Epoch 343/1000, Training Loss (NLML): -959.1553\n",
      "convergence dfGPdfNN Run 5/10, Epoch 344/1000, Training Loss (NLML): -959.1702\n",
      "convergence dfGPdfNN Run 5/10, Epoch 345/1000, Training Loss (NLML): -959.2058\n",
      "convergence dfGPdfNN Run 5/10, Epoch 346/1000, Training Loss (NLML): -959.2549\n",
      "convergence dfGPdfNN Run 5/10, Epoch 347/1000, Training Loss (NLML): -959.2850\n",
      "convergence dfGPdfNN Run 5/10, Epoch 348/1000, Training Loss (NLML): -959.2971\n",
      "convergence dfGPdfNN Run 5/10, Epoch 349/1000, Training Loss (NLML): -959.3235\n",
      "convergence dfGPdfNN Run 5/10, Epoch 350/1000, Training Loss (NLML): -959.3243\n",
      "convergence dfGPdfNN Run 5/10, Epoch 351/1000, Training Loss (NLML): -959.3280\n",
      "convergence dfGPdfNN Run 5/10, Epoch 352/1000, Training Loss (NLML): -959.3522\n",
      "convergence dfGPdfNN Run 5/10, Epoch 353/1000, Training Loss (NLML): -959.3630\n",
      "convergence dfGPdfNN Run 5/10, Epoch 354/1000, Training Loss (NLML): -959.3728\n",
      "convergence dfGPdfNN Run 5/10, Epoch 355/1000, Training Loss (NLML): -959.3861\n",
      "convergence dfGPdfNN Run 5/10, Epoch 356/1000, Training Loss (NLML): -959.4329\n",
      "convergence dfGPdfNN Run 5/10, Epoch 357/1000, Training Loss (NLML): -959.4229\n",
      "convergence dfGPdfNN Run 5/10, Epoch 358/1000, Training Loss (NLML): -959.4303\n",
      "convergence dfGPdfNN Run 5/10, Epoch 359/1000, Training Loss (NLML): -959.4370\n",
      "convergence dfGPdfNN Run 5/10, Epoch 360/1000, Training Loss (NLML): -959.4465\n",
      "convergence dfGPdfNN Run 5/10, Epoch 361/1000, Training Loss (NLML): -959.4629\n",
      "convergence dfGPdfNN Run 5/10, Epoch 362/1000, Training Loss (NLML): -959.4810\n",
      "convergence dfGPdfNN Run 5/10, Epoch 363/1000, Training Loss (NLML): -959.4900\n",
      "convergence dfGPdfNN Run 5/10, Epoch 364/1000, Training Loss (NLML): -959.4994\n",
      "convergence dfGPdfNN Run 5/10, Epoch 365/1000, Training Loss (NLML): -959.5023\n",
      "convergence dfGPdfNN Run 5/10, Epoch 366/1000, Training Loss (NLML): -959.5052\n",
      "convergence dfGPdfNN Run 5/10, Epoch 367/1000, Training Loss (NLML): -959.5096\n",
      "convergence dfGPdfNN Run 5/10, Epoch 368/1000, Training Loss (NLML): -959.5178\n",
      "convergence dfGPdfNN Run 5/10, Epoch 369/1000, Training Loss (NLML): -959.5219\n",
      "convergence dfGPdfNN Run 5/10, Epoch 370/1000, Training Loss (NLML): -959.5310\n",
      "convergence dfGPdfNN Run 5/10, Epoch 371/1000, Training Loss (NLML): -959.5436\n",
      "convergence dfGPdfNN Run 5/10, Epoch 372/1000, Training Loss (NLML): -959.5283\n",
      "convergence dfGPdfNN Run 5/10, Epoch 373/1000, Training Loss (NLML): -959.5271\n",
      "convergence dfGPdfNN Run 5/10, Epoch 374/1000, Training Loss (NLML): -959.5262\n",
      "convergence dfGPdfNN Run 5/10, Epoch 375/1000, Training Loss (NLML): -959.5343\n",
      "convergence dfGPdfNN Run 5/10, Epoch 376/1000, Training Loss (NLML): -959.5376\n",
      "convergence dfGPdfNN Run 5/10, Epoch 377/1000, Training Loss (NLML): -959.5560\n",
      "convergence dfGPdfNN Run 5/10, Epoch 378/1000, Training Loss (NLML): -959.5614\n",
      "convergence dfGPdfNN Run 5/10, Epoch 379/1000, Training Loss (NLML): -959.5804\n",
      "convergence dfGPdfNN Run 5/10, Epoch 380/1000, Training Loss (NLML): -959.5718\n",
      "convergence dfGPdfNN Run 5/10, Epoch 381/1000, Training Loss (NLML): -959.5747\n",
      "convergence dfGPdfNN Run 5/10, Epoch 382/1000, Training Loss (NLML): -959.5807\n",
      "convergence dfGPdfNN Run 5/10, Epoch 383/1000, Training Loss (NLML): -959.5881\n",
      "convergence dfGPdfNN Run 5/10, Epoch 384/1000, Training Loss (NLML): -959.5513\n",
      "convergence dfGPdfNN Run 5/10, Epoch 385/1000, Training Loss (NLML): -959.6030\n",
      "convergence dfGPdfNN Run 5/10, Epoch 386/1000, Training Loss (NLML): -959.5892\n",
      "convergence dfGPdfNN Run 5/10, Epoch 387/1000, Training Loss (NLML): -959.5831\n",
      "convergence dfGPdfNN Run 5/10, Epoch 388/1000, Training Loss (NLML): -959.5891\n",
      "convergence dfGPdfNN Run 5/10, Epoch 389/1000, Training Loss (NLML): -959.5991\n",
      "convergence dfGPdfNN Run 5/10, Epoch 390/1000, Training Loss (NLML): -959.5961\n",
      "convergence dfGPdfNN Run 5/10, Epoch 391/1000, Training Loss (NLML): -959.5709\n",
      "convergence dfGPdfNN Run 5/10, Epoch 392/1000, Training Loss (NLML): -959.5752\n",
      "convergence dfGPdfNN Run 5/10, Epoch 393/1000, Training Loss (NLML): -959.5703\n",
      "convergence dfGPdfNN Run 5/10, Epoch 394/1000, Training Loss (NLML): -959.5836\n",
      "convergence dfGPdfNN Run 5/10, Epoch 395/1000, Training Loss (NLML): -959.5818\n",
      "convergence dfGPdfNN Run 5/10, Epoch 396/1000, Training Loss (NLML): -959.5798\n",
      "convergence dfGPdfNN Run 5/10, Epoch 397/1000, Training Loss (NLML): -959.5994\n",
      "convergence dfGPdfNN Run 5/10, Epoch 398/1000, Training Loss (NLML): -959.5942\n",
      "convergence dfGPdfNN Run 5/10, Epoch 399/1000, Training Loss (NLML): -959.5959\n",
      "convergence dfGPdfNN Run 5/10, Epoch 400/1000, Training Loss (NLML): -959.6104\n",
      "convergence dfGPdfNN Run 5/10, Epoch 401/1000, Training Loss (NLML): -959.6191\n",
      "convergence dfGPdfNN Run 5/10, Epoch 402/1000, Training Loss (NLML): -959.6324\n",
      "convergence dfGPdfNN Run 5/10, Epoch 403/1000, Training Loss (NLML): -959.6375\n",
      "convergence dfGPdfNN Run 5/10, Epoch 404/1000, Training Loss (NLML): -959.6394\n",
      "convergence dfGPdfNN Run 5/10, Epoch 405/1000, Training Loss (NLML): -959.6350\n",
      "convergence dfGPdfNN Run 5/10, Epoch 406/1000, Training Loss (NLML): -959.6179\n",
      "convergence dfGPdfNN Run 5/10, Epoch 407/1000, Training Loss (NLML): -959.6062\n",
      "convergence dfGPdfNN Run 5/10, Epoch 408/1000, Training Loss (NLML): -959.6057\n",
      "convergence dfGPdfNN Run 5/10, Epoch 409/1000, Training Loss (NLML): -959.5903\n",
      "convergence dfGPdfNN Run 5/10, Epoch 410/1000, Training Loss (NLML): -959.5868\n",
      "convergence dfGPdfNN Run 5/10, Epoch 411/1000, Training Loss (NLML): -959.5692\n",
      "convergence dfGPdfNN Run 5/10, Epoch 412/1000, Training Loss (NLML): -959.6672\n",
      "convergence dfGPdfNN Run 5/10, Epoch 413/1000, Training Loss (NLML): -959.6777\n",
      "convergence dfGPdfNN Run 5/10, Epoch 414/1000, Training Loss (NLML): -959.6101\n",
      "convergence dfGPdfNN Run 5/10, Epoch 415/1000, Training Loss (NLML): -959.6049\n",
      "convergence dfGPdfNN Run 5/10, Epoch 416/1000, Training Loss (NLML): -959.6525\n",
      "convergence dfGPdfNN Run 5/10, Epoch 417/1000, Training Loss (NLML): -959.6495\n",
      "convergence dfGPdfNN Run 5/10, Epoch 418/1000, Training Loss (NLML): -959.6492\n",
      "convergence dfGPdfNN Run 5/10, Epoch 419/1000, Training Loss (NLML): -959.6769\n",
      "convergence dfGPdfNN Run 5/10, Epoch 420/1000, Training Loss (NLML): -959.6746\n",
      "convergence dfGPdfNN Run 5/10, Epoch 421/1000, Training Loss (NLML): -959.6793\n",
      "convergence dfGPdfNN Run 5/10, Epoch 422/1000, Training Loss (NLML): -959.6932\n",
      "convergence dfGPdfNN Run 5/10, Epoch 423/1000, Training Loss (NLML): -959.7090\n",
      "convergence dfGPdfNN Run 5/10, Epoch 424/1000, Training Loss (NLML): -959.7251\n",
      "convergence dfGPdfNN Run 5/10, Epoch 425/1000, Training Loss (NLML): -959.7568\n",
      "convergence dfGPdfNN Run 5/10, Epoch 426/1000, Training Loss (NLML): -959.7555\n",
      "convergence dfGPdfNN Run 5/10, Epoch 427/1000, Training Loss (NLML): -959.7738\n",
      "convergence dfGPdfNN Run 5/10, Epoch 428/1000, Training Loss (NLML): -959.7817\n",
      "convergence dfGPdfNN Run 5/10, Epoch 429/1000, Training Loss (NLML): -959.7885\n",
      "convergence dfGPdfNN Run 5/10, Epoch 430/1000, Training Loss (NLML): -959.7563\n",
      "convergence dfGPdfNN Run 5/10, Epoch 431/1000, Training Loss (NLML): -959.7548\n",
      "convergence dfGPdfNN Run 5/10, Epoch 432/1000, Training Loss (NLML): -959.7592\n",
      "convergence dfGPdfNN Run 5/10, Epoch 433/1000, Training Loss (NLML): -959.7649\n",
      "convergence dfGPdfNN Run 5/10, Epoch 434/1000, Training Loss (NLML): -959.7858\n",
      "convergence dfGPdfNN Run 5/10, Epoch 435/1000, Training Loss (NLML): -959.7958\n",
      "convergence dfGPdfNN Run 5/10, Epoch 436/1000, Training Loss (NLML): -959.8015\n",
      "convergence dfGPdfNN Run 5/10, Epoch 437/1000, Training Loss (NLML): -959.7980\n",
      "convergence dfGPdfNN Run 5/10, Epoch 438/1000, Training Loss (NLML): -959.8026\n",
      "convergence dfGPdfNN Run 5/10, Epoch 439/1000, Training Loss (NLML): -959.8394\n",
      "convergence dfGPdfNN Run 5/10, Epoch 440/1000, Training Loss (NLML): -959.8459\n",
      "convergence dfGPdfNN Run 5/10, Epoch 441/1000, Training Loss (NLML): -959.8505\n",
      "convergence dfGPdfNN Run 5/10, Epoch 442/1000, Training Loss (NLML): -959.8536\n",
      "convergence dfGPdfNN Run 5/10, Epoch 443/1000, Training Loss (NLML): -959.8606\n",
      "convergence dfGPdfNN Run 5/10, Epoch 444/1000, Training Loss (NLML): -959.8690\n",
      "convergence dfGPdfNN Run 5/10, Epoch 445/1000, Training Loss (NLML): -959.8759\n",
      "convergence dfGPdfNN Run 5/10, Epoch 446/1000, Training Loss (NLML): -959.8792\n",
      "convergence dfGPdfNN Run 5/10, Epoch 447/1000, Training Loss (NLML): -959.8901\n",
      "convergence dfGPdfNN Run 5/10, Epoch 448/1000, Training Loss (NLML): -959.8928\n",
      "convergence dfGPdfNN Run 5/10, Epoch 449/1000, Training Loss (NLML): -959.8796\n",
      "convergence dfGPdfNN Run 5/10, Epoch 450/1000, Training Loss (NLML): -959.8862\n",
      "convergence dfGPdfNN Run 5/10, Epoch 451/1000, Training Loss (NLML): -959.8804\n",
      "convergence dfGPdfNN Run 5/10, Epoch 452/1000, Training Loss (NLML): -959.8877\n",
      "convergence dfGPdfNN Run 5/10, Epoch 453/1000, Training Loss (NLML): -959.8781\n",
      "convergence dfGPdfNN Run 5/10, Epoch 454/1000, Training Loss (NLML): -959.8846\n",
      "convergence dfGPdfNN Run 5/10, Epoch 455/1000, Training Loss (NLML): -959.8900\n",
      "convergence dfGPdfNN Run 5/10, Epoch 456/1000, Training Loss (NLML): -959.8958\n",
      "convergence dfGPdfNN Run 5/10, Epoch 457/1000, Training Loss (NLML): -959.9283\n",
      "convergence dfGPdfNN Run 5/10, Epoch 458/1000, Training Loss (NLML): -959.9301\n",
      "convergence dfGPdfNN Run 5/10, Epoch 459/1000, Training Loss (NLML): -959.9452\n",
      "convergence dfGPdfNN Run 5/10, Epoch 460/1000, Training Loss (NLML): -959.9509\n",
      "convergence dfGPdfNN Run 5/10, Epoch 461/1000, Training Loss (NLML): -959.9666\n",
      "convergence dfGPdfNN Run 5/10, Epoch 462/1000, Training Loss (NLML): -959.9719\n",
      "convergence dfGPdfNN Run 5/10, Epoch 463/1000, Training Loss (NLML): -959.9741\n",
      "convergence dfGPdfNN Run 5/10, Epoch 464/1000, Training Loss (NLML): -959.9803\n",
      "convergence dfGPdfNN Run 5/10, Epoch 465/1000, Training Loss (NLML): -959.9545\n",
      "convergence dfGPdfNN Run 5/10, Epoch 466/1000, Training Loss (NLML): -959.9589\n",
      "convergence dfGPdfNN Run 5/10, Epoch 467/1000, Training Loss (NLML): -959.9620\n",
      "convergence dfGPdfNN Run 5/10, Epoch 468/1000, Training Loss (NLML): -959.9562\n",
      "convergence dfGPdfNN Run 5/10, Epoch 469/1000, Training Loss (NLML): -959.9634\n",
      "convergence dfGPdfNN Run 5/10, Epoch 470/1000, Training Loss (NLML): -959.9630\n",
      "convergence dfGPdfNN Run 5/10, Epoch 471/1000, Training Loss (NLML): -959.9569\n",
      "convergence dfGPdfNN Run 5/10, Epoch 472/1000, Training Loss (NLML): -959.9642\n",
      "convergence dfGPdfNN Run 5/10, Epoch 473/1000, Training Loss (NLML): -959.9655\n",
      "convergence dfGPdfNN Run 5/10, Epoch 474/1000, Training Loss (NLML): -959.9672\n",
      "convergence dfGPdfNN Run 5/10, Epoch 475/1000, Training Loss (NLML): -959.9773\n",
      "convergence dfGPdfNN Run 5/10, Epoch 476/1000, Training Loss (NLML): -959.9913\n",
      "convergence dfGPdfNN Run 5/10, Epoch 477/1000, Training Loss (NLML): -959.9969\n",
      "convergence dfGPdfNN Run 5/10, Epoch 478/1000, Training Loss (NLML): -960.0176\n",
      "convergence dfGPdfNN Run 5/10, Epoch 479/1000, Training Loss (NLML): -960.0204\n",
      "convergence dfGPdfNN Run 5/10, Epoch 480/1000, Training Loss (NLML): -960.0243\n",
      "convergence dfGPdfNN Run 5/10, Epoch 481/1000, Training Loss (NLML): -960.0211\n",
      "convergence dfGPdfNN Run 5/10, Epoch 482/1000, Training Loss (NLML): -960.0258\n",
      "convergence dfGPdfNN Run 5/10, Epoch 483/1000, Training Loss (NLML): -960.0286\n",
      "convergence dfGPdfNN Run 5/10, Epoch 484/1000, Training Loss (NLML): -960.0358\n",
      "convergence dfGPdfNN Run 5/10, Epoch 485/1000, Training Loss (NLML): -960.0399\n",
      "convergence dfGPdfNN Run 5/10, Epoch 486/1000, Training Loss (NLML): -960.0363\n",
      "convergence dfGPdfNN Run 5/10, Epoch 487/1000, Training Loss (NLML): -960.0503\n",
      "convergence dfGPdfNN Run 5/10, Epoch 488/1000, Training Loss (NLML): -960.0503\n",
      "convergence dfGPdfNN Run 5/10, Epoch 489/1000, Training Loss (NLML): -960.0537\n",
      "convergence dfGPdfNN Run 5/10, Epoch 490/1000, Training Loss (NLML): -960.0580\n",
      "convergence dfGPdfNN Run 5/10, Epoch 491/1000, Training Loss (NLML): -960.0630\n",
      "convergence dfGPdfNN Run 5/10, Epoch 492/1000, Training Loss (NLML): -960.0653\n",
      "convergence dfGPdfNN Run 5/10, Epoch 493/1000, Training Loss (NLML): -960.0663\n",
      "convergence dfGPdfNN Run 5/10, Epoch 494/1000, Training Loss (NLML): -960.0715\n",
      "convergence dfGPdfNN Run 5/10, Epoch 495/1000, Training Loss (NLML): -960.0791\n",
      "convergence dfGPdfNN Run 5/10, Epoch 496/1000, Training Loss (NLML): -960.0815\n",
      "convergence dfGPdfNN Run 5/10, Epoch 497/1000, Training Loss (NLML): -960.1219\n",
      "convergence dfGPdfNN Run 5/10, Epoch 498/1000, Training Loss (NLML): -960.1210\n",
      "convergence dfGPdfNN Run 5/10, Epoch 499/1000, Training Loss (NLML): -960.1348\n",
      "convergence dfGPdfNN Run 5/10, Epoch 500/1000, Training Loss (NLML): -960.1422\n",
      "convergence dfGPdfNN Run 5/10, Epoch 501/1000, Training Loss (NLML): -960.1401\n",
      "convergence dfGPdfNN Run 5/10, Epoch 502/1000, Training Loss (NLML): -960.1454\n",
      "convergence dfGPdfNN Run 5/10, Epoch 503/1000, Training Loss (NLML): -960.1375\n",
      "convergence dfGPdfNN Run 5/10, Epoch 504/1000, Training Loss (NLML): -960.1412\n",
      "convergence dfGPdfNN Run 5/10, Epoch 505/1000, Training Loss (NLML): -960.1461\n",
      "convergence dfGPdfNN Run 5/10, Epoch 506/1000, Training Loss (NLML): -960.1520\n",
      "convergence dfGPdfNN Run 5/10, Epoch 507/1000, Training Loss (NLML): -960.1583\n",
      "convergence dfGPdfNN Run 5/10, Epoch 508/1000, Training Loss (NLML): -960.1794\n",
      "convergence dfGPdfNN Run 5/10, Epoch 509/1000, Training Loss (NLML): -960.1863\n",
      "convergence dfGPdfNN Run 5/10, Epoch 510/1000, Training Loss (NLML): -960.1942\n",
      "convergence dfGPdfNN Run 5/10, Epoch 511/1000, Training Loss (NLML): -960.2034\n",
      "convergence dfGPdfNN Run 5/10, Epoch 512/1000, Training Loss (NLML): -960.2118\n",
      "convergence dfGPdfNN Run 5/10, Epoch 513/1000, Training Loss (NLML): -960.2178\n",
      "convergence dfGPdfNN Run 5/10, Epoch 514/1000, Training Loss (NLML): -960.2034\n",
      "convergence dfGPdfNN Run 5/10, Epoch 515/1000, Training Loss (NLML): -960.1957\n",
      "convergence dfGPdfNN Run 5/10, Epoch 516/1000, Training Loss (NLML): -960.2095\n",
      "convergence dfGPdfNN Run 5/10, Epoch 517/1000, Training Loss (NLML): -960.2220\n",
      "convergence dfGPdfNN Run 5/10, Epoch 518/1000, Training Loss (NLML): -960.2194\n",
      "convergence dfGPdfNN Run 5/10, Epoch 519/1000, Training Loss (NLML): -960.2181\n",
      "convergence dfGPdfNN Run 5/10, Epoch 520/1000, Training Loss (NLML): -960.1936\n",
      "convergence dfGPdfNN Run 5/10, Epoch 521/1000, Training Loss (NLML): -960.2477\n",
      "convergence dfGPdfNN Run 5/10, Epoch 522/1000, Training Loss (NLML): -960.2520\n",
      "convergence dfGPdfNN Run 5/10, Epoch 523/1000, Training Loss (NLML): -960.2579\n",
      "convergence dfGPdfNN Run 5/10, Epoch 524/1000, Training Loss (NLML): -960.2644\n",
      "convergence dfGPdfNN Run 5/10, Epoch 525/1000, Training Loss (NLML): -960.2717\n",
      "convergence dfGPdfNN Run 5/10, Epoch 526/1000, Training Loss (NLML): -960.2765\n",
      "convergence dfGPdfNN Run 5/10, Epoch 527/1000, Training Loss (NLML): -960.2842\n",
      "convergence dfGPdfNN Run 5/10, Epoch 528/1000, Training Loss (NLML): -960.2911\n",
      "convergence dfGPdfNN Run 5/10, Epoch 529/1000, Training Loss (NLML): -960.2950\n",
      "convergence dfGPdfNN Run 5/10, Epoch 530/1000, Training Loss (NLML): -960.2987\n",
      "convergence dfGPdfNN Run 5/10, Epoch 531/1000, Training Loss (NLML): -960.3027\n",
      "convergence dfGPdfNN Run 5/10, Epoch 532/1000, Training Loss (NLML): -960.2983\n",
      "convergence dfGPdfNN Run 5/10, Epoch 533/1000, Training Loss (NLML): -960.3036\n",
      "convergence dfGPdfNN Run 5/10, Epoch 534/1000, Training Loss (NLML): -960.3081\n",
      "convergence dfGPdfNN Run 5/10, Epoch 535/1000, Training Loss (NLML): -960.3114\n",
      "convergence dfGPdfNN Run 5/10, Epoch 536/1000, Training Loss (NLML): -960.3156\n",
      "convergence dfGPdfNN Run 5/10, Epoch 537/1000, Training Loss (NLML): -960.3198\n",
      "convergence dfGPdfNN Run 5/10, Epoch 538/1000, Training Loss (NLML): -960.3232\n",
      "convergence dfGPdfNN Run 5/10, Epoch 539/1000, Training Loss (NLML): -960.3059\n",
      "convergence dfGPdfNN Run 5/10, Epoch 540/1000, Training Loss (NLML): -960.3104\n",
      "convergence dfGPdfNN Run 5/10, Epoch 541/1000, Training Loss (NLML): -960.3041\n",
      "convergence dfGPdfNN Run 5/10, Epoch 542/1000, Training Loss (NLML): -960.3083\n",
      "convergence dfGPdfNN Run 5/10, Epoch 543/1000, Training Loss (NLML): -960.3348\n",
      "convergence dfGPdfNN Run 5/10, Epoch 544/1000, Training Loss (NLML): -960.3339\n",
      "convergence dfGPdfNN Run 5/10, Epoch 545/1000, Training Loss (NLML): -960.3395\n",
      "convergence dfGPdfNN Run 5/10, Epoch 546/1000, Training Loss (NLML): -960.3409\n",
      "convergence dfGPdfNN Run 5/10, Epoch 547/1000, Training Loss (NLML): -960.3459\n",
      "convergence dfGPdfNN Run 5/10, Epoch 548/1000, Training Loss (NLML): -960.3458\n",
      "convergence dfGPdfNN Run 5/10, Epoch 549/1000, Training Loss (NLML): -960.3475\n",
      "convergence dfGPdfNN Run 5/10, Epoch 550/1000, Training Loss (NLML): -960.3534\n",
      "convergence dfGPdfNN Run 5/10, Epoch 551/1000, Training Loss (NLML): -960.3661\n",
      "convergence dfGPdfNN Run 5/10, Epoch 552/1000, Training Loss (NLML): -960.3682\n",
      "convergence dfGPdfNN Run 5/10, Epoch 553/1000, Training Loss (NLML): -960.3727\n",
      "convergence dfGPdfNN Run 5/10, Epoch 554/1000, Training Loss (NLML): -960.3805\n",
      "convergence dfGPdfNN Run 5/10, Epoch 555/1000, Training Loss (NLML): -960.3846\n",
      "convergence dfGPdfNN Run 5/10, Epoch 556/1000, Training Loss (NLML): -960.3875\n",
      "convergence dfGPdfNN Run 5/10, Epoch 557/1000, Training Loss (NLML): -960.3910\n",
      "convergence dfGPdfNN Run 5/10, Epoch 558/1000, Training Loss (NLML): -960.3940\n",
      "convergence dfGPdfNN Run 5/10, Epoch 559/1000, Training Loss (NLML): -960.3965\n",
      "convergence dfGPdfNN Run 5/10, Epoch 560/1000, Training Loss (NLML): -960.3995\n",
      "convergence dfGPdfNN Run 5/10, Epoch 561/1000, Training Loss (NLML): -960.4037\n",
      "convergence dfGPdfNN Run 5/10, Epoch 562/1000, Training Loss (NLML): -960.4073\n",
      "convergence dfGPdfNN Run 5/10, Epoch 563/1000, Training Loss (NLML): -960.4114\n",
      "convergence dfGPdfNN Run 5/10, Epoch 564/1000, Training Loss (NLML): -960.4137\n",
      "convergence dfGPdfNN Run 5/10, Epoch 565/1000, Training Loss (NLML): -960.4170\n",
      "convergence dfGPdfNN Run 5/10, Epoch 566/1000, Training Loss (NLML): -960.4192\n",
      "convergence dfGPdfNN Run 5/10, Epoch 567/1000, Training Loss (NLML): -960.4225\n",
      "convergence dfGPdfNN Run 5/10, Epoch 568/1000, Training Loss (NLML): -960.4250\n",
      "convergence dfGPdfNN Run 5/10, Epoch 569/1000, Training Loss (NLML): -960.4294\n",
      "convergence dfGPdfNN Run 5/10, Epoch 570/1000, Training Loss (NLML): -960.4310\n",
      "convergence dfGPdfNN Run 5/10, Epoch 571/1000, Training Loss (NLML): -960.4351\n",
      "convergence dfGPdfNN Run 5/10, Epoch 572/1000, Training Loss (NLML): -960.4392\n",
      "convergence dfGPdfNN Run 5/10, Epoch 573/1000, Training Loss (NLML): -960.4429\n",
      "convergence dfGPdfNN Run 5/10, Epoch 574/1000, Training Loss (NLML): -960.4447\n",
      "convergence dfGPdfNN Run 5/10, Epoch 575/1000, Training Loss (NLML): -960.4475\n",
      "convergence dfGPdfNN Run 5/10, Epoch 576/1000, Training Loss (NLML): -960.4510\n",
      "convergence dfGPdfNN Run 5/10, Epoch 577/1000, Training Loss (NLML): -960.4541\n",
      "convergence dfGPdfNN Run 5/10, Epoch 578/1000, Training Loss (NLML): -960.4574\n",
      "convergence dfGPdfNN Run 5/10, Epoch 579/1000, Training Loss (NLML): -960.4619\n",
      "convergence dfGPdfNN Run 5/10, Epoch 580/1000, Training Loss (NLML): -960.4633\n",
      "convergence dfGPdfNN Run 5/10, Epoch 581/1000, Training Loss (NLML): -960.4652\n",
      "convergence dfGPdfNN Run 5/10, Epoch 582/1000, Training Loss (NLML): -960.4694\n",
      "convergence dfGPdfNN Run 5/10, Epoch 583/1000, Training Loss (NLML): -960.4731\n",
      "convergence dfGPdfNN Run 5/10, Epoch 584/1000, Training Loss (NLML): -960.4761\n",
      "convergence dfGPdfNN Run 5/10, Epoch 585/1000, Training Loss (NLML): -960.4789\n",
      "convergence dfGPdfNN Run 5/10, Epoch 586/1000, Training Loss (NLML): -960.4808\n",
      "convergence dfGPdfNN Run 5/10, Epoch 587/1000, Training Loss (NLML): -960.4846\n",
      "convergence dfGPdfNN Run 5/10, Epoch 588/1000, Training Loss (NLML): -960.4878\n",
      "convergence dfGPdfNN Run 5/10, Epoch 589/1000, Training Loss (NLML): -960.4899\n",
      "convergence dfGPdfNN Run 5/10, Epoch 590/1000, Training Loss (NLML): -960.4927\n",
      "convergence dfGPdfNN Run 5/10, Epoch 591/1000, Training Loss (NLML): -960.4952\n",
      "convergence dfGPdfNN Run 5/10, Epoch 592/1000, Training Loss (NLML): -960.4983\n",
      "convergence dfGPdfNN Run 5/10, Epoch 593/1000, Training Loss (NLML): -960.5018\n",
      "convergence dfGPdfNN Run 5/10, Epoch 594/1000, Training Loss (NLML): -960.5055\n",
      "convergence dfGPdfNN Run 5/10, Epoch 595/1000, Training Loss (NLML): -960.5073\n",
      "convergence dfGPdfNN Run 5/10, Epoch 596/1000, Training Loss (NLML): -960.5100\n",
      "convergence dfGPdfNN Run 5/10, Epoch 597/1000, Training Loss (NLML): -960.5138\n",
      "convergence dfGPdfNN Run 5/10, Epoch 598/1000, Training Loss (NLML): -960.5155\n",
      "convergence dfGPdfNN Run 5/10, Epoch 599/1000, Training Loss (NLML): -960.5192\n",
      "convergence dfGPdfNN Run 5/10, Epoch 600/1000, Training Loss (NLML): -960.5208\n",
      "convergence dfGPdfNN Run 5/10, Epoch 601/1000, Training Loss (NLML): -960.5258\n",
      "convergence dfGPdfNN Run 5/10, Epoch 602/1000, Training Loss (NLML): -960.5280\n",
      "convergence dfGPdfNN Run 5/10, Epoch 603/1000, Training Loss (NLML): -960.5300\n",
      "convergence dfGPdfNN Run 5/10, Epoch 604/1000, Training Loss (NLML): -960.5337\n",
      "convergence dfGPdfNN Run 5/10, Epoch 605/1000, Training Loss (NLML): -960.5359\n",
      "convergence dfGPdfNN Run 5/10, Epoch 606/1000, Training Loss (NLML): -960.5380\n",
      "convergence dfGPdfNN Run 5/10, Epoch 607/1000, Training Loss (NLML): -960.5413\n",
      "convergence dfGPdfNN Run 5/10, Epoch 608/1000, Training Loss (NLML): -960.5436\n",
      "convergence dfGPdfNN Run 5/10, Epoch 609/1000, Training Loss (NLML): -960.5480\n",
      "convergence dfGPdfNN Run 5/10, Epoch 610/1000, Training Loss (NLML): -960.5505\n",
      "convergence dfGPdfNN Run 5/10, Epoch 611/1000, Training Loss (NLML): -960.5532\n",
      "convergence dfGPdfNN Run 5/10, Epoch 612/1000, Training Loss (NLML): -960.5551\n",
      "convergence dfGPdfNN Run 5/10, Epoch 613/1000, Training Loss (NLML): -960.5593\n",
      "convergence dfGPdfNN Run 5/10, Epoch 614/1000, Training Loss (NLML): -960.5630\n",
      "convergence dfGPdfNN Run 5/10, Epoch 615/1000, Training Loss (NLML): -960.5656\n",
      "convergence dfGPdfNN Run 5/10, Epoch 616/1000, Training Loss (NLML): -960.5687\n",
      "convergence dfGPdfNN Run 5/10, Epoch 617/1000, Training Loss (NLML): -960.5713\n",
      "convergence dfGPdfNN Run 5/10, Epoch 618/1000, Training Loss (NLML): -960.5751\n",
      "convergence dfGPdfNN Run 5/10, Epoch 619/1000, Training Loss (NLML): -960.5763\n",
      "convergence dfGPdfNN Run 5/10, Epoch 620/1000, Training Loss (NLML): -960.5785\n",
      "convergence dfGPdfNN Run 5/10, Epoch 621/1000, Training Loss (NLML): -960.5792\n",
      "convergence dfGPdfNN Run 5/10, Epoch 622/1000, Training Loss (NLML): -960.5835\n",
      "convergence dfGPdfNN Run 5/10, Epoch 623/1000, Training Loss (NLML): -960.5856\n",
      "convergence dfGPdfNN Run 5/10, Epoch 624/1000, Training Loss (NLML): -960.5980\n",
      "convergence dfGPdfNN Run 5/10, Epoch 625/1000, Training Loss (NLML): -960.6006\n",
      "convergence dfGPdfNN Run 5/10, Epoch 626/1000, Training Loss (NLML): -960.6041\n",
      "convergence dfGPdfNN Run 5/10, Epoch 627/1000, Training Loss (NLML): -960.5978\n",
      "convergence dfGPdfNN Run 5/10, Epoch 628/1000, Training Loss (NLML): -960.6005\n",
      "convergence dfGPdfNN Run 5/10, Epoch 629/1000, Training Loss (NLML): -960.6041\n",
      "convergence dfGPdfNN Run 5/10, Epoch 630/1000, Training Loss (NLML): -960.6052\n",
      "convergence dfGPdfNN Run 5/10, Epoch 631/1000, Training Loss (NLML): -960.6063\n",
      "convergence dfGPdfNN Run 5/10, Epoch 632/1000, Training Loss (NLML): -960.6093\n",
      "convergence dfGPdfNN Run 5/10, Epoch 633/1000, Training Loss (NLML): -960.6124\n",
      "convergence dfGPdfNN Run 5/10, Epoch 634/1000, Training Loss (NLML): -960.6160\n",
      "convergence dfGPdfNN Run 5/10, Epoch 635/1000, Training Loss (NLML): -960.6195\n",
      "convergence dfGPdfNN Run 5/10, Epoch 636/1000, Training Loss (NLML): -960.6230\n",
      "convergence dfGPdfNN Run 5/10, Epoch 637/1000, Training Loss (NLML): -960.6255\n",
      "convergence dfGPdfNN Run 5/10, Epoch 638/1000, Training Loss (NLML): -960.6290\n",
      "convergence dfGPdfNN Run 5/10, Epoch 639/1000, Training Loss (NLML): -960.6302\n",
      "convergence dfGPdfNN Run 5/10, Epoch 640/1000, Training Loss (NLML): -960.6310\n",
      "convergence dfGPdfNN Run 5/10, Epoch 641/1000, Training Loss (NLML): -960.6346\n",
      "convergence dfGPdfNN Run 5/10, Epoch 642/1000, Training Loss (NLML): -960.6389\n",
      "convergence dfGPdfNN Run 5/10, Epoch 643/1000, Training Loss (NLML): -960.6409\n",
      "convergence dfGPdfNN Run 5/10, Epoch 644/1000, Training Loss (NLML): -960.6426\n",
      "convergence dfGPdfNN Run 5/10, Epoch 645/1000, Training Loss (NLML): -960.6448\n",
      "convergence dfGPdfNN Run 5/10, Epoch 646/1000, Training Loss (NLML): -960.6471\n",
      "convergence dfGPdfNN Run 5/10, Epoch 647/1000, Training Loss (NLML): -960.6498\n",
      "convergence dfGPdfNN Run 5/10, Epoch 648/1000, Training Loss (NLML): -960.6547\n",
      "convergence dfGPdfNN Run 5/10, Epoch 649/1000, Training Loss (NLML): -960.6564\n",
      "convergence dfGPdfNN Run 5/10, Epoch 650/1000, Training Loss (NLML): -960.6520\n",
      "convergence dfGPdfNN Run 5/10, Epoch 651/1000, Training Loss (NLML): -960.6573\n",
      "convergence dfGPdfNN Run 5/10, Epoch 652/1000, Training Loss (NLML): -960.6603\n",
      "convergence dfGPdfNN Run 5/10, Epoch 653/1000, Training Loss (NLML): -960.6606\n",
      "convergence dfGPdfNN Run 5/10, Epoch 654/1000, Training Loss (NLML): -960.6725\n",
      "convergence dfGPdfNN Run 5/10, Epoch 655/1000, Training Loss (NLML): -960.6765\n",
      "convergence dfGPdfNN Run 5/10, Epoch 656/1000, Training Loss (NLML): -960.6692\n",
      "convergence dfGPdfNN Run 5/10, Epoch 657/1000, Training Loss (NLML): -960.6731\n",
      "convergence dfGPdfNN Run 5/10, Epoch 658/1000, Training Loss (NLML): -960.6747\n",
      "convergence dfGPdfNN Run 5/10, Epoch 659/1000, Training Loss (NLML): -960.6775\n",
      "convergence dfGPdfNN Run 5/10, Epoch 660/1000, Training Loss (NLML): -960.6815\n",
      "convergence dfGPdfNN Run 5/10, Epoch 661/1000, Training Loss (NLML): -960.6833\n",
      "convergence dfGPdfNN Run 5/10, Epoch 662/1000, Training Loss (NLML): -960.6855\n",
      "convergence dfGPdfNN Run 5/10, Epoch 663/1000, Training Loss (NLML): -960.6852\n",
      "convergence dfGPdfNN Run 5/10, Epoch 664/1000, Training Loss (NLML): -960.6887\n",
      "convergence dfGPdfNN Run 5/10, Epoch 665/1000, Training Loss (NLML): -960.6907\n",
      "convergence dfGPdfNN Run 5/10, Epoch 666/1000, Training Loss (NLML): -960.6932\n",
      "convergence dfGPdfNN Run 5/10, Epoch 667/1000, Training Loss (NLML): -960.6964\n",
      "convergence dfGPdfNN Run 5/10, Epoch 668/1000, Training Loss (NLML): -960.6992\n",
      "convergence dfGPdfNN Run 5/10, Epoch 669/1000, Training Loss (NLML): -960.7017\n",
      "convergence dfGPdfNN Run 5/10, Epoch 670/1000, Training Loss (NLML): -960.7034\n",
      "convergence dfGPdfNN Run 5/10, Epoch 671/1000, Training Loss (NLML): -960.7152\n",
      "convergence dfGPdfNN Run 5/10, Epoch 672/1000, Training Loss (NLML): -960.7188\n",
      "convergence dfGPdfNN Run 5/10, Epoch 673/1000, Training Loss (NLML): -960.7104\n",
      "convergence dfGPdfNN Run 5/10, Epoch 674/1000, Training Loss (NLML): -960.7147\n",
      "convergence dfGPdfNN Run 5/10, Epoch 675/1000, Training Loss (NLML): -960.7150\n",
      "convergence dfGPdfNN Run 5/10, Epoch 676/1000, Training Loss (NLML): -960.7186\n",
      "convergence dfGPdfNN Run 5/10, Epoch 677/1000, Training Loss (NLML): -960.7201\n",
      "convergence dfGPdfNN Run 5/10, Epoch 678/1000, Training Loss (NLML): -960.7250\n",
      "convergence dfGPdfNN Run 5/10, Epoch 679/1000, Training Loss (NLML): -960.7251\n",
      "convergence dfGPdfNN Run 5/10, Epoch 680/1000, Training Loss (NLML): -960.7294\n",
      "convergence dfGPdfNN Run 5/10, Epoch 681/1000, Training Loss (NLML): -960.7301\n",
      "convergence dfGPdfNN Run 5/10, Epoch 682/1000, Training Loss (NLML): -960.7419\n",
      "convergence dfGPdfNN Run 5/10, Epoch 683/1000, Training Loss (NLML): -960.7357\n",
      "convergence dfGPdfNN Run 5/10, Epoch 684/1000, Training Loss (NLML): -960.7369\n",
      "convergence dfGPdfNN Run 5/10, Epoch 685/1000, Training Loss (NLML): -960.7408\n",
      "convergence dfGPdfNN Run 5/10, Epoch 686/1000, Training Loss (NLML): -960.7434\n",
      "convergence dfGPdfNN Run 5/10, Epoch 687/1000, Training Loss (NLML): -960.7462\n",
      "convergence dfGPdfNN Run 5/10, Epoch 688/1000, Training Loss (NLML): -960.7567\n",
      "convergence dfGPdfNN Run 5/10, Epoch 689/1000, Training Loss (NLML): -960.7494\n",
      "convergence dfGPdfNN Run 5/10, Epoch 690/1000, Training Loss (NLML): -960.7522\n",
      "convergence dfGPdfNN Run 5/10, Epoch 691/1000, Training Loss (NLML): -960.7554\n",
      "convergence dfGPdfNN Run 5/10, Epoch 692/1000, Training Loss (NLML): -960.7571\n",
      "convergence dfGPdfNN Run 5/10, Epoch 693/1000, Training Loss (NLML): -960.7573\n",
      "convergence dfGPdfNN Run 5/10, Epoch 694/1000, Training Loss (NLML): -960.7622\n",
      "convergence dfGPdfNN Run 5/10, Epoch 695/1000, Training Loss (NLML): -960.7721\n",
      "convergence dfGPdfNN Run 5/10, Epoch 696/1000, Training Loss (NLML): -960.7740\n",
      "convergence dfGPdfNN Run 5/10, Epoch 697/1000, Training Loss (NLML): -960.7694\n",
      "convergence dfGPdfNN Run 5/10, Epoch 698/1000, Training Loss (NLML): -960.7708\n",
      "convergence dfGPdfNN Run 5/10, Epoch 699/1000, Training Loss (NLML): -960.7727\n",
      "convergence dfGPdfNN Run 5/10, Epoch 700/1000, Training Loss (NLML): -960.7764\n",
      "convergence dfGPdfNN Run 5/10, Epoch 701/1000, Training Loss (NLML): -960.7794\n",
      "convergence dfGPdfNN Run 5/10, Epoch 702/1000, Training Loss (NLML): -960.7802\n",
      "convergence dfGPdfNN Run 5/10, Epoch 703/1000, Training Loss (NLML): -960.7897\n",
      "convergence dfGPdfNN Run 5/10, Epoch 704/1000, Training Loss (NLML): -960.7914\n",
      "convergence dfGPdfNN Run 5/10, Epoch 705/1000, Training Loss (NLML): -960.7850\n",
      "convergence dfGPdfNN Run 5/10, Epoch 706/1000, Training Loss (NLML): -960.7889\n",
      "convergence dfGPdfNN Run 5/10, Epoch 707/1000, Training Loss (NLML): -960.7894\n",
      "convergence dfGPdfNN Run 5/10, Epoch 708/1000, Training Loss (NLML): -960.7933\n",
      "convergence dfGPdfNN Run 5/10, Epoch 709/1000, Training Loss (NLML): -960.7952\n",
      "convergence dfGPdfNN Run 5/10, Epoch 710/1000, Training Loss (NLML): -960.7966\n",
      "convergence dfGPdfNN Run 5/10, Epoch 711/1000, Training Loss (NLML): -960.8063\n",
      "convergence dfGPdfNN Run 5/10, Epoch 712/1000, Training Loss (NLML): -960.8102\n",
      "convergence dfGPdfNN Run 5/10, Epoch 713/1000, Training Loss (NLML): -960.8044\n",
      "convergence dfGPdfNN Run 5/10, Epoch 714/1000, Training Loss (NLML): -960.8065\n",
      "convergence dfGPdfNN Run 5/10, Epoch 715/1000, Training Loss (NLML): -960.8094\n",
      "convergence dfGPdfNN Run 5/10, Epoch 716/1000, Training Loss (NLML): -960.8113\n",
      "convergence dfGPdfNN Run 5/10, Epoch 717/1000, Training Loss (NLML): -960.8118\n",
      "convergence dfGPdfNN Run 5/10, Epoch 718/1000, Training Loss (NLML): -960.8237\n",
      "convergence dfGPdfNN Run 5/10, Epoch 719/1000, Training Loss (NLML): -960.8253\n",
      "convergence dfGPdfNN Run 5/10, Epoch 720/1000, Training Loss (NLML): -960.8171\n",
      "convergence dfGPdfNN Run 5/10, Epoch 721/1000, Training Loss (NLML): -960.8234\n",
      "convergence dfGPdfNN Run 5/10, Epoch 722/1000, Training Loss (NLML): -960.8239\n",
      "convergence dfGPdfNN Run 5/10, Epoch 723/1000, Training Loss (NLML): -960.8262\n",
      "convergence dfGPdfNN Run 5/10, Epoch 724/1000, Training Loss (NLML): -960.8369\n",
      "convergence dfGPdfNN Run 5/10, Epoch 725/1000, Training Loss (NLML): -960.8392\n",
      "convergence dfGPdfNN Run 5/10, Epoch 726/1000, Training Loss (NLML): -960.8336\n",
      "convergence dfGPdfNN Run 5/10, Epoch 727/1000, Training Loss (NLML): -960.8353\n",
      "convergence dfGPdfNN Run 5/10, Epoch 728/1000, Training Loss (NLML): -960.8390\n",
      "convergence dfGPdfNN Run 5/10, Epoch 729/1000, Training Loss (NLML): -960.8390\n",
      "convergence dfGPdfNN Run 5/10, Epoch 730/1000, Training Loss (NLML): -960.8505\n",
      "convergence dfGPdfNN Run 5/10, Epoch 731/1000, Training Loss (NLML): -960.8513\n",
      "convergence dfGPdfNN Run 5/10, Epoch 732/1000, Training Loss (NLML): -960.8481\n",
      "convergence dfGPdfNN Run 5/10, Epoch 733/1000, Training Loss (NLML): -960.8474\n",
      "convergence dfGPdfNN Run 5/10, Epoch 734/1000, Training Loss (NLML): -960.8446\n",
      "convergence dfGPdfNN Run 5/10, Epoch 735/1000, Training Loss (NLML): -960.8544\n",
      "convergence dfGPdfNN Run 5/10, Epoch 736/1000, Training Loss (NLML): -960.8510\n",
      "convergence dfGPdfNN Run 5/10, Epoch 737/1000, Training Loss (NLML): -960.8538\n",
      "convergence dfGPdfNN Run 5/10, Epoch 738/1000, Training Loss (NLML): -960.8654\n",
      "convergence dfGPdfNN Run 5/10, Epoch 739/1000, Training Loss (NLML): -960.8666\n",
      "convergence dfGPdfNN Run 5/10, Epoch 740/1000, Training Loss (NLML): -960.8682\n",
      "convergence dfGPdfNN Run 5/10, Epoch 741/1000, Training Loss (NLML): -960.8656\n",
      "convergence dfGPdfNN Run 5/10, Epoch 742/1000, Training Loss (NLML): -960.8723\n",
      "convergence dfGPdfNN Run 5/10, Epoch 743/1000, Training Loss (NLML): -960.8724\n",
      "convergence dfGPdfNN Run 5/10, Epoch 744/1000, Training Loss (NLML): -960.8732\n",
      "convergence dfGPdfNN Run 5/10, Epoch 745/1000, Training Loss (NLML): -960.8749\n",
      "convergence dfGPdfNN Run 5/10, Epoch 746/1000, Training Loss (NLML): -960.8765\n",
      "convergence dfGPdfNN Run 5/10, Epoch 747/1000, Training Loss (NLML): -960.8790\n",
      "convergence dfGPdfNN Run 5/10, Epoch 748/1000, Training Loss (NLML): -960.8827\n",
      "convergence dfGPdfNN Run 5/10, Epoch 749/1000, Training Loss (NLML): -960.8843\n",
      "convergence dfGPdfNN Run 5/10, Epoch 750/1000, Training Loss (NLML): -960.8859\n",
      "convergence dfGPdfNN Run 5/10, Epoch 751/1000, Training Loss (NLML): -960.8940\n",
      "convergence dfGPdfNN Run 5/10, Epoch 752/1000, Training Loss (NLML): -960.8993\n",
      "convergence dfGPdfNN Run 5/10, Epoch 753/1000, Training Loss (NLML): -960.9041\n",
      "convergence dfGPdfNN Run 5/10, Epoch 754/1000, Training Loss (NLML): -960.8934\n",
      "convergence dfGPdfNN Run 5/10, Epoch 755/1000, Training Loss (NLML): -960.8938\n",
      "convergence dfGPdfNN Run 5/10, Epoch 756/1000, Training Loss (NLML): -960.8983\n",
      "convergence dfGPdfNN Run 5/10, Epoch 757/1000, Training Loss (NLML): -960.9021\n",
      "convergence dfGPdfNN Run 5/10, Epoch 758/1000, Training Loss (NLML): -960.9058\n",
      "convergence dfGPdfNN Run 5/10, Epoch 759/1000, Training Loss (NLML): -960.9039\n",
      "convergence dfGPdfNN Run 5/10, Epoch 760/1000, Training Loss (NLML): -960.9067\n",
      "convergence dfGPdfNN Run 5/10, Epoch 761/1000, Training Loss (NLML): -960.9095\n",
      "convergence dfGPdfNN Run 5/10, Epoch 762/1000, Training Loss (NLML): -960.9087\n",
      "convergence dfGPdfNN Run 5/10, Epoch 763/1000, Training Loss (NLML): -960.9193\n",
      "convergence dfGPdfNN Run 5/10, Epoch 764/1000, Training Loss (NLML): -960.9222\n",
      "convergence dfGPdfNN Run 5/10, Epoch 765/1000, Training Loss (NLML): -960.9238\n",
      "convergence dfGPdfNN Run 5/10, Epoch 766/1000, Training Loss (NLML): -960.9170\n",
      "convergence dfGPdfNN Run 5/10, Epoch 767/1000, Training Loss (NLML): -960.9143\n",
      "convergence dfGPdfNN Run 5/10, Epoch 768/1000, Training Loss (NLML): -960.9207\n",
      "convergence dfGPdfNN Run 5/10, Epoch 769/1000, Training Loss (NLML): -960.9307\n",
      "convergence dfGPdfNN Run 5/10, Epoch 770/1000, Training Loss (NLML): -960.9353\n",
      "convergence dfGPdfNN Run 5/10, Epoch 771/1000, Training Loss (NLML): -960.9292\n",
      "convergence dfGPdfNN Run 5/10, Epoch 772/1000, Training Loss (NLML): -960.9309\n",
      "convergence dfGPdfNN Run 5/10, Epoch 773/1000, Training Loss (NLML): -960.9315\n",
      "convergence dfGPdfNN Run 5/10, Epoch 774/1000, Training Loss (NLML): -960.9323\n",
      "convergence dfGPdfNN Run 5/10, Epoch 775/1000, Training Loss (NLML): -960.9468\n",
      "convergence dfGPdfNN Run 5/10, Epoch 776/1000, Training Loss (NLML): -960.9373\n",
      "convergence dfGPdfNN Run 5/10, Epoch 777/1000, Training Loss (NLML): -960.9473\n",
      "convergence dfGPdfNN Run 5/10, Epoch 778/1000, Training Loss (NLML): -960.9410\n",
      "convergence dfGPdfNN Run 5/10, Epoch 779/1000, Training Loss (NLML): -960.9408\n",
      "convergence dfGPdfNN Run 5/10, Epoch 780/1000, Training Loss (NLML): -960.9528\n",
      "convergence dfGPdfNN Run 5/10, Epoch 781/1000, Training Loss (NLML): -960.9540\n",
      "convergence dfGPdfNN Run 5/10, Epoch 782/1000, Training Loss (NLML): -960.9496\n",
      "convergence dfGPdfNN Run 5/10, Epoch 783/1000, Training Loss (NLML): -960.9513\n",
      "convergence dfGPdfNN Run 5/10, Epoch 784/1000, Training Loss (NLML): -960.9532\n",
      "convergence dfGPdfNN Run 5/10, Epoch 785/1000, Training Loss (NLML): -960.9543\n",
      "convergence dfGPdfNN Run 5/10, Epoch 786/1000, Training Loss (NLML): -960.9553\n",
      "convergence dfGPdfNN Run 5/10, Epoch 787/1000, Training Loss (NLML): -960.9663\n",
      "convergence dfGPdfNN Run 5/10, Epoch 788/1000, Training Loss (NLML): -960.9673\n",
      "convergence dfGPdfNN Run 5/10, Epoch 789/1000, Training Loss (NLML): -960.9681\n",
      "convergence dfGPdfNN Run 5/10, Epoch 790/1000, Training Loss (NLML): -960.9656\n",
      "convergence dfGPdfNN Run 5/10, Epoch 791/1000, Training Loss (NLML): -960.9657\n",
      "convergence dfGPdfNN Run 5/10, Epoch 792/1000, Training Loss (NLML): -960.9673\n",
      "convergence dfGPdfNN Run 5/10, Epoch 793/1000, Training Loss (NLML): -960.9661\n",
      "convergence dfGPdfNN Run 5/10, Epoch 794/1000, Training Loss (NLML): -960.9761\n",
      "convergence dfGPdfNN Run 5/10, Epoch 795/1000, Training Loss (NLML): -960.9801\n",
      "convergence dfGPdfNN Run 5/10, Epoch 796/1000, Training Loss (NLML): -960.9835\n",
      "convergence dfGPdfNN Run 5/10, Epoch 797/1000, Training Loss (NLML): -960.9857\n",
      "convergence dfGPdfNN Run 5/10, Epoch 798/1000, Training Loss (NLML): -960.9778\n",
      "convergence dfGPdfNN Run 5/10, Epoch 799/1000, Training Loss (NLML): -960.9823\n",
      "convergence dfGPdfNN Run 5/10, Epoch 800/1000, Training Loss (NLML): -960.9825\n",
      "convergence dfGPdfNN Run 5/10, Epoch 801/1000, Training Loss (NLML): -960.9838\n",
      "convergence dfGPdfNN Run 5/10, Epoch 802/1000, Training Loss (NLML): -960.9950\n",
      "convergence dfGPdfNN Run 5/10, Epoch 803/1000, Training Loss (NLML): -960.9963\n",
      "convergence dfGPdfNN Run 5/10, Epoch 804/1000, Training Loss (NLML): -960.9985\n",
      "convergence dfGPdfNN Run 5/10, Epoch 805/1000, Training Loss (NLML): -960.9974\n",
      "convergence dfGPdfNN Run 5/10, Epoch 806/1000, Training Loss (NLML): -960.9940\n",
      "convergence dfGPdfNN Run 5/10, Epoch 807/1000, Training Loss (NLML): -961.0021\n",
      "convergence dfGPdfNN Run 5/10, Epoch 808/1000, Training Loss (NLML): -960.9985\n",
      "convergence dfGPdfNN Run 5/10, Epoch 809/1000, Training Loss (NLML): -961.0071\n",
      "convergence dfGPdfNN Run 5/10, Epoch 810/1000, Training Loss (NLML): -960.9993\n",
      "convergence dfGPdfNN Run 5/10, Epoch 811/1000, Training Loss (NLML): -961.0103\n",
      "convergence dfGPdfNN Run 5/10, Epoch 812/1000, Training Loss (NLML): -961.0121\n",
      "convergence dfGPdfNN Run 5/10, Epoch 813/1000, Training Loss (NLML): -961.0140\n",
      "convergence dfGPdfNN Run 5/10, Epoch 814/1000, Training Loss (NLML): -961.0164\n",
      "convergence dfGPdfNN Run 5/10, Epoch 815/1000, Training Loss (NLML): -961.0115\n",
      "convergence dfGPdfNN Run 5/10, Epoch 816/1000, Training Loss (NLML): -961.0192\n",
      "convergence dfGPdfNN Run 5/10, Epoch 817/1000, Training Loss (NLML): -961.0154\n",
      "convergence dfGPdfNN Run 5/10, Epoch 818/1000, Training Loss (NLML): -961.0146\n",
      "convergence dfGPdfNN Run 5/10, Epoch 819/1000, Training Loss (NLML): -961.0250\n",
      "convergence dfGPdfNN Run 5/10, Epoch 820/1000, Training Loss (NLML): -961.0256\n",
      "convergence dfGPdfNN Run 5/10, Epoch 821/1000, Training Loss (NLML): -961.0323\n",
      "convergence dfGPdfNN Run 5/10, Epoch 822/1000, Training Loss (NLML): -961.0204\n",
      "convergence dfGPdfNN Run 5/10, Epoch 823/1000, Training Loss (NLML): -961.0232\n",
      "convergence dfGPdfNN Run 5/10, Epoch 824/1000, Training Loss (NLML): -961.0333\n",
      "convergence dfGPdfNN Run 5/10, Epoch 825/1000, Training Loss (NLML): -961.0355\n",
      "convergence dfGPdfNN Run 5/10, Epoch 826/1000, Training Loss (NLML): -961.0293\n",
      "convergence dfGPdfNN Run 5/10, Epoch 827/1000, Training Loss (NLML): -961.0298\n",
      "convergence dfGPdfNN Run 5/10, Epoch 828/1000, Training Loss (NLML): -961.0414\n",
      "convergence dfGPdfNN Run 5/10, Epoch 829/1000, Training Loss (NLML): -961.0441\n",
      "convergence dfGPdfNN Run 5/10, Epoch 830/1000, Training Loss (NLML): -961.0435\n",
      "convergence dfGPdfNN Run 5/10, Epoch 831/1000, Training Loss (NLML): -961.0404\n",
      "convergence dfGPdfNN Run 5/10, Epoch 832/1000, Training Loss (NLML): -961.0471\n",
      "convergence dfGPdfNN Run 5/10, Epoch 833/1000, Training Loss (NLML): -961.0432\n",
      "convergence dfGPdfNN Run 5/10, Epoch 834/1000, Training Loss (NLML): -961.0508\n",
      "convergence dfGPdfNN Run 5/10, Epoch 835/1000, Training Loss (NLML): -961.0516\n",
      "convergence dfGPdfNN Run 5/10, Epoch 836/1000, Training Loss (NLML): -961.0483\n",
      "convergence dfGPdfNN Run 5/10, Epoch 837/1000, Training Loss (NLML): -961.0522\n",
      "convergence dfGPdfNN Run 5/10, Epoch 838/1000, Training Loss (NLML): -961.0631\n",
      "convergence dfGPdfNN Run 5/10, Epoch 839/1000, Training Loss (NLML): -961.0592\n",
      "convergence dfGPdfNN Run 5/10, Epoch 840/1000, Training Loss (NLML): -961.0605\n",
      "convergence dfGPdfNN Run 5/10, Epoch 841/1000, Training Loss (NLML): -961.0648\n",
      "convergence dfGPdfNN Run 5/10, Epoch 842/1000, Training Loss (NLML): -961.0658\n",
      "convergence dfGPdfNN Run 5/10, Epoch 843/1000, Training Loss (NLML): -961.0663\n",
      "convergence dfGPdfNN Run 5/10, Epoch 844/1000, Training Loss (NLML): -961.0607\n",
      "convergence dfGPdfNN Run 5/10, Epoch 845/1000, Training Loss (NLML): -961.0621\n",
      "convergence dfGPdfNN Run 5/10, Epoch 846/1000, Training Loss (NLML): -961.0709\n",
      "convergence dfGPdfNN Run 5/10, Epoch 847/1000, Training Loss (NLML): -961.0737\n",
      "convergence dfGPdfNN Run 5/10, Epoch 848/1000, Training Loss (NLML): -961.0819\n",
      "convergence dfGPdfNN Run 5/10, Epoch 849/1000, Training Loss (NLML): -961.0780\n",
      "convergence dfGPdfNN Run 5/10, Epoch 850/1000, Training Loss (NLML): -961.0806\n",
      "convergence dfGPdfNN Run 5/10, Epoch 851/1000, Training Loss (NLML): -961.0780\n",
      "convergence dfGPdfNN Run 5/10, Epoch 852/1000, Training Loss (NLML): -961.0730\n",
      "convergence dfGPdfNN Run 5/10, Epoch 853/1000, Training Loss (NLML): -961.0774\n",
      "convergence dfGPdfNN Run 5/10, Epoch 854/1000, Training Loss (NLML): -961.0830\n",
      "convergence dfGPdfNN Run 5/10, Epoch 855/1000, Training Loss (NLML): -961.0891\n",
      "convergence dfGPdfNN Run 5/10, Epoch 856/1000, Training Loss (NLML): -961.0878\n",
      "convergence dfGPdfNN Run 5/10, Epoch 857/1000, Training Loss (NLML): -961.0906\n",
      "convergence dfGPdfNN Run 5/10, Epoch 858/1000, Training Loss (NLML): -961.0930\n",
      "convergence dfGPdfNN Run 5/10, Epoch 859/1000, Training Loss (NLML): -961.1002\n",
      "convergence dfGPdfNN Run 5/10, Epoch 860/1000, Training Loss (NLML): -961.0975\n",
      "convergence dfGPdfNN Run 5/10, Epoch 861/1000, Training Loss (NLML): -961.0895\n",
      "convergence dfGPdfNN Run 5/10, Epoch 862/1000, Training Loss (NLML): -961.0911\n",
      "convergence dfGPdfNN Run 5/10, Epoch 863/1000, Training Loss (NLML): -961.1069\n",
      "convergence dfGPdfNN Run 5/10, Epoch 864/1000, Training Loss (NLML): -961.1021\n",
      "convergence dfGPdfNN Run 5/10, Epoch 865/1000, Training Loss (NLML): -961.1047\n",
      "convergence dfGPdfNN Run 5/10, Epoch 866/1000, Training Loss (NLML): -961.1062\n",
      "convergence dfGPdfNN Run 5/10, Epoch 867/1000, Training Loss (NLML): -961.1136\n",
      "convergence dfGPdfNN Run 5/10, Epoch 868/1000, Training Loss (NLML): -961.1024\n",
      "convergence dfGPdfNN Run 5/10, Epoch 869/1000, Training Loss (NLML): -961.1095\n",
      "convergence dfGPdfNN Run 5/10, Epoch 870/1000, Training Loss (NLML): -961.1125\n",
      "convergence dfGPdfNN Run 5/10, Epoch 871/1000, Training Loss (NLML): -961.1196\n",
      "convergence dfGPdfNN Run 5/10, Epoch 872/1000, Training Loss (NLML): -961.1061\n",
      "convergence dfGPdfNN Run 5/10, Epoch 873/1000, Training Loss (NLML): -961.1180\n",
      "convergence dfGPdfNN Run 5/10, Epoch 874/1000, Training Loss (NLML): -961.1187\n",
      "convergence dfGPdfNN Run 5/10, Epoch 875/1000, Training Loss (NLML): -961.1201\n",
      "convergence dfGPdfNN Run 5/10, Epoch 876/1000, Training Loss (NLML): -961.1224\n",
      "convergence dfGPdfNN Run 5/10, Epoch 877/1000, Training Loss (NLML): -961.1268\n",
      "convergence dfGPdfNN Run 5/10, Epoch 878/1000, Training Loss (NLML): -961.1259\n",
      "convergence dfGPdfNN Run 5/10, Epoch 879/1000, Training Loss (NLML): -961.1283\n",
      "convergence dfGPdfNN Run 5/10, Epoch 880/1000, Training Loss (NLML): -961.1287\n",
      "convergence dfGPdfNN Run 5/10, Epoch 881/1000, Training Loss (NLML): -961.1226\n",
      "convergence dfGPdfNN Run 5/10, Epoch 882/1000, Training Loss (NLML): -961.1250\n",
      "convergence dfGPdfNN Run 5/10, Epoch 883/1000, Training Loss (NLML): -961.1312\n",
      "convergence dfGPdfNN Run 5/10, Epoch 884/1000, Training Loss (NLML): -961.1338\n",
      "convergence dfGPdfNN Run 5/10, Epoch 885/1000, Training Loss (NLML): -961.1379\n",
      "convergence dfGPdfNN Run 5/10, Epoch 886/1000, Training Loss (NLML): -961.1360\n",
      "convergence dfGPdfNN Run 5/10, Epoch 887/1000, Training Loss (NLML): -961.1445\n",
      "convergence dfGPdfNN Run 5/10, Epoch 888/1000, Training Loss (NLML): -961.1416\n",
      "convergence dfGPdfNN Run 5/10, Epoch 889/1000, Training Loss (NLML): -961.1422\n",
      "convergence dfGPdfNN Run 5/10, Epoch 890/1000, Training Loss (NLML): -961.1515\n",
      "convergence dfGPdfNN Run 5/10, Epoch 891/1000, Training Loss (NLML): -961.1445\n",
      "convergence dfGPdfNN Run 5/10, Epoch 892/1000, Training Loss (NLML): -961.1519\n",
      "convergence dfGPdfNN Run 5/10, Epoch 893/1000, Training Loss (NLML): -961.1499\n",
      "convergence dfGPdfNN Run 5/10, Epoch 894/1000, Training Loss (NLML): -961.1425\n",
      "convergence dfGPdfNN Run 5/10, Epoch 895/1000, Training Loss (NLML): -961.1521\n",
      "convergence dfGPdfNN Run 5/10, Epoch 896/1000, Training Loss (NLML): -961.1578\n",
      "convergence dfGPdfNN Run 5/10, Epoch 897/1000, Training Loss (NLML): -961.1550\n",
      "convergence dfGPdfNN Run 5/10, Epoch 898/1000, Training Loss (NLML): -961.1584\n",
      "convergence dfGPdfNN Run 5/10, Epoch 899/1000, Training Loss (NLML): -961.1637\n",
      "convergence dfGPdfNN Run 5/10, Epoch 900/1000, Training Loss (NLML): -961.1636\n",
      "convergence dfGPdfNN Run 5/10, Epoch 901/1000, Training Loss (NLML): -961.1636\n",
      "convergence dfGPdfNN Run 5/10, Epoch 902/1000, Training Loss (NLML): -961.1658\n",
      "convergence dfGPdfNN Run 5/10, Epoch 903/1000, Training Loss (NLML): -961.1648\n",
      "convergence dfGPdfNN Run 5/10, Epoch 904/1000, Training Loss (NLML): -961.1711\n",
      "convergence dfGPdfNN Run 5/10, Epoch 905/1000, Training Loss (NLML): -961.1725\n",
      "convergence dfGPdfNN Run 5/10, Epoch 906/1000, Training Loss (NLML): -961.1672\n",
      "convergence dfGPdfNN Run 5/10, Epoch 907/1000, Training Loss (NLML): -961.1703\n",
      "convergence dfGPdfNN Run 5/10, Epoch 908/1000, Training Loss (NLML): -961.1617\n",
      "convergence dfGPdfNN Run 5/10, Epoch 909/1000, Training Loss (NLML): -961.1793\n",
      "convergence dfGPdfNN Run 5/10, Epoch 910/1000, Training Loss (NLML): -961.1758\n",
      "convergence dfGPdfNN Run 5/10, Epoch 911/1000, Training Loss (NLML): -961.1812\n",
      "convergence dfGPdfNN Run 5/10, Epoch 912/1000, Training Loss (NLML): -961.1782\n",
      "convergence dfGPdfNN Run 5/10, Epoch 913/1000, Training Loss (NLML): -961.1792\n",
      "convergence dfGPdfNN Run 5/10, Epoch 914/1000, Training Loss (NLML): -961.1864\n",
      "convergence dfGPdfNN Run 5/10, Epoch 915/1000, Training Loss (NLML): -961.1880\n",
      "convergence dfGPdfNN Run 5/10, Epoch 916/1000, Training Loss (NLML): -961.1823\n",
      "convergence dfGPdfNN Run 5/10, Epoch 917/1000, Training Loss (NLML): -961.1835\n",
      "convergence dfGPdfNN Run 5/10, Epoch 918/1000, Training Loss (NLML): -961.1879\n",
      "convergence dfGPdfNN Run 5/10, Epoch 919/1000, Training Loss (NLML): -961.1924\n",
      "convergence dfGPdfNN Run 5/10, Epoch 920/1000, Training Loss (NLML): -961.1935\n",
      "convergence dfGPdfNN Run 5/10, Epoch 921/1000, Training Loss (NLML): -961.1970\n",
      "convergence dfGPdfNN Run 5/10, Epoch 922/1000, Training Loss (NLML): -961.1917\n",
      "convergence dfGPdfNN Run 5/10, Epoch 923/1000, Training Loss (NLML): -961.1946\n",
      "convergence dfGPdfNN Run 5/10, Epoch 924/1000, Training Loss (NLML): -961.1957\n",
      "convergence dfGPdfNN Run 5/10, Epoch 925/1000, Training Loss (NLML): -961.2041\n",
      "convergence dfGPdfNN Run 5/10, Epoch 926/1000, Training Loss (NLML): -961.2029\n",
      "convergence dfGPdfNN Run 5/10, Epoch 927/1000, Training Loss (NLML): -961.2069\n",
      "convergence dfGPdfNN Run 5/10, Epoch 928/1000, Training Loss (NLML): -961.1951\n",
      "convergence dfGPdfNN Run 5/10, Epoch 929/1000, Training Loss (NLML): -961.2023\n",
      "convergence dfGPdfNN Run 5/10, Epoch 930/1000, Training Loss (NLML): -961.2024\n",
      "convergence dfGPdfNN Run 5/10, Epoch 931/1000, Training Loss (NLML): -961.2098\n",
      "convergence dfGPdfNN Run 5/10, Epoch 932/1000, Training Loss (NLML): -961.2113\n",
      "convergence dfGPdfNN Run 5/10, Epoch 933/1000, Training Loss (NLML): -961.2100\n",
      "convergence dfGPdfNN Run 5/10, Epoch 934/1000, Training Loss (NLML): -961.2101\n",
      "convergence dfGPdfNN Run 5/10, Epoch 935/1000, Training Loss (NLML): -961.2112\n",
      "convergence dfGPdfNN Run 5/10, Epoch 936/1000, Training Loss (NLML): -961.2137\n",
      "convergence dfGPdfNN Run 5/10, Epoch 937/1000, Training Loss (NLML): -961.2180\n",
      "convergence dfGPdfNN Run 5/10, Epoch 938/1000, Training Loss (NLML): -961.2214\n",
      "convergence dfGPdfNN Run 5/10, Epoch 939/1000, Training Loss (NLML): -961.2181\n",
      "convergence dfGPdfNN Run 5/10, Epoch 940/1000, Training Loss (NLML): -961.2206\n",
      "convergence dfGPdfNN Run 5/10, Epoch 941/1000, Training Loss (NLML): -961.2261\n",
      "convergence dfGPdfNN Run 5/10, Epoch 942/1000, Training Loss (NLML): -961.2284\n",
      "convergence dfGPdfNN Run 5/10, Epoch 943/1000, Training Loss (NLML): -961.2283\n",
      "convergence dfGPdfNN Run 5/10, Epoch 944/1000, Training Loss (NLML): -961.2214\n",
      "convergence dfGPdfNN Run 5/10, Epoch 945/1000, Training Loss (NLML): -961.2266\n",
      "convergence dfGPdfNN Run 5/10, Epoch 946/1000, Training Loss (NLML): -961.2301\n",
      "convergence dfGPdfNN Run 5/10, Epoch 947/1000, Training Loss (NLML): -961.2365\n",
      "convergence dfGPdfNN Run 5/10, Epoch 948/1000, Training Loss (NLML): -961.2354\n",
      "convergence dfGPdfNN Run 5/10, Epoch 949/1000, Training Loss (NLML): -961.2397\n",
      "convergence dfGPdfNN Run 5/10, Epoch 950/1000, Training Loss (NLML): -961.2311\n",
      "convergence dfGPdfNN Run 5/10, Epoch 951/1000, Training Loss (NLML): -961.2343\n",
      "convergence dfGPdfNN Run 5/10, Epoch 952/1000, Training Loss (NLML): -961.2335\n",
      "convergence dfGPdfNN Run 5/10, Epoch 953/1000, Training Loss (NLML): -961.2422\n",
      "convergence dfGPdfNN Run 5/10, Epoch 954/1000, Training Loss (NLML): -961.2441\n",
      "convergence dfGPdfNN Run 5/10, Epoch 955/1000, Training Loss (NLML): -961.2439\n",
      "convergence dfGPdfNN Run 5/10, Epoch 956/1000, Training Loss (NLML): -961.2479\n",
      "convergence dfGPdfNN Run 5/10, Epoch 957/1000, Training Loss (NLML): -961.2429\n",
      "convergence dfGPdfNN Run 5/10, Epoch 958/1000, Training Loss (NLML): -961.2458\n",
      "convergence dfGPdfNN Run 5/10, Epoch 959/1000, Training Loss (NLML): -961.2444\n",
      "convergence dfGPdfNN Run 5/10, Epoch 960/1000, Training Loss (NLML): -961.2499\n",
      "convergence dfGPdfNN Run 5/10, Epoch 961/1000, Training Loss (NLML): -961.2540\n",
      "convergence dfGPdfNN Run 5/10, Epoch 962/1000, Training Loss (NLML): -961.2560\n",
      "convergence dfGPdfNN Run 5/10, Epoch 963/1000, Training Loss (NLML): -961.2590\n",
      "convergence dfGPdfNN Run 5/10, Epoch 964/1000, Training Loss (NLML): -961.2584\n",
      "convergence dfGPdfNN Run 5/10, Epoch 965/1000, Training Loss (NLML): -961.2606\n",
      "convergence dfGPdfNN Run 5/10, Epoch 966/1000, Training Loss (NLML): -961.2532\n",
      "convergence dfGPdfNN Run 5/10, Epoch 967/1000, Training Loss (NLML): -961.2612\n",
      "convergence dfGPdfNN Run 5/10, Epoch 968/1000, Training Loss (NLML): -961.2646\n",
      "convergence dfGPdfNN Run 5/10, Epoch 969/1000, Training Loss (NLML): -961.2670\n",
      "convergence dfGPdfNN Run 5/10, Epoch 970/1000, Training Loss (NLML): -961.2662\n",
      "convergence dfGPdfNN Run 5/10, Epoch 971/1000, Training Loss (NLML): -961.2623\n",
      "convergence dfGPdfNN Run 5/10, Epoch 972/1000, Training Loss (NLML): -961.2665\n",
      "convergence dfGPdfNN Run 5/10, Epoch 973/1000, Training Loss (NLML): -961.2681\n",
      "convergence dfGPdfNN Run 5/10, Epoch 974/1000, Training Loss (NLML): -961.2698\n",
      "convergence dfGPdfNN Run 5/10, Epoch 975/1000, Training Loss (NLML): -961.2744\n",
      "convergence dfGPdfNN Run 5/10, Epoch 976/1000, Training Loss (NLML): -961.2682\n",
      "convergence dfGPdfNN Run 5/10, Epoch 977/1000, Training Loss (NLML): -961.2727\n",
      "convergence dfGPdfNN Run 5/10, Epoch 978/1000, Training Loss (NLML): -961.2732\n",
      "convergence dfGPdfNN Run 5/10, Epoch 979/1000, Training Loss (NLML): -961.2772\n",
      "convergence dfGPdfNN Run 5/10, Epoch 980/1000, Training Loss (NLML): -961.2798\n",
      "convergence dfGPdfNN Run 5/10, Epoch 981/1000, Training Loss (NLML): -961.2781\n",
      "convergence dfGPdfNN Run 5/10, Epoch 982/1000, Training Loss (NLML): -961.2816\n",
      "convergence dfGPdfNN Run 5/10, Epoch 983/1000, Training Loss (NLML): -961.2784\n",
      "convergence dfGPdfNN Run 5/10, Epoch 984/1000, Training Loss (NLML): -961.2826\n",
      "convergence dfGPdfNN Run 5/10, Epoch 985/1000, Training Loss (NLML): -961.2877\n",
      "convergence dfGPdfNN Run 5/10, Epoch 986/1000, Training Loss (NLML): -961.2902\n",
      "convergence dfGPdfNN Run 5/10, Epoch 987/1000, Training Loss (NLML): -961.2898\n",
      "convergence dfGPdfNN Run 5/10, Epoch 988/1000, Training Loss (NLML): -961.2910\n",
      "convergence dfGPdfNN Run 5/10, Epoch 989/1000, Training Loss (NLML): -961.2856\n",
      "convergence dfGPdfNN Run 5/10, Epoch 990/1000, Training Loss (NLML): -961.2891\n",
      "convergence dfGPdfNN Run 5/10, Epoch 991/1000, Training Loss (NLML): -961.2938\n",
      "convergence dfGPdfNN Run 5/10, Epoch 992/1000, Training Loss (NLML): -961.2976\n",
      "convergence dfGPdfNN Run 5/10, Epoch 993/1000, Training Loss (NLML): -961.2960\n",
      "convergence dfGPdfNN Run 5/10, Epoch 994/1000, Training Loss (NLML): -961.3010\n",
      "convergence dfGPdfNN Run 5/10, Epoch 995/1000, Training Loss (NLML): -961.3013\n",
      "convergence dfGPdfNN Run 5/10, Epoch 996/1000, Training Loss (NLML): -961.2970\n",
      "convergence dfGPdfNN Run 5/10, Epoch 997/1000, Training Loss (NLML): -961.2976\n",
      "convergence dfGPdfNN Run 5/10, Epoch 998/1000, Training Loss (NLML): -961.3031\n",
      "convergence dfGPdfNN Run 5/10, Epoch 999/1000, Training Loss (NLML): -961.3043\n",
      "convergence dfGPdfNN Run 5/10, Epoch 1000/1000, Training Loss (NLML): -961.3062\n",
      "\n",
      "--- Training Run 6/10 ---\n",
      "\n",
      "Start Training\n",
      "convergence dfGPdfNN Run 6/10, Epoch 1/1000, Training Loss (NLML): -886.5521\n",
      "convergence dfGPdfNN Run 6/10, Epoch 2/1000, Training Loss (NLML): -893.1039\n",
      "convergence dfGPdfNN Run 6/10, Epoch 3/1000, Training Loss (NLML): -897.0354\n",
      "convergence dfGPdfNN Run 6/10, Epoch 4/1000, Training Loss (NLML): -900.4486\n",
      "convergence dfGPdfNN Run 6/10, Epoch 5/1000, Training Loss (NLML): -902.7788\n",
      "convergence dfGPdfNN Run 6/10, Epoch 6/1000, Training Loss (NLML): -905.1437\n",
      "convergence dfGPdfNN Run 6/10, Epoch 7/1000, Training Loss (NLML): -906.4940\n",
      "convergence dfGPdfNN Run 6/10, Epoch 8/1000, Training Loss (NLML): -907.6323\n",
      "convergence dfGPdfNN Run 6/10, Epoch 9/1000, Training Loss (NLML): -909.5288\n",
      "convergence dfGPdfNN Run 6/10, Epoch 10/1000, Training Loss (NLML): -911.1191\n",
      "convergence dfGPdfNN Run 6/10, Epoch 11/1000, Training Loss (NLML): -912.7743\n",
      "convergence dfGPdfNN Run 6/10, Epoch 12/1000, Training Loss (NLML): -914.2386\n",
      "convergence dfGPdfNN Run 6/10, Epoch 13/1000, Training Loss (NLML): -915.5397\n",
      "convergence dfGPdfNN Run 6/10, Epoch 14/1000, Training Loss (NLML): -916.8224\n",
      "convergence dfGPdfNN Run 6/10, Epoch 15/1000, Training Loss (NLML): -918.0403\n",
      "convergence dfGPdfNN Run 6/10, Epoch 16/1000, Training Loss (NLML): -919.2117\n",
      "convergence dfGPdfNN Run 6/10, Epoch 17/1000, Training Loss (NLML): -920.0729\n",
      "convergence dfGPdfNN Run 6/10, Epoch 18/1000, Training Loss (NLML): -920.9659\n",
      "convergence dfGPdfNN Run 6/10, Epoch 19/1000, Training Loss (NLML): -921.9161\n",
      "convergence dfGPdfNN Run 6/10, Epoch 20/1000, Training Loss (NLML): -922.7164\n",
      "convergence dfGPdfNN Run 6/10, Epoch 21/1000, Training Loss (NLML): -923.8347\n",
      "convergence dfGPdfNN Run 6/10, Epoch 22/1000, Training Loss (NLML): -924.9926\n",
      "convergence dfGPdfNN Run 6/10, Epoch 23/1000, Training Loss (NLML): -925.8722\n",
      "convergence dfGPdfNN Run 6/10, Epoch 24/1000, Training Loss (NLML): -926.7081\n",
      "convergence dfGPdfNN Run 6/10, Epoch 25/1000, Training Loss (NLML): -927.5498\n",
      "convergence dfGPdfNN Run 6/10, Epoch 26/1000, Training Loss (NLML): -928.3245\n",
      "convergence dfGPdfNN Run 6/10, Epoch 27/1000, Training Loss (NLML): -929.0441\n",
      "convergence dfGPdfNN Run 6/10, Epoch 28/1000, Training Loss (NLML): -929.7520\n",
      "convergence dfGPdfNN Run 6/10, Epoch 29/1000, Training Loss (NLML): -930.4374\n",
      "convergence dfGPdfNN Run 6/10, Epoch 30/1000, Training Loss (NLML): -931.0737\n",
      "convergence dfGPdfNN Run 6/10, Epoch 31/1000, Training Loss (NLML): -931.7380\n",
      "convergence dfGPdfNN Run 6/10, Epoch 32/1000, Training Loss (NLML): -932.3843\n",
      "convergence dfGPdfNN Run 6/10, Epoch 33/1000, Training Loss (NLML): -933.0015\n",
      "convergence dfGPdfNN Run 6/10, Epoch 34/1000, Training Loss (NLML): -933.6073\n",
      "convergence dfGPdfNN Run 6/10, Epoch 35/1000, Training Loss (NLML): -934.1870\n",
      "convergence dfGPdfNN Run 6/10, Epoch 36/1000, Training Loss (NLML): -934.7412\n",
      "convergence dfGPdfNN Run 6/10, Epoch 37/1000, Training Loss (NLML): -935.2888\n",
      "convergence dfGPdfNN Run 6/10, Epoch 38/1000, Training Loss (NLML): -935.8069\n",
      "convergence dfGPdfNN Run 6/10, Epoch 39/1000, Training Loss (NLML): -936.3196\n",
      "convergence dfGPdfNN Run 6/10, Epoch 40/1000, Training Loss (NLML): -936.8080\n",
      "convergence dfGPdfNN Run 6/10, Epoch 41/1000, Training Loss (NLML): -937.2731\n",
      "convergence dfGPdfNN Run 6/10, Epoch 42/1000, Training Loss (NLML): -937.7363\n",
      "convergence dfGPdfNN Run 6/10, Epoch 43/1000, Training Loss (NLML): -938.1757\n",
      "convergence dfGPdfNN Run 6/10, Epoch 44/1000, Training Loss (NLML): -938.6096\n",
      "convergence dfGPdfNN Run 6/10, Epoch 45/1000, Training Loss (NLML): -939.0323\n",
      "convergence dfGPdfNN Run 6/10, Epoch 46/1000, Training Loss (NLML): -939.4038\n",
      "convergence dfGPdfNN Run 6/10, Epoch 47/1000, Training Loss (NLML): -939.8116\n",
      "convergence dfGPdfNN Run 6/10, Epoch 48/1000, Training Loss (NLML): -940.2075\n",
      "convergence dfGPdfNN Run 6/10, Epoch 49/1000, Training Loss (NLML): -940.5901\n",
      "convergence dfGPdfNN Run 6/10, Epoch 50/1000, Training Loss (NLML): -940.9669\n",
      "convergence dfGPdfNN Run 6/10, Epoch 51/1000, Training Loss (NLML): -941.3318\n",
      "convergence dfGPdfNN Run 6/10, Epoch 52/1000, Training Loss (NLML): -941.6841\n",
      "convergence dfGPdfNN Run 6/10, Epoch 53/1000, Training Loss (NLML): -942.0297\n",
      "convergence dfGPdfNN Run 6/10, Epoch 54/1000, Training Loss (NLML): -942.3584\n",
      "convergence dfGPdfNN Run 6/10, Epoch 55/1000, Training Loss (NLML): -942.6840\n",
      "convergence dfGPdfNN Run 6/10, Epoch 56/1000, Training Loss (NLML): -943.0020\n",
      "convergence dfGPdfNN Run 6/10, Epoch 57/1000, Training Loss (NLML): -942.0930\n",
      "convergence dfGPdfNN Run 6/10, Epoch 58/1000, Training Loss (NLML): -940.0685\n",
      "convergence dfGPdfNN Run 6/10, Epoch 59/1000, Training Loss (NLML): -939.9238\n",
      "convergence dfGPdfNN Run 6/10, Epoch 60/1000, Training Loss (NLML): -940.7235\n",
      "convergence dfGPdfNN Run 6/10, Epoch 61/1000, Training Loss (NLML): -942.3132\n",
      "convergence dfGPdfNN Run 6/10, Epoch 62/1000, Training Loss (NLML): -943.6466\n",
      "convergence dfGPdfNN Run 6/10, Epoch 63/1000, Training Loss (NLML): -944.2101\n",
      "convergence dfGPdfNN Run 6/10, Epoch 64/1000, Training Loss (NLML): -944.5939\n",
      "convergence dfGPdfNN Run 6/10, Epoch 65/1000, Training Loss (NLML): -944.8420\n",
      "convergence dfGPdfNN Run 6/10, Epoch 66/1000, Training Loss (NLML): -945.0764\n",
      "convergence dfGPdfNN Run 6/10, Epoch 67/1000, Training Loss (NLML): -945.3351\n",
      "convergence dfGPdfNN Run 6/10, Epoch 68/1000, Training Loss (NLML): -945.5952\n",
      "convergence dfGPdfNN Run 6/10, Epoch 69/1000, Training Loss (NLML): -945.8094\n",
      "convergence dfGPdfNN Run 6/10, Epoch 70/1000, Training Loss (NLML): -946.0253\n",
      "convergence dfGPdfNN Run 6/10, Epoch 71/1000, Training Loss (NLML): -946.1680\n",
      "convergence dfGPdfNN Run 6/10, Epoch 72/1000, Training Loss (NLML): -946.3292\n",
      "convergence dfGPdfNN Run 6/10, Epoch 73/1000, Training Loss (NLML): -946.4852\n",
      "convergence dfGPdfNN Run 6/10, Epoch 74/1000, Training Loss (NLML): -946.7643\n",
      "convergence dfGPdfNN Run 6/10, Epoch 75/1000, Training Loss (NLML): -946.9508\n",
      "convergence dfGPdfNN Run 6/10, Epoch 76/1000, Training Loss (NLML): -947.1512\n",
      "convergence dfGPdfNN Run 6/10, Epoch 77/1000, Training Loss (NLML): -947.3224\n",
      "convergence dfGPdfNN Run 6/10, Epoch 78/1000, Training Loss (NLML): -947.3835\n",
      "convergence dfGPdfNN Run 6/10, Epoch 79/1000, Training Loss (NLML): -947.5449\n",
      "convergence dfGPdfNN Run 6/10, Epoch 80/1000, Training Loss (NLML): -947.7408\n",
      "convergence dfGPdfNN Run 6/10, Epoch 81/1000, Training Loss (NLML): -948.0293\n",
      "convergence dfGPdfNN Run 6/10, Epoch 82/1000, Training Loss (NLML): -948.3416\n",
      "convergence dfGPdfNN Run 6/10, Epoch 83/1000, Training Loss (NLML): -948.4937\n",
      "convergence dfGPdfNN Run 6/10, Epoch 84/1000, Training Loss (NLML): -948.5740\n",
      "convergence dfGPdfNN Run 6/10, Epoch 85/1000, Training Loss (NLML): -948.6726\n",
      "convergence dfGPdfNN Run 6/10, Epoch 86/1000, Training Loss (NLML): -948.9346\n",
      "convergence dfGPdfNN Run 6/10, Epoch 87/1000, Training Loss (NLML): -949.1075\n",
      "convergence dfGPdfNN Run 6/10, Epoch 88/1000, Training Loss (NLML): -949.2844\n",
      "convergence dfGPdfNN Run 6/10, Epoch 89/1000, Training Loss (NLML): -949.4525\n",
      "convergence dfGPdfNN Run 6/10, Epoch 90/1000, Training Loss (NLML): -949.5656\n",
      "convergence dfGPdfNN Run 6/10, Epoch 91/1000, Training Loss (NLML): -949.6721\n",
      "convergence dfGPdfNN Run 6/10, Epoch 92/1000, Training Loss (NLML): -949.7738\n",
      "convergence dfGPdfNN Run 6/10, Epoch 93/1000, Training Loss (NLML): -949.9316\n",
      "convergence dfGPdfNN Run 6/10, Epoch 94/1000, Training Loss (NLML): -950.1552\n",
      "convergence dfGPdfNN Run 6/10, Epoch 95/1000, Training Loss (NLML): -950.3571\n",
      "convergence dfGPdfNN Run 6/10, Epoch 96/1000, Training Loss (NLML): -950.2026\n",
      "convergence dfGPdfNN Run 6/10, Epoch 97/1000, Training Loss (NLML): -950.3254\n",
      "convergence dfGPdfNN Run 6/10, Epoch 98/1000, Training Loss (NLML): -950.5126\n",
      "convergence dfGPdfNN Run 6/10, Epoch 99/1000, Training Loss (NLML): -950.6354\n",
      "convergence dfGPdfNN Run 6/10, Epoch 100/1000, Training Loss (NLML): -950.7668\n",
      "convergence dfGPdfNN Run 6/10, Epoch 101/1000, Training Loss (NLML): -951.1334\n",
      "convergence dfGPdfNN Run 6/10, Epoch 102/1000, Training Loss (NLML): -951.1758\n",
      "convergence dfGPdfNN Run 6/10, Epoch 103/1000, Training Loss (NLML): -951.3054\n",
      "convergence dfGPdfNN Run 6/10, Epoch 104/1000, Training Loss (NLML): -951.5321\n",
      "convergence dfGPdfNN Run 6/10, Epoch 105/1000, Training Loss (NLML): -951.5957\n",
      "convergence dfGPdfNN Run 6/10, Epoch 106/1000, Training Loss (NLML): -951.6183\n",
      "convergence dfGPdfNN Run 6/10, Epoch 107/1000, Training Loss (NLML): -951.6056\n",
      "convergence dfGPdfNN Run 6/10, Epoch 108/1000, Training Loss (NLML): -951.8701\n",
      "convergence dfGPdfNN Run 6/10, Epoch 109/1000, Training Loss (NLML): -952.0532\n",
      "convergence dfGPdfNN Run 6/10, Epoch 110/1000, Training Loss (NLML): -952.1696\n",
      "convergence dfGPdfNN Run 6/10, Epoch 111/1000, Training Loss (NLML): -952.2766\n",
      "convergence dfGPdfNN Run 6/10, Epoch 112/1000, Training Loss (NLML): -952.6495\n",
      "convergence dfGPdfNN Run 6/10, Epoch 113/1000, Training Loss (NLML): -952.8020\n",
      "convergence dfGPdfNN Run 6/10, Epoch 114/1000, Training Loss (NLML): -952.9979\n",
      "convergence dfGPdfNN Run 6/10, Epoch 115/1000, Training Loss (NLML): -952.9546\n",
      "convergence dfGPdfNN Run 6/10, Epoch 116/1000, Training Loss (NLML): -953.1084\n",
      "convergence dfGPdfNN Run 6/10, Epoch 117/1000, Training Loss (NLML): -953.2308\n",
      "convergence dfGPdfNN Run 6/10, Epoch 118/1000, Training Loss (NLML): -953.2410\n",
      "convergence dfGPdfNN Run 6/10, Epoch 119/1000, Training Loss (NLML): -953.0344\n",
      "convergence dfGPdfNN Run 6/10, Epoch 120/1000, Training Loss (NLML): -952.9318\n",
      "convergence dfGPdfNN Run 6/10, Epoch 121/1000, Training Loss (NLML): -952.9327\n",
      "convergence dfGPdfNN Run 6/10, Epoch 122/1000, Training Loss (NLML): -953.1251\n",
      "convergence dfGPdfNN Run 6/10, Epoch 123/1000, Training Loss (NLML): -953.2653\n",
      "convergence dfGPdfNN Run 6/10, Epoch 124/1000, Training Loss (NLML): -953.3705\n",
      "convergence dfGPdfNN Run 6/10, Epoch 125/1000, Training Loss (NLML): -953.5143\n",
      "convergence dfGPdfNN Run 6/10, Epoch 126/1000, Training Loss (NLML): -953.6178\n",
      "convergence dfGPdfNN Run 6/10, Epoch 127/1000, Training Loss (NLML): -953.7491\n",
      "convergence dfGPdfNN Run 6/10, Epoch 128/1000, Training Loss (NLML): -953.8612\n",
      "convergence dfGPdfNN Run 6/10, Epoch 129/1000, Training Loss (NLML): -953.9785\n",
      "convergence dfGPdfNN Run 6/10, Epoch 130/1000, Training Loss (NLML): -954.0299\n",
      "convergence dfGPdfNN Run 6/10, Epoch 131/1000, Training Loss (NLML): -954.1982\n",
      "convergence dfGPdfNN Run 6/10, Epoch 132/1000, Training Loss (NLML): -954.2194\n",
      "convergence dfGPdfNN Run 6/10, Epoch 133/1000, Training Loss (NLML): -954.3005\n",
      "convergence dfGPdfNN Run 6/10, Epoch 134/1000, Training Loss (NLML): -953.8351\n",
      "convergence dfGPdfNN Run 6/10, Epoch 135/1000, Training Loss (NLML): -954.3024\n",
      "convergence dfGPdfNN Run 6/10, Epoch 136/1000, Training Loss (NLML): -954.5627\n",
      "convergence dfGPdfNN Run 6/10, Epoch 137/1000, Training Loss (NLML): -954.7109\n",
      "convergence dfGPdfNN Run 6/10, Epoch 138/1000, Training Loss (NLML): -954.8013\n",
      "convergence dfGPdfNN Run 6/10, Epoch 139/1000, Training Loss (NLML): -954.9401\n",
      "convergence dfGPdfNN Run 6/10, Epoch 140/1000, Training Loss (NLML): -955.0667\n",
      "convergence dfGPdfNN Run 6/10, Epoch 141/1000, Training Loss (NLML): -955.1240\n",
      "convergence dfGPdfNN Run 6/10, Epoch 142/1000, Training Loss (NLML): -955.0605\n",
      "convergence dfGPdfNN Run 6/10, Epoch 143/1000, Training Loss (NLML): -955.2080\n",
      "convergence dfGPdfNN Run 6/10, Epoch 144/1000, Training Loss (NLML): -955.2959\n",
      "convergence dfGPdfNN Run 6/10, Epoch 145/1000, Training Loss (NLML): -955.2617\n",
      "convergence dfGPdfNN Run 6/10, Epoch 146/1000, Training Loss (NLML): -955.1927\n",
      "convergence dfGPdfNN Run 6/10, Epoch 147/1000, Training Loss (NLML): -955.4678\n",
      "convergence dfGPdfNN Run 6/10, Epoch 148/1000, Training Loss (NLML): -955.7202\n",
      "convergence dfGPdfNN Run 6/10, Epoch 149/1000, Training Loss (NLML): -955.8342\n",
      "convergence dfGPdfNN Run 6/10, Epoch 150/1000, Training Loss (NLML): -955.7216\n",
      "convergence dfGPdfNN Run 6/10, Epoch 151/1000, Training Loss (NLML): -955.6488\n",
      "convergence dfGPdfNN Run 6/10, Epoch 152/1000, Training Loss (NLML): -955.5844\n",
      "convergence dfGPdfNN Run 6/10, Epoch 153/1000, Training Loss (NLML): -955.8110\n",
      "convergence dfGPdfNN Run 6/10, Epoch 154/1000, Training Loss (NLML): -956.0037\n",
      "convergence dfGPdfNN Run 6/10, Epoch 155/1000, Training Loss (NLML): -956.1969\n",
      "convergence dfGPdfNN Run 6/10, Epoch 156/1000, Training Loss (NLML): -956.1146\n",
      "convergence dfGPdfNN Run 6/10, Epoch 157/1000, Training Loss (NLML): -956.1116\n",
      "convergence dfGPdfNN Run 6/10, Epoch 158/1000, Training Loss (NLML): -956.1385\n",
      "convergence dfGPdfNN Run 6/10, Epoch 159/1000, Training Loss (NLML): -956.3401\n",
      "convergence dfGPdfNN Run 6/10, Epoch 160/1000, Training Loss (NLML): -908.4000\n",
      "convergence dfGPdfNN Run 6/10, Epoch 161/1000, Training Loss (NLML): -956.0559\n",
      "convergence dfGPdfNN Run 6/10, Epoch 162/1000, Training Loss (NLML): -955.5271\n",
      "convergence dfGPdfNN Run 6/10, Epoch 163/1000, Training Loss (NLML): -955.1891\n",
      "convergence dfGPdfNN Run 6/10, Epoch 164/1000, Training Loss (NLML): -954.8281\n",
      "convergence dfGPdfNN Run 6/10, Epoch 165/1000, Training Loss (NLML): -954.4038\n",
      "convergence dfGPdfNN Run 6/10, Epoch 166/1000, Training Loss (NLML): -954.4048\n",
      "convergence dfGPdfNN Run 6/10, Epoch 167/1000, Training Loss (NLML): -954.4911\n",
      "convergence dfGPdfNN Run 6/10, Epoch 168/1000, Training Loss (NLML): -954.5942\n",
      "convergence dfGPdfNN Run 6/10, Epoch 169/1000, Training Loss (NLML): -954.8314\n",
      "convergence dfGPdfNN Run 6/10, Epoch 170/1000, Training Loss (NLML): -954.9738\n",
      "convergence dfGPdfNN Run 6/10, Epoch 171/1000, Training Loss (NLML): -954.8339\n",
      "convergence dfGPdfNN Run 6/10, Epoch 172/1000, Training Loss (NLML): -954.7374\n",
      "convergence dfGPdfNN Run 6/10, Epoch 173/1000, Training Loss (NLML): -954.9583\n",
      "convergence dfGPdfNN Run 6/10, Epoch 174/1000, Training Loss (NLML): -955.1785\n",
      "convergence dfGPdfNN Run 6/10, Epoch 175/1000, Training Loss (NLML): -955.3217\n",
      "convergence dfGPdfNN Run 6/10, Epoch 176/1000, Training Loss (NLML): -955.4812\n",
      "convergence dfGPdfNN Run 6/10, Epoch 177/1000, Training Loss (NLML): -955.6036\n",
      "convergence dfGPdfNN Run 6/10, Epoch 178/1000, Training Loss (NLML): -955.7952\n",
      "convergence dfGPdfNN Run 6/10, Epoch 179/1000, Training Loss (NLML): -956.0459\n",
      "convergence dfGPdfNN Run 6/10, Epoch 180/1000, Training Loss (NLML): -956.2217\n",
      "convergence dfGPdfNN Run 6/10, Epoch 181/1000, Training Loss (NLML): -956.2623\n",
      "convergence dfGPdfNN Run 6/10, Epoch 182/1000, Training Loss (NLML): -956.1759\n",
      "convergence dfGPdfNN Run 6/10, Epoch 183/1000, Training Loss (NLML): -956.2245\n",
      "convergence dfGPdfNN Run 6/10, Epoch 184/1000, Training Loss (NLML): -956.4178\n",
      "convergence dfGPdfNN Run 6/10, Epoch 185/1000, Training Loss (NLML): -956.6172\n",
      "convergence dfGPdfNN Run 6/10, Epoch 186/1000, Training Loss (NLML): -956.7052\n",
      "convergence dfGPdfNN Run 6/10, Epoch 187/1000, Training Loss (NLML): -956.6124\n",
      "convergence dfGPdfNN Run 6/10, Epoch 188/1000, Training Loss (NLML): -956.6376\n",
      "convergence dfGPdfNN Run 6/10, Epoch 189/1000, Training Loss (NLML): -956.7090\n",
      "convergence dfGPdfNN Run 6/10, Epoch 190/1000, Training Loss (NLML): -956.7894\n",
      "convergence dfGPdfNN Run 6/10, Epoch 191/1000, Training Loss (NLML): -956.9714\n",
      "convergence dfGPdfNN Run 6/10, Epoch 192/1000, Training Loss (NLML): -957.0586\n",
      "convergence dfGPdfNN Run 6/10, Epoch 193/1000, Training Loss (NLML): -957.1113\n",
      "convergence dfGPdfNN Run 6/10, Epoch 194/1000, Training Loss (NLML): -957.0969\n",
      "convergence dfGPdfNN Run 6/10, Epoch 195/1000, Training Loss (NLML): -957.1074\n",
      "convergence dfGPdfNN Run 6/10, Epoch 196/1000, Training Loss (NLML): -957.1021\n",
      "convergence dfGPdfNN Run 6/10, Epoch 197/1000, Training Loss (NLML): -955.0293\n",
      "convergence dfGPdfNN Run 6/10, Epoch 198/1000, Training Loss (NLML): -955.4093\n",
      "convergence dfGPdfNN Run 6/10, Epoch 199/1000, Training Loss (NLML): -956.6554\n",
      "convergence dfGPdfNN Run 6/10, Epoch 200/1000, Training Loss (NLML): -956.9806\n",
      "convergence dfGPdfNN Run 6/10, Epoch 201/1000, Training Loss (NLML): -957.1494\n",
      "convergence dfGPdfNN Run 6/10, Epoch 202/1000, Training Loss (NLML): -957.1404\n",
      "convergence dfGPdfNN Run 6/10, Epoch 203/1000, Training Loss (NLML): -957.1794\n",
      "convergence dfGPdfNN Run 6/10, Epoch 204/1000, Training Loss (NLML): -957.2871\n",
      "convergence dfGPdfNN Run 6/10, Epoch 205/1000, Training Loss (NLML): -957.3473\n",
      "convergence dfGPdfNN Run 6/10, Epoch 206/1000, Training Loss (NLML): -957.3353\n",
      "convergence dfGPdfNN Run 6/10, Epoch 207/1000, Training Loss (NLML): -957.3536\n",
      "convergence dfGPdfNN Run 6/10, Epoch 208/1000, Training Loss (NLML): -957.3857\n",
      "convergence dfGPdfNN Run 6/10, Epoch 209/1000, Training Loss (NLML): -957.3285\n",
      "convergence dfGPdfNN Run 6/10, Epoch 210/1000, Training Loss (NLML): -957.3770\n",
      "convergence dfGPdfNN Run 6/10, Epoch 211/1000, Training Loss (NLML): -957.4530\n",
      "convergence dfGPdfNN Run 6/10, Epoch 212/1000, Training Loss (NLML): -957.4199\n",
      "convergence dfGPdfNN Run 6/10, Epoch 213/1000, Training Loss (NLML): -957.2410\n",
      "convergence dfGPdfNN Run 6/10, Epoch 214/1000, Training Loss (NLML): -957.2614\n",
      "convergence dfGPdfNN Run 6/10, Epoch 215/1000, Training Loss (NLML): -957.3207\n",
      "convergence dfGPdfNN Run 6/10, Epoch 216/1000, Training Loss (NLML): -957.2927\n",
      "convergence dfGPdfNN Run 6/10, Epoch 217/1000, Training Loss (NLML): -957.2987\n",
      "convergence dfGPdfNN Run 6/10, Epoch 218/1000, Training Loss (NLML): -945.6627\n",
      "convergence dfGPdfNN Run 6/10, Epoch 219/1000, Training Loss (NLML): -957.3300\n",
      "convergence dfGPdfNN Run 6/10, Epoch 220/1000, Training Loss (NLML): -957.0922\n",
      "convergence dfGPdfNN Run 6/10, Epoch 221/1000, Training Loss (NLML): -957.0800\n",
      "convergence dfGPdfNN Run 6/10, Epoch 222/1000, Training Loss (NLML): -957.0531\n",
      "convergence dfGPdfNN Run 6/10, Epoch 223/1000, Training Loss (NLML): -956.9471\n",
      "convergence dfGPdfNN Run 6/10, Epoch 224/1000, Training Loss (NLML): -956.9775\n",
      "convergence dfGPdfNN Run 6/10, Epoch 225/1000, Training Loss (NLML): -956.9618\n",
      "convergence dfGPdfNN Run 6/10, Epoch 226/1000, Training Loss (NLML): -957.0527\n",
      "convergence dfGPdfNN Run 6/10, Epoch 227/1000, Training Loss (NLML): -957.1593\n",
      "convergence dfGPdfNN Run 6/10, Epoch 228/1000, Training Loss (NLML): -957.1185\n",
      "convergence dfGPdfNN Run 6/10, Epoch 229/1000, Training Loss (NLML): -957.1249\n",
      "convergence dfGPdfNN Run 6/10, Epoch 230/1000, Training Loss (NLML): -957.1721\n",
      "convergence dfGPdfNN Run 6/10, Epoch 231/1000, Training Loss (NLML): -957.2001\n",
      "convergence dfGPdfNN Run 6/10, Epoch 232/1000, Training Loss (NLML): -957.2708\n",
      "convergence dfGPdfNN Run 6/10, Epoch 233/1000, Training Loss (NLML): -957.4781\n",
      "convergence dfGPdfNN Run 6/10, Epoch 234/1000, Training Loss (NLML): -957.5170\n",
      "convergence dfGPdfNN Run 6/10, Epoch 235/1000, Training Loss (NLML): -957.5052\n",
      "convergence dfGPdfNN Run 6/10, Epoch 236/1000, Training Loss (NLML): -957.3778\n",
      "convergence dfGPdfNN Run 6/10, Epoch 237/1000, Training Loss (NLML): -957.4056\n",
      "convergence dfGPdfNN Run 6/10, Epoch 238/1000, Training Loss (NLML): -957.4320\n",
      "convergence dfGPdfNN Run 6/10, Epoch 239/1000, Training Loss (NLML): -957.4480\n",
      "convergence dfGPdfNN Run 6/10, Epoch 240/1000, Training Loss (NLML): -957.4487\n",
      "convergence dfGPdfNN Run 6/10, Epoch 241/1000, Training Loss (NLML): -956.7217\n",
      "convergence dfGPdfNN Run 6/10, Epoch 242/1000, Training Loss (NLML): -956.7589\n",
      "convergence dfGPdfNN Run 6/10, Epoch 243/1000, Training Loss (NLML): -957.3180\n",
      "convergence dfGPdfNN Run 6/10, Epoch 244/1000, Training Loss (NLML): -957.5452\n",
      "convergence dfGPdfNN Run 6/10, Epoch 245/1000, Training Loss (NLML): -957.6544\n",
      "convergence dfGPdfNN Run 6/10, Epoch 246/1000, Training Loss (NLML): -957.7135\n",
      "convergence dfGPdfNN Run 6/10, Epoch 247/1000, Training Loss (NLML): -957.6746\n",
      "convergence dfGPdfNN Run 6/10, Epoch 248/1000, Training Loss (NLML): -957.5222\n",
      "convergence dfGPdfNN Run 6/10, Epoch 249/1000, Training Loss (NLML): -957.4978\n",
      "convergence dfGPdfNN Run 6/10, Epoch 250/1000, Training Loss (NLML): -957.4253\n",
      "convergence dfGPdfNN Run 6/10, Epoch 251/1000, Training Loss (NLML): -957.4976\n",
      "convergence dfGPdfNN Run 6/10, Epoch 252/1000, Training Loss (NLML): -957.6036\n",
      "convergence dfGPdfNN Run 6/10, Epoch 253/1000, Training Loss (NLML): -957.6021\n",
      "convergence dfGPdfNN Run 6/10, Epoch 254/1000, Training Loss (NLML): -957.8612\n",
      "convergence dfGPdfNN Run 6/10, Epoch 255/1000, Training Loss (NLML): -958.0051\n",
      "convergence dfGPdfNN Run 6/10, Epoch 256/1000, Training Loss (NLML): -958.0282\n",
      "convergence dfGPdfNN Run 6/10, Epoch 257/1000, Training Loss (NLML): -957.9915\n",
      "convergence dfGPdfNN Run 6/10, Epoch 258/1000, Training Loss (NLML): -957.8252\n",
      "convergence dfGPdfNN Run 6/10, Epoch 259/1000, Training Loss (NLML): -957.8026\n",
      "convergence dfGPdfNN Run 6/10, Epoch 260/1000, Training Loss (NLML): -957.8198\n",
      "convergence dfGPdfNN Run 6/10, Epoch 261/1000, Training Loss (NLML): -957.8561\n",
      "convergence dfGPdfNN Run 6/10, Epoch 262/1000, Training Loss (NLML): -957.9482\n",
      "convergence dfGPdfNN Run 6/10, Epoch 263/1000, Training Loss (NLML): -958.0349\n",
      "convergence dfGPdfNN Run 6/10, Epoch 264/1000, Training Loss (NLML): -957.9910\n",
      "convergence dfGPdfNN Run 6/10, Epoch 265/1000, Training Loss (NLML): -957.9597\n",
      "convergence dfGPdfNN Run 6/10, Epoch 266/1000, Training Loss (NLML): -957.9083\n",
      "convergence dfGPdfNN Run 6/10, Epoch 267/1000, Training Loss (NLML): -957.8695\n",
      "convergence dfGPdfNN Run 6/10, Epoch 268/1000, Training Loss (NLML): -957.9639\n",
      "convergence dfGPdfNN Run 6/10, Epoch 269/1000, Training Loss (NLML): -958.0033\n",
      "convergence dfGPdfNN Run 6/10, Epoch 270/1000, Training Loss (NLML): -958.0734\n",
      "convergence dfGPdfNN Run 6/10, Epoch 271/1000, Training Loss (NLML): -958.0679\n",
      "convergence dfGPdfNN Run 6/10, Epoch 272/1000, Training Loss (NLML): -958.1697\n",
      "convergence dfGPdfNN Run 6/10, Epoch 273/1000, Training Loss (NLML): -958.2023\n",
      "convergence dfGPdfNN Run 6/10, Epoch 274/1000, Training Loss (NLML): -958.2216\n",
      "convergence dfGPdfNN Run 6/10, Epoch 275/1000, Training Loss (NLML): -958.2231\n",
      "convergence dfGPdfNN Run 6/10, Epoch 276/1000, Training Loss (NLML): -958.1960\n",
      "convergence dfGPdfNN Run 6/10, Epoch 277/1000, Training Loss (NLML): -958.2225\n",
      "convergence dfGPdfNN Run 6/10, Epoch 278/1000, Training Loss (NLML): -958.1880\n",
      "convergence dfGPdfNN Run 6/10, Epoch 279/1000, Training Loss (NLML): -958.0967\n",
      "convergence dfGPdfNN Run 6/10, Epoch 280/1000, Training Loss (NLML): -958.2412\n",
      "convergence dfGPdfNN Run 6/10, Epoch 281/1000, Training Loss (NLML): -958.2216\n",
      "convergence dfGPdfNN Run 6/10, Epoch 282/1000, Training Loss (NLML): -958.2311\n",
      "convergence dfGPdfNN Run 6/10, Epoch 283/1000, Training Loss (NLML): -958.1781\n",
      "convergence dfGPdfNN Run 6/10, Epoch 284/1000, Training Loss (NLML): -958.1769\n",
      "convergence dfGPdfNN Run 6/10, Epoch 285/1000, Training Loss (NLML): -958.2346\n",
      "convergence dfGPdfNN Run 6/10, Epoch 286/1000, Training Loss (NLML): -958.2747\n",
      "convergence dfGPdfNN Run 6/10, Epoch 287/1000, Training Loss (NLML): -958.2640\n",
      "convergence dfGPdfNN Run 6/10, Epoch 288/1000, Training Loss (NLML): -958.3470\n",
      "convergence dfGPdfNN Run 6/10, Epoch 289/1000, Training Loss (NLML): -958.3660\n",
      "convergence dfGPdfNN Run 6/10, Epoch 290/1000, Training Loss (NLML): -958.4086\n",
      "convergence dfGPdfNN Run 6/10, Epoch 291/1000, Training Loss (NLML): -958.4338\n",
      "convergence dfGPdfNN Run 6/10, Epoch 292/1000, Training Loss (NLML): -958.4872\n",
      "convergence dfGPdfNN Run 6/10, Epoch 293/1000, Training Loss (NLML): -958.5100\n",
      "convergence dfGPdfNN Run 6/10, Epoch 294/1000, Training Loss (NLML): -958.5142\n",
      "convergence dfGPdfNN Run 6/10, Epoch 295/1000, Training Loss (NLML): -958.4769\n",
      "convergence dfGPdfNN Run 6/10, Epoch 296/1000, Training Loss (NLML): -958.4951\n",
      "convergence dfGPdfNN Run 6/10, Epoch 297/1000, Training Loss (NLML): -958.5140\n",
      "convergence dfGPdfNN Run 6/10, Epoch 298/1000, Training Loss (NLML): -958.5300\n",
      "convergence dfGPdfNN Run 6/10, Epoch 299/1000, Training Loss (NLML): -958.5182\n",
      "convergence dfGPdfNN Run 6/10, Epoch 300/1000, Training Loss (NLML): -958.5883\n",
      "convergence dfGPdfNN Run 6/10, Epoch 301/1000, Training Loss (NLML): -958.6029\n",
      "convergence dfGPdfNN Run 6/10, Epoch 302/1000, Training Loss (NLML): -958.5977\n",
      "convergence dfGPdfNN Run 6/10, Epoch 303/1000, Training Loss (NLML): -958.5966\n",
      "convergence dfGPdfNN Run 6/10, Epoch 304/1000, Training Loss (NLML): -958.5608\n",
      "convergence dfGPdfNN Run 6/10, Epoch 305/1000, Training Loss (NLML): -958.5637\n",
      "convergence dfGPdfNN Run 6/10, Epoch 306/1000, Training Loss (NLML): -958.5876\n",
      "convergence dfGPdfNN Run 6/10, Epoch 307/1000, Training Loss (NLML): -958.5699\n",
      "convergence dfGPdfNN Run 6/10, Epoch 308/1000, Training Loss (NLML): -958.5142\n",
      "convergence dfGPdfNN Run 6/10, Epoch 309/1000, Training Loss (NLML): -958.4132\n",
      "convergence dfGPdfNN Run 6/10, Epoch 310/1000, Training Loss (NLML): -958.3698\n",
      "convergence dfGPdfNN Run 6/10, Epoch 311/1000, Training Loss (NLML): -958.3140\n",
      "convergence dfGPdfNN Run 6/10, Epoch 312/1000, Training Loss (NLML): -958.3044\n",
      "convergence dfGPdfNN Run 6/10, Epoch 313/1000, Training Loss (NLML): -958.3138\n",
      "convergence dfGPdfNN Run 6/10, Epoch 314/1000, Training Loss (NLML): -958.3479\n",
      "convergence dfGPdfNN Run 6/10, Epoch 315/1000, Training Loss (NLML): -958.3955\n",
      "convergence dfGPdfNN Run 6/10, Epoch 316/1000, Training Loss (NLML): -958.3767\n",
      "convergence dfGPdfNN Run 6/10, Epoch 317/1000, Training Loss (NLML): -958.4298\n",
      "convergence dfGPdfNN Run 6/10, Epoch 318/1000, Training Loss (NLML): -958.4714\n",
      "convergence dfGPdfNN Run 6/10, Epoch 319/1000, Training Loss (NLML): -958.4719\n",
      "convergence dfGPdfNN Run 6/10, Epoch 320/1000, Training Loss (NLML): -958.5591\n",
      "convergence dfGPdfNN Run 6/10, Epoch 321/1000, Training Loss (NLML): -958.5552\n",
      "convergence dfGPdfNN Run 6/10, Epoch 322/1000, Training Loss (NLML): -958.5824\n",
      "convergence dfGPdfNN Run 6/10, Epoch 323/1000, Training Loss (NLML): -958.5662\n",
      "convergence dfGPdfNN Run 6/10, Epoch 324/1000, Training Loss (NLML): -958.5695\n",
      "convergence dfGPdfNN Run 6/10, Epoch 325/1000, Training Loss (NLML): -958.6029\n",
      "convergence dfGPdfNN Run 6/10, Epoch 326/1000, Training Loss (NLML): -958.6061\n",
      "convergence dfGPdfNN Run 6/10, Epoch 327/1000, Training Loss (NLML): -958.6212\n",
      "convergence dfGPdfNN Run 6/10, Epoch 328/1000, Training Loss (NLML): -958.5801\n",
      "convergence dfGPdfNN Run 6/10, Epoch 329/1000, Training Loss (NLML): -958.5980\n",
      "convergence dfGPdfNN Run 6/10, Epoch 330/1000, Training Loss (NLML): -958.6010\n",
      "convergence dfGPdfNN Run 6/10, Epoch 331/1000, Training Loss (NLML): -958.5709\n",
      "convergence dfGPdfNN Run 6/10, Epoch 332/1000, Training Loss (NLML): -958.5181\n",
      "convergence dfGPdfNN Run 6/10, Epoch 333/1000, Training Loss (NLML): -958.5125\n",
      "convergence dfGPdfNN Run 6/10, Epoch 334/1000, Training Loss (NLML): -958.5442\n",
      "convergence dfGPdfNN Run 6/10, Epoch 335/1000, Training Loss (NLML): -958.5963\n",
      "convergence dfGPdfNN Run 6/10, Epoch 336/1000, Training Loss (NLML): -958.6584\n",
      "convergence dfGPdfNN Run 6/10, Epoch 337/1000, Training Loss (NLML): -958.7312\n",
      "convergence dfGPdfNN Run 6/10, Epoch 338/1000, Training Loss (NLML): -958.7520\n",
      "convergence dfGPdfNN Run 6/10, Epoch 339/1000, Training Loss (NLML): -958.7062\n",
      "convergence dfGPdfNN Run 6/10, Epoch 340/1000, Training Loss (NLML): -958.6851\n",
      "convergence dfGPdfNN Run 6/10, Epoch 341/1000, Training Loss (NLML): -958.7192\n",
      "convergence dfGPdfNN Run 6/10, Epoch 342/1000, Training Loss (NLML): -958.7213\n",
      "convergence dfGPdfNN Run 6/10, Epoch 343/1000, Training Loss (NLML): -958.7117\n",
      "convergence dfGPdfNN Run 6/10, Epoch 344/1000, Training Loss (NLML): -958.7224\n",
      "convergence dfGPdfNN Run 6/10, Epoch 345/1000, Training Loss (NLML): -958.7932\n",
      "convergence dfGPdfNN Run 6/10, Epoch 346/1000, Training Loss (NLML): -958.8699\n",
      "convergence dfGPdfNN Run 6/10, Epoch 347/1000, Training Loss (NLML): -958.9238\n",
      "convergence dfGPdfNN Run 6/10, Epoch 348/1000, Training Loss (NLML): -958.9806\n",
      "convergence dfGPdfNN Run 6/10, Epoch 349/1000, Training Loss (NLML): -958.9948\n",
      "convergence dfGPdfNN Run 6/10, Epoch 350/1000, Training Loss (NLML): -958.9950\n",
      "convergence dfGPdfNN Run 6/10, Epoch 351/1000, Training Loss (NLML): -959.0150\n",
      "convergence dfGPdfNN Run 6/10, Epoch 352/1000, Training Loss (NLML): -959.0240\n",
      "convergence dfGPdfNN Run 6/10, Epoch 353/1000, Training Loss (NLML): -959.0251\n",
      "convergence dfGPdfNN Run 6/10, Epoch 354/1000, Training Loss (NLML): -959.0133\n",
      "convergence dfGPdfNN Run 6/10, Epoch 355/1000, Training Loss (NLML): -959.0396\n",
      "convergence dfGPdfNN Run 6/10, Epoch 356/1000, Training Loss (NLML): -959.0918\n",
      "convergence dfGPdfNN Run 6/10, Epoch 357/1000, Training Loss (NLML): -959.1260\n",
      "convergence dfGPdfNN Run 6/10, Epoch 358/1000, Training Loss (NLML): -959.1106\n",
      "convergence dfGPdfNN Run 6/10, Epoch 359/1000, Training Loss (NLML): -959.0271\n",
      "convergence dfGPdfNN Run 6/10, Epoch 360/1000, Training Loss (NLML): -958.9871\n",
      "convergence dfGPdfNN Run 6/10, Epoch 361/1000, Training Loss (NLML): -958.9640\n",
      "convergence dfGPdfNN Run 6/10, Epoch 362/1000, Training Loss (NLML): -958.9910\n",
      "convergence dfGPdfNN Run 6/10, Epoch 363/1000, Training Loss (NLML): -959.0049\n",
      "convergence dfGPdfNN Run 6/10, Epoch 364/1000, Training Loss (NLML): -959.0369\n",
      "convergence dfGPdfNN Run 6/10, Epoch 365/1000, Training Loss (NLML): -959.0684\n",
      "convergence dfGPdfNN Run 6/10, Epoch 366/1000, Training Loss (NLML): -959.0541\n",
      "convergence dfGPdfNN Run 6/10, Epoch 367/1000, Training Loss (NLML): -959.1083\n",
      "convergence dfGPdfNN Run 6/10, Epoch 368/1000, Training Loss (NLML): -959.0824\n",
      "convergence dfGPdfNN Run 6/10, Epoch 369/1000, Training Loss (NLML): -959.1257\n",
      "convergence dfGPdfNN Run 6/10, Epoch 370/1000, Training Loss (NLML): -959.1038\n",
      "convergence dfGPdfNN Run 6/10, Epoch 371/1000, Training Loss (NLML): -959.0841\n",
      "convergence dfGPdfNN Run 6/10, Epoch 372/1000, Training Loss (NLML): -959.0461\n",
      "convergence dfGPdfNN Run 6/10, Epoch 373/1000, Training Loss (NLML): -959.0189\n",
      "convergence dfGPdfNN Run 6/10, Epoch 374/1000, Training Loss (NLML): -959.0918\n",
      "convergence dfGPdfNN Run 6/10, Epoch 375/1000, Training Loss (NLML): -959.0911\n",
      "convergence dfGPdfNN Run 6/10, Epoch 376/1000, Training Loss (NLML): -959.1433\n",
      "convergence dfGPdfNN Run 6/10, Epoch 377/1000, Training Loss (NLML): -959.1570\n",
      "convergence dfGPdfNN Run 6/10, Epoch 378/1000, Training Loss (NLML): -959.1880\n",
      "convergence dfGPdfNN Run 6/10, Epoch 379/1000, Training Loss (NLML): -959.1577\n",
      "convergence dfGPdfNN Run 6/10, Epoch 380/1000, Training Loss (NLML): -959.2114\n",
      "convergence dfGPdfNN Run 6/10, Epoch 381/1000, Training Loss (NLML): -959.2096\n",
      "convergence dfGPdfNN Run 6/10, Epoch 382/1000, Training Loss (NLML): -959.2332\n",
      "convergence dfGPdfNN Run 6/10, Epoch 383/1000, Training Loss (NLML): -959.2672\n",
      "convergence dfGPdfNN Run 6/10, Epoch 384/1000, Training Loss (NLML): -959.2435\n",
      "convergence dfGPdfNN Run 6/10, Epoch 385/1000, Training Loss (NLML): -959.2394\n",
      "convergence dfGPdfNN Run 6/10, Epoch 386/1000, Training Loss (NLML): -959.2365\n",
      "convergence dfGPdfNN Run 6/10, Epoch 387/1000, Training Loss (NLML): -959.2065\n",
      "convergence dfGPdfNN Run 6/10, Epoch 388/1000, Training Loss (NLML): -959.2123\n",
      "convergence dfGPdfNN Run 6/10, Epoch 389/1000, Training Loss (NLML): -959.1847\n",
      "convergence dfGPdfNN Run 6/10, Epoch 390/1000, Training Loss (NLML): -959.1937\n",
      "convergence dfGPdfNN Run 6/10, Epoch 391/1000, Training Loss (NLML): -959.1774\n",
      "convergence dfGPdfNN Run 6/10, Epoch 392/1000, Training Loss (NLML): -959.2091\n",
      "convergence dfGPdfNN Run 6/10, Epoch 393/1000, Training Loss (NLML): -959.2124\n",
      "convergence dfGPdfNN Run 6/10, Epoch 394/1000, Training Loss (NLML): -959.2540\n",
      "convergence dfGPdfNN Run 6/10, Epoch 395/1000, Training Loss (NLML): -959.3063\n",
      "convergence dfGPdfNN Run 6/10, Epoch 396/1000, Training Loss (NLML): -959.2252\n",
      "convergence dfGPdfNN Run 6/10, Epoch 397/1000, Training Loss (NLML): -959.3013\n",
      "convergence dfGPdfNN Run 6/10, Epoch 398/1000, Training Loss (NLML): -959.2819\n",
      "convergence dfGPdfNN Run 6/10, Epoch 399/1000, Training Loss (NLML): -959.1710\n",
      "convergence dfGPdfNN Run 6/10, Epoch 400/1000, Training Loss (NLML): -959.0802\n",
      "convergence dfGPdfNN Run 6/10, Epoch 401/1000, Training Loss (NLML): -959.1060\n",
      "convergence dfGPdfNN Run 6/10, Epoch 402/1000, Training Loss (NLML): -959.1503\n",
      "convergence dfGPdfNN Run 6/10, Epoch 403/1000, Training Loss (NLML): -959.2000\n",
      "convergence dfGPdfNN Run 6/10, Epoch 404/1000, Training Loss (NLML): -959.2031\n",
      "convergence dfGPdfNN Run 6/10, Epoch 405/1000, Training Loss (NLML): -959.2769\n",
      "convergence dfGPdfNN Run 6/10, Epoch 406/1000, Training Loss (NLML): -959.2942\n",
      "convergence dfGPdfNN Run 6/10, Epoch 407/1000, Training Loss (NLML): -959.2639\n",
      "convergence dfGPdfNN Run 6/10, Epoch 408/1000, Training Loss (NLML): -959.2822\n",
      "convergence dfGPdfNN Run 6/10, Epoch 409/1000, Training Loss (NLML): -959.3071\n",
      "convergence dfGPdfNN Run 6/10, Epoch 410/1000, Training Loss (NLML): -959.3097\n",
      "convergence dfGPdfNN Run 6/10, Epoch 411/1000, Training Loss (NLML): -959.2828\n",
      "convergence dfGPdfNN Run 6/10, Epoch 412/1000, Training Loss (NLML): -959.2496\n",
      "convergence dfGPdfNN Run 6/10, Epoch 413/1000, Training Loss (NLML): -959.2982\n",
      "convergence dfGPdfNN Run 6/10, Epoch 414/1000, Training Loss (NLML): -959.3649\n",
      "convergence dfGPdfNN Run 6/10, Epoch 415/1000, Training Loss (NLML): -959.2777\n",
      "convergence dfGPdfNN Run 6/10, Epoch 416/1000, Training Loss (NLML): -959.2081\n",
      "convergence dfGPdfNN Run 6/10, Epoch 417/1000, Training Loss (NLML): -959.1428\n",
      "convergence dfGPdfNN Run 6/10, Epoch 418/1000, Training Loss (NLML): -959.2555\n",
      "convergence dfGPdfNN Run 6/10, Epoch 419/1000, Training Loss (NLML): -959.2848\n",
      "convergence dfGPdfNN Run 6/10, Epoch 420/1000, Training Loss (NLML): -959.3062\n",
      "convergence dfGPdfNN Run 6/10, Epoch 421/1000, Training Loss (NLML): -959.3260\n",
      "convergence dfGPdfNN Run 6/10, Epoch 422/1000, Training Loss (NLML): -959.3436\n",
      "convergence dfGPdfNN Run 6/10, Epoch 423/1000, Training Loss (NLML): -959.3997\n",
      "convergence dfGPdfNN Run 6/10, Epoch 424/1000, Training Loss (NLML): -959.4222\n",
      "convergence dfGPdfNN Run 6/10, Epoch 425/1000, Training Loss (NLML): -959.4354\n",
      "convergence dfGPdfNN Run 6/10, Epoch 426/1000, Training Loss (NLML): -959.4746\n",
      "convergence dfGPdfNN Run 6/10, Epoch 427/1000, Training Loss (NLML): -959.4464\n",
      "convergence dfGPdfNN Run 6/10, Epoch 428/1000, Training Loss (NLML): -959.4850\n",
      "convergence dfGPdfNN Run 6/10, Epoch 429/1000, Training Loss (NLML): -959.4941\n",
      "convergence dfGPdfNN Run 6/10, Epoch 430/1000, Training Loss (NLML): -959.5808\n",
      "convergence dfGPdfNN Run 6/10, Epoch 431/1000, Training Loss (NLML): -959.6219\n",
      "convergence dfGPdfNN Run 6/10, Epoch 432/1000, Training Loss (NLML): -959.6506\n",
      "convergence dfGPdfNN Run 6/10, Epoch 433/1000, Training Loss (NLML): -959.6670\n",
      "convergence dfGPdfNN Run 6/10, Epoch 434/1000, Training Loss (NLML): -959.6648\n",
      "convergence dfGPdfNN Run 6/10, Epoch 435/1000, Training Loss (NLML): -959.6774\n",
      "convergence dfGPdfNN Run 6/10, Epoch 436/1000, Training Loss (NLML): -959.6537\n",
      "convergence dfGPdfNN Run 6/10, Epoch 437/1000, Training Loss (NLML): -959.6072\n",
      "convergence dfGPdfNN Run 6/10, Epoch 438/1000, Training Loss (NLML): -959.5005\n",
      "convergence dfGPdfNN Run 6/10, Epoch 439/1000, Training Loss (NLML): -959.4873\n",
      "convergence dfGPdfNN Run 6/10, Epoch 440/1000, Training Loss (NLML): -959.5168\n",
      "convergence dfGPdfNN Run 6/10, Epoch 441/1000, Training Loss (NLML): -959.5383\n",
      "convergence dfGPdfNN Run 6/10, Epoch 442/1000, Training Loss (NLML): -959.4885\n",
      "convergence dfGPdfNN Run 6/10, Epoch 443/1000, Training Loss (NLML): -959.4509\n",
      "convergence dfGPdfNN Run 6/10, Epoch 444/1000, Training Loss (NLML): -959.4003\n",
      "convergence dfGPdfNN Run 6/10, Epoch 445/1000, Training Loss (NLML): -959.4110\n",
      "convergence dfGPdfNN Run 6/10, Epoch 446/1000, Training Loss (NLML): -959.4315\n",
      "convergence dfGPdfNN Run 6/10, Epoch 447/1000, Training Loss (NLML): -959.4503\n",
      "convergence dfGPdfNN Run 6/10, Epoch 448/1000, Training Loss (NLML): -959.4657\n",
      "convergence dfGPdfNN Run 6/10, Epoch 449/1000, Training Loss (NLML): -959.4408\n",
      "convergence dfGPdfNN Run 6/10, Epoch 450/1000, Training Loss (NLML): -959.4200\n",
      "convergence dfGPdfNN Run 6/10, Epoch 451/1000, Training Loss (NLML): -959.4453\n",
      "convergence dfGPdfNN Run 6/10, Epoch 452/1000, Training Loss (NLML): -959.4794\n",
      "convergence dfGPdfNN Run 6/10, Epoch 453/1000, Training Loss (NLML): -959.5245\n",
      "convergence dfGPdfNN Run 6/10, Epoch 454/1000, Training Loss (NLML): -959.5430\n",
      "convergence dfGPdfNN Run 6/10, Epoch 455/1000, Training Loss (NLML): -959.5513\n",
      "convergence dfGPdfNN Run 6/10, Epoch 456/1000, Training Loss (NLML): -959.5525\n",
      "convergence dfGPdfNN Run 6/10, Epoch 457/1000, Training Loss (NLML): -959.5419\n",
      "convergence dfGPdfNN Run 6/10, Epoch 458/1000, Training Loss (NLML): -959.5740\n",
      "convergence dfGPdfNN Run 6/10, Epoch 459/1000, Training Loss (NLML): -959.6080\n",
      "convergence dfGPdfNN Run 6/10, Epoch 460/1000, Training Loss (NLML): -959.6533\n",
      "convergence dfGPdfNN Run 6/10, Epoch 461/1000, Training Loss (NLML): -959.6926\n",
      "convergence dfGPdfNN Run 6/10, Epoch 462/1000, Training Loss (NLML): -959.6875\n",
      "convergence dfGPdfNN Run 6/10, Epoch 463/1000, Training Loss (NLML): -959.6976\n",
      "convergence dfGPdfNN Run 6/10, Epoch 464/1000, Training Loss (NLML): -959.7045\n",
      "convergence dfGPdfNN Run 6/10, Epoch 465/1000, Training Loss (NLML): -959.7629\n",
      "convergence dfGPdfNN Run 6/10, Epoch 466/1000, Training Loss (NLML): -959.8059\n",
      "convergence dfGPdfNN Run 6/10, Epoch 467/1000, Training Loss (NLML): -959.8125\n",
      "convergence dfGPdfNN Run 6/10, Epoch 468/1000, Training Loss (NLML): -959.8107\n",
      "convergence dfGPdfNN Run 6/10, Epoch 469/1000, Training Loss (NLML): -959.7933\n",
      "convergence dfGPdfNN Run 6/10, Epoch 470/1000, Training Loss (NLML): -959.7625\n",
      "convergence dfGPdfNN Run 6/10, Epoch 471/1000, Training Loss (NLML): -959.7465\n",
      "convergence dfGPdfNN Run 6/10, Epoch 472/1000, Training Loss (NLML): -959.7820\n",
      "convergence dfGPdfNN Run 6/10, Epoch 473/1000, Training Loss (NLML): -959.7640\n",
      "convergence dfGPdfNN Run 6/10, Epoch 474/1000, Training Loss (NLML): -959.6923\n",
      "convergence dfGPdfNN Run 6/10, Epoch 475/1000, Training Loss (NLML): -959.6171\n",
      "convergence dfGPdfNN Run 6/10, Epoch 476/1000, Training Loss (NLML): -959.6667\n",
      "convergence dfGPdfNN Run 6/10, Epoch 477/1000, Training Loss (NLML): -959.6815\n",
      "convergence dfGPdfNN Run 6/10, Epoch 478/1000, Training Loss (NLML): -959.7180\n",
      "convergence dfGPdfNN Run 6/10, Epoch 479/1000, Training Loss (NLML): -959.7047\n",
      "convergence dfGPdfNN Run 6/10, Epoch 480/1000, Training Loss (NLML): -959.6805\n",
      "convergence dfGPdfNN Run 6/10, Epoch 481/1000, Training Loss (NLML): -959.6377\n",
      "convergence dfGPdfNN Run 6/10, Epoch 482/1000, Training Loss (NLML): -959.6489\n",
      "convergence dfGPdfNN Run 6/10, Epoch 483/1000, Training Loss (NLML): -959.6235\n",
      "convergence dfGPdfNN Run 6/10, Epoch 484/1000, Training Loss (NLML): -959.6593\n",
      "convergence dfGPdfNN Run 6/10, Epoch 485/1000, Training Loss (NLML): -959.6714\n",
      "convergence dfGPdfNN Run 6/10, Epoch 486/1000, Training Loss (NLML): -959.6720\n",
      "convergence dfGPdfNN Run 6/10, Epoch 487/1000, Training Loss (NLML): -959.6801\n",
      "convergence dfGPdfNN Run 6/10, Epoch 488/1000, Training Loss (NLML): -959.6779\n",
      "convergence dfGPdfNN Run 6/10, Epoch 489/1000, Training Loss (NLML): -959.7283\n",
      "convergence dfGPdfNN Run 6/10, Epoch 490/1000, Training Loss (NLML): -959.7283\n",
      "convergence dfGPdfNN Run 6/10, Epoch 491/1000, Training Loss (NLML): -959.7520\n",
      "convergence dfGPdfNN Run 6/10, Epoch 492/1000, Training Loss (NLML): -959.7499\n",
      "convergence dfGPdfNN Run 6/10, Epoch 493/1000, Training Loss (NLML): -959.7719\n",
      "convergence dfGPdfNN Run 6/10, Epoch 494/1000, Training Loss (NLML): -959.7736\n",
      "convergence dfGPdfNN Run 6/10, Epoch 495/1000, Training Loss (NLML): -959.7726\n",
      "convergence dfGPdfNN Run 6/10, Epoch 496/1000, Training Loss (NLML): -959.7780\n",
      "convergence dfGPdfNN Run 6/10, Epoch 497/1000, Training Loss (NLML): -959.8423\n",
      "convergence dfGPdfNN Run 6/10, Epoch 498/1000, Training Loss (NLML): -959.8971\n",
      "convergence dfGPdfNN Run 6/10, Epoch 499/1000, Training Loss (NLML): -959.9044\n",
      "convergence dfGPdfNN Run 6/10, Epoch 500/1000, Training Loss (NLML): -959.9202\n",
      "convergence dfGPdfNN Run 6/10, Epoch 501/1000, Training Loss (NLML): -959.9288\n",
      "convergence dfGPdfNN Run 6/10, Epoch 502/1000, Training Loss (NLML): -959.8837\n",
      "convergence dfGPdfNN Run 6/10, Epoch 503/1000, Training Loss (NLML): -959.8757\n",
      "convergence dfGPdfNN Run 6/10, Epoch 504/1000, Training Loss (NLML): -959.8429\n",
      "convergence dfGPdfNN Run 6/10, Epoch 505/1000, Training Loss (NLML): -959.8505\n",
      "convergence dfGPdfNN Run 6/10, Epoch 506/1000, Training Loss (NLML): -959.8669\n",
      "convergence dfGPdfNN Run 6/10, Epoch 507/1000, Training Loss (NLML): -959.9163\n",
      "convergence dfGPdfNN Run 6/10, Epoch 508/1000, Training Loss (NLML): -959.9465\n",
      "convergence dfGPdfNN Run 6/10, Epoch 509/1000, Training Loss (NLML): -959.9890\n",
      "convergence dfGPdfNN Run 6/10, Epoch 510/1000, Training Loss (NLML): -960.0029\n",
      "convergence dfGPdfNN Run 6/10, Epoch 511/1000, Training Loss (NLML): -960.0129\n",
      "convergence dfGPdfNN Run 6/10, Epoch 512/1000, Training Loss (NLML): -959.9973\n",
      "convergence dfGPdfNN Run 6/10, Epoch 513/1000, Training Loss (NLML): -959.9531\n",
      "convergence dfGPdfNN Run 6/10, Epoch 514/1000, Training Loss (NLML): -959.8690\n",
      "convergence dfGPdfNN Run 6/10, Epoch 515/1000, Training Loss (NLML): -959.8525\n",
      "convergence dfGPdfNN Run 6/10, Epoch 516/1000, Training Loss (NLML): -959.8610\n",
      "convergence dfGPdfNN Run 6/10, Epoch 517/1000, Training Loss (NLML): -959.8820\n",
      "convergence dfGPdfNN Run 6/10, Epoch 518/1000, Training Loss (NLML): -959.8888\n",
      "convergence dfGPdfNN Run 6/10, Epoch 519/1000, Training Loss (NLML): -959.8778\n",
      "convergence dfGPdfNN Run 6/10, Epoch 520/1000, Training Loss (NLML): -959.8783\n",
      "convergence dfGPdfNN Run 6/10, Epoch 521/1000, Training Loss (NLML): -959.8726\n",
      "convergence dfGPdfNN Run 6/10, Epoch 522/1000, Training Loss (NLML): -959.8718\n",
      "convergence dfGPdfNN Run 6/10, Epoch 523/1000, Training Loss (NLML): -959.8888\n",
      "convergence dfGPdfNN Run 6/10, Epoch 524/1000, Training Loss (NLML): -959.9045\n",
      "convergence dfGPdfNN Run 6/10, Epoch 525/1000, Training Loss (NLML): -959.9150\n",
      "convergence dfGPdfNN Run 6/10, Epoch 526/1000, Training Loss (NLML): -959.9266\n",
      "convergence dfGPdfNN Run 6/10, Epoch 527/1000, Training Loss (NLML): -959.9360\n",
      "convergence dfGPdfNN Run 6/10, Epoch 528/1000, Training Loss (NLML): -959.9404\n",
      "convergence dfGPdfNN Run 6/10, Epoch 529/1000, Training Loss (NLML): -959.9312\n",
      "convergence dfGPdfNN Run 6/10, Epoch 530/1000, Training Loss (NLML): -959.9420\n",
      "convergence dfGPdfNN Run 6/10, Epoch 531/1000, Training Loss (NLML): -959.9398\n",
      "convergence dfGPdfNN Run 6/10, Epoch 532/1000, Training Loss (NLML): -959.9098\n",
      "convergence dfGPdfNN Run 6/10, Epoch 533/1000, Training Loss (NLML): -959.9016\n",
      "convergence dfGPdfNN Run 6/10, Epoch 534/1000, Training Loss (NLML): -959.9119\n",
      "convergence dfGPdfNN Run 6/10, Epoch 535/1000, Training Loss (NLML): -959.8953\n",
      "convergence dfGPdfNN Run 6/10, Epoch 536/1000, Training Loss (NLML): -959.9263\n",
      "convergence dfGPdfNN Run 6/10, Epoch 537/1000, Training Loss (NLML): -959.9387\n",
      "convergence dfGPdfNN Run 6/10, Epoch 538/1000, Training Loss (NLML): -959.9473\n",
      "convergence dfGPdfNN Run 6/10, Epoch 539/1000, Training Loss (NLML): -959.9404\n",
      "convergence dfGPdfNN Run 6/10, Epoch 540/1000, Training Loss (NLML): -959.9381\n",
      "convergence dfGPdfNN Run 6/10, Epoch 541/1000, Training Loss (NLML): -959.9435\n",
      "convergence dfGPdfNN Run 6/10, Epoch 542/1000, Training Loss (NLML): -959.9556\n",
      "convergence dfGPdfNN Run 6/10, Epoch 543/1000, Training Loss (NLML): -959.9673\n",
      "convergence dfGPdfNN Run 6/10, Epoch 544/1000, Training Loss (NLML): -959.9790\n",
      "convergence dfGPdfNN Run 6/10, Epoch 545/1000, Training Loss (NLML): -959.9662\n",
      "convergence dfGPdfNN Run 6/10, Epoch 546/1000, Training Loss (NLML): -959.9685\n",
      "convergence dfGPdfNN Run 6/10, Epoch 547/1000, Training Loss (NLML): -959.9785\n",
      "convergence dfGPdfNN Run 6/10, Epoch 548/1000, Training Loss (NLML): -959.9851\n",
      "convergence dfGPdfNN Run 6/10, Epoch 549/1000, Training Loss (NLML): -959.9797\n",
      "convergence dfGPdfNN Run 6/10, Epoch 550/1000, Training Loss (NLML): -959.9850\n",
      "convergence dfGPdfNN Run 6/10, Epoch 551/1000, Training Loss (NLML): -959.9868\n",
      "convergence dfGPdfNN Run 6/10, Epoch 552/1000, Training Loss (NLML): -959.9922\n",
      "convergence dfGPdfNN Run 6/10, Epoch 553/1000, Training Loss (NLML): -959.9930\n",
      "convergence dfGPdfNN Run 6/10, Epoch 554/1000, Training Loss (NLML): -960.0015\n",
      "convergence dfGPdfNN Run 6/10, Epoch 555/1000, Training Loss (NLML): -960.0128\n",
      "convergence dfGPdfNN Run 6/10, Epoch 556/1000, Training Loss (NLML): -960.0234\n",
      "convergence dfGPdfNN Run 6/10, Epoch 557/1000, Training Loss (NLML): -960.0280\n",
      "convergence dfGPdfNN Run 6/10, Epoch 558/1000, Training Loss (NLML): -960.0283\n",
      "convergence dfGPdfNN Run 6/10, Epoch 559/1000, Training Loss (NLML): -960.0331\n",
      "convergence dfGPdfNN Run 6/10, Epoch 560/1000, Training Loss (NLML): -960.0410\n",
      "convergence dfGPdfNN Run 6/10, Epoch 561/1000, Training Loss (NLML): -960.0507\n",
      "convergence dfGPdfNN Run 6/10, Epoch 562/1000, Training Loss (NLML): -960.0560\n",
      "convergence dfGPdfNN Run 6/10, Epoch 563/1000, Training Loss (NLML): -960.0577\n",
      "convergence dfGPdfNN Run 6/10, Epoch 564/1000, Training Loss (NLML): -960.0601\n",
      "convergence dfGPdfNN Run 6/10, Epoch 565/1000, Training Loss (NLML): -960.0629\n",
      "convergence dfGPdfNN Run 6/10, Epoch 566/1000, Training Loss (NLML): -960.0652\n",
      "convergence dfGPdfNN Run 6/10, Epoch 567/1000, Training Loss (NLML): -960.0635\n",
      "convergence dfGPdfNN Run 6/10, Epoch 568/1000, Training Loss (NLML): -960.0631\n",
      "convergence dfGPdfNN Run 6/10, Epoch 569/1000, Training Loss (NLML): -960.0652\n",
      "convergence dfGPdfNN Run 6/10, Epoch 570/1000, Training Loss (NLML): -960.0699\n",
      "convergence dfGPdfNN Run 6/10, Epoch 571/1000, Training Loss (NLML): -960.0691\n",
      "convergence dfGPdfNN Run 6/10, Epoch 572/1000, Training Loss (NLML): -960.0741\n",
      "convergence dfGPdfNN Run 6/10, Epoch 573/1000, Training Loss (NLML): -960.0757\n",
      "convergence dfGPdfNN Run 6/10, Epoch 574/1000, Training Loss (NLML): -960.0792\n",
      "convergence dfGPdfNN Run 6/10, Epoch 575/1000, Training Loss (NLML): -960.0847\n",
      "convergence dfGPdfNN Run 6/10, Epoch 576/1000, Training Loss (NLML): -960.0961\n",
      "convergence dfGPdfNN Run 6/10, Epoch 577/1000, Training Loss (NLML): -960.1002\n",
      "convergence dfGPdfNN Run 6/10, Epoch 578/1000, Training Loss (NLML): -960.1040\n",
      "convergence dfGPdfNN Run 6/10, Epoch 579/1000, Training Loss (NLML): -960.0934\n",
      "convergence dfGPdfNN Run 6/10, Epoch 580/1000, Training Loss (NLML): -960.0853\n",
      "convergence dfGPdfNN Run 6/10, Epoch 581/1000, Training Loss (NLML): -960.1000\n",
      "convergence dfGPdfNN Run 6/10, Epoch 582/1000, Training Loss (NLML): -960.0938\n",
      "convergence dfGPdfNN Run 6/10, Epoch 583/1000, Training Loss (NLML): -960.0991\n",
      "convergence dfGPdfNN Run 6/10, Epoch 584/1000, Training Loss (NLML): -960.1030\n",
      "convergence dfGPdfNN Run 6/10, Epoch 585/1000, Training Loss (NLML): -960.1050\n",
      "convergence dfGPdfNN Run 6/10, Epoch 586/1000, Training Loss (NLML): -960.1116\n",
      "convergence dfGPdfNN Run 6/10, Epoch 587/1000, Training Loss (NLML): -960.1140\n",
      "convergence dfGPdfNN Run 6/10, Epoch 588/1000, Training Loss (NLML): -960.1176\n",
      "convergence dfGPdfNN Run 6/10, Epoch 589/1000, Training Loss (NLML): -960.1239\n",
      "convergence dfGPdfNN Run 6/10, Epoch 590/1000, Training Loss (NLML): -960.1436\n",
      "convergence dfGPdfNN Run 6/10, Epoch 591/1000, Training Loss (NLML): -960.1545\n",
      "convergence dfGPdfNN Run 6/10, Epoch 592/1000, Training Loss (NLML): -960.1478\n",
      "convergence dfGPdfNN Run 6/10, Epoch 593/1000, Training Loss (NLML): -960.1494\n",
      "convergence dfGPdfNN Run 6/10, Epoch 594/1000, Training Loss (NLML): -960.1580\n",
      "convergence dfGPdfNN Run 6/10, Epoch 595/1000, Training Loss (NLML): -960.1641\n",
      "convergence dfGPdfNN Run 6/10, Epoch 596/1000, Training Loss (NLML): -960.1705\n",
      "convergence dfGPdfNN Run 6/10, Epoch 597/1000, Training Loss (NLML): -960.1729\n",
      "convergence dfGPdfNN Run 6/10, Epoch 598/1000, Training Loss (NLML): -960.1750\n",
      "convergence dfGPdfNN Run 6/10, Epoch 599/1000, Training Loss (NLML): -960.1792\n",
      "convergence dfGPdfNN Run 6/10, Epoch 600/1000, Training Loss (NLML): -960.1848\n",
      "convergence dfGPdfNN Run 6/10, Epoch 601/1000, Training Loss (NLML): -960.1863\n",
      "convergence dfGPdfNN Run 6/10, Epoch 602/1000, Training Loss (NLML): -960.1897\n",
      "convergence dfGPdfNN Run 6/10, Epoch 603/1000, Training Loss (NLML): -960.1935\n",
      "convergence dfGPdfNN Run 6/10, Epoch 604/1000, Training Loss (NLML): -960.1958\n",
      "convergence dfGPdfNN Run 6/10, Epoch 605/1000, Training Loss (NLML): -960.2021\n",
      "convergence dfGPdfNN Run 6/10, Epoch 606/1000, Training Loss (NLML): -960.2109\n",
      "convergence dfGPdfNN Run 6/10, Epoch 607/1000, Training Loss (NLML): -960.2157\n",
      "convergence dfGPdfNN Run 6/10, Epoch 608/1000, Training Loss (NLML): -960.2179\n",
      "convergence dfGPdfNN Run 6/10, Epoch 609/1000, Training Loss (NLML): -960.2207\n",
      "convergence dfGPdfNN Run 6/10, Epoch 610/1000, Training Loss (NLML): -960.2260\n",
      "convergence dfGPdfNN Run 6/10, Epoch 611/1000, Training Loss (NLML): -960.2297\n",
      "convergence dfGPdfNN Run 6/10, Epoch 612/1000, Training Loss (NLML): -960.2313\n",
      "convergence dfGPdfNN Run 6/10, Epoch 613/1000, Training Loss (NLML): -960.2356\n",
      "convergence dfGPdfNN Run 6/10, Epoch 614/1000, Training Loss (NLML): -960.2445\n",
      "convergence dfGPdfNN Run 6/10, Epoch 615/1000, Training Loss (NLML): -960.2462\n",
      "convergence dfGPdfNN Run 6/10, Epoch 616/1000, Training Loss (NLML): -960.2473\n",
      "convergence dfGPdfNN Run 6/10, Epoch 617/1000, Training Loss (NLML): -960.2484\n",
      "convergence dfGPdfNN Run 6/10, Epoch 618/1000, Training Loss (NLML): -960.2372\n",
      "convergence dfGPdfNN Run 6/10, Epoch 619/1000, Training Loss (NLML): -960.2582\n",
      "convergence dfGPdfNN Run 6/10, Epoch 620/1000, Training Loss (NLML): -960.2599\n",
      "convergence dfGPdfNN Run 6/10, Epoch 621/1000, Training Loss (NLML): -960.2670\n",
      "convergence dfGPdfNN Run 6/10, Epoch 622/1000, Training Loss (NLML): -960.2689\n",
      "convergence dfGPdfNN Run 6/10, Epoch 623/1000, Training Loss (NLML): -960.2726\n",
      "convergence dfGPdfNN Run 6/10, Epoch 624/1000, Training Loss (NLML): -960.2775\n",
      "convergence dfGPdfNN Run 6/10, Epoch 625/1000, Training Loss (NLML): -960.2804\n",
      "convergence dfGPdfNN Run 6/10, Epoch 626/1000, Training Loss (NLML): -960.2834\n",
      "convergence dfGPdfNN Run 6/10, Epoch 627/1000, Training Loss (NLML): -960.2842\n",
      "convergence dfGPdfNN Run 6/10, Epoch 628/1000, Training Loss (NLML): -960.2872\n",
      "convergence dfGPdfNN Run 6/10, Epoch 629/1000, Training Loss (NLML): -960.2859\n",
      "convergence dfGPdfNN Run 6/10, Epoch 630/1000, Training Loss (NLML): -960.2881\n",
      "convergence dfGPdfNN Run 6/10, Epoch 631/1000, Training Loss (NLML): -960.2915\n",
      "convergence dfGPdfNN Run 6/10, Epoch 632/1000, Training Loss (NLML): -960.2961\n",
      "convergence dfGPdfNN Run 6/10, Epoch 633/1000, Training Loss (NLML): -960.2986\n",
      "convergence dfGPdfNN Run 6/10, Epoch 634/1000, Training Loss (NLML): -960.2897\n",
      "convergence dfGPdfNN Run 6/10, Epoch 635/1000, Training Loss (NLML): -960.2941\n",
      "convergence dfGPdfNN Run 6/10, Epoch 636/1000, Training Loss (NLML): -960.2983\n",
      "convergence dfGPdfNN Run 6/10, Epoch 637/1000, Training Loss (NLML): -960.3029\n",
      "convergence dfGPdfNN Run 6/10, Epoch 638/1000, Training Loss (NLML): -960.3055\n",
      "convergence dfGPdfNN Run 6/10, Epoch 639/1000, Training Loss (NLML): -960.3224\n",
      "convergence dfGPdfNN Run 6/10, Epoch 640/1000, Training Loss (NLML): -960.3247\n",
      "convergence dfGPdfNN Run 6/10, Epoch 641/1000, Training Loss (NLML): -960.3297\n",
      "convergence dfGPdfNN Run 6/10, Epoch 642/1000, Training Loss (NLML): -960.3245\n",
      "convergence dfGPdfNN Run 6/10, Epoch 643/1000, Training Loss (NLML): -960.3286\n",
      "convergence dfGPdfNN Run 6/10, Epoch 644/1000, Training Loss (NLML): -960.3378\n",
      "convergence dfGPdfNN Run 6/10, Epoch 645/1000, Training Loss (NLML): -960.3326\n",
      "convergence dfGPdfNN Run 6/10, Epoch 646/1000, Training Loss (NLML): -960.3379\n",
      "convergence dfGPdfNN Run 6/10, Epoch 647/1000, Training Loss (NLML): -960.3430\n",
      "convergence dfGPdfNN Run 6/10, Epoch 648/1000, Training Loss (NLML): -960.3475\n",
      "convergence dfGPdfNN Run 6/10, Epoch 649/1000, Training Loss (NLML): -960.3510\n",
      "convergence dfGPdfNN Run 6/10, Epoch 650/1000, Training Loss (NLML): -960.3511\n",
      "convergence dfGPdfNN Run 6/10, Epoch 651/1000, Training Loss (NLML): -960.3539\n",
      "convergence dfGPdfNN Run 6/10, Epoch 652/1000, Training Loss (NLML): -960.3527\n",
      "convergence dfGPdfNN Run 6/10, Epoch 653/1000, Training Loss (NLML): -960.3550\n",
      "convergence dfGPdfNN Run 6/10, Epoch 654/1000, Training Loss (NLML): -960.3586\n",
      "convergence dfGPdfNN Run 6/10, Epoch 655/1000, Training Loss (NLML): -960.3607\n",
      "convergence dfGPdfNN Run 6/10, Epoch 656/1000, Training Loss (NLML): -960.3698\n",
      "convergence dfGPdfNN Run 6/10, Epoch 657/1000, Training Loss (NLML): -960.3724\n",
      "convergence dfGPdfNN Run 6/10, Epoch 658/1000, Training Loss (NLML): -960.3763\n",
      "convergence dfGPdfNN Run 6/10, Epoch 659/1000, Training Loss (NLML): -960.3793\n",
      "convergence dfGPdfNN Run 6/10, Epoch 660/1000, Training Loss (NLML): -960.3817\n",
      "convergence dfGPdfNN Run 6/10, Epoch 661/1000, Training Loss (NLML): -960.3875\n",
      "convergence dfGPdfNN Run 6/10, Epoch 662/1000, Training Loss (NLML): -960.3844\n",
      "convergence dfGPdfNN Run 6/10, Epoch 663/1000, Training Loss (NLML): -960.3765\n",
      "convergence dfGPdfNN Run 6/10, Epoch 664/1000, Training Loss (NLML): -960.3694\n",
      "convergence dfGPdfNN Run 6/10, Epoch 665/1000, Training Loss (NLML): -960.3857\n",
      "convergence dfGPdfNN Run 6/10, Epoch 666/1000, Training Loss (NLML): -960.3954\n",
      "convergence dfGPdfNN Run 6/10, Epoch 667/1000, Training Loss (NLML): -960.3969\n",
      "convergence dfGPdfNN Run 6/10, Epoch 668/1000, Training Loss (NLML): -960.4001\n",
      "convergence dfGPdfNN Run 6/10, Epoch 669/1000, Training Loss (NLML): -960.4030\n",
      "convergence dfGPdfNN Run 6/10, Epoch 670/1000, Training Loss (NLML): -960.3901\n",
      "convergence dfGPdfNN Run 6/10, Epoch 671/1000, Training Loss (NLML): -960.3921\n",
      "convergence dfGPdfNN Run 6/10, Epoch 672/1000, Training Loss (NLML): -960.3959\n",
      "convergence dfGPdfNN Run 6/10, Epoch 673/1000, Training Loss (NLML): -960.4058\n",
      "convergence dfGPdfNN Run 6/10, Epoch 674/1000, Training Loss (NLML): -960.4098\n",
      "convergence dfGPdfNN Run 6/10, Epoch 675/1000, Training Loss (NLML): -960.4119\n",
      "convergence dfGPdfNN Run 6/10, Epoch 676/1000, Training Loss (NLML): -960.4172\n",
      "convergence dfGPdfNN Run 6/10, Epoch 677/1000, Training Loss (NLML): -960.4230\n",
      "convergence dfGPdfNN Run 6/10, Epoch 678/1000, Training Loss (NLML): -960.4279\n",
      "convergence dfGPdfNN Run 6/10, Epoch 679/1000, Training Loss (NLML): -960.4288\n",
      "convergence dfGPdfNN Run 6/10, Epoch 680/1000, Training Loss (NLML): -960.4337\n",
      "convergence dfGPdfNN Run 6/10, Epoch 681/1000, Training Loss (NLML): -960.4382\n",
      "convergence dfGPdfNN Run 6/10, Epoch 682/1000, Training Loss (NLML): -960.4410\n",
      "convergence dfGPdfNN Run 6/10, Epoch 683/1000, Training Loss (NLML): -960.4434\n",
      "convergence dfGPdfNN Run 6/10, Epoch 684/1000, Training Loss (NLML): -960.4404\n",
      "convergence dfGPdfNN Run 6/10, Epoch 685/1000, Training Loss (NLML): -960.4404\n",
      "convergence dfGPdfNN Run 6/10, Epoch 686/1000, Training Loss (NLML): -960.4456\n",
      "convergence dfGPdfNN Run 6/10, Epoch 687/1000, Training Loss (NLML): -960.4517\n",
      "convergence dfGPdfNN Run 6/10, Epoch 688/1000, Training Loss (NLML): -960.4546\n",
      "convergence dfGPdfNN Run 6/10, Epoch 689/1000, Training Loss (NLML): -960.4584\n",
      "convergence dfGPdfNN Run 6/10, Epoch 690/1000, Training Loss (NLML): -960.4624\n",
      "convergence dfGPdfNN Run 6/10, Epoch 691/1000, Training Loss (NLML): -960.4667\n",
      "convergence dfGPdfNN Run 6/10, Epoch 692/1000, Training Loss (NLML): -960.4695\n",
      "convergence dfGPdfNN Run 6/10, Epoch 693/1000, Training Loss (NLML): -960.4512\n",
      "convergence dfGPdfNN Run 6/10, Epoch 694/1000, Training Loss (NLML): -960.4561\n",
      "convergence dfGPdfNN Run 6/10, Epoch 695/1000, Training Loss (NLML): -960.4557\n",
      "convergence dfGPdfNN Run 6/10, Epoch 696/1000, Training Loss (NLML): -960.4569\n",
      "convergence dfGPdfNN Run 6/10, Epoch 697/1000, Training Loss (NLML): -960.4601\n",
      "convergence dfGPdfNN Run 6/10, Epoch 698/1000, Training Loss (NLML): -960.4340\n",
      "convergence dfGPdfNN Run 6/10, Epoch 699/1000, Training Loss (NLML): -960.4235\n",
      "convergence dfGPdfNN Run 6/10, Epoch 700/1000, Training Loss (NLML): -960.4288\n",
      "convergence dfGPdfNN Run 6/10, Epoch 701/1000, Training Loss (NLML): -960.4623\n",
      "convergence dfGPdfNN Run 6/10, Epoch 702/1000, Training Loss (NLML): -960.4346\n",
      "convergence dfGPdfNN Run 6/10, Epoch 703/1000, Training Loss (NLML): -960.4384\n",
      "convergence dfGPdfNN Run 6/10, Epoch 704/1000, Training Loss (NLML): -960.4412\n",
      "convergence dfGPdfNN Run 6/10, Epoch 705/1000, Training Loss (NLML): -960.4200\n",
      "convergence dfGPdfNN Run 6/10, Epoch 706/1000, Training Loss (NLML): -960.4231\n",
      "convergence dfGPdfNN Run 6/10, Epoch 707/1000, Training Loss (NLML): -960.4260\n",
      "convergence dfGPdfNN Run 6/10, Epoch 708/1000, Training Loss (NLML): -960.4137\n",
      "convergence dfGPdfNN Run 6/10, Epoch 709/1000, Training Loss (NLML): -960.4353\n",
      "convergence dfGPdfNN Run 6/10, Epoch 710/1000, Training Loss (NLML): -960.4609\n",
      "convergence dfGPdfNN Run 6/10, Epoch 711/1000, Training Loss (NLML): -960.4629\n",
      "convergence dfGPdfNN Run 6/10, Epoch 712/1000, Training Loss (NLML): -960.4667\n",
      "convergence dfGPdfNN Run 6/10, Epoch 713/1000, Training Loss (NLML): -960.4669\n",
      "convergence dfGPdfNN Run 6/10, Epoch 714/1000, Training Loss (NLML): -960.4713\n",
      "convergence dfGPdfNN Run 6/10, Epoch 715/1000, Training Loss (NLML): -960.4729\n",
      "convergence dfGPdfNN Run 6/10, Epoch 716/1000, Training Loss (NLML): -960.4766\n",
      "convergence dfGPdfNN Run 6/10, Epoch 717/1000, Training Loss (NLML): -960.4823\n",
      "convergence dfGPdfNN Run 6/10, Epoch 718/1000, Training Loss (NLML): -960.4851\n",
      "convergence dfGPdfNN Run 6/10, Epoch 719/1000, Training Loss (NLML): -960.4872\n",
      "convergence dfGPdfNN Run 6/10, Epoch 720/1000, Training Loss (NLML): -960.4908\n",
      "convergence dfGPdfNN Run 6/10, Epoch 721/1000, Training Loss (NLML): -960.4924\n",
      "convergence dfGPdfNN Run 6/10, Epoch 722/1000, Training Loss (NLML): -960.4956\n",
      "convergence dfGPdfNN Run 6/10, Epoch 723/1000, Training Loss (NLML): -960.4998\n",
      "convergence dfGPdfNN Run 6/10, Epoch 724/1000, Training Loss (NLML): -960.5012\n",
      "convergence dfGPdfNN Run 6/10, Epoch 725/1000, Training Loss (NLML): -960.5035\n",
      "convergence dfGPdfNN Run 6/10, Epoch 726/1000, Training Loss (NLML): -960.5061\n",
      "convergence dfGPdfNN Run 6/10, Epoch 727/1000, Training Loss (NLML): -960.5283\n",
      "convergence dfGPdfNN Run 6/10, Epoch 728/1000, Training Loss (NLML): -960.5293\n",
      "convergence dfGPdfNN Run 6/10, Epoch 729/1000, Training Loss (NLML): -960.5142\n",
      "convergence dfGPdfNN Run 6/10, Epoch 730/1000, Training Loss (NLML): -960.5175\n",
      "convergence dfGPdfNN Run 6/10, Epoch 731/1000, Training Loss (NLML): -960.5211\n",
      "convergence dfGPdfNN Run 6/10, Epoch 732/1000, Training Loss (NLML): -960.5248\n",
      "convergence dfGPdfNN Run 6/10, Epoch 733/1000, Training Loss (NLML): -960.5272\n",
      "convergence dfGPdfNN Run 6/10, Epoch 734/1000, Training Loss (NLML): -960.5289\n",
      "convergence dfGPdfNN Run 6/10, Epoch 735/1000, Training Loss (NLML): -960.5354\n",
      "convergence dfGPdfNN Run 6/10, Epoch 736/1000, Training Loss (NLML): -960.5353\n",
      "convergence dfGPdfNN Run 6/10, Epoch 737/1000, Training Loss (NLML): -960.5389\n",
      "convergence dfGPdfNN Run 6/10, Epoch 738/1000, Training Loss (NLML): -960.5392\n",
      "convergence dfGPdfNN Run 6/10, Epoch 739/1000, Training Loss (NLML): -960.5437\n",
      "convergence dfGPdfNN Run 6/10, Epoch 740/1000, Training Loss (NLML): -960.5459\n",
      "convergence dfGPdfNN Run 6/10, Epoch 741/1000, Training Loss (NLML): -960.5531\n",
      "convergence dfGPdfNN Run 6/10, Epoch 742/1000, Training Loss (NLML): -960.5524\n",
      "convergence dfGPdfNN Run 6/10, Epoch 743/1000, Training Loss (NLML): -960.5558\n",
      "convergence dfGPdfNN Run 6/10, Epoch 744/1000, Training Loss (NLML): -960.5575\n",
      "convergence dfGPdfNN Run 6/10, Epoch 745/1000, Training Loss (NLML): -960.5581\n",
      "convergence dfGPdfNN Run 6/10, Epoch 746/1000, Training Loss (NLML): -960.5626\n",
      "convergence dfGPdfNN Run 6/10, Epoch 747/1000, Training Loss (NLML): -960.5658\n",
      "convergence dfGPdfNN Run 6/10, Epoch 748/1000, Training Loss (NLML): -960.5691\n",
      "convergence dfGPdfNN Run 6/10, Epoch 749/1000, Training Loss (NLML): -960.5736\n",
      "convergence dfGPdfNN Run 6/10, Epoch 750/1000, Training Loss (NLML): -960.5751\n",
      "convergence dfGPdfNN Run 6/10, Epoch 751/1000, Training Loss (NLML): -960.5760\n",
      "convergence dfGPdfNN Run 6/10, Epoch 752/1000, Training Loss (NLML): -960.5773\n",
      "convergence dfGPdfNN Run 6/10, Epoch 753/1000, Training Loss (NLML): -960.5807\n",
      "convergence dfGPdfNN Run 6/10, Epoch 754/1000, Training Loss (NLML): -960.5845\n",
      "convergence dfGPdfNN Run 6/10, Epoch 755/1000, Training Loss (NLML): -960.5852\n",
      "convergence dfGPdfNN Run 6/10, Epoch 756/1000, Training Loss (NLML): -960.5884\n",
      "convergence dfGPdfNN Run 6/10, Epoch 757/1000, Training Loss (NLML): -960.5907\n",
      "convergence dfGPdfNN Run 6/10, Epoch 758/1000, Training Loss (NLML): -960.5962\n",
      "convergence dfGPdfNN Run 6/10, Epoch 759/1000, Training Loss (NLML): -960.5981\n",
      "convergence dfGPdfNN Run 6/10, Epoch 760/1000, Training Loss (NLML): -960.5975\n",
      "convergence dfGPdfNN Run 6/10, Epoch 761/1000, Training Loss (NLML): -960.6001\n",
      "convergence dfGPdfNN Run 6/10, Epoch 762/1000, Training Loss (NLML): -960.6040\n",
      "convergence dfGPdfNN Run 6/10, Epoch 763/1000, Training Loss (NLML): -960.6067\n",
      "convergence dfGPdfNN Run 6/10, Epoch 764/1000, Training Loss (NLML): -960.6152\n",
      "convergence dfGPdfNN Run 6/10, Epoch 765/1000, Training Loss (NLML): -960.6023\n",
      "convergence dfGPdfNN Run 6/10, Epoch 766/1000, Training Loss (NLML): -960.6046\n",
      "convergence dfGPdfNN Run 6/10, Epoch 767/1000, Training Loss (NLML): -960.6104\n",
      "convergence dfGPdfNN Run 6/10, Epoch 768/1000, Training Loss (NLML): -960.6108\n",
      "convergence dfGPdfNN Run 6/10, Epoch 769/1000, Training Loss (NLML): -960.6117\n",
      "convergence dfGPdfNN Run 6/10, Epoch 770/1000, Training Loss (NLML): -960.6191\n",
      "convergence dfGPdfNN Run 6/10, Epoch 771/1000, Training Loss (NLML): -960.6204\n",
      "convergence dfGPdfNN Run 6/10, Epoch 772/1000, Training Loss (NLML): -960.6226\n",
      "convergence dfGPdfNN Run 6/10, Epoch 773/1000, Training Loss (NLML): -960.6239\n",
      "convergence dfGPdfNN Run 6/10, Epoch 774/1000, Training Loss (NLML): -960.6277\n",
      "convergence dfGPdfNN Run 6/10, Epoch 775/1000, Training Loss (NLML): -960.6327\n",
      "convergence dfGPdfNN Run 6/10, Epoch 776/1000, Training Loss (NLML): -960.6345\n",
      "convergence dfGPdfNN Run 6/10, Epoch 777/1000, Training Loss (NLML): -960.6350\n",
      "convergence dfGPdfNN Run 6/10, Epoch 778/1000, Training Loss (NLML): -960.6362\n",
      "convergence dfGPdfNN Run 6/10, Epoch 779/1000, Training Loss (NLML): -960.6400\n",
      "convergence dfGPdfNN Run 6/10, Epoch 780/1000, Training Loss (NLML): -960.6161\n",
      "convergence dfGPdfNN Run 6/10, Epoch 781/1000, Training Loss (NLML): -960.6237\n",
      "convergence dfGPdfNN Run 6/10, Epoch 782/1000, Training Loss (NLML): -960.6238\n",
      "convergence dfGPdfNN Run 6/10, Epoch 783/1000, Training Loss (NLML): -960.6249\n",
      "convergence dfGPdfNN Run 6/10, Epoch 784/1000, Training Loss (NLML): -960.6274\n",
      "convergence dfGPdfNN Run 6/10, Epoch 785/1000, Training Loss (NLML): -960.6400\n",
      "convergence dfGPdfNN Run 6/10, Epoch 786/1000, Training Loss (NLML): -960.6433\n",
      "convergence dfGPdfNN Run 6/10, Epoch 787/1000, Training Loss (NLML): -960.6482\n",
      "convergence dfGPdfNN Run 6/10, Epoch 788/1000, Training Loss (NLML): -960.6371\n",
      "convergence dfGPdfNN Run 6/10, Epoch 789/1000, Training Loss (NLML): -960.6381\n",
      "convergence dfGPdfNN Run 6/10, Epoch 790/1000, Training Loss (NLML): -960.6555\n",
      "convergence dfGPdfNN Run 6/10, Epoch 791/1000, Training Loss (NLML): -960.6564\n",
      "convergence dfGPdfNN Run 6/10, Epoch 792/1000, Training Loss (NLML): -960.6581\n",
      "convergence dfGPdfNN Run 6/10, Epoch 793/1000, Training Loss (NLML): -960.6611\n",
      "convergence dfGPdfNN Run 6/10, Epoch 794/1000, Training Loss (NLML): -960.6680\n",
      "convergence dfGPdfNN Run 6/10, Epoch 795/1000, Training Loss (NLML): -960.6693\n",
      "convergence dfGPdfNN Run 6/10, Epoch 796/1000, Training Loss (NLML): -960.6553\n",
      "convergence dfGPdfNN Run 6/10, Epoch 797/1000, Training Loss (NLML): -960.6597\n",
      "convergence dfGPdfNN Run 6/10, Epoch 798/1000, Training Loss (NLML): -960.6663\n",
      "convergence dfGPdfNN Run 6/10, Epoch 799/1000, Training Loss (NLML): -960.6700\n",
      "convergence dfGPdfNN Run 6/10, Epoch 800/1000, Training Loss (NLML): -960.6777\n",
      "convergence dfGPdfNN Run 6/10, Epoch 801/1000, Training Loss (NLML): -960.6816\n",
      "convergence dfGPdfNN Run 6/10, Epoch 802/1000, Training Loss (NLML): -960.6791\n",
      "convergence dfGPdfNN Run 6/10, Epoch 803/1000, Training Loss (NLML): -960.6796\n",
      "convergence dfGPdfNN Run 6/10, Epoch 804/1000, Training Loss (NLML): -960.6851\n",
      "convergence dfGPdfNN Run 6/10, Epoch 805/1000, Training Loss (NLML): -960.6902\n",
      "convergence dfGPdfNN Run 6/10, Epoch 806/1000, Training Loss (NLML): -960.6915\n",
      "convergence dfGPdfNN Run 6/10, Epoch 807/1000, Training Loss (NLML): -960.6947\n",
      "convergence dfGPdfNN Run 6/10, Epoch 808/1000, Training Loss (NLML): -960.6946\n",
      "convergence dfGPdfNN Run 6/10, Epoch 809/1000, Training Loss (NLML): -960.6884\n",
      "convergence dfGPdfNN Run 6/10, Epoch 810/1000, Training Loss (NLML): -960.6891\n",
      "convergence dfGPdfNN Run 6/10, Epoch 811/1000, Training Loss (NLML): -960.6953\n",
      "convergence dfGPdfNN Run 6/10, Epoch 812/1000, Training Loss (NLML): -960.6984\n",
      "convergence dfGPdfNN Run 6/10, Epoch 813/1000, Training Loss (NLML): -960.6963\n",
      "convergence dfGPdfNN Run 6/10, Epoch 814/1000, Training Loss (NLML): -960.7037\n",
      "convergence dfGPdfNN Run 6/10, Epoch 815/1000, Training Loss (NLML): -960.7054\n",
      "convergence dfGPdfNN Run 6/10, Epoch 816/1000, Training Loss (NLML): -960.7089\n",
      "convergence dfGPdfNN Run 6/10, Epoch 817/1000, Training Loss (NLML): -960.7043\n",
      "convergence dfGPdfNN Run 6/10, Epoch 818/1000, Training Loss (NLML): -960.7114\n",
      "convergence dfGPdfNN Run 6/10, Epoch 819/1000, Training Loss (NLML): -960.7113\n",
      "convergence dfGPdfNN Run 6/10, Epoch 820/1000, Training Loss (NLML): -960.7175\n",
      "convergence dfGPdfNN Run 6/10, Epoch 821/1000, Training Loss (NLML): -960.7195\n",
      "convergence dfGPdfNN Run 6/10, Epoch 822/1000, Training Loss (NLML): -960.7211\n",
      "convergence dfGPdfNN Run 6/10, Epoch 823/1000, Training Loss (NLML): -960.7224\n",
      "convergence dfGPdfNN Run 6/10, Epoch 824/1000, Training Loss (NLML): -960.7246\n",
      "convergence dfGPdfNN Run 6/10, Epoch 825/1000, Training Loss (NLML): -960.7292\n",
      "convergence dfGPdfNN Run 6/10, Epoch 826/1000, Training Loss (NLML): -960.7305\n",
      "convergence dfGPdfNN Run 6/10, Epoch 827/1000, Training Loss (NLML): -960.7340\n",
      "convergence dfGPdfNN Run 6/10, Epoch 828/1000, Training Loss (NLML): -960.7351\n",
      "convergence dfGPdfNN Run 6/10, Epoch 829/1000, Training Loss (NLML): -960.7361\n",
      "convergence dfGPdfNN Run 6/10, Epoch 830/1000, Training Loss (NLML): -960.7366\n",
      "convergence dfGPdfNN Run 6/10, Epoch 831/1000, Training Loss (NLML): -960.7406\n",
      "convergence dfGPdfNN Run 6/10, Epoch 832/1000, Training Loss (NLML): -960.7456\n",
      "convergence dfGPdfNN Run 6/10, Epoch 833/1000, Training Loss (NLML): -960.7471\n",
      "convergence dfGPdfNN Run 6/10, Epoch 834/1000, Training Loss (NLML): -960.7498\n",
      "convergence dfGPdfNN Run 6/10, Epoch 835/1000, Training Loss (NLML): -960.7526\n",
      "convergence dfGPdfNN Run 6/10, Epoch 836/1000, Training Loss (NLML): -960.7548\n",
      "convergence dfGPdfNN Run 6/10, Epoch 837/1000, Training Loss (NLML): -960.7544\n",
      "convergence dfGPdfNN Run 6/10, Epoch 838/1000, Training Loss (NLML): -960.7559\n",
      "convergence dfGPdfNN Run 6/10, Epoch 839/1000, Training Loss (NLML): -960.7594\n",
      "convergence dfGPdfNN Run 6/10, Epoch 840/1000, Training Loss (NLML): -960.7648\n",
      "convergence dfGPdfNN Run 6/10, Epoch 841/1000, Training Loss (NLML): -960.7657\n",
      "convergence dfGPdfNN Run 6/10, Epoch 842/1000, Training Loss (NLML): -960.7653\n",
      "convergence dfGPdfNN Run 6/10, Epoch 843/1000, Training Loss (NLML): -960.7656\n",
      "convergence dfGPdfNN Run 6/10, Epoch 844/1000, Training Loss (NLML): -960.7701\n",
      "convergence dfGPdfNN Run 6/10, Epoch 845/1000, Training Loss (NLML): -960.7750\n",
      "convergence dfGPdfNN Run 6/10, Epoch 846/1000, Training Loss (NLML): -960.7771\n",
      "convergence dfGPdfNN Run 6/10, Epoch 847/1000, Training Loss (NLML): -960.7800\n",
      "convergence dfGPdfNN Run 6/10, Epoch 848/1000, Training Loss (NLML): -960.7816\n",
      "convergence dfGPdfNN Run 6/10, Epoch 849/1000, Training Loss (NLML): -960.7823\n",
      "convergence dfGPdfNN Run 6/10, Epoch 850/1000, Training Loss (NLML): -960.7833\n",
      "convergence dfGPdfNN Run 6/10, Epoch 851/1000, Training Loss (NLML): -960.7839\n",
      "convergence dfGPdfNN Run 6/10, Epoch 852/1000, Training Loss (NLML): -960.7887\n",
      "convergence dfGPdfNN Run 6/10, Epoch 853/1000, Training Loss (NLML): -960.8082\n",
      "convergence dfGPdfNN Run 6/10, Epoch 854/1000, Training Loss (NLML): -960.8092\n",
      "convergence dfGPdfNN Run 6/10, Epoch 855/1000, Training Loss (NLML): -960.8105\n",
      "convergence dfGPdfNN Run 6/10, Epoch 856/1000, Training Loss (NLML): -960.8151\n",
      "convergence dfGPdfNN Run 6/10, Epoch 857/1000, Training Loss (NLML): -960.8134\n",
      "convergence dfGPdfNN Run 6/10, Epoch 858/1000, Training Loss (NLML): -960.8186\n",
      "convergence dfGPdfNN Run 6/10, Epoch 859/1000, Training Loss (NLML): -960.8206\n",
      "convergence dfGPdfNN Run 6/10, Epoch 860/1000, Training Loss (NLML): -960.8345\n",
      "convergence dfGPdfNN Run 6/10, Epoch 861/1000, Training Loss (NLML): -960.8390\n",
      "convergence dfGPdfNN Run 6/10, Epoch 862/1000, Training Loss (NLML): -960.8317\n",
      "convergence dfGPdfNN Run 6/10, Epoch 863/1000, Training Loss (NLML): -960.8352\n",
      "convergence dfGPdfNN Run 6/10, Epoch 864/1000, Training Loss (NLML): -960.8350\n",
      "convergence dfGPdfNN Run 6/10, Epoch 865/1000, Training Loss (NLML): -960.8445\n",
      "convergence dfGPdfNN Run 6/10, Epoch 866/1000, Training Loss (NLML): -960.8162\n",
      "convergence dfGPdfNN Run 6/10, Epoch 867/1000, Training Loss (NLML): -960.8279\n",
      "convergence dfGPdfNN Run 6/10, Epoch 868/1000, Training Loss (NLML): -960.8303\n",
      "convergence dfGPdfNN Run 6/10, Epoch 869/1000, Training Loss (NLML): -960.8458\n",
      "convergence dfGPdfNN Run 6/10, Epoch 870/1000, Training Loss (NLML): -960.8512\n",
      "convergence dfGPdfNN Run 6/10, Epoch 871/1000, Training Loss (NLML): -960.8533\n",
      "convergence dfGPdfNN Run 6/10, Epoch 872/1000, Training Loss (NLML): -960.8372\n",
      "convergence dfGPdfNN Run 6/10, Epoch 873/1000, Training Loss (NLML): -960.8364\n",
      "convergence dfGPdfNN Run 6/10, Epoch 874/1000, Training Loss (NLML): -960.8594\n",
      "convergence dfGPdfNN Run 6/10, Epoch 875/1000, Training Loss (NLML): -960.8588\n",
      "convergence dfGPdfNN Run 6/10, Epoch 876/1000, Training Loss (NLML): -960.8604\n",
      "convergence dfGPdfNN Run 6/10, Epoch 877/1000, Training Loss (NLML): -960.8622\n",
      "convergence dfGPdfNN Run 6/10, Epoch 878/1000, Training Loss (NLML): -960.8702\n",
      "convergence dfGPdfNN Run 6/10, Epoch 879/1000, Training Loss (NLML): -960.8507\n",
      "convergence dfGPdfNN Run 6/10, Epoch 880/1000, Training Loss (NLML): -960.8694\n",
      "convergence dfGPdfNN Run 6/10, Epoch 881/1000, Training Loss (NLML): -960.8702\n",
      "convergence dfGPdfNN Run 6/10, Epoch 882/1000, Training Loss (NLML): -960.8765\n",
      "convergence dfGPdfNN Run 6/10, Epoch 883/1000, Training Loss (NLML): -960.8757\n",
      "convergence dfGPdfNN Run 6/10, Epoch 884/1000, Training Loss (NLML): -960.8818\n",
      "convergence dfGPdfNN Run 6/10, Epoch 885/1000, Training Loss (NLML): -960.8824\n",
      "convergence dfGPdfNN Run 6/10, Epoch 886/1000, Training Loss (NLML): -960.8839\n",
      "convergence dfGPdfNN Run 6/10, Epoch 887/1000, Training Loss (NLML): -960.8666\n",
      "convergence dfGPdfNN Run 6/10, Epoch 888/1000, Training Loss (NLML): -960.8717\n",
      "convergence dfGPdfNN Run 6/10, Epoch 889/1000, Training Loss (NLML): -960.8925\n",
      "convergence dfGPdfNN Run 6/10, Epoch 890/1000, Training Loss (NLML): -960.8936\n",
      "convergence dfGPdfNN Run 6/10, Epoch 891/1000, Training Loss (NLML): -960.8927\n",
      "convergence dfGPdfNN Run 6/10, Epoch 892/1000, Training Loss (NLML): -960.8806\n",
      "convergence dfGPdfNN Run 6/10, Epoch 893/1000, Training Loss (NLML): -960.8944\n",
      "convergence dfGPdfNN Run 6/10, Epoch 894/1000, Training Loss (NLML): -960.8829\n",
      "convergence dfGPdfNN Run 6/10, Epoch 895/1000, Training Loss (NLML): -960.9000\n",
      "convergence dfGPdfNN Run 6/10, Epoch 896/1000, Training Loss (NLML): -960.9022\n",
      "convergence dfGPdfNN Run 6/10, Epoch 897/1000, Training Loss (NLML): -960.9045\n",
      "convergence dfGPdfNN Run 6/10, Epoch 898/1000, Training Loss (NLML): -960.9081\n",
      "convergence dfGPdfNN Run 6/10, Epoch 899/1000, Training Loss (NLML): -960.9108\n",
      "convergence dfGPdfNN Run 6/10, Epoch 900/1000, Training Loss (NLML): -960.8940\n",
      "convergence dfGPdfNN Run 6/10, Epoch 901/1000, Training Loss (NLML): -960.9128\n",
      "convergence dfGPdfNN Run 6/10, Epoch 902/1000, Training Loss (NLML): -960.9115\n",
      "convergence dfGPdfNN Run 6/10, Epoch 903/1000, Training Loss (NLML): -960.9023\n",
      "convergence dfGPdfNN Run 6/10, Epoch 904/1000, Training Loss (NLML): -960.9208\n",
      "convergence dfGPdfNN Run 6/10, Epoch 905/1000, Training Loss (NLML): -960.9207\n",
      "convergence dfGPdfNN Run 6/10, Epoch 906/1000, Training Loss (NLML): -960.9087\n",
      "convergence dfGPdfNN Run 6/10, Epoch 907/1000, Training Loss (NLML): -960.9215\n",
      "convergence dfGPdfNN Run 6/10, Epoch 908/1000, Training Loss (NLML): -960.9253\n",
      "convergence dfGPdfNN Run 6/10, Epoch 909/1000, Training Loss (NLML): -960.9319\n",
      "convergence dfGPdfNN Run 6/10, Epoch 910/1000, Training Loss (NLML): -960.9346\n",
      "convergence dfGPdfNN Run 6/10, Epoch 911/1000, Training Loss (NLML): -960.9174\n",
      "convergence dfGPdfNN Run 6/10, Epoch 912/1000, Training Loss (NLML): -960.9153\n",
      "convergence dfGPdfNN Run 6/10, Epoch 913/1000, Training Loss (NLML): -960.9353\n",
      "convergence dfGPdfNN Run 6/10, Epoch 914/1000, Training Loss (NLML): -960.9365\n",
      "convergence dfGPdfNN Run 6/10, Epoch 915/1000, Training Loss (NLML): -960.9366\n",
      "convergence dfGPdfNN Run 6/10, Epoch 916/1000, Training Loss (NLML): -960.9382\n",
      "convergence dfGPdfNN Run 6/10, Epoch 917/1000, Training Loss (NLML): -960.9484\n",
      "convergence dfGPdfNN Run 6/10, Epoch 918/1000, Training Loss (NLML): -960.9459\n",
      "convergence dfGPdfNN Run 6/10, Epoch 919/1000, Training Loss (NLML): -960.9482\n",
      "convergence dfGPdfNN Run 6/10, Epoch 920/1000, Training Loss (NLML): -960.9506\n",
      "convergence dfGPdfNN Run 6/10, Epoch 921/1000, Training Loss (NLML): -960.9546\n",
      "convergence dfGPdfNN Run 6/10, Epoch 922/1000, Training Loss (NLML): -960.9392\n",
      "convergence dfGPdfNN Run 6/10, Epoch 923/1000, Training Loss (NLML): -960.9395\n",
      "convergence dfGPdfNN Run 6/10, Epoch 924/1000, Training Loss (NLML): -960.9447\n",
      "convergence dfGPdfNN Run 6/10, Epoch 925/1000, Training Loss (NLML): -960.9425\n",
      "convergence dfGPdfNN Run 6/10, Epoch 926/1000, Training Loss (NLML): -960.9572\n",
      "convergence dfGPdfNN Run 6/10, Epoch 927/1000, Training Loss (NLML): -960.9624\n",
      "convergence dfGPdfNN Run 6/10, Epoch 928/1000, Training Loss (NLML): -960.9669\n",
      "convergence dfGPdfNN Run 6/10, Epoch 929/1000, Training Loss (NLML): -960.9674\n",
      "convergence dfGPdfNN Run 6/10, Epoch 930/1000, Training Loss (NLML): -960.9689\n",
      "convergence dfGPdfNN Run 6/10, Epoch 931/1000, Training Loss (NLML): -960.9681\n",
      "convergence dfGPdfNN Run 6/10, Epoch 932/1000, Training Loss (NLML): -960.9717\n",
      "convergence dfGPdfNN Run 6/10, Epoch 933/1000, Training Loss (NLML): -960.9733\n",
      "convergence dfGPdfNN Run 6/10, Epoch 934/1000, Training Loss (NLML): -960.9739\n",
      "convergence dfGPdfNN Run 6/10, Epoch 935/1000, Training Loss (NLML): -960.9803\n",
      "convergence dfGPdfNN Run 6/10, Epoch 936/1000, Training Loss (NLML): -960.9834\n",
      "convergence dfGPdfNN Run 6/10, Epoch 937/1000, Training Loss (NLML): -960.9816\n",
      "convergence dfGPdfNN Run 6/10, Epoch 938/1000, Training Loss (NLML): -960.9845\n",
      "convergence dfGPdfNN Run 6/10, Epoch 939/1000, Training Loss (NLML): -960.9774\n",
      "convergence dfGPdfNN Run 6/10, Epoch 940/1000, Training Loss (NLML): -960.9844\n",
      "convergence dfGPdfNN Run 6/10, Epoch 941/1000, Training Loss (NLML): -960.9861\n",
      "convergence dfGPdfNN Run 6/10, Epoch 942/1000, Training Loss (NLML): -960.9902\n",
      "convergence dfGPdfNN Run 6/10, Epoch 943/1000, Training Loss (NLML): -960.9916\n",
      "convergence dfGPdfNN Run 6/10, Epoch 944/1000, Training Loss (NLML): -960.9943\n",
      "convergence dfGPdfNN Run 6/10, Epoch 945/1000, Training Loss (NLML): -960.9803\n",
      "convergence dfGPdfNN Run 6/10, Epoch 946/1000, Training Loss (NLML): -960.9980\n",
      "convergence dfGPdfNN Run 6/10, Epoch 947/1000, Training Loss (NLML): -960.9999\n",
      "convergence dfGPdfNN Run 6/10, Epoch 948/1000, Training Loss (NLML): -961.0031\n",
      "convergence dfGPdfNN Run 6/10, Epoch 949/1000, Training Loss (NLML): -961.0015\n",
      "convergence dfGPdfNN Run 6/10, Epoch 950/1000, Training Loss (NLML): -961.0039\n",
      "convergence dfGPdfNN Run 6/10, Epoch 951/1000, Training Loss (NLML): -961.0052\n",
      "convergence dfGPdfNN Run 6/10, Epoch 952/1000, Training Loss (NLML): -961.0082\n",
      "convergence dfGPdfNN Run 6/10, Epoch 953/1000, Training Loss (NLML): -961.0106\n",
      "convergence dfGPdfNN Run 6/10, Epoch 954/1000, Training Loss (NLML): -961.0116\n",
      "convergence dfGPdfNN Run 6/10, Epoch 955/1000, Training Loss (NLML): -961.0134\n",
      "convergence dfGPdfNN Run 6/10, Epoch 956/1000, Training Loss (NLML): -961.0194\n",
      "convergence dfGPdfNN Run 6/10, Epoch 957/1000, Training Loss (NLML): -961.0157\n",
      "convergence dfGPdfNN Run 6/10, Epoch 958/1000, Training Loss (NLML): -961.0168\n",
      "convergence dfGPdfNN Run 6/10, Epoch 959/1000, Training Loss (NLML): -961.0173\n",
      "convergence dfGPdfNN Run 6/10, Epoch 960/1000, Training Loss (NLML): -961.0219\n",
      "convergence dfGPdfNN Run 6/10, Epoch 961/1000, Training Loss (NLML): -961.0237\n",
      "convergence dfGPdfNN Run 6/10, Epoch 962/1000, Training Loss (NLML): -961.0265\n",
      "convergence dfGPdfNN Run 6/10, Epoch 963/1000, Training Loss (NLML): -961.0308\n",
      "convergence dfGPdfNN Run 6/10, Epoch 964/1000, Training Loss (NLML): -961.0275\n",
      "convergence dfGPdfNN Run 6/10, Epoch 965/1000, Training Loss (NLML): -961.0304\n",
      "convergence dfGPdfNN Run 6/10, Epoch 966/1000, Training Loss (NLML): -961.0288\n",
      "convergence dfGPdfNN Run 6/10, Epoch 967/1000, Training Loss (NLML): -961.0348\n",
      "convergence dfGPdfNN Run 6/10, Epoch 968/1000, Training Loss (NLML): -961.0367\n",
      "convergence dfGPdfNN Run 6/10, Epoch 969/1000, Training Loss (NLML): -961.0398\n",
      "convergence dfGPdfNN Run 6/10, Epoch 970/1000, Training Loss (NLML): -961.0397\n",
      "convergence dfGPdfNN Run 6/10, Epoch 971/1000, Training Loss (NLML): -961.0386\n",
      "convergence dfGPdfNN Run 6/10, Epoch 972/1000, Training Loss (NLML): -961.0409\n",
      "convergence dfGPdfNN Run 6/10, Epoch 973/1000, Training Loss (NLML): -961.0442\n",
      "convergence dfGPdfNN Run 6/10, Epoch 974/1000, Training Loss (NLML): -961.0460\n",
      "convergence dfGPdfNN Run 6/10, Epoch 975/1000, Training Loss (NLML): -961.0491\n",
      "convergence dfGPdfNN Run 6/10, Epoch 976/1000, Training Loss (NLML): -961.0500\n",
      "convergence dfGPdfNN Run 6/10, Epoch 977/1000, Training Loss (NLML): -961.0519\n",
      "convergence dfGPdfNN Run 6/10, Epoch 978/1000, Training Loss (NLML): -961.0529\n",
      "convergence dfGPdfNN Run 6/10, Epoch 979/1000, Training Loss (NLML): -961.0519\n",
      "convergence dfGPdfNN Run 6/10, Epoch 980/1000, Training Loss (NLML): -961.0558\n",
      "convergence dfGPdfNN Run 6/10, Epoch 981/1000, Training Loss (NLML): -961.0570\n",
      "convergence dfGPdfNN Run 6/10, Epoch 982/1000, Training Loss (NLML): -961.0619\n",
      "convergence dfGPdfNN Run 6/10, Epoch 983/1000, Training Loss (NLML): -961.0601\n",
      "convergence dfGPdfNN Run 6/10, Epoch 984/1000, Training Loss (NLML): -961.0624\n",
      "convergence dfGPdfNN Run 6/10, Epoch 985/1000, Training Loss (NLML): -961.0637\n",
      "convergence dfGPdfNN Run 6/10, Epoch 986/1000, Training Loss (NLML): -961.0651\n",
      "convergence dfGPdfNN Run 6/10, Epoch 987/1000, Training Loss (NLML): -961.0657\n",
      "convergence dfGPdfNN Run 6/10, Epoch 988/1000, Training Loss (NLML): -961.0682\n",
      "convergence dfGPdfNN Run 6/10, Epoch 989/1000, Training Loss (NLML): -961.0737\n",
      "convergence dfGPdfNN Run 6/10, Epoch 990/1000, Training Loss (NLML): -961.0732\n",
      "convergence dfGPdfNN Run 6/10, Epoch 991/1000, Training Loss (NLML): -961.0730\n",
      "convergence dfGPdfNN Run 6/10, Epoch 992/1000, Training Loss (NLML): -961.0774\n",
      "convergence dfGPdfNN Run 6/10, Epoch 993/1000, Training Loss (NLML): -961.0779\n",
      "convergence dfGPdfNN Run 6/10, Epoch 994/1000, Training Loss (NLML): -961.0771\n",
      "convergence dfGPdfNN Run 6/10, Epoch 995/1000, Training Loss (NLML): -961.0770\n",
      "convergence dfGPdfNN Run 6/10, Epoch 996/1000, Training Loss (NLML): -961.0807\n",
      "convergence dfGPdfNN Run 6/10, Epoch 997/1000, Training Loss (NLML): -961.0867\n",
      "convergence dfGPdfNN Run 6/10, Epoch 998/1000, Training Loss (NLML): -961.0836\n",
      "convergence dfGPdfNN Run 6/10, Epoch 999/1000, Training Loss (NLML): -961.0880\n",
      "convergence dfGPdfNN Run 6/10, Epoch 1000/1000, Training Loss (NLML): -961.0917\n",
      "\n",
      "--- Training Run 7/10 ---\n",
      "\n",
      "Start Training\n",
      "convergence dfGPdfNN Run 7/10, Epoch 1/1000, Training Loss (NLML): -872.2396\n",
      "convergence dfGPdfNN Run 7/10, Epoch 2/1000, Training Loss (NLML): -894.9019\n",
      "convergence dfGPdfNN Run 7/10, Epoch 3/1000, Training Loss (NLML): -904.0054\n",
      "convergence dfGPdfNN Run 7/10, Epoch 4/1000, Training Loss (NLML): -905.8263\n",
      "convergence dfGPdfNN Run 7/10, Epoch 5/1000, Training Loss (NLML): -905.2135\n",
      "convergence dfGPdfNN Run 7/10, Epoch 6/1000, Training Loss (NLML): -907.8118\n",
      "convergence dfGPdfNN Run 7/10, Epoch 7/1000, Training Loss (NLML): -911.3348\n",
      "convergence dfGPdfNN Run 7/10, Epoch 8/1000, Training Loss (NLML): -913.7272\n",
      "convergence dfGPdfNN Run 7/10, Epoch 9/1000, Training Loss (NLML): -915.4587\n",
      "convergence dfGPdfNN Run 7/10, Epoch 10/1000, Training Loss (NLML): -916.6794\n",
      "convergence dfGPdfNN Run 7/10, Epoch 11/1000, Training Loss (NLML): -917.9767\n",
      "convergence dfGPdfNN Run 7/10, Epoch 12/1000, Training Loss (NLML): -919.1060\n",
      "convergence dfGPdfNN Run 7/10, Epoch 13/1000, Training Loss (NLML): -920.3397\n",
      "convergence dfGPdfNN Run 7/10, Epoch 14/1000, Training Loss (NLML): -921.4768\n",
      "convergence dfGPdfNN Run 7/10, Epoch 15/1000, Training Loss (NLML): -922.3230\n",
      "convergence dfGPdfNN Run 7/10, Epoch 16/1000, Training Loss (NLML): -922.7747\n",
      "convergence dfGPdfNN Run 7/10, Epoch 17/1000, Training Loss (NLML): -923.9191\n",
      "convergence dfGPdfNN Run 7/10, Epoch 18/1000, Training Loss (NLML): -925.2954\n",
      "convergence dfGPdfNN Run 7/10, Epoch 19/1000, Training Loss (NLML): -926.3226\n",
      "convergence dfGPdfNN Run 7/10, Epoch 20/1000, Training Loss (NLML): -927.2965\n",
      "convergence dfGPdfNN Run 7/10, Epoch 21/1000, Training Loss (NLML): -928.1694\n",
      "convergence dfGPdfNN Run 7/10, Epoch 22/1000, Training Loss (NLML): -929.0256\n",
      "convergence dfGPdfNN Run 7/10, Epoch 23/1000, Training Loss (NLML): -929.8330\n",
      "convergence dfGPdfNN Run 7/10, Epoch 24/1000, Training Loss (NLML): -930.5808\n",
      "convergence dfGPdfNN Run 7/10, Epoch 25/1000, Training Loss (NLML): -931.2783\n",
      "convergence dfGPdfNN Run 7/10, Epoch 26/1000, Training Loss (NLML): -931.9933\n",
      "convergence dfGPdfNN Run 7/10, Epoch 27/1000, Training Loss (NLML): -932.6606\n",
      "convergence dfGPdfNN Run 7/10, Epoch 28/1000, Training Loss (NLML): -933.3353\n",
      "convergence dfGPdfNN Run 7/10, Epoch 29/1000, Training Loss (NLML): -933.9888\n",
      "convergence dfGPdfNN Run 7/10, Epoch 30/1000, Training Loss (NLML): -934.6050\n",
      "convergence dfGPdfNN Run 7/10, Epoch 31/1000, Training Loss (NLML): -935.0939\n",
      "convergence dfGPdfNN Run 7/10, Epoch 32/1000, Training Loss (NLML): -935.7469\n",
      "convergence dfGPdfNN Run 7/10, Epoch 33/1000, Training Loss (NLML): -936.3053\n",
      "convergence dfGPdfNN Run 7/10, Epoch 34/1000, Training Loss (NLML): -936.8920\n",
      "convergence dfGPdfNN Run 7/10, Epoch 35/1000, Training Loss (NLML): -937.4091\n",
      "convergence dfGPdfNN Run 7/10, Epoch 36/1000, Training Loss (NLML): -937.9097\n",
      "convergence dfGPdfNN Run 7/10, Epoch 37/1000, Training Loss (NLML): -938.4053\n",
      "convergence dfGPdfNN Run 7/10, Epoch 38/1000, Training Loss (NLML): -938.8768\n",
      "convergence dfGPdfNN Run 7/10, Epoch 39/1000, Training Loss (NLML): -939.3091\n",
      "convergence dfGPdfNN Run 7/10, Epoch 40/1000, Training Loss (NLML): -939.7805\n",
      "convergence dfGPdfNN Run 7/10, Epoch 41/1000, Training Loss (NLML): -940.0826\n",
      "convergence dfGPdfNN Run 7/10, Epoch 42/1000, Training Loss (NLML): -940.4694\n",
      "convergence dfGPdfNN Run 7/10, Epoch 43/1000, Training Loss (NLML): -940.8462\n",
      "convergence dfGPdfNN Run 7/10, Epoch 44/1000, Training Loss (NLML): -941.2958\n",
      "convergence dfGPdfNN Run 7/10, Epoch 45/1000, Training Loss (NLML): -941.7513\n",
      "convergence dfGPdfNN Run 7/10, Epoch 46/1000, Training Loss (NLML): -942.1398\n",
      "convergence dfGPdfNN Run 7/10, Epoch 47/1000, Training Loss (NLML): -942.5067\n",
      "convergence dfGPdfNN Run 7/10, Epoch 48/1000, Training Loss (NLML): -942.9070\n",
      "convergence dfGPdfNN Run 7/10, Epoch 49/1000, Training Loss (NLML): -943.2528\n",
      "convergence dfGPdfNN Run 7/10, Epoch 50/1000, Training Loss (NLML): -943.5996\n",
      "convergence dfGPdfNN Run 7/10, Epoch 51/1000, Training Loss (NLML): -943.9500\n",
      "convergence dfGPdfNN Run 7/10, Epoch 52/1000, Training Loss (NLML): -944.2623\n",
      "convergence dfGPdfNN Run 7/10, Epoch 53/1000, Training Loss (NLML): -943.3440\n",
      "convergence dfGPdfNN Run 7/10, Epoch 54/1000, Training Loss (NLML): -943.7572\n",
      "convergence dfGPdfNN Run 7/10, Epoch 55/1000, Training Loss (NLML): -944.2816\n",
      "convergence dfGPdfNN Run 7/10, Epoch 56/1000, Training Loss (NLML): -944.5315\n",
      "convergence dfGPdfNN Run 7/10, Epoch 57/1000, Training Loss (NLML): -945.1107\n",
      "convergence dfGPdfNN Run 7/10, Epoch 58/1000, Training Loss (NLML): -945.6239\n",
      "convergence dfGPdfNN Run 7/10, Epoch 59/1000, Training Loss (NLML): -946.0699\n",
      "convergence dfGPdfNN Run 7/10, Epoch 60/1000, Training Loss (NLML): -946.3079\n",
      "convergence dfGPdfNN Run 7/10, Epoch 61/1000, Training Loss (NLML): -946.4933\n",
      "convergence dfGPdfNN Run 7/10, Epoch 62/1000, Training Loss (NLML): -946.6805\n",
      "convergence dfGPdfNN Run 7/10, Epoch 63/1000, Training Loss (NLML): -946.9194\n",
      "convergence dfGPdfNN Run 7/10, Epoch 64/1000, Training Loss (NLML): -947.2439\n",
      "convergence dfGPdfNN Run 7/10, Epoch 65/1000, Training Loss (NLML): -947.5179\n",
      "convergence dfGPdfNN Run 7/10, Epoch 66/1000, Training Loss (NLML): -947.9099\n",
      "convergence dfGPdfNN Run 7/10, Epoch 67/1000, Training Loss (NLML): -948.1083\n",
      "convergence dfGPdfNN Run 7/10, Epoch 68/1000, Training Loss (NLML): -945.2928\n",
      "convergence dfGPdfNN Run 7/10, Epoch 69/1000, Training Loss (NLML): -946.0488\n",
      "convergence dfGPdfNN Run 7/10, Epoch 70/1000, Training Loss (NLML): -947.0570\n",
      "convergence dfGPdfNN Run 7/10, Epoch 71/1000, Training Loss (NLML): -947.8389\n",
      "convergence dfGPdfNN Run 7/10, Epoch 72/1000, Training Loss (NLML): -948.3131\n",
      "convergence dfGPdfNN Run 7/10, Epoch 73/1000, Training Loss (NLML): -948.6007\n",
      "convergence dfGPdfNN Run 7/10, Epoch 74/1000, Training Loss (NLML): -948.9718\n",
      "convergence dfGPdfNN Run 7/10, Epoch 75/1000, Training Loss (NLML): -949.2275\n",
      "convergence dfGPdfNN Run 7/10, Epoch 76/1000, Training Loss (NLML): -949.5317\n",
      "convergence dfGPdfNN Run 7/10, Epoch 77/1000, Training Loss (NLML): -949.8116\n",
      "convergence dfGPdfNN Run 7/10, Epoch 78/1000, Training Loss (NLML): -950.3221\n",
      "convergence dfGPdfNN Run 7/10, Epoch 79/1000, Training Loss (NLML): -950.3030\n",
      "convergence dfGPdfNN Run 7/10, Epoch 80/1000, Training Loss (NLML): -950.4191\n",
      "convergence dfGPdfNN Run 7/10, Epoch 81/1000, Training Loss (NLML): -950.6055\n",
      "convergence dfGPdfNN Run 7/10, Epoch 82/1000, Training Loss (NLML): -950.8560\n",
      "convergence dfGPdfNN Run 7/10, Epoch 83/1000, Training Loss (NLML): -950.9543\n",
      "convergence dfGPdfNN Run 7/10, Epoch 84/1000, Training Loss (NLML): -951.1342\n",
      "convergence dfGPdfNN Run 7/10, Epoch 85/1000, Training Loss (NLML): -950.8805\n",
      "convergence dfGPdfNN Run 7/10, Epoch 86/1000, Training Loss (NLML): -951.3198\n",
      "convergence dfGPdfNN Run 7/10, Epoch 87/1000, Training Loss (NLML): -951.7181\n",
      "convergence dfGPdfNN Run 7/10, Epoch 88/1000, Training Loss (NLML): -951.9601\n",
      "convergence dfGPdfNN Run 7/10, Epoch 89/1000, Training Loss (NLML): -952.2047\n",
      "convergence dfGPdfNN Run 7/10, Epoch 90/1000, Training Loss (NLML): -952.1129\n",
      "convergence dfGPdfNN Run 7/10, Epoch 91/1000, Training Loss (NLML): -952.2335\n",
      "convergence dfGPdfNN Run 7/10, Epoch 92/1000, Training Loss (NLML): -952.3378\n",
      "convergence dfGPdfNN Run 7/10, Epoch 93/1000, Training Loss (NLML): -952.7355\n",
      "convergence dfGPdfNN Run 7/10, Epoch 94/1000, Training Loss (NLML): -953.0193\n",
      "convergence dfGPdfNN Run 7/10, Epoch 95/1000, Training Loss (NLML): -953.1799\n",
      "convergence dfGPdfNN Run 7/10, Epoch 96/1000, Training Loss (NLML): -947.3268\n",
      "convergence dfGPdfNN Run 7/10, Epoch 97/1000, Training Loss (NLML): -949.3232\n",
      "convergence dfGPdfNN Run 7/10, Epoch 98/1000, Training Loss (NLML): -953.0990\n",
      "convergence dfGPdfNN Run 7/10, Epoch 99/1000, Training Loss (NLML): -952.9795\n",
      "convergence dfGPdfNN Run 7/10, Epoch 100/1000, Training Loss (NLML): -952.9722\n",
      "convergence dfGPdfNN Run 7/10, Epoch 101/1000, Training Loss (NLML): -953.0786\n",
      "convergence dfGPdfNN Run 7/10, Epoch 102/1000, Training Loss (NLML): -953.3595\n",
      "convergence dfGPdfNN Run 7/10, Epoch 103/1000, Training Loss (NLML): -953.5930\n",
      "convergence dfGPdfNN Run 7/10, Epoch 104/1000, Training Loss (NLML): -953.9520\n",
      "convergence dfGPdfNN Run 7/10, Epoch 105/1000, Training Loss (NLML): -954.0339\n",
      "convergence dfGPdfNN Run 7/10, Epoch 106/1000, Training Loss (NLML): -954.2223\n",
      "convergence dfGPdfNN Run 7/10, Epoch 107/1000, Training Loss (NLML): -954.1742\n",
      "convergence dfGPdfNN Run 7/10, Epoch 108/1000, Training Loss (NLML): -954.3251\n",
      "convergence dfGPdfNN Run 7/10, Epoch 109/1000, Training Loss (NLML): -954.5900\n",
      "convergence dfGPdfNN Run 7/10, Epoch 110/1000, Training Loss (NLML): -954.4703\n",
      "convergence dfGPdfNN Run 7/10, Epoch 111/1000, Training Loss (NLML): -954.3738\n",
      "convergence dfGPdfNN Run 7/10, Epoch 112/1000, Training Loss (NLML): -954.4054\n",
      "convergence dfGPdfNN Run 7/10, Epoch 113/1000, Training Loss (NLML): -954.4772\n",
      "convergence dfGPdfNN Run 7/10, Epoch 114/1000, Training Loss (NLML): -954.5781\n",
      "convergence dfGPdfNN Run 7/10, Epoch 115/1000, Training Loss (NLML): -954.9092\n",
      "convergence dfGPdfNN Run 7/10, Epoch 116/1000, Training Loss (NLML): -954.9327\n",
      "convergence dfGPdfNN Run 7/10, Epoch 117/1000, Training Loss (NLML): -954.9792\n",
      "convergence dfGPdfNN Run 7/10, Epoch 118/1000, Training Loss (NLML): -955.1855\n",
      "convergence dfGPdfNN Run 7/10, Epoch 119/1000, Training Loss (NLML): -955.3967\n",
      "convergence dfGPdfNN Run 7/10, Epoch 120/1000, Training Loss (NLML): -955.4807\n",
      "convergence dfGPdfNN Run 7/10, Epoch 121/1000, Training Loss (NLML): -955.2000\n",
      "convergence dfGPdfNN Run 7/10, Epoch 122/1000, Training Loss (NLML): -939.9893\n",
      "convergence dfGPdfNN Run 7/10, Epoch 123/1000, Training Loss (NLML): -955.3422\n",
      "convergence dfGPdfNN Run 7/10, Epoch 124/1000, Training Loss (NLML): -955.0208\n",
      "convergence dfGPdfNN Run 7/10, Epoch 125/1000, Training Loss (NLML): -954.9589\n",
      "convergence dfGPdfNN Run 7/10, Epoch 126/1000, Training Loss (NLML): -955.0022\n",
      "convergence dfGPdfNN Run 7/10, Epoch 127/1000, Training Loss (NLML): -955.1982\n",
      "convergence dfGPdfNN Run 7/10, Epoch 128/1000, Training Loss (NLML): -955.1436\n",
      "convergence dfGPdfNN Run 7/10, Epoch 129/1000, Training Loss (NLML): -955.3795\n",
      "convergence dfGPdfNN Run 7/10, Epoch 130/1000, Training Loss (NLML): -955.5090\n",
      "convergence dfGPdfNN Run 7/10, Epoch 131/1000, Training Loss (NLML): -955.5216\n",
      "convergence dfGPdfNN Run 7/10, Epoch 132/1000, Training Loss (NLML): -955.4972\n",
      "convergence dfGPdfNN Run 7/10, Epoch 133/1000, Training Loss (NLML): -955.3185\n",
      "convergence dfGPdfNN Run 7/10, Epoch 134/1000, Training Loss (NLML): -955.0101\n",
      "convergence dfGPdfNN Run 7/10, Epoch 135/1000, Training Loss (NLML): -954.4642\n",
      "convergence dfGPdfNN Run 7/10, Epoch 136/1000, Training Loss (NLML): -954.2292\n",
      "convergence dfGPdfNN Run 7/10, Epoch 137/1000, Training Loss (NLML): -954.5704\n",
      "convergence dfGPdfNN Run 7/10, Epoch 138/1000, Training Loss (NLML): -955.0428\n",
      "convergence dfGPdfNN Run 7/10, Epoch 139/1000, Training Loss (NLML): -955.3837\n",
      "convergence dfGPdfNN Run 7/10, Epoch 140/1000, Training Loss (NLML): -955.6753\n",
      "convergence dfGPdfNN Run 7/10, Epoch 141/1000, Training Loss (NLML): -955.8036\n",
      "convergence dfGPdfNN Run 7/10, Epoch 142/1000, Training Loss (NLML): -955.8362\n",
      "convergence dfGPdfNN Run 7/10, Epoch 143/1000, Training Loss (NLML): -955.8292\n",
      "convergence dfGPdfNN Run 7/10, Epoch 144/1000, Training Loss (NLML): -955.7937\n",
      "convergence dfGPdfNN Run 7/10, Epoch 145/1000, Training Loss (NLML): -955.7496\n",
      "convergence dfGPdfNN Run 7/10, Epoch 146/1000, Training Loss (NLML): -955.7648\n",
      "convergence dfGPdfNN Run 7/10, Epoch 147/1000, Training Loss (NLML): -955.9347\n",
      "convergence dfGPdfNN Run 7/10, Epoch 148/1000, Training Loss (NLML): -955.9344\n",
      "convergence dfGPdfNN Run 7/10, Epoch 149/1000, Training Loss (NLML): -955.4696\n",
      "convergence dfGPdfNN Run 7/10, Epoch 150/1000, Training Loss (NLML): -955.6636\n",
      "convergence dfGPdfNN Run 7/10, Epoch 151/1000, Training Loss (NLML): -955.7800\n",
      "convergence dfGPdfNN Run 7/10, Epoch 152/1000, Training Loss (NLML): -956.0107\n",
      "convergence dfGPdfNN Run 7/10, Epoch 153/1000, Training Loss (NLML): -956.2367\n",
      "convergence dfGPdfNN Run 7/10, Epoch 154/1000, Training Loss (NLML): -956.3081\n",
      "convergence dfGPdfNN Run 7/10, Epoch 155/1000, Training Loss (NLML): -956.3691\n",
      "convergence dfGPdfNN Run 7/10, Epoch 156/1000, Training Loss (NLML): -956.4327\n",
      "convergence dfGPdfNN Run 7/10, Epoch 157/1000, Training Loss (NLML): -956.4821\n",
      "convergence dfGPdfNN Run 7/10, Epoch 158/1000, Training Loss (NLML): -956.5688\n",
      "convergence dfGPdfNN Run 7/10, Epoch 159/1000, Training Loss (NLML): -956.6162\n",
      "convergence dfGPdfNN Run 7/10, Epoch 160/1000, Training Loss (NLML): -956.5901\n",
      "convergence dfGPdfNN Run 7/10, Epoch 161/1000, Training Loss (NLML): -956.5151\n",
      "convergence dfGPdfNN Run 7/10, Epoch 162/1000, Training Loss (NLML): -956.4791\n",
      "convergence dfGPdfNN Run 7/10, Epoch 163/1000, Training Loss (NLML): -956.5193\n",
      "convergence dfGPdfNN Run 7/10, Epoch 164/1000, Training Loss (NLML): -956.5698\n",
      "convergence dfGPdfNN Run 7/10, Epoch 165/1000, Training Loss (NLML): -956.5688\n",
      "convergence dfGPdfNN Run 7/10, Epoch 166/1000, Training Loss (NLML): -956.6169\n",
      "convergence dfGPdfNN Run 7/10, Epoch 167/1000, Training Loss (NLML): -956.6361\n",
      "convergence dfGPdfNN Run 7/10, Epoch 168/1000, Training Loss (NLML): -956.6641\n",
      "convergence dfGPdfNN Run 7/10, Epoch 169/1000, Training Loss (NLML): -956.6366\n",
      "convergence dfGPdfNN Run 7/10, Epoch 170/1000, Training Loss (NLML): -956.6677\n",
      "convergence dfGPdfNN Run 7/10, Epoch 171/1000, Training Loss (NLML): -956.6772\n",
      "convergence dfGPdfNN Run 7/10, Epoch 172/1000, Training Loss (NLML): -956.7449\n",
      "convergence dfGPdfNN Run 7/10, Epoch 173/1000, Training Loss (NLML): -956.7861\n",
      "convergence dfGPdfNN Run 7/10, Epoch 174/1000, Training Loss (NLML): -956.8193\n",
      "convergence dfGPdfNN Run 7/10, Epoch 175/1000, Training Loss (NLML): -956.7532\n",
      "convergence dfGPdfNN Run 7/10, Epoch 176/1000, Training Loss (NLML): -956.6543\n",
      "convergence dfGPdfNN Run 7/10, Epoch 177/1000, Training Loss (NLML): -956.6450\n",
      "convergence dfGPdfNN Run 7/10, Epoch 178/1000, Training Loss (NLML): -956.7542\n",
      "convergence dfGPdfNN Run 7/10, Epoch 179/1000, Training Loss (NLML): -956.7391\n",
      "convergence dfGPdfNN Run 7/10, Epoch 180/1000, Training Loss (NLML): -956.7661\n",
      "convergence dfGPdfNN Run 7/10, Epoch 181/1000, Training Loss (NLML): -956.7896\n",
      "convergence dfGPdfNN Run 7/10, Epoch 182/1000, Training Loss (NLML): -956.7699\n",
      "convergence dfGPdfNN Run 7/10, Epoch 183/1000, Training Loss (NLML): -956.8198\n",
      "convergence dfGPdfNN Run 7/10, Epoch 184/1000, Training Loss (NLML): -956.8864\n",
      "convergence dfGPdfNN Run 7/10, Epoch 185/1000, Training Loss (NLML): -956.9543\n",
      "convergence dfGPdfNN Run 7/10, Epoch 186/1000, Training Loss (NLML): -956.8855\n",
      "convergence dfGPdfNN Run 7/10, Epoch 187/1000, Training Loss (NLML): -956.7162\n",
      "convergence dfGPdfNN Run 7/10, Epoch 188/1000, Training Loss (NLML): -956.5734\n",
      "convergence dfGPdfNN Run 7/10, Epoch 189/1000, Training Loss (NLML): -956.5571\n",
      "convergence dfGPdfNN Run 7/10, Epoch 190/1000, Training Loss (NLML): -956.5980\n",
      "convergence dfGPdfNN Run 7/10, Epoch 191/1000, Training Loss (NLML): -956.7084\n",
      "convergence dfGPdfNN Run 7/10, Epoch 192/1000, Training Loss (NLML): -956.7327\n",
      "convergence dfGPdfNN Run 7/10, Epoch 193/1000, Training Loss (NLML): -956.6793\n",
      "convergence dfGPdfNN Run 7/10, Epoch 194/1000, Training Loss (NLML): -956.6945\n",
      "convergence dfGPdfNN Run 7/10, Epoch 195/1000, Training Loss (NLML): -956.6802\n",
      "convergence dfGPdfNN Run 7/10, Epoch 196/1000, Training Loss (NLML): -956.8147\n",
      "convergence dfGPdfNN Run 7/10, Epoch 197/1000, Training Loss (NLML): -956.8693\n",
      "convergence dfGPdfNN Run 7/10, Epoch 198/1000, Training Loss (NLML): -956.8782\n",
      "convergence dfGPdfNN Run 7/10, Epoch 199/1000, Training Loss (NLML): -956.9255\n",
      "convergence dfGPdfNN Run 7/10, Epoch 200/1000, Training Loss (NLML): -956.9446\n",
      "convergence dfGPdfNN Run 7/10, Epoch 201/1000, Training Loss (NLML): -956.9885\n",
      "convergence dfGPdfNN Run 7/10, Epoch 202/1000, Training Loss (NLML): -957.0289\n",
      "convergence dfGPdfNN Run 7/10, Epoch 203/1000, Training Loss (NLML): -957.0164\n",
      "convergence dfGPdfNN Run 7/10, Epoch 204/1000, Training Loss (NLML): -957.0219\n",
      "convergence dfGPdfNN Run 7/10, Epoch 205/1000, Training Loss (NLML): -957.0265\n",
      "convergence dfGPdfNN Run 7/10, Epoch 206/1000, Training Loss (NLML): -957.1497\n",
      "convergence dfGPdfNN Run 7/10, Epoch 207/1000, Training Loss (NLML): -957.1790\n",
      "convergence dfGPdfNN Run 7/10, Epoch 208/1000, Training Loss (NLML): -957.2144\n",
      "convergence dfGPdfNN Run 7/10, Epoch 209/1000, Training Loss (NLML): -957.2355\n",
      "convergence dfGPdfNN Run 7/10, Epoch 210/1000, Training Loss (NLML): -957.2174\n",
      "convergence dfGPdfNN Run 7/10, Epoch 211/1000, Training Loss (NLML): -957.1721\n",
      "convergence dfGPdfNN Run 7/10, Epoch 212/1000, Training Loss (NLML): -957.1224\n",
      "convergence dfGPdfNN Run 7/10, Epoch 213/1000, Training Loss (NLML): -957.1127\n",
      "convergence dfGPdfNN Run 7/10, Epoch 214/1000, Training Loss (NLML): -957.1710\n",
      "convergence dfGPdfNN Run 7/10, Epoch 215/1000, Training Loss (NLML): -957.2576\n",
      "convergence dfGPdfNN Run 7/10, Epoch 216/1000, Training Loss (NLML): -957.2717\n",
      "convergence dfGPdfNN Run 7/10, Epoch 217/1000, Training Loss (NLML): -957.2935\n",
      "convergence dfGPdfNN Run 7/10, Epoch 218/1000, Training Loss (NLML): -957.3350\n",
      "convergence dfGPdfNN Run 7/10, Epoch 219/1000, Training Loss (NLML): -957.3889\n",
      "convergence dfGPdfNN Run 7/10, Epoch 220/1000, Training Loss (NLML): -957.4073\n",
      "convergence dfGPdfNN Run 7/10, Epoch 221/1000, Training Loss (NLML): -957.4021\n",
      "convergence dfGPdfNN Run 7/10, Epoch 222/1000, Training Loss (NLML): -957.4435\n",
      "convergence dfGPdfNN Run 7/10, Epoch 223/1000, Training Loss (NLML): -957.4352\n",
      "convergence dfGPdfNN Run 7/10, Epoch 224/1000, Training Loss (NLML): -957.4781\n",
      "convergence dfGPdfNN Run 7/10, Epoch 225/1000, Training Loss (NLML): -957.4650\n",
      "convergence dfGPdfNN Run 7/10, Epoch 226/1000, Training Loss (NLML): -957.4503\n",
      "convergence dfGPdfNN Run 7/10, Epoch 227/1000, Training Loss (NLML): -957.4626\n",
      "convergence dfGPdfNN Run 7/10, Epoch 228/1000, Training Loss (NLML): -957.4796\n",
      "convergence dfGPdfNN Run 7/10, Epoch 229/1000, Training Loss (NLML): -957.5490\n",
      "convergence dfGPdfNN Run 7/10, Epoch 230/1000, Training Loss (NLML): -957.5995\n",
      "convergence dfGPdfNN Run 7/10, Epoch 231/1000, Training Loss (NLML): -957.6772\n",
      "convergence dfGPdfNN Run 7/10, Epoch 232/1000, Training Loss (NLML): -957.7120\n",
      "convergence dfGPdfNN Run 7/10, Epoch 233/1000, Training Loss (NLML): -957.7283\n",
      "convergence dfGPdfNN Run 7/10, Epoch 234/1000, Training Loss (NLML): -957.7567\n",
      "convergence dfGPdfNN Run 7/10, Epoch 235/1000, Training Loss (NLML): -957.7139\n",
      "convergence dfGPdfNN Run 7/10, Epoch 236/1000, Training Loss (NLML): -957.7413\n",
      "convergence dfGPdfNN Run 7/10, Epoch 237/1000, Training Loss (NLML): -957.8079\n",
      "convergence dfGPdfNN Run 7/10, Epoch 238/1000, Training Loss (NLML): -957.7775\n",
      "convergence dfGPdfNN Run 7/10, Epoch 239/1000, Training Loss (NLML): -957.6664\n",
      "convergence dfGPdfNN Run 7/10, Epoch 240/1000, Training Loss (NLML): -957.6864\n",
      "convergence dfGPdfNN Run 7/10, Epoch 241/1000, Training Loss (NLML): -957.6945\n",
      "convergence dfGPdfNN Run 7/10, Epoch 242/1000, Training Loss (NLML): -957.7764\n",
      "convergence dfGPdfNN Run 7/10, Epoch 243/1000, Training Loss (NLML): -957.7825\n",
      "convergence dfGPdfNN Run 7/10, Epoch 244/1000, Training Loss (NLML): -957.7930\n",
      "convergence dfGPdfNN Run 7/10, Epoch 245/1000, Training Loss (NLML): -957.8094\n",
      "convergence dfGPdfNN Run 7/10, Epoch 246/1000, Training Loss (NLML): -957.8425\n",
      "convergence dfGPdfNN Run 7/10, Epoch 247/1000, Training Loss (NLML): -957.8536\n",
      "convergence dfGPdfNN Run 7/10, Epoch 248/1000, Training Loss (NLML): -957.8666\n",
      "convergence dfGPdfNN Run 7/10, Epoch 249/1000, Training Loss (NLML): -957.7888\n",
      "convergence dfGPdfNN Run 7/10, Epoch 250/1000, Training Loss (NLML): -957.8137\n",
      "convergence dfGPdfNN Run 7/10, Epoch 251/1000, Training Loss (NLML): -957.8435\n",
      "convergence dfGPdfNN Run 7/10, Epoch 252/1000, Training Loss (NLML): -957.9128\n",
      "convergence dfGPdfNN Run 7/10, Epoch 253/1000, Training Loss (NLML): -957.9075\n",
      "convergence dfGPdfNN Run 7/10, Epoch 254/1000, Training Loss (NLML): -957.9502\n",
      "convergence dfGPdfNN Run 7/10, Epoch 255/1000, Training Loss (NLML): -957.9265\n",
      "convergence dfGPdfNN Run 7/10, Epoch 256/1000, Training Loss (NLML): -957.8690\n",
      "convergence dfGPdfNN Run 7/10, Epoch 257/1000, Training Loss (NLML): -957.8843\n",
      "convergence dfGPdfNN Run 7/10, Epoch 258/1000, Training Loss (NLML): -957.8812\n",
      "convergence dfGPdfNN Run 7/10, Epoch 259/1000, Training Loss (NLML): -957.9695\n",
      "convergence dfGPdfNN Run 7/10, Epoch 260/1000, Training Loss (NLML): -957.9967\n",
      "convergence dfGPdfNN Run 7/10, Epoch 261/1000, Training Loss (NLML): -957.9337\n",
      "convergence dfGPdfNN Run 7/10, Epoch 262/1000, Training Loss (NLML): -957.9990\n",
      "convergence dfGPdfNN Run 7/10, Epoch 263/1000, Training Loss (NLML): -958.0167\n",
      "convergence dfGPdfNN Run 7/10, Epoch 264/1000, Training Loss (NLML): -958.1071\n",
      "convergence dfGPdfNN Run 7/10, Epoch 265/1000, Training Loss (NLML): -958.1318\n",
      "convergence dfGPdfNN Run 7/10, Epoch 266/1000, Training Loss (NLML): -958.1458\n",
      "convergence dfGPdfNN Run 7/10, Epoch 267/1000, Training Loss (NLML): -958.1501\n",
      "convergence dfGPdfNN Run 7/10, Epoch 268/1000, Training Loss (NLML): -958.1263\n",
      "convergence dfGPdfNN Run 7/10, Epoch 269/1000, Training Loss (NLML): -958.1205\n",
      "convergence dfGPdfNN Run 7/10, Epoch 270/1000, Training Loss (NLML): -958.1064\n",
      "convergence dfGPdfNN Run 7/10, Epoch 271/1000, Training Loss (NLML): -958.1224\n",
      "convergence dfGPdfNN Run 7/10, Epoch 272/1000, Training Loss (NLML): -958.1697\n",
      "convergence dfGPdfNN Run 7/10, Epoch 273/1000, Training Loss (NLML): -958.1846\n",
      "convergence dfGPdfNN Run 7/10, Epoch 274/1000, Training Loss (NLML): -958.2202\n",
      "convergence dfGPdfNN Run 7/10, Epoch 275/1000, Training Loss (NLML): -958.2467\n",
      "convergence dfGPdfNN Run 7/10, Epoch 276/1000, Training Loss (NLML): -958.2582\n",
      "convergence dfGPdfNN Run 7/10, Epoch 277/1000, Training Loss (NLML): -958.2368\n",
      "convergence dfGPdfNN Run 7/10, Epoch 278/1000, Training Loss (NLML): -958.2361\n",
      "convergence dfGPdfNN Run 7/10, Epoch 279/1000, Training Loss (NLML): -958.2462\n",
      "convergence dfGPdfNN Run 7/10, Epoch 280/1000, Training Loss (NLML): -958.2372\n",
      "convergence dfGPdfNN Run 7/10, Epoch 281/1000, Training Loss (NLML): -958.3025\n",
      "convergence dfGPdfNN Run 7/10, Epoch 282/1000, Training Loss (NLML): -958.3278\n",
      "convergence dfGPdfNN Run 7/10, Epoch 283/1000, Training Loss (NLML): -958.3081\n",
      "convergence dfGPdfNN Run 7/10, Epoch 284/1000, Training Loss (NLML): -958.3213\n",
      "convergence dfGPdfNN Run 7/10, Epoch 285/1000, Training Loss (NLML): -958.3641\n",
      "convergence dfGPdfNN Run 7/10, Epoch 286/1000, Training Loss (NLML): -958.3759\n",
      "convergence dfGPdfNN Run 7/10, Epoch 287/1000, Training Loss (NLML): -958.3551\n",
      "convergence dfGPdfNN Run 7/10, Epoch 288/1000, Training Loss (NLML): -958.3667\n",
      "convergence dfGPdfNN Run 7/10, Epoch 289/1000, Training Loss (NLML): -958.4102\n",
      "convergence dfGPdfNN Run 7/10, Epoch 290/1000, Training Loss (NLML): -958.3828\n",
      "convergence dfGPdfNN Run 7/10, Epoch 291/1000, Training Loss (NLML): -958.3934\n",
      "convergence dfGPdfNN Run 7/10, Epoch 292/1000, Training Loss (NLML): -958.3937\n",
      "convergence dfGPdfNN Run 7/10, Epoch 293/1000, Training Loss (NLML): -958.4053\n",
      "convergence dfGPdfNN Run 7/10, Epoch 294/1000, Training Loss (NLML): -958.4644\n",
      "convergence dfGPdfNN Run 7/10, Epoch 295/1000, Training Loss (NLML): -958.4741\n",
      "convergence dfGPdfNN Run 7/10, Epoch 296/1000, Training Loss (NLML): -958.4852\n",
      "convergence dfGPdfNN Run 7/10, Epoch 297/1000, Training Loss (NLML): -958.4957\n",
      "convergence dfGPdfNN Run 7/10, Epoch 298/1000, Training Loss (NLML): -958.4744\n",
      "convergence dfGPdfNN Run 7/10, Epoch 299/1000, Training Loss (NLML): -958.4899\n",
      "convergence dfGPdfNN Run 7/10, Epoch 300/1000, Training Loss (NLML): -958.5005\n",
      "convergence dfGPdfNN Run 7/10, Epoch 301/1000, Training Loss (NLML): -958.5104\n",
      "convergence dfGPdfNN Run 7/10, Epoch 302/1000, Training Loss (NLML): -958.5469\n",
      "convergence dfGPdfNN Run 7/10, Epoch 303/1000, Training Loss (NLML): -958.5583\n",
      "convergence dfGPdfNN Run 7/10, Epoch 304/1000, Training Loss (NLML): -958.5687\n",
      "convergence dfGPdfNN Run 7/10, Epoch 305/1000, Training Loss (NLML): -958.5781\n",
      "convergence dfGPdfNN Run 7/10, Epoch 306/1000, Training Loss (NLML): -958.5731\n",
      "convergence dfGPdfNN Run 7/10, Epoch 307/1000, Training Loss (NLML): -958.5824\n",
      "convergence dfGPdfNN Run 7/10, Epoch 308/1000, Training Loss (NLML): -958.5918\n",
      "convergence dfGPdfNN Run 7/10, Epoch 309/1000, Training Loss (NLML): -958.6044\n",
      "convergence dfGPdfNN Run 7/10, Epoch 310/1000, Training Loss (NLML): -958.6307\n",
      "convergence dfGPdfNN Run 7/10, Epoch 311/1000, Training Loss (NLML): -958.6399\n",
      "convergence dfGPdfNN Run 7/10, Epoch 312/1000, Training Loss (NLML): -958.6500\n",
      "convergence dfGPdfNN Run 7/10, Epoch 313/1000, Training Loss (NLML): -958.6606\n",
      "convergence dfGPdfNN Run 7/10, Epoch 314/1000, Training Loss (NLML): -958.6324\n",
      "convergence dfGPdfNN Run 7/10, Epoch 315/1000, Training Loss (NLML): -958.6475\n",
      "convergence dfGPdfNN Run 7/10, Epoch 316/1000, Training Loss (NLML): -958.6571\n",
      "convergence dfGPdfNN Run 7/10, Epoch 317/1000, Training Loss (NLML): -958.6832\n",
      "convergence dfGPdfNN Run 7/10, Epoch 318/1000, Training Loss (NLML): -958.7019\n",
      "convergence dfGPdfNN Run 7/10, Epoch 319/1000, Training Loss (NLML): -958.7109\n",
      "convergence dfGPdfNN Run 7/10, Epoch 320/1000, Training Loss (NLML): -958.7230\n",
      "convergence dfGPdfNN Run 7/10, Epoch 321/1000, Training Loss (NLML): -958.7333\n",
      "convergence dfGPdfNN Run 7/10, Epoch 322/1000, Training Loss (NLML): -958.7317\n",
      "convergence dfGPdfNN Run 7/10, Epoch 323/1000, Training Loss (NLML): -958.7434\n",
      "convergence dfGPdfNN Run 7/10, Epoch 324/1000, Training Loss (NLML): -958.7458\n",
      "convergence dfGPdfNN Run 7/10, Epoch 325/1000, Training Loss (NLML): -958.7692\n",
      "convergence dfGPdfNN Run 7/10, Epoch 326/1000, Training Loss (NLML): -958.7799\n",
      "convergence dfGPdfNN Run 7/10, Epoch 327/1000, Training Loss (NLML): -958.7985\n",
      "convergence dfGPdfNN Run 7/10, Epoch 328/1000, Training Loss (NLML): -958.8079\n",
      "convergence dfGPdfNN Run 7/10, Epoch 329/1000, Training Loss (NLML): -958.8169\n",
      "convergence dfGPdfNN Run 7/10, Epoch 330/1000, Training Loss (NLML): -958.8253\n",
      "convergence dfGPdfNN Run 7/10, Epoch 331/1000, Training Loss (NLML): -958.8278\n",
      "convergence dfGPdfNN Run 7/10, Epoch 332/1000, Training Loss (NLML): -958.8356\n",
      "convergence dfGPdfNN Run 7/10, Epoch 333/1000, Training Loss (NLML): -958.8400\n",
      "convergence dfGPdfNN Run 7/10, Epoch 334/1000, Training Loss (NLML): -958.8324\n",
      "convergence dfGPdfNN Run 7/10, Epoch 335/1000, Training Loss (NLML): -958.8389\n",
      "convergence dfGPdfNN Run 7/10, Epoch 336/1000, Training Loss (NLML): -958.8553\n",
      "convergence dfGPdfNN Run 7/10, Epoch 337/1000, Training Loss (NLML): -958.8569\n",
      "convergence dfGPdfNN Run 7/10, Epoch 338/1000, Training Loss (NLML): -958.8658\n",
      "convergence dfGPdfNN Run 7/10, Epoch 339/1000, Training Loss (NLML): -958.8849\n",
      "convergence dfGPdfNN Run 7/10, Epoch 340/1000, Training Loss (NLML): -958.8884\n",
      "convergence dfGPdfNN Run 7/10, Epoch 341/1000, Training Loss (NLML): -958.8871\n",
      "convergence dfGPdfNN Run 7/10, Epoch 342/1000, Training Loss (NLML): -958.9066\n",
      "convergence dfGPdfNN Run 7/10, Epoch 343/1000, Training Loss (NLML): -958.8912\n",
      "convergence dfGPdfNN Run 7/10, Epoch 344/1000, Training Loss (NLML): -958.9254\n",
      "convergence dfGPdfNN Run 7/10, Epoch 345/1000, Training Loss (NLML): -958.9404\n",
      "convergence dfGPdfNN Run 7/10, Epoch 346/1000, Training Loss (NLML): -958.9388\n",
      "convergence dfGPdfNN Run 7/10, Epoch 347/1000, Training Loss (NLML): -958.9482\n",
      "convergence dfGPdfNN Run 7/10, Epoch 348/1000, Training Loss (NLML): -958.9692\n",
      "convergence dfGPdfNN Run 7/10, Epoch 349/1000, Training Loss (NLML): -958.9785\n",
      "convergence dfGPdfNN Run 7/10, Epoch 350/1000, Training Loss (NLML): -958.9879\n",
      "convergence dfGPdfNN Run 7/10, Epoch 351/1000, Training Loss (NLML): -959.0200\n",
      "convergence dfGPdfNN Run 7/10, Epoch 352/1000, Training Loss (NLML): -959.0044\n",
      "convergence dfGPdfNN Run 7/10, Epoch 353/1000, Training Loss (NLML): -959.0107\n",
      "convergence dfGPdfNN Run 7/10, Epoch 354/1000, Training Loss (NLML): -959.0187\n",
      "convergence dfGPdfNN Run 7/10, Epoch 355/1000, Training Loss (NLML): -959.0291\n",
      "convergence dfGPdfNN Run 7/10, Epoch 356/1000, Training Loss (NLML): -959.0374\n",
      "convergence dfGPdfNN Run 7/10, Epoch 357/1000, Training Loss (NLML): -959.0466\n",
      "convergence dfGPdfNN Run 7/10, Epoch 358/1000, Training Loss (NLML): -959.0570\n",
      "convergence dfGPdfNN Run 7/10, Epoch 359/1000, Training Loss (NLML): -959.0651\n",
      "convergence dfGPdfNN Run 7/10, Epoch 360/1000, Training Loss (NLML): -959.0720\n",
      "convergence dfGPdfNN Run 7/10, Epoch 361/1000, Training Loss (NLML): -959.0814\n",
      "convergence dfGPdfNN Run 7/10, Epoch 362/1000, Training Loss (NLML): -959.0886\n",
      "convergence dfGPdfNN Run 7/10, Epoch 363/1000, Training Loss (NLML): -959.0973\n",
      "convergence dfGPdfNN Run 7/10, Epoch 364/1000, Training Loss (NLML): -959.1068\n",
      "convergence dfGPdfNN Run 7/10, Epoch 365/1000, Training Loss (NLML): -959.1128\n",
      "convergence dfGPdfNN Run 7/10, Epoch 366/1000, Training Loss (NLML): -959.1228\n",
      "convergence dfGPdfNN Run 7/10, Epoch 367/1000, Training Loss (NLML): -959.1293\n",
      "convergence dfGPdfNN Run 7/10, Epoch 368/1000, Training Loss (NLML): -959.1368\n",
      "convergence dfGPdfNN Run 7/10, Epoch 369/1000, Training Loss (NLML): -959.1432\n",
      "convergence dfGPdfNN Run 7/10, Epoch 370/1000, Training Loss (NLML): -959.1514\n",
      "convergence dfGPdfNN Run 7/10, Epoch 371/1000, Training Loss (NLML): -959.1681\n",
      "convergence dfGPdfNN Run 7/10, Epoch 372/1000, Training Loss (NLML): -959.1788\n",
      "convergence dfGPdfNN Run 7/10, Epoch 373/1000, Training Loss (NLML): -959.1854\n",
      "convergence dfGPdfNN Run 7/10, Epoch 374/1000, Training Loss (NLML): -959.1918\n",
      "convergence dfGPdfNN Run 7/10, Epoch 375/1000, Training Loss (NLML): -959.2003\n",
      "convergence dfGPdfNN Run 7/10, Epoch 376/1000, Training Loss (NLML): -959.2078\n",
      "convergence dfGPdfNN Run 7/10, Epoch 377/1000, Training Loss (NLML): -959.2050\n",
      "convergence dfGPdfNN Run 7/10, Epoch 378/1000, Training Loss (NLML): -959.2120\n",
      "convergence dfGPdfNN Run 7/10, Epoch 379/1000, Training Loss (NLML): -959.2172\n",
      "convergence dfGPdfNN Run 7/10, Epoch 380/1000, Training Loss (NLML): -959.2213\n",
      "convergence dfGPdfNN Run 7/10, Epoch 381/1000, Training Loss (NLML): -959.2041\n",
      "convergence dfGPdfNN Run 7/10, Epoch 382/1000, Training Loss (NLML): -959.2035\n",
      "convergence dfGPdfNN Run 7/10, Epoch 383/1000, Training Loss (NLML): -959.2091\n",
      "convergence dfGPdfNN Run 7/10, Epoch 384/1000, Training Loss (NLML): -959.2323\n",
      "convergence dfGPdfNN Run 7/10, Epoch 385/1000, Training Loss (NLML): -959.2351\n",
      "convergence dfGPdfNN Run 7/10, Epoch 386/1000, Training Loss (NLML): -959.2456\n",
      "convergence dfGPdfNN Run 7/10, Epoch 387/1000, Training Loss (NLML): -959.2598\n",
      "convergence dfGPdfNN Run 7/10, Epoch 388/1000, Training Loss (NLML): -959.2704\n",
      "convergence dfGPdfNN Run 7/10, Epoch 389/1000, Training Loss (NLML): -959.2788\n",
      "convergence dfGPdfNN Run 7/10, Epoch 390/1000, Training Loss (NLML): -959.2865\n",
      "convergence dfGPdfNN Run 7/10, Epoch 391/1000, Training Loss (NLML): -959.2950\n",
      "convergence dfGPdfNN Run 7/10, Epoch 392/1000, Training Loss (NLML): -959.3032\n",
      "convergence dfGPdfNN Run 7/10, Epoch 393/1000, Training Loss (NLML): -959.3101\n",
      "convergence dfGPdfNN Run 7/10, Epoch 394/1000, Training Loss (NLML): -959.3180\n",
      "convergence dfGPdfNN Run 7/10, Epoch 395/1000, Training Loss (NLML): -959.3274\n",
      "convergence dfGPdfNN Run 7/10, Epoch 396/1000, Training Loss (NLML): -959.3419\n",
      "convergence dfGPdfNN Run 7/10, Epoch 397/1000, Training Loss (NLML): -959.3469\n",
      "convergence dfGPdfNN Run 7/10, Epoch 398/1000, Training Loss (NLML): -959.3472\n",
      "convergence dfGPdfNN Run 7/10, Epoch 399/1000, Training Loss (NLML): -959.3516\n",
      "convergence dfGPdfNN Run 7/10, Epoch 400/1000, Training Loss (NLML): -959.3674\n",
      "convergence dfGPdfNN Run 7/10, Epoch 401/1000, Training Loss (NLML): -959.3759\n",
      "convergence dfGPdfNN Run 7/10, Epoch 402/1000, Training Loss (NLML): -959.3813\n",
      "convergence dfGPdfNN Run 7/10, Epoch 403/1000, Training Loss (NLML): -959.3776\n",
      "convergence dfGPdfNN Run 7/10, Epoch 404/1000, Training Loss (NLML): -959.3855\n",
      "convergence dfGPdfNN Run 7/10, Epoch 405/1000, Training Loss (NLML): -959.3936\n",
      "convergence dfGPdfNN Run 7/10, Epoch 406/1000, Training Loss (NLML): -959.3942\n",
      "convergence dfGPdfNN Run 7/10, Epoch 407/1000, Training Loss (NLML): -959.4099\n",
      "convergence dfGPdfNN Run 7/10, Epoch 408/1000, Training Loss (NLML): -959.4163\n",
      "convergence dfGPdfNN Run 7/10, Epoch 409/1000, Training Loss (NLML): -959.4225\n",
      "convergence dfGPdfNN Run 7/10, Epoch 410/1000, Training Loss (NLML): -959.4304\n",
      "convergence dfGPdfNN Run 7/10, Epoch 411/1000, Training Loss (NLML): -956.3047\n",
      "convergence dfGPdfNN Run 7/10, Epoch 412/1000, Training Loss (NLML): -959.4210\n",
      "convergence dfGPdfNN Run 7/10, Epoch 413/1000, Training Loss (NLML): -959.4047\n",
      "convergence dfGPdfNN Run 7/10, Epoch 414/1000, Training Loss (NLML): -959.3877\n",
      "convergence dfGPdfNN Run 7/10, Epoch 415/1000, Training Loss (NLML): -959.3749\n",
      "convergence dfGPdfNN Run 7/10, Epoch 416/1000, Training Loss (NLML): -959.3684\n",
      "convergence dfGPdfNN Run 7/10, Epoch 417/1000, Training Loss (NLML): -959.3694\n",
      "convergence dfGPdfNN Run 7/10, Epoch 418/1000, Training Loss (NLML): -959.3671\n",
      "convergence dfGPdfNN Run 7/10, Epoch 419/1000, Training Loss (NLML): -959.3752\n",
      "convergence dfGPdfNN Run 7/10, Epoch 420/1000, Training Loss (NLML): -959.3710\n",
      "convergence dfGPdfNN Run 7/10, Epoch 421/1000, Training Loss (NLML): -959.3877\n",
      "convergence dfGPdfNN Run 7/10, Epoch 422/1000, Training Loss (NLML): -959.3915\n",
      "convergence dfGPdfNN Run 7/10, Epoch 423/1000, Training Loss (NLML): -959.4084\n",
      "convergence dfGPdfNN Run 7/10, Epoch 424/1000, Training Loss (NLML): -959.4210\n",
      "convergence dfGPdfNN Run 7/10, Epoch 425/1000, Training Loss (NLML): -959.4709\n",
      "convergence dfGPdfNN Run 7/10, Epoch 426/1000, Training Loss (NLML): -959.5026\n",
      "convergence dfGPdfNN Run 7/10, Epoch 427/1000, Training Loss (NLML): -959.5292\n",
      "convergence dfGPdfNN Run 7/10, Epoch 428/1000, Training Loss (NLML): -959.5430\n",
      "convergence dfGPdfNN Run 7/10, Epoch 429/1000, Training Loss (NLML): -959.5638\n",
      "convergence dfGPdfNN Run 7/10, Epoch 430/1000, Training Loss (NLML): -959.5720\n",
      "convergence dfGPdfNN Run 7/10, Epoch 431/1000, Training Loss (NLML): -959.5835\n",
      "convergence dfGPdfNN Run 7/10, Epoch 432/1000, Training Loss (NLML): -959.5908\n",
      "convergence dfGPdfNN Run 7/10, Epoch 433/1000, Training Loss (NLML): -959.4541\n",
      "convergence dfGPdfNN Run 7/10, Epoch 434/1000, Training Loss (NLML): -959.4819\n",
      "convergence dfGPdfNN Run 7/10, Epoch 435/1000, Training Loss (NLML): -959.5250\n",
      "convergence dfGPdfNN Run 7/10, Epoch 436/1000, Training Loss (NLML): -959.5586\n",
      "convergence dfGPdfNN Run 7/10, Epoch 437/1000, Training Loss (NLML): -959.5767\n",
      "convergence dfGPdfNN Run 7/10, Epoch 438/1000, Training Loss (NLML): -959.5852\n",
      "convergence dfGPdfNN Run 7/10, Epoch 439/1000, Training Loss (NLML): -959.5824\n",
      "convergence dfGPdfNN Run 7/10, Epoch 440/1000, Training Loss (NLML): -959.5828\n",
      "convergence dfGPdfNN Run 7/10, Epoch 441/1000, Training Loss (NLML): -959.5829\n",
      "convergence dfGPdfNN Run 7/10, Epoch 442/1000, Training Loss (NLML): -959.5968\n",
      "convergence dfGPdfNN Run 7/10, Epoch 443/1000, Training Loss (NLML): -959.6227\n",
      "convergence dfGPdfNN Run 7/10, Epoch 444/1000, Training Loss (NLML): -959.6309\n",
      "convergence dfGPdfNN Run 7/10, Epoch 445/1000, Training Loss (NLML): -959.6400\n",
      "convergence dfGPdfNN Run 7/10, Epoch 446/1000, Training Loss (NLML): -959.6383\n",
      "convergence dfGPdfNN Run 7/10, Epoch 447/1000, Training Loss (NLML): -959.6477\n",
      "convergence dfGPdfNN Run 7/10, Epoch 448/1000, Training Loss (NLML): -959.6493\n",
      "convergence dfGPdfNN Run 7/10, Epoch 449/1000, Training Loss (NLML): -959.6846\n",
      "convergence dfGPdfNN Run 7/10, Epoch 450/1000, Training Loss (NLML): -959.7113\n",
      "convergence dfGPdfNN Run 7/10, Epoch 451/1000, Training Loss (NLML): -959.7229\n",
      "convergence dfGPdfNN Run 7/10, Epoch 452/1000, Training Loss (NLML): -959.7347\n",
      "convergence dfGPdfNN Run 7/10, Epoch 453/1000, Training Loss (NLML): -959.7004\n",
      "convergence dfGPdfNN Run 7/10, Epoch 454/1000, Training Loss (NLML): -959.7152\n",
      "convergence dfGPdfNN Run 7/10, Epoch 455/1000, Training Loss (NLML): -959.8240\n",
      "convergence dfGPdfNN Run 7/10, Epoch 456/1000, Training Loss (NLML): -959.8695\n",
      "convergence dfGPdfNN Run 7/10, Epoch 457/1000, Training Loss (NLML): -959.9521\n",
      "convergence dfGPdfNN Run 7/10, Epoch 458/1000, Training Loss (NLML): -959.9087\n",
      "convergence dfGPdfNN Run 7/10, Epoch 459/1000, Training Loss (NLML): -959.7964\n",
      "convergence dfGPdfNN Run 7/10, Epoch 460/1000, Training Loss (NLML): -959.7889\n",
      "convergence dfGPdfNN Run 7/10, Epoch 461/1000, Training Loss (NLML): -959.7688\n",
      "convergence dfGPdfNN Run 7/10, Epoch 462/1000, Training Loss (NLML): -959.7031\n",
      "convergence dfGPdfNN Run 7/10, Epoch 463/1000, Training Loss (NLML): -959.6866\n",
      "convergence dfGPdfNN Run 7/10, Epoch 464/1000, Training Loss (NLML): -959.7255\n",
      "convergence dfGPdfNN Run 7/10, Epoch 465/1000, Training Loss (NLML): -959.7571\n",
      "convergence dfGPdfNN Run 7/10, Epoch 466/1000, Training Loss (NLML): -959.7798\n",
      "convergence dfGPdfNN Run 7/10, Epoch 467/1000, Training Loss (NLML): -959.8119\n",
      "convergence dfGPdfNN Run 7/10, Epoch 468/1000, Training Loss (NLML): -959.8170\n",
      "convergence dfGPdfNN Run 7/10, Epoch 469/1000, Training Loss (NLML): -959.7903\n",
      "convergence dfGPdfNN Run 7/10, Epoch 470/1000, Training Loss (NLML): -959.7985\n",
      "convergence dfGPdfNN Run 7/10, Epoch 471/1000, Training Loss (NLML): -959.7611\n",
      "convergence dfGPdfNN Run 7/10, Epoch 472/1000, Training Loss (NLML): -959.7367\n",
      "convergence dfGPdfNN Run 7/10, Epoch 473/1000, Training Loss (NLML): -959.7169\n",
      "convergence dfGPdfNN Run 7/10, Epoch 474/1000, Training Loss (NLML): -959.7681\n",
      "convergence dfGPdfNN Run 7/10, Epoch 475/1000, Training Loss (NLML): -959.7860\n",
      "convergence dfGPdfNN Run 7/10, Epoch 476/1000, Training Loss (NLML): -959.7662\n",
      "convergence dfGPdfNN Run 7/10, Epoch 477/1000, Training Loss (NLML): -959.7256\n",
      "convergence dfGPdfNN Run 7/10, Epoch 478/1000, Training Loss (NLML): -959.7812\n",
      "convergence dfGPdfNN Run 7/10, Epoch 479/1000, Training Loss (NLML): -959.8419\n",
      "convergence dfGPdfNN Run 7/10, Epoch 480/1000, Training Loss (NLML): -959.8469\n",
      "convergence dfGPdfNN Run 7/10, Epoch 481/1000, Training Loss (NLML): -959.8557\n",
      "convergence dfGPdfNN Run 7/10, Epoch 482/1000, Training Loss (NLML): -959.8253\n",
      "convergence dfGPdfNN Run 7/10, Epoch 483/1000, Training Loss (NLML): -959.8031\n",
      "convergence dfGPdfNN Run 7/10, Epoch 484/1000, Training Loss (NLML): -959.8126\n",
      "convergence dfGPdfNN Run 7/10, Epoch 485/1000, Training Loss (NLML): -959.7902\n",
      "convergence dfGPdfNN Run 7/10, Epoch 486/1000, Training Loss (NLML): -959.8154\n",
      "convergence dfGPdfNN Run 7/10, Epoch 487/1000, Training Loss (NLML): -959.8254\n",
      "convergence dfGPdfNN Run 7/10, Epoch 488/1000, Training Loss (NLML): -959.8390\n",
      "convergence dfGPdfNN Run 7/10, Epoch 489/1000, Training Loss (NLML): -959.8529\n",
      "convergence dfGPdfNN Run 7/10, Epoch 490/1000, Training Loss (NLML): -959.8564\n",
      "convergence dfGPdfNN Run 7/10, Epoch 491/1000, Training Loss (NLML): -959.8270\n",
      "convergence dfGPdfNN Run 7/10, Epoch 492/1000, Training Loss (NLML): -959.8517\n",
      "convergence dfGPdfNN Run 7/10, Epoch 493/1000, Training Loss (NLML): -959.8552\n",
      "convergence dfGPdfNN Run 7/10, Epoch 494/1000, Training Loss (NLML): -959.8536\n",
      "convergence dfGPdfNN Run 7/10, Epoch 495/1000, Training Loss (NLML): -959.8708\n",
      "convergence dfGPdfNN Run 7/10, Epoch 496/1000, Training Loss (NLML): -959.8750\n",
      "convergence dfGPdfNN Run 7/10, Epoch 497/1000, Training Loss (NLML): -959.8844\n",
      "convergence dfGPdfNN Run 7/10, Epoch 498/1000, Training Loss (NLML): -959.8915\n",
      "convergence dfGPdfNN Run 7/10, Epoch 499/1000, Training Loss (NLML): -959.8970\n",
      "convergence dfGPdfNN Run 7/10, Epoch 500/1000, Training Loss (NLML): -959.8949\n",
      "convergence dfGPdfNN Run 7/10, Epoch 501/1000, Training Loss (NLML): -959.9563\n",
      "convergence dfGPdfNN Run 7/10, Epoch 502/1000, Training Loss (NLML): -959.9652\n",
      "convergence dfGPdfNN Run 7/10, Epoch 503/1000, Training Loss (NLML): -959.9742\n",
      "convergence dfGPdfNN Run 7/10, Epoch 504/1000, Training Loss (NLML): -959.9857\n",
      "convergence dfGPdfNN Run 7/10, Epoch 505/1000, Training Loss (NLML): -959.9922\n",
      "convergence dfGPdfNN Run 7/10, Epoch 506/1000, Training Loss (NLML): -960.0311\n",
      "convergence dfGPdfNN Run 7/10, Epoch 507/1000, Training Loss (NLML): -960.0800\n",
      "convergence dfGPdfNN Run 7/10, Epoch 508/1000, Training Loss (NLML): -960.0793\n",
      "convergence dfGPdfNN Run 7/10, Epoch 509/1000, Training Loss (NLML): -960.0430\n",
      "convergence dfGPdfNN Run 7/10, Epoch 510/1000, Training Loss (NLML): -959.9985\n",
      "convergence dfGPdfNN Run 7/10, Epoch 511/1000, Training Loss (NLML): -959.9556\n",
      "convergence dfGPdfNN Run 7/10, Epoch 512/1000, Training Loss (NLML): -959.9225\n",
      "convergence dfGPdfNN Run 7/10, Epoch 513/1000, Training Loss (NLML): -959.9180\n",
      "convergence dfGPdfNN Run 7/10, Epoch 514/1000, Training Loss (NLML): -959.9487\n",
      "convergence dfGPdfNN Run 7/10, Epoch 515/1000, Training Loss (NLML): -959.9744\n",
      "convergence dfGPdfNN Run 7/10, Epoch 516/1000, Training Loss (NLML): -959.9845\n",
      "convergence dfGPdfNN Run 7/10, Epoch 517/1000, Training Loss (NLML): -960.0348\n",
      "convergence dfGPdfNN Run 7/10, Epoch 518/1000, Training Loss (NLML): -960.0742\n",
      "convergence dfGPdfNN Run 7/10, Epoch 519/1000, Training Loss (NLML): -960.0898\n",
      "convergence dfGPdfNN Run 7/10, Epoch 520/1000, Training Loss (NLML): -960.1193\n",
      "convergence dfGPdfNN Run 7/10, Epoch 521/1000, Training Loss (NLML): -960.1320\n",
      "convergence dfGPdfNN Run 7/10, Epoch 522/1000, Training Loss (NLML): -960.1285\n",
      "convergence dfGPdfNN Run 7/10, Epoch 523/1000, Training Loss (NLML): -960.1168\n",
      "convergence dfGPdfNN Run 7/10, Epoch 524/1000, Training Loss (NLML): -960.1096\n",
      "convergence dfGPdfNN Run 7/10, Epoch 525/1000, Training Loss (NLML): -960.0739\n",
      "convergence dfGPdfNN Run 7/10, Epoch 526/1000, Training Loss (NLML): -960.0566\n",
      "convergence dfGPdfNN Run 7/10, Epoch 527/1000, Training Loss (NLML): -960.0508\n",
      "convergence dfGPdfNN Run 7/10, Epoch 528/1000, Training Loss (NLML): -960.0580\n",
      "convergence dfGPdfNN Run 7/10, Epoch 529/1000, Training Loss (NLML): -960.0663\n",
      "convergence dfGPdfNN Run 7/10, Epoch 530/1000, Training Loss (NLML): -960.0396\n",
      "convergence dfGPdfNN Run 7/10, Epoch 531/1000, Training Loss (NLML): -960.0664\n",
      "convergence dfGPdfNN Run 7/10, Epoch 532/1000, Training Loss (NLML): -960.1055\n",
      "convergence dfGPdfNN Run 7/10, Epoch 533/1000, Training Loss (NLML): -960.1292\n",
      "convergence dfGPdfNN Run 7/10, Epoch 534/1000, Training Loss (NLML): -960.1326\n",
      "convergence dfGPdfNN Run 7/10, Epoch 535/1000, Training Loss (NLML): -960.1353\n",
      "convergence dfGPdfNN Run 7/10, Epoch 536/1000, Training Loss (NLML): -960.1465\n",
      "convergence dfGPdfNN Run 7/10, Epoch 537/1000, Training Loss (NLML): -960.1622\n",
      "convergence dfGPdfNN Run 7/10, Epoch 538/1000, Training Loss (NLML): -960.1699\n",
      "convergence dfGPdfNN Run 7/10, Epoch 539/1000, Training Loss (NLML): -960.1147\n",
      "convergence dfGPdfNN Run 7/10, Epoch 540/1000, Training Loss (NLML): -960.1056\n",
      "convergence dfGPdfNN Run 7/10, Epoch 541/1000, Training Loss (NLML): -960.1240\n",
      "convergence dfGPdfNN Run 7/10, Epoch 542/1000, Training Loss (NLML): -960.1576\n",
      "convergence dfGPdfNN Run 7/10, Epoch 543/1000, Training Loss (NLML): -960.1523\n",
      "convergence dfGPdfNN Run 7/10, Epoch 544/1000, Training Loss (NLML): -960.1550\n",
      "convergence dfGPdfNN Run 7/10, Epoch 545/1000, Training Loss (NLML): -960.1667\n",
      "convergence dfGPdfNN Run 7/10, Epoch 546/1000, Training Loss (NLML): -960.1721\n",
      "convergence dfGPdfNN Run 7/10, Epoch 547/1000, Training Loss (NLML): -960.1763\n",
      "convergence dfGPdfNN Run 7/10, Epoch 548/1000, Training Loss (NLML): -960.1772\n",
      "convergence dfGPdfNN Run 7/10, Epoch 549/1000, Training Loss (NLML): -960.1895\n",
      "convergence dfGPdfNN Run 7/10, Epoch 550/1000, Training Loss (NLML): -960.1907\n",
      "convergence dfGPdfNN Run 7/10, Epoch 551/1000, Training Loss (NLML): -960.1956\n",
      "convergence dfGPdfNN Run 7/10, Epoch 552/1000, Training Loss (NLML): -960.2003\n",
      "convergence dfGPdfNN Run 7/10, Epoch 553/1000, Training Loss (NLML): -960.2036\n",
      "convergence dfGPdfNN Run 7/10, Epoch 554/1000, Training Loss (NLML): -960.2092\n",
      "convergence dfGPdfNN Run 7/10, Epoch 555/1000, Training Loss (NLML): -960.2081\n",
      "convergence dfGPdfNN Run 7/10, Epoch 556/1000, Training Loss (NLML): -960.1973\n",
      "convergence dfGPdfNN Run 7/10, Epoch 557/1000, Training Loss (NLML): -960.1853\n",
      "convergence dfGPdfNN Run 7/10, Epoch 558/1000, Training Loss (NLML): -960.2091\n",
      "convergence dfGPdfNN Run 7/10, Epoch 559/1000, Training Loss (NLML): -960.2117\n",
      "convergence dfGPdfNN Run 7/10, Epoch 560/1000, Training Loss (NLML): -960.2166\n",
      "convergence dfGPdfNN Run 7/10, Epoch 561/1000, Training Loss (NLML): -960.2236\n",
      "convergence dfGPdfNN Run 7/10, Epoch 562/1000, Training Loss (NLML): -960.2379\n",
      "convergence dfGPdfNN Run 7/10, Epoch 563/1000, Training Loss (NLML): -960.2423\n",
      "convergence dfGPdfNN Run 7/10, Epoch 564/1000, Training Loss (NLML): -960.2472\n",
      "convergence dfGPdfNN Run 7/10, Epoch 565/1000, Training Loss (NLML): -960.2498\n",
      "convergence dfGPdfNN Run 7/10, Epoch 566/1000, Training Loss (NLML): -960.2533\n",
      "convergence dfGPdfNN Run 7/10, Epoch 567/1000, Training Loss (NLML): -960.2596\n",
      "convergence dfGPdfNN Run 7/10, Epoch 568/1000, Training Loss (NLML): -960.2289\n",
      "convergence dfGPdfNN Run 7/10, Epoch 569/1000, Training Loss (NLML): -960.2661\n",
      "convergence dfGPdfNN Run 7/10, Epoch 570/1000, Training Loss (NLML): -960.2701\n",
      "convergence dfGPdfNN Run 7/10, Epoch 571/1000, Training Loss (NLML): -960.2717\n",
      "convergence dfGPdfNN Run 7/10, Epoch 572/1000, Training Loss (NLML): -960.2755\n",
      "convergence dfGPdfNN Run 7/10, Epoch 573/1000, Training Loss (NLML): -960.2756\n",
      "convergence dfGPdfNN Run 7/10, Epoch 574/1000, Training Loss (NLML): -960.2728\n",
      "convergence dfGPdfNN Run 7/10, Epoch 575/1000, Training Loss (NLML): -960.2772\n",
      "convergence dfGPdfNN Run 7/10, Epoch 576/1000, Training Loss (NLML): -960.2809\n",
      "convergence dfGPdfNN Run 7/10, Epoch 577/1000, Training Loss (NLML): -960.2825\n",
      "convergence dfGPdfNN Run 7/10, Epoch 578/1000, Training Loss (NLML): -960.2859\n",
      "convergence dfGPdfNN Run 7/10, Epoch 579/1000, Training Loss (NLML): -960.2998\n",
      "convergence dfGPdfNN Run 7/10, Epoch 580/1000, Training Loss (NLML): -960.3031\n",
      "convergence dfGPdfNN Run 7/10, Epoch 581/1000, Training Loss (NLML): -960.3087\n",
      "convergence dfGPdfNN Run 7/10, Epoch 582/1000, Training Loss (NLML): -960.3112\n",
      "convergence dfGPdfNN Run 7/10, Epoch 583/1000, Training Loss (NLML): -960.3138\n",
      "convergence dfGPdfNN Run 7/10, Epoch 584/1000, Training Loss (NLML): -960.3083\n",
      "convergence dfGPdfNN Run 7/10, Epoch 585/1000, Training Loss (NLML): -960.3102\n",
      "convergence dfGPdfNN Run 7/10, Epoch 586/1000, Training Loss (NLML): -960.3140\n",
      "convergence dfGPdfNN Run 7/10, Epoch 587/1000, Training Loss (NLML): -960.3190\n",
      "convergence dfGPdfNN Run 7/10, Epoch 588/1000, Training Loss (NLML): -960.3214\n",
      "convergence dfGPdfNN Run 7/10, Epoch 589/1000, Training Loss (NLML): -960.3152\n",
      "convergence dfGPdfNN Run 7/10, Epoch 590/1000, Training Loss (NLML): -960.3115\n",
      "convergence dfGPdfNN Run 7/10, Epoch 591/1000, Training Loss (NLML): -960.3160\n",
      "convergence dfGPdfNN Run 7/10, Epoch 592/1000, Training Loss (NLML): -960.3210\n",
      "convergence dfGPdfNN Run 7/10, Epoch 593/1000, Training Loss (NLML): -960.3317\n",
      "convergence dfGPdfNN Run 7/10, Epoch 594/1000, Training Loss (NLML): -960.3413\n",
      "convergence dfGPdfNN Run 7/10, Epoch 595/1000, Training Loss (NLML): -960.3325\n",
      "convergence dfGPdfNN Run 7/10, Epoch 596/1000, Training Loss (NLML): -960.3549\n",
      "convergence dfGPdfNN Run 7/10, Epoch 597/1000, Training Loss (NLML): -960.3580\n",
      "convergence dfGPdfNN Run 7/10, Epoch 598/1000, Training Loss (NLML): -960.3617\n",
      "convergence dfGPdfNN Run 7/10, Epoch 599/1000, Training Loss (NLML): -960.3666\n",
      "convergence dfGPdfNN Run 7/10, Epoch 600/1000, Training Loss (NLML): -960.3676\n",
      "convergence dfGPdfNN Run 7/10, Epoch 601/1000, Training Loss (NLML): -960.3752\n",
      "convergence dfGPdfNN Run 7/10, Epoch 602/1000, Training Loss (NLML): -960.3783\n",
      "convergence dfGPdfNN Run 7/10, Epoch 603/1000, Training Loss (NLML): -960.3804\n",
      "convergence dfGPdfNN Run 7/10, Epoch 604/1000, Training Loss (NLML): -960.3853\n",
      "convergence dfGPdfNN Run 7/10, Epoch 605/1000, Training Loss (NLML): -960.3885\n",
      "convergence dfGPdfNN Run 7/10, Epoch 606/1000, Training Loss (NLML): -960.3920\n",
      "convergence dfGPdfNN Run 7/10, Epoch 607/1000, Training Loss (NLML): -960.3951\n",
      "convergence dfGPdfNN Run 7/10, Epoch 608/1000, Training Loss (NLML): -960.3978\n",
      "convergence dfGPdfNN Run 7/10, Epoch 609/1000, Training Loss (NLML): -960.3986\n",
      "convergence dfGPdfNN Run 7/10, Epoch 610/1000, Training Loss (NLML): -960.4025\n",
      "convergence dfGPdfNN Run 7/10, Epoch 611/1000, Training Loss (NLML): -960.4050\n",
      "convergence dfGPdfNN Run 7/10, Epoch 612/1000, Training Loss (NLML): -960.4088\n",
      "convergence dfGPdfNN Run 7/10, Epoch 613/1000, Training Loss (NLML): -960.4100\n",
      "convergence dfGPdfNN Run 7/10, Epoch 614/1000, Training Loss (NLML): -960.4148\n",
      "convergence dfGPdfNN Run 7/10, Epoch 615/1000, Training Loss (NLML): -960.4183\n",
      "convergence dfGPdfNN Run 7/10, Epoch 616/1000, Training Loss (NLML): -960.4138\n",
      "convergence dfGPdfNN Run 7/10, Epoch 617/1000, Training Loss (NLML): -960.4250\n",
      "convergence dfGPdfNN Run 7/10, Epoch 618/1000, Training Loss (NLML): -960.4281\n",
      "convergence dfGPdfNN Run 7/10, Epoch 619/1000, Training Loss (NLML): -960.4308\n",
      "convergence dfGPdfNN Run 7/10, Epoch 620/1000, Training Loss (NLML): -960.4335\n",
      "convergence dfGPdfNN Run 7/10, Epoch 621/1000, Training Loss (NLML): -960.4362\n",
      "convergence dfGPdfNN Run 7/10, Epoch 622/1000, Training Loss (NLML): -960.4387\n",
      "convergence dfGPdfNN Run 7/10, Epoch 623/1000, Training Loss (NLML): -960.4426\n",
      "convergence dfGPdfNN Run 7/10, Epoch 624/1000, Training Loss (NLML): -960.4451\n",
      "convergence dfGPdfNN Run 7/10, Epoch 625/1000, Training Loss (NLML): -960.4482\n",
      "convergence dfGPdfNN Run 7/10, Epoch 626/1000, Training Loss (NLML): -960.4512\n",
      "convergence dfGPdfNN Run 7/10, Epoch 627/1000, Training Loss (NLML): -960.4547\n",
      "convergence dfGPdfNN Run 7/10, Epoch 628/1000, Training Loss (NLML): -960.4592\n",
      "convergence dfGPdfNN Run 7/10, Epoch 629/1000, Training Loss (NLML): -960.4609\n",
      "convergence dfGPdfNN Run 7/10, Epoch 630/1000, Training Loss (NLML): -960.4635\n",
      "convergence dfGPdfNN Run 7/10, Epoch 631/1000, Training Loss (NLML): -960.4673\n",
      "convergence dfGPdfNN Run 7/10, Epoch 632/1000, Training Loss (NLML): -960.4695\n",
      "convergence dfGPdfNN Run 7/10, Epoch 633/1000, Training Loss (NLML): -960.4733\n",
      "convergence dfGPdfNN Run 7/10, Epoch 634/1000, Training Loss (NLML): -960.4772\n",
      "convergence dfGPdfNN Run 7/10, Epoch 635/1000, Training Loss (NLML): -960.4792\n",
      "convergence dfGPdfNN Run 7/10, Epoch 636/1000, Training Loss (NLML): -960.4834\n",
      "convergence dfGPdfNN Run 7/10, Epoch 637/1000, Training Loss (NLML): -960.4865\n",
      "convergence dfGPdfNN Run 7/10, Epoch 638/1000, Training Loss (NLML): -960.4889\n",
      "convergence dfGPdfNN Run 7/10, Epoch 639/1000, Training Loss (NLML): -960.4921\n",
      "convergence dfGPdfNN Run 7/10, Epoch 640/1000, Training Loss (NLML): -960.4941\n",
      "convergence dfGPdfNN Run 7/10, Epoch 641/1000, Training Loss (NLML): -960.4980\n",
      "convergence dfGPdfNN Run 7/10, Epoch 642/1000, Training Loss (NLML): -960.5009\n",
      "convergence dfGPdfNN Run 7/10, Epoch 643/1000, Training Loss (NLML): -960.5038\n",
      "convergence dfGPdfNN Run 7/10, Epoch 644/1000, Training Loss (NLML): -960.5055\n",
      "convergence dfGPdfNN Run 7/10, Epoch 645/1000, Training Loss (NLML): -960.5087\n",
      "convergence dfGPdfNN Run 7/10, Epoch 646/1000, Training Loss (NLML): -960.5116\n",
      "convergence dfGPdfNN Run 7/10, Epoch 647/1000, Training Loss (NLML): -960.5140\n",
      "convergence dfGPdfNN Run 7/10, Epoch 648/1000, Training Loss (NLML): -960.5179\n",
      "convergence dfGPdfNN Run 7/10, Epoch 649/1000, Training Loss (NLML): -960.5210\n",
      "convergence dfGPdfNN Run 7/10, Epoch 650/1000, Training Loss (NLML): -960.5254\n",
      "convergence dfGPdfNN Run 7/10, Epoch 651/1000, Training Loss (NLML): -960.5289\n",
      "convergence dfGPdfNN Run 7/10, Epoch 652/1000, Training Loss (NLML): -960.5304\n",
      "convergence dfGPdfNN Run 7/10, Epoch 653/1000, Training Loss (NLML): -960.5337\n",
      "convergence dfGPdfNN Run 7/10, Epoch 654/1000, Training Loss (NLML): -960.5348\n",
      "convergence dfGPdfNN Run 7/10, Epoch 655/1000, Training Loss (NLML): -960.5387\n",
      "convergence dfGPdfNN Run 7/10, Epoch 656/1000, Training Loss (NLML): -960.5416\n",
      "convergence dfGPdfNN Run 7/10, Epoch 657/1000, Training Loss (NLML): -960.5454\n",
      "convergence dfGPdfNN Run 7/10, Epoch 658/1000, Training Loss (NLML): -960.5475\n",
      "convergence dfGPdfNN Run 7/10, Epoch 659/1000, Training Loss (NLML): -960.5486\n",
      "convergence dfGPdfNN Run 7/10, Epoch 660/1000, Training Loss (NLML): -960.5532\n",
      "convergence dfGPdfNN Run 7/10, Epoch 661/1000, Training Loss (NLML): -960.5565\n",
      "convergence dfGPdfNN Run 7/10, Epoch 662/1000, Training Loss (NLML): -960.5468\n",
      "convergence dfGPdfNN Run 7/10, Epoch 663/1000, Training Loss (NLML): -960.5555\n",
      "convergence dfGPdfNN Run 7/10, Epoch 664/1000, Training Loss (NLML): -960.5597\n",
      "convergence dfGPdfNN Run 7/10, Epoch 665/1000, Training Loss (NLML): -960.5623\n",
      "convergence dfGPdfNN Run 7/10, Epoch 666/1000, Training Loss (NLML): -960.5627\n",
      "convergence dfGPdfNN Run 7/10, Epoch 667/1000, Training Loss (NLML): -960.5649\n",
      "convergence dfGPdfNN Run 7/10, Epoch 668/1000, Training Loss (NLML): -960.5656\n",
      "convergence dfGPdfNN Run 7/10, Epoch 669/1000, Training Loss (NLML): -960.5710\n",
      "convergence dfGPdfNN Run 7/10, Epoch 670/1000, Training Loss (NLML): -960.5735\n",
      "convergence dfGPdfNN Run 7/10, Epoch 671/1000, Training Loss (NLML): -960.5778\n",
      "convergence dfGPdfNN Run 7/10, Epoch 672/1000, Training Loss (NLML): -960.5797\n",
      "convergence dfGPdfNN Run 7/10, Epoch 673/1000, Training Loss (NLML): -960.5857\n",
      "convergence dfGPdfNN Run 7/10, Epoch 674/1000, Training Loss (NLML): -960.5867\n",
      "convergence dfGPdfNN Run 7/10, Epoch 675/1000, Training Loss (NLML): -960.5890\n",
      "convergence dfGPdfNN Run 7/10, Epoch 676/1000, Training Loss (NLML): -960.5925\n",
      "convergence dfGPdfNN Run 7/10, Epoch 677/1000, Training Loss (NLML): -960.5920\n",
      "convergence dfGPdfNN Run 7/10, Epoch 678/1000, Training Loss (NLML): -960.5989\n",
      "convergence dfGPdfNN Run 7/10, Epoch 679/1000, Training Loss (NLML): -960.5861\n",
      "convergence dfGPdfNN Run 7/10, Epoch 680/1000, Training Loss (NLML): -960.6060\n",
      "convergence dfGPdfNN Run 7/10, Epoch 681/1000, Training Loss (NLML): -960.6105\n",
      "convergence dfGPdfNN Run 7/10, Epoch 682/1000, Training Loss (NLML): -960.6096\n",
      "convergence dfGPdfNN Run 7/10, Epoch 683/1000, Training Loss (NLML): -960.6195\n",
      "convergence dfGPdfNN Run 7/10, Epoch 684/1000, Training Loss (NLML): -960.6230\n",
      "convergence dfGPdfNN Run 7/10, Epoch 685/1000, Training Loss (NLML): -960.6012\n",
      "convergence dfGPdfNN Run 7/10, Epoch 686/1000, Training Loss (NLML): -960.6161\n",
      "convergence dfGPdfNN Run 7/10, Epoch 687/1000, Training Loss (NLML): -960.6211\n",
      "convergence dfGPdfNN Run 7/10, Epoch 688/1000, Training Loss (NLML): -960.5999\n",
      "convergence dfGPdfNN Run 7/10, Epoch 689/1000, Training Loss (NLML): -960.6277\n",
      "convergence dfGPdfNN Run 7/10, Epoch 690/1000, Training Loss (NLML): -960.6356\n",
      "convergence dfGPdfNN Run 7/10, Epoch 691/1000, Training Loss (NLML): -960.5996\n",
      "convergence dfGPdfNN Run 7/10, Epoch 692/1000, Training Loss (NLML): -960.6151\n",
      "convergence dfGPdfNN Run 7/10, Epoch 693/1000, Training Loss (NLML): -960.6453\n",
      "convergence dfGPdfNN Run 7/10, Epoch 694/1000, Training Loss (NLML): -960.6475\n",
      "convergence dfGPdfNN Run 7/10, Epoch 695/1000, Training Loss (NLML): -960.6482\n",
      "convergence dfGPdfNN Run 7/10, Epoch 696/1000, Training Loss (NLML): -960.6162\n",
      "convergence dfGPdfNN Run 7/10, Epoch 697/1000, Training Loss (NLML): -960.6442\n",
      "convergence dfGPdfNN Run 7/10, Epoch 698/1000, Training Loss (NLML): -960.6124\n",
      "convergence dfGPdfNN Run 7/10, Epoch 699/1000, Training Loss (NLML): -960.6519\n",
      "convergence dfGPdfNN Run 7/10, Epoch 700/1000, Training Loss (NLML): -960.6434\n",
      "convergence dfGPdfNN Run 7/10, Epoch 701/1000, Training Loss (NLML): -960.6349\n",
      "convergence dfGPdfNN Run 7/10, Epoch 702/1000, Training Loss (NLML): -960.6367\n",
      "convergence dfGPdfNN Run 7/10, Epoch 703/1000, Training Loss (NLML): -960.6240\n",
      "convergence dfGPdfNN Run 7/10, Epoch 704/1000, Training Loss (NLML): -960.6506\n",
      "convergence dfGPdfNN Run 7/10, Epoch 705/1000, Training Loss (NLML): -960.6469\n",
      "convergence dfGPdfNN Run 7/10, Epoch 706/1000, Training Loss (NLML): -960.6469\n",
      "convergence dfGPdfNN Run 7/10, Epoch 707/1000, Training Loss (NLML): -960.6342\n",
      "convergence dfGPdfNN Run 7/10, Epoch 708/1000, Training Loss (NLML): -960.6279\n",
      "convergence dfGPdfNN Run 7/10, Epoch 709/1000, Training Loss (NLML): -960.6289\n",
      "convergence dfGPdfNN Run 7/10, Epoch 710/1000, Training Loss (NLML): -960.6139\n",
      "convergence dfGPdfNN Run 7/10, Epoch 711/1000, Training Loss (NLML): -960.6213\n",
      "convergence dfGPdfNN Run 7/10, Epoch 712/1000, Training Loss (NLML): -960.6234\n",
      "convergence dfGPdfNN Run 7/10, Epoch 713/1000, Training Loss (NLML): -960.6281\n",
      "convergence dfGPdfNN Run 7/10, Epoch 714/1000, Training Loss (NLML): -960.6324\n",
      "convergence dfGPdfNN Run 7/10, Epoch 715/1000, Training Loss (NLML): -960.6550\n",
      "convergence dfGPdfNN Run 7/10, Epoch 716/1000, Training Loss (NLML): -960.6610\n",
      "convergence dfGPdfNN Run 7/10, Epoch 717/1000, Training Loss (NLML): -960.6755\n",
      "convergence dfGPdfNN Run 7/10, Epoch 718/1000, Training Loss (NLML): -960.6790\n",
      "convergence dfGPdfNN Run 7/10, Epoch 719/1000, Training Loss (NLML): -960.6770\n",
      "convergence dfGPdfNN Run 7/10, Epoch 720/1000, Training Loss (NLML): -960.6591\n",
      "convergence dfGPdfNN Run 7/10, Epoch 721/1000, Training Loss (NLML): -960.6594\n",
      "convergence dfGPdfNN Run 7/10, Epoch 722/1000, Training Loss (NLML): -960.6426\n",
      "convergence dfGPdfNN Run 7/10, Epoch 723/1000, Training Loss (NLML): -960.6345\n",
      "convergence dfGPdfNN Run 7/10, Epoch 724/1000, Training Loss (NLML): -960.6803\n",
      "convergence dfGPdfNN Run 7/10, Epoch 725/1000, Training Loss (NLML): -960.6813\n",
      "convergence dfGPdfNN Run 7/10, Epoch 726/1000, Training Loss (NLML): -960.6688\n",
      "convergence dfGPdfNN Run 7/10, Epoch 727/1000, Training Loss (NLML): -960.6733\n",
      "convergence dfGPdfNN Run 7/10, Epoch 728/1000, Training Loss (NLML): -960.6759\n",
      "convergence dfGPdfNN Run 7/10, Epoch 729/1000, Training Loss (NLML): -960.6925\n",
      "convergence dfGPdfNN Run 7/10, Epoch 730/1000, Training Loss (NLML): -960.6936\n",
      "convergence dfGPdfNN Run 7/10, Epoch 731/1000, Training Loss (NLML): -960.7108\n",
      "convergence dfGPdfNN Run 7/10, Epoch 732/1000, Training Loss (NLML): -960.7133\n",
      "convergence dfGPdfNN Run 7/10, Epoch 733/1000, Training Loss (NLML): -960.7036\n",
      "convergence dfGPdfNN Run 7/10, Epoch 734/1000, Training Loss (NLML): -960.7030\n",
      "convergence dfGPdfNN Run 7/10, Epoch 735/1000, Training Loss (NLML): -960.7083\n",
      "convergence dfGPdfNN Run 7/10, Epoch 736/1000, Training Loss (NLML): -960.7095\n",
      "convergence dfGPdfNN Run 7/10, Epoch 737/1000, Training Loss (NLML): -960.7173\n",
      "convergence dfGPdfNN Run 7/10, Epoch 738/1000, Training Loss (NLML): -960.7183\n",
      "convergence dfGPdfNN Run 7/10, Epoch 739/1000, Training Loss (NLML): -960.7197\n",
      "convergence dfGPdfNN Run 7/10, Epoch 740/1000, Training Loss (NLML): -960.7194\n",
      "convergence dfGPdfNN Run 7/10, Epoch 741/1000, Training Loss (NLML): -960.7235\n",
      "convergence dfGPdfNN Run 7/10, Epoch 742/1000, Training Loss (NLML): -960.7264\n",
      "convergence dfGPdfNN Run 7/10, Epoch 743/1000, Training Loss (NLML): -960.7279\n",
      "convergence dfGPdfNN Run 7/10, Epoch 744/1000, Training Loss (NLML): -960.7317\n",
      "convergence dfGPdfNN Run 7/10, Epoch 745/1000, Training Loss (NLML): -960.7338\n",
      "convergence dfGPdfNN Run 7/10, Epoch 746/1000, Training Loss (NLML): -960.7333\n",
      "convergence dfGPdfNN Run 7/10, Epoch 747/1000, Training Loss (NLML): -960.7386\n",
      "convergence dfGPdfNN Run 7/10, Epoch 748/1000, Training Loss (NLML): -960.7404\n",
      "convergence dfGPdfNN Run 7/10, Epoch 749/1000, Training Loss (NLML): -960.7441\n",
      "convergence dfGPdfNN Run 7/10, Epoch 750/1000, Training Loss (NLML): -960.7283\n",
      "convergence dfGPdfNN Run 7/10, Epoch 751/1000, Training Loss (NLML): -960.7466\n",
      "convergence dfGPdfNN Run 7/10, Epoch 752/1000, Training Loss (NLML): -960.7498\n",
      "convergence dfGPdfNN Run 7/10, Epoch 753/1000, Training Loss (NLML): -960.7499\n",
      "convergence dfGPdfNN Run 7/10, Epoch 754/1000, Training Loss (NLML): -960.7548\n",
      "convergence dfGPdfNN Run 7/10, Epoch 755/1000, Training Loss (NLML): -960.7390\n",
      "convergence dfGPdfNN Run 7/10, Epoch 756/1000, Training Loss (NLML): -960.7589\n",
      "convergence dfGPdfNN Run 7/10, Epoch 757/1000, Training Loss (NLML): -960.7478\n",
      "convergence dfGPdfNN Run 7/10, Epoch 758/1000, Training Loss (NLML): -960.7627\n",
      "convergence dfGPdfNN Run 7/10, Epoch 759/1000, Training Loss (NLML): -960.7679\n",
      "convergence dfGPdfNN Run 7/10, Epoch 760/1000, Training Loss (NLML): -960.7700\n",
      "convergence dfGPdfNN Run 7/10, Epoch 761/1000, Training Loss (NLML): -960.7729\n",
      "convergence dfGPdfNN Run 7/10, Epoch 762/1000, Training Loss (NLML): -960.7614\n",
      "convergence dfGPdfNN Run 7/10, Epoch 763/1000, Training Loss (NLML): -960.7749\n",
      "convergence dfGPdfNN Run 7/10, Epoch 764/1000, Training Loss (NLML): -960.7793\n",
      "convergence dfGPdfNN Run 7/10, Epoch 765/1000, Training Loss (NLML): -960.7922\n",
      "convergence dfGPdfNN Run 7/10, Epoch 766/1000, Training Loss (NLML): -960.7899\n",
      "convergence dfGPdfNN Run 7/10, Epoch 767/1000, Training Loss (NLML): -960.7960\n",
      "convergence dfGPdfNN Run 7/10, Epoch 768/1000, Training Loss (NLML): -960.7859\n",
      "convergence dfGPdfNN Run 7/10, Epoch 769/1000, Training Loss (NLML): -960.7904\n",
      "convergence dfGPdfNN Run 7/10, Epoch 770/1000, Training Loss (NLML): -960.7921\n",
      "convergence dfGPdfNN Run 7/10, Epoch 771/1000, Training Loss (NLML): -960.7955\n",
      "convergence dfGPdfNN Run 7/10, Epoch 772/1000, Training Loss (NLML): -960.7827\n",
      "convergence dfGPdfNN Run 7/10, Epoch 773/1000, Training Loss (NLML): -960.7968\n",
      "convergence dfGPdfNN Run 7/10, Epoch 774/1000, Training Loss (NLML): -960.8109\n",
      "convergence dfGPdfNN Run 7/10, Epoch 775/1000, Training Loss (NLML): -960.8152\n",
      "convergence dfGPdfNN Run 7/10, Epoch 776/1000, Training Loss (NLML): -960.8169\n",
      "convergence dfGPdfNN Run 7/10, Epoch 777/1000, Training Loss (NLML): -960.8212\n",
      "convergence dfGPdfNN Run 7/10, Epoch 778/1000, Training Loss (NLML): -960.8065\n",
      "convergence dfGPdfNN Run 7/10, Epoch 779/1000, Training Loss (NLML): -960.8224\n",
      "convergence dfGPdfNN Run 7/10, Epoch 780/1000, Training Loss (NLML): -960.8011\n",
      "convergence dfGPdfNN Run 7/10, Epoch 781/1000, Training Loss (NLML): -960.8187\n",
      "convergence dfGPdfNN Run 7/10, Epoch 782/1000, Training Loss (NLML): -960.8184\n",
      "convergence dfGPdfNN Run 7/10, Epoch 783/1000, Training Loss (NLML): -960.8069\n",
      "convergence dfGPdfNN Run 7/10, Epoch 784/1000, Training Loss (NLML): -960.8098\n",
      "convergence dfGPdfNN Run 7/10, Epoch 785/1000, Training Loss (NLML): -960.8274\n",
      "convergence dfGPdfNN Run 7/10, Epoch 786/1000, Training Loss (NLML): -960.8278\n",
      "convergence dfGPdfNN Run 7/10, Epoch 787/1000, Training Loss (NLML): -960.8312\n",
      "convergence dfGPdfNN Run 7/10, Epoch 788/1000, Training Loss (NLML): -960.8447\n",
      "convergence dfGPdfNN Run 7/10, Epoch 789/1000, Training Loss (NLML): -960.8467\n",
      "convergence dfGPdfNN Run 7/10, Epoch 790/1000, Training Loss (NLML): -960.8225\n",
      "convergence dfGPdfNN Run 7/10, Epoch 791/1000, Training Loss (NLML): -960.8414\n",
      "convergence dfGPdfNN Run 7/10, Epoch 792/1000, Training Loss (NLML): -960.8420\n",
      "convergence dfGPdfNN Run 7/10, Epoch 793/1000, Training Loss (NLML): -960.8448\n",
      "convergence dfGPdfNN Run 7/10, Epoch 794/1000, Training Loss (NLML): -960.8473\n",
      "convergence dfGPdfNN Run 7/10, Epoch 795/1000, Training Loss (NLML): -960.8502\n",
      "convergence dfGPdfNN Run 7/10, Epoch 796/1000, Training Loss (NLML): -960.8608\n",
      "convergence dfGPdfNN Run 7/10, Epoch 797/1000, Training Loss (NLML): -960.8657\n",
      "convergence dfGPdfNN Run 7/10, Epoch 798/1000, Training Loss (NLML): -960.8557\n",
      "convergence dfGPdfNN Run 7/10, Epoch 799/1000, Training Loss (NLML): -960.8571\n",
      "convergence dfGPdfNN Run 7/10, Epoch 800/1000, Training Loss (NLML): -960.8610\n",
      "convergence dfGPdfNN Run 7/10, Epoch 801/1000, Training Loss (NLML): -960.8621\n",
      "convergence dfGPdfNN Run 7/10, Epoch 802/1000, Training Loss (NLML): -960.8636\n",
      "convergence dfGPdfNN Run 7/10, Epoch 803/1000, Training Loss (NLML): -960.8671\n",
      "convergence dfGPdfNN Run 7/10, Epoch 804/1000, Training Loss (NLML): -960.8541\n",
      "convergence dfGPdfNN Run 7/10, Epoch 805/1000, Training Loss (NLML): -960.8636\n",
      "convergence dfGPdfNN Run 7/10, Epoch 806/1000, Training Loss (NLML): -960.8824\n",
      "convergence dfGPdfNN Run 7/10, Epoch 807/1000, Training Loss (NLML): -960.8844\n",
      "convergence dfGPdfNN Run 7/10, Epoch 808/1000, Training Loss (NLML): -960.8846\n",
      "convergence dfGPdfNN Run 7/10, Epoch 809/1000, Training Loss (NLML): -960.8903\n",
      "convergence dfGPdfNN Run 7/10, Epoch 810/1000, Training Loss (NLML): -960.8798\n",
      "convergence dfGPdfNN Run 7/10, Epoch 811/1000, Training Loss (NLML): -960.8839\n",
      "convergence dfGPdfNN Run 7/10, Epoch 812/1000, Training Loss (NLML): -960.8716\n",
      "convergence dfGPdfNN Run 7/10, Epoch 813/1000, Training Loss (NLML): -960.8723\n",
      "convergence dfGPdfNN Run 7/10, Epoch 814/1000, Training Loss (NLML): -960.8914\n",
      "convergence dfGPdfNN Run 7/10, Epoch 815/1000, Training Loss (NLML): -960.8901\n",
      "convergence dfGPdfNN Run 7/10, Epoch 816/1000, Training Loss (NLML): -960.8944\n",
      "convergence dfGPdfNN Run 7/10, Epoch 817/1000, Training Loss (NLML): -960.9052\n",
      "convergence dfGPdfNN Run 7/10, Epoch 818/1000, Training Loss (NLML): -960.9102\n",
      "convergence dfGPdfNN Run 7/10, Epoch 819/1000, Training Loss (NLML): -960.8955\n",
      "convergence dfGPdfNN Run 7/10, Epoch 820/1000, Training Loss (NLML): -960.9108\n",
      "convergence dfGPdfNN Run 7/10, Epoch 821/1000, Training Loss (NLML): -960.9147\n",
      "convergence dfGPdfNN Run 7/10, Epoch 822/1000, Training Loss (NLML): -960.9177\n",
      "convergence dfGPdfNN Run 7/10, Epoch 823/1000, Training Loss (NLML): -960.9182\n",
      "convergence dfGPdfNN Run 7/10, Epoch 824/1000, Training Loss (NLML): -960.9233\n",
      "convergence dfGPdfNN Run 7/10, Epoch 825/1000, Training Loss (NLML): -960.9216\n",
      "convergence dfGPdfNN Run 7/10, Epoch 826/1000, Training Loss (NLML): -960.9026\n",
      "convergence dfGPdfNN Run 7/10, Epoch 827/1000, Training Loss (NLML): -960.9191\n",
      "convergence dfGPdfNN Run 7/10, Epoch 828/1000, Training Loss (NLML): -960.9188\n",
      "convergence dfGPdfNN Run 7/10, Epoch 829/1000, Training Loss (NLML): -960.9180\n",
      "convergence dfGPdfNN Run 7/10, Epoch 830/1000, Training Loss (NLML): -960.9227\n",
      "convergence dfGPdfNN Run 7/10, Epoch 831/1000, Training Loss (NLML): -960.9242\n",
      "convergence dfGPdfNN Run 7/10, Epoch 832/1000, Training Loss (NLML): -960.9305\n",
      "convergence dfGPdfNN Run 7/10, Epoch 833/1000, Training Loss (NLML): -960.9304\n",
      "convergence dfGPdfNN Run 7/10, Epoch 834/1000, Training Loss (NLML): -960.9424\n",
      "convergence dfGPdfNN Run 7/10, Epoch 835/1000, Training Loss (NLML): -960.9443\n",
      "convergence dfGPdfNN Run 7/10, Epoch 836/1000, Training Loss (NLML): -960.9438\n",
      "convergence dfGPdfNN Run 7/10, Epoch 837/1000, Training Loss (NLML): -960.9456\n",
      "convergence dfGPdfNN Run 7/10, Epoch 838/1000, Training Loss (NLML): -960.9480\n",
      "convergence dfGPdfNN Run 7/10, Epoch 839/1000, Training Loss (NLML): -960.9513\n",
      "convergence dfGPdfNN Run 7/10, Epoch 840/1000, Training Loss (NLML): -960.9531\n",
      "convergence dfGPdfNN Run 7/10, Epoch 841/1000, Training Loss (NLML): -960.9540\n",
      "convergence dfGPdfNN Run 7/10, Epoch 842/1000, Training Loss (NLML): -960.9579\n",
      "convergence dfGPdfNN Run 7/10, Epoch 843/1000, Training Loss (NLML): -960.9584\n",
      "convergence dfGPdfNN Run 7/10, Epoch 844/1000, Training Loss (NLML): -960.9614\n",
      "convergence dfGPdfNN Run 7/10, Epoch 845/1000, Training Loss (NLML): -960.9652\n",
      "convergence dfGPdfNN Run 7/10, Epoch 846/1000, Training Loss (NLML): -960.9629\n",
      "convergence dfGPdfNN Run 7/10, Epoch 847/1000, Training Loss (NLML): -960.9675\n",
      "convergence dfGPdfNN Run 7/10, Epoch 848/1000, Training Loss (NLML): -960.9688\n",
      "convergence dfGPdfNN Run 7/10, Epoch 849/1000, Training Loss (NLML): -960.9705\n",
      "convergence dfGPdfNN Run 7/10, Epoch 850/1000, Training Loss (NLML): -960.9741\n",
      "convergence dfGPdfNN Run 7/10, Epoch 851/1000, Training Loss (NLML): -960.9761\n",
      "convergence dfGPdfNN Run 7/10, Epoch 852/1000, Training Loss (NLML): -960.9761\n",
      "convergence dfGPdfNN Run 7/10, Epoch 853/1000, Training Loss (NLML): -960.9790\n",
      "convergence dfGPdfNN Run 7/10, Epoch 854/1000, Training Loss (NLML): -960.9752\n",
      "convergence dfGPdfNN Run 7/10, Epoch 855/1000, Training Loss (NLML): -960.9824\n",
      "convergence dfGPdfNN Run 7/10, Epoch 856/1000, Training Loss (NLML): -960.9839\n",
      "convergence dfGPdfNN Run 7/10, Epoch 857/1000, Training Loss (NLML): -960.9838\n",
      "convergence dfGPdfNN Run 7/10, Epoch 858/1000, Training Loss (NLML): -960.9879\n",
      "convergence dfGPdfNN Run 7/10, Epoch 859/1000, Training Loss (NLML): -960.9910\n",
      "convergence dfGPdfNN Run 7/10, Epoch 860/1000, Training Loss (NLML): -960.9915\n",
      "convergence dfGPdfNN Run 7/10, Epoch 861/1000, Training Loss (NLML): -960.9944\n",
      "convergence dfGPdfNN Run 7/10, Epoch 862/1000, Training Loss (NLML): -960.9951\n",
      "convergence dfGPdfNN Run 7/10, Epoch 863/1000, Training Loss (NLML): -960.9963\n",
      "convergence dfGPdfNN Run 7/10, Epoch 864/1000, Training Loss (NLML): -960.9968\n",
      "convergence dfGPdfNN Run 7/10, Epoch 865/1000, Training Loss (NLML): -961.0017\n",
      "convergence dfGPdfNN Run 7/10, Epoch 866/1000, Training Loss (NLML): -961.0045\n",
      "convergence dfGPdfNN Run 7/10, Epoch 867/1000, Training Loss (NLML): -961.0054\n",
      "convergence dfGPdfNN Run 7/10, Epoch 868/1000, Training Loss (NLML): -961.0073\n",
      "convergence dfGPdfNN Run 7/10, Epoch 869/1000, Training Loss (NLML): -961.0092\n",
      "convergence dfGPdfNN Run 7/10, Epoch 870/1000, Training Loss (NLML): -961.0122\n",
      "convergence dfGPdfNN Run 7/10, Epoch 871/1000, Training Loss (NLML): -961.0155\n",
      "convergence dfGPdfNN Run 7/10, Epoch 872/1000, Training Loss (NLML): -961.0166\n",
      "convergence dfGPdfNN Run 7/10, Epoch 873/1000, Training Loss (NLML): -961.0161\n",
      "convergence dfGPdfNN Run 7/10, Epoch 874/1000, Training Loss (NLML): -961.0179\n",
      "convergence dfGPdfNN Run 7/10, Epoch 875/1000, Training Loss (NLML): -961.0098\n",
      "convergence dfGPdfNN Run 7/10, Epoch 876/1000, Training Loss (NLML): -961.0106\n",
      "convergence dfGPdfNN Run 7/10, Epoch 877/1000, Training Loss (NLML): -961.0132\n",
      "convergence dfGPdfNN Run 7/10, Epoch 878/1000, Training Loss (NLML): -961.0172\n",
      "convergence dfGPdfNN Run 7/10, Epoch 879/1000, Training Loss (NLML): -961.0265\n",
      "convergence dfGPdfNN Run 7/10, Epoch 880/1000, Training Loss (NLML): -961.0295\n",
      "convergence dfGPdfNN Run 7/10, Epoch 881/1000, Training Loss (NLML): -961.0302\n",
      "convergence dfGPdfNN Run 7/10, Epoch 882/1000, Training Loss (NLML): -961.0361\n",
      "convergence dfGPdfNN Run 7/10, Epoch 883/1000, Training Loss (NLML): -961.0361\n",
      "convergence dfGPdfNN Run 7/10, Epoch 884/1000, Training Loss (NLML): -961.0371\n",
      "convergence dfGPdfNN Run 7/10, Epoch 885/1000, Training Loss (NLML): -961.0366\n",
      "convergence dfGPdfNN Run 7/10, Epoch 886/1000, Training Loss (NLML): -961.0414\n",
      "convergence dfGPdfNN Run 7/10, Epoch 887/1000, Training Loss (NLML): -961.0413\n",
      "convergence dfGPdfNN Run 7/10, Epoch 888/1000, Training Loss (NLML): -961.0369\n",
      "convergence dfGPdfNN Run 7/10, Epoch 889/1000, Training Loss (NLML): -961.0452\n",
      "convergence dfGPdfNN Run 7/10, Epoch 890/1000, Training Loss (NLML): -961.0476\n",
      "convergence dfGPdfNN Run 7/10, Epoch 891/1000, Training Loss (NLML): -961.0509\n",
      "convergence dfGPdfNN Run 7/10, Epoch 892/1000, Training Loss (NLML): -961.0513\n",
      "convergence dfGPdfNN Run 7/10, Epoch 893/1000, Training Loss (NLML): -961.0531\n",
      "convergence dfGPdfNN Run 7/10, Epoch 894/1000, Training Loss (NLML): -961.0544\n",
      "convergence dfGPdfNN Run 7/10, Epoch 895/1000, Training Loss (NLML): -961.0548\n",
      "convergence dfGPdfNN Run 7/10, Epoch 896/1000, Training Loss (NLML): -961.0592\n",
      "convergence dfGPdfNN Run 7/10, Epoch 897/1000, Training Loss (NLML): -961.0586\n",
      "convergence dfGPdfNN Run 7/10, Epoch 898/1000, Training Loss (NLML): -961.0603\n",
      "convergence dfGPdfNN Run 7/10, Epoch 899/1000, Training Loss (NLML): -961.0627\n",
      "convergence dfGPdfNN Run 7/10, Epoch 900/1000, Training Loss (NLML): -961.0629\n",
      "convergence dfGPdfNN Run 7/10, Epoch 901/1000, Training Loss (NLML): -961.0670\n",
      "convergence dfGPdfNN Run 7/10, Epoch 902/1000, Training Loss (NLML): -961.0657\n",
      "convergence dfGPdfNN Run 7/10, Epoch 903/1000, Training Loss (NLML): -961.0718\n",
      "convergence dfGPdfNN Run 7/10, Epoch 904/1000, Training Loss (NLML): -961.0739\n",
      "convergence dfGPdfNN Run 7/10, Epoch 905/1000, Training Loss (NLML): -961.0741\n",
      "convergence dfGPdfNN Run 7/10, Epoch 906/1000, Training Loss (NLML): -961.0748\n",
      "convergence dfGPdfNN Run 7/10, Epoch 907/1000, Training Loss (NLML): -961.0784\n",
      "convergence dfGPdfNN Run 7/10, Epoch 908/1000, Training Loss (NLML): -961.0778\n",
      "convergence dfGPdfNN Run 7/10, Epoch 909/1000, Training Loss (NLML): -961.0790\n",
      "convergence dfGPdfNN Run 7/10, Epoch 910/1000, Training Loss (NLML): -961.0806\n",
      "convergence dfGPdfNN Run 7/10, Epoch 911/1000, Training Loss (NLML): -961.0848\n",
      "convergence dfGPdfNN Run 7/10, Epoch 912/1000, Training Loss (NLML): -961.0853\n",
      "convergence dfGPdfNN Run 7/10, Epoch 913/1000, Training Loss (NLML): -961.0851\n",
      "convergence dfGPdfNN Run 7/10, Epoch 914/1000, Training Loss (NLML): -961.0898\n",
      "convergence dfGPdfNN Run 7/10, Epoch 915/1000, Training Loss (NLML): -961.0919\n",
      "convergence dfGPdfNN Run 7/10, Epoch 916/1000, Training Loss (NLML): -961.0914\n",
      "convergence dfGPdfNN Run 7/10, Epoch 917/1000, Training Loss (NLML): -961.0953\n",
      "convergence dfGPdfNN Run 7/10, Epoch 918/1000, Training Loss (NLML): -961.0950\n",
      "convergence dfGPdfNN Run 7/10, Epoch 919/1000, Training Loss (NLML): -961.0981\n",
      "convergence dfGPdfNN Run 7/10, Epoch 920/1000, Training Loss (NLML): -961.0999\n",
      "convergence dfGPdfNN Run 7/10, Epoch 921/1000, Training Loss (NLML): -961.1006\n",
      "convergence dfGPdfNN Run 7/10, Epoch 922/1000, Training Loss (NLML): -961.1022\n",
      "convergence dfGPdfNN Run 7/10, Epoch 923/1000, Training Loss (NLML): -961.1045\n",
      "convergence dfGPdfNN Run 7/10, Epoch 924/1000, Training Loss (NLML): -961.1079\n",
      "convergence dfGPdfNN Run 7/10, Epoch 925/1000, Training Loss (NLML): -961.1058\n",
      "convergence dfGPdfNN Run 7/10, Epoch 926/1000, Training Loss (NLML): -961.1215\n",
      "convergence dfGPdfNN Run 7/10, Epoch 927/1000, Training Loss (NLML): -961.1232\n",
      "convergence dfGPdfNN Run 7/10, Epoch 928/1000, Training Loss (NLML): -961.1268\n",
      "convergence dfGPdfNN Run 7/10, Epoch 929/1000, Training Loss (NLML): -961.1290\n",
      "convergence dfGPdfNN Run 7/10, Epoch 930/1000, Training Loss (NLML): -961.1294\n",
      "convergence dfGPdfNN Run 7/10, Epoch 931/1000, Training Loss (NLML): -961.1307\n",
      "convergence dfGPdfNN Run 7/10, Epoch 932/1000, Training Loss (NLML): -961.1342\n",
      "convergence dfGPdfNN Run 7/10, Epoch 933/1000, Training Loss (NLML): -961.1355\n",
      "convergence dfGPdfNN Run 7/10, Epoch 934/1000, Training Loss (NLML): -961.1361\n",
      "convergence dfGPdfNN Run 7/10, Epoch 935/1000, Training Loss (NLML): -961.1333\n",
      "convergence dfGPdfNN Run 7/10, Epoch 936/1000, Training Loss (NLML): -961.1362\n",
      "convergence dfGPdfNN Run 7/10, Epoch 937/1000, Training Loss (NLML): -961.1373\n",
      "convergence dfGPdfNN Run 7/10, Epoch 938/1000, Training Loss (NLML): -961.1406\n",
      "convergence dfGPdfNN Run 7/10, Epoch 939/1000, Training Loss (NLML): -961.1407\n",
      "convergence dfGPdfNN Run 7/10, Epoch 940/1000, Training Loss (NLML): -961.1444\n",
      "convergence dfGPdfNN Run 7/10, Epoch 941/1000, Training Loss (NLML): -961.1440\n",
      "convergence dfGPdfNN Run 7/10, Epoch 942/1000, Training Loss (NLML): -961.1481\n",
      "convergence dfGPdfNN Run 7/10, Epoch 943/1000, Training Loss (NLML): -961.1458\n",
      "convergence dfGPdfNN Run 7/10, Epoch 944/1000, Training Loss (NLML): -961.1479\n",
      "convergence dfGPdfNN Run 7/10, Epoch 945/1000, Training Loss (NLML): -961.1519\n",
      "convergence dfGPdfNN Run 7/10, Epoch 946/1000, Training Loss (NLML): -961.1533\n",
      "convergence dfGPdfNN Run 7/10, Epoch 947/1000, Training Loss (NLML): -961.1577\n",
      "convergence dfGPdfNN Run 7/10, Epoch 948/1000, Training Loss (NLML): -961.1565\n",
      "convergence dfGPdfNN Run 7/10, Epoch 949/1000, Training Loss (NLML): -961.1575\n",
      "convergence dfGPdfNN Run 7/10, Epoch 950/1000, Training Loss (NLML): -961.1609\n",
      "convergence dfGPdfNN Run 7/10, Epoch 951/1000, Training Loss (NLML): -961.1603\n",
      "convergence dfGPdfNN Run 7/10, Epoch 952/1000, Training Loss (NLML): -961.1639\n",
      "convergence dfGPdfNN Run 7/10, Epoch 953/1000, Training Loss (NLML): -961.1667\n",
      "convergence dfGPdfNN Run 7/10, Epoch 954/1000, Training Loss (NLML): -961.1642\n",
      "convergence dfGPdfNN Run 7/10, Epoch 955/1000, Training Loss (NLML): -961.1669\n",
      "convergence dfGPdfNN Run 7/10, Epoch 956/1000, Training Loss (NLML): -961.1676\n",
      "convergence dfGPdfNN Run 7/10, Epoch 957/1000, Training Loss (NLML): -961.1702\n",
      "convergence dfGPdfNN Run 7/10, Epoch 958/1000, Training Loss (NLML): -961.1746\n",
      "convergence dfGPdfNN Run 7/10, Epoch 959/1000, Training Loss (NLML): -961.1737\n",
      "convergence dfGPdfNN Run 7/10, Epoch 960/1000, Training Loss (NLML): -961.1768\n",
      "convergence dfGPdfNN Run 7/10, Epoch 961/1000, Training Loss (NLML): -961.1760\n",
      "convergence dfGPdfNN Run 7/10, Epoch 962/1000, Training Loss (NLML): -961.1804\n",
      "convergence dfGPdfNN Run 7/10, Epoch 963/1000, Training Loss (NLML): -961.1804\n",
      "convergence dfGPdfNN Run 7/10, Epoch 964/1000, Training Loss (NLML): -961.1820\n",
      "convergence dfGPdfNN Run 7/10, Epoch 965/1000, Training Loss (NLML): -961.1857\n",
      "convergence dfGPdfNN Run 7/10, Epoch 966/1000, Training Loss (NLML): -961.2008\n",
      "convergence dfGPdfNN Run 7/10, Epoch 967/1000, Training Loss (NLML): -961.1865\n",
      "convergence dfGPdfNN Run 7/10, Epoch 968/1000, Training Loss (NLML): -961.1859\n",
      "convergence dfGPdfNN Run 7/10, Epoch 969/1000, Training Loss (NLML): -961.1866\n",
      "convergence dfGPdfNN Run 7/10, Epoch 970/1000, Training Loss (NLML): -961.1913\n",
      "convergence dfGPdfNN Run 7/10, Epoch 971/1000, Training Loss (NLML): -961.1915\n",
      "convergence dfGPdfNN Run 7/10, Epoch 972/1000, Training Loss (NLML): -961.1936\n",
      "convergence dfGPdfNN Run 7/10, Epoch 973/1000, Training Loss (NLML): -961.1936\n",
      "convergence dfGPdfNN Run 7/10, Epoch 974/1000, Training Loss (NLML): -961.1981\n",
      "convergence dfGPdfNN Run 7/10, Epoch 975/1000, Training Loss (NLML): -961.1969\n",
      "convergence dfGPdfNN Run 7/10, Epoch 976/1000, Training Loss (NLML): -961.2030\n",
      "convergence dfGPdfNN Run 7/10, Epoch 977/1000, Training Loss (NLML): -961.2114\n",
      "convergence dfGPdfNN Run 7/10, Epoch 978/1000, Training Loss (NLML): -961.2040\n",
      "convergence dfGPdfNN Run 7/10, Epoch 979/1000, Training Loss (NLML): -961.2045\n",
      "convergence dfGPdfNN Run 7/10, Epoch 980/1000, Training Loss (NLML): -961.2144\n",
      "convergence dfGPdfNN Run 7/10, Epoch 981/1000, Training Loss (NLML): -961.2152\n",
      "convergence dfGPdfNN Run 7/10, Epoch 982/1000, Training Loss (NLML): -961.2086\n",
      "convergence dfGPdfNN Run 7/10, Epoch 983/1000, Training Loss (NLML): -961.2096\n",
      "convergence dfGPdfNN Run 7/10, Epoch 984/1000, Training Loss (NLML): -961.2202\n",
      "convergence dfGPdfNN Run 7/10, Epoch 985/1000, Training Loss (NLML): -961.2220\n",
      "convergence dfGPdfNN Run 7/10, Epoch 986/1000, Training Loss (NLML): -961.2152\n",
      "convergence dfGPdfNN Run 7/10, Epoch 987/1000, Training Loss (NLML): -961.2162\n",
      "convergence dfGPdfNN Run 7/10, Epoch 988/1000, Training Loss (NLML): -961.2184\n",
      "convergence dfGPdfNN Run 7/10, Epoch 989/1000, Training Loss (NLML): -961.2213\n",
      "convergence dfGPdfNN Run 7/10, Epoch 990/1000, Training Loss (NLML): -961.2217\n",
      "convergence dfGPdfNN Run 7/10, Epoch 991/1000, Training Loss (NLML): -961.2239\n",
      "convergence dfGPdfNN Run 7/10, Epoch 992/1000, Training Loss (NLML): -961.2242\n",
      "convergence dfGPdfNN Run 7/10, Epoch 993/1000, Training Loss (NLML): -961.2250\n",
      "convergence dfGPdfNN Run 7/10, Epoch 994/1000, Training Loss (NLML): -961.2262\n",
      "convergence dfGPdfNN Run 7/10, Epoch 995/1000, Training Loss (NLML): -961.2291\n",
      "convergence dfGPdfNN Run 7/10, Epoch 996/1000, Training Loss (NLML): -961.2390\n",
      "convergence dfGPdfNN Run 7/10, Epoch 997/1000, Training Loss (NLML): -961.2397\n",
      "convergence dfGPdfNN Run 7/10, Epoch 998/1000, Training Loss (NLML): -961.2400\n",
      "convergence dfGPdfNN Run 7/10, Epoch 999/1000, Training Loss (NLML): -961.2427\n",
      "convergence dfGPdfNN Run 7/10, Epoch 1000/1000, Training Loss (NLML): -961.2448\n",
      "\n",
      "--- Training Run 8/10 ---\n",
      "\n",
      "Start Training\n",
      "convergence dfGPdfNN Run 8/10, Epoch 1/1000, Training Loss (NLML): -902.2197\n",
      "convergence dfGPdfNN Run 8/10, Epoch 2/1000, Training Loss (NLML): -903.8193\n",
      "convergence dfGPdfNN Run 8/10, Epoch 3/1000, Training Loss (NLML): -909.5345\n",
      "convergence dfGPdfNN Run 8/10, Epoch 4/1000, Training Loss (NLML): -914.0803\n",
      "convergence dfGPdfNN Run 8/10, Epoch 5/1000, Training Loss (NLML): -917.7003\n",
      "convergence dfGPdfNN Run 8/10, Epoch 6/1000, Training Loss (NLML): -919.3894\n",
      "convergence dfGPdfNN Run 8/10, Epoch 7/1000, Training Loss (NLML): -920.6093\n",
      "convergence dfGPdfNN Run 8/10, Epoch 8/1000, Training Loss (NLML): -922.4015\n",
      "convergence dfGPdfNN Run 8/10, Epoch 9/1000, Training Loss (NLML): -923.6508\n",
      "convergence dfGPdfNN Run 8/10, Epoch 10/1000, Training Loss (NLML): -924.1084\n",
      "convergence dfGPdfNN Run 8/10, Epoch 11/1000, Training Loss (NLML): -925.1301\n",
      "convergence dfGPdfNN Run 8/10, Epoch 12/1000, Training Loss (NLML): -926.0027\n",
      "convergence dfGPdfNN Run 8/10, Epoch 13/1000, Training Loss (NLML): -927.1559\n",
      "convergence dfGPdfNN Run 8/10, Epoch 14/1000, Training Loss (NLML): -928.2465\n",
      "convergence dfGPdfNN Run 8/10, Epoch 15/1000, Training Loss (NLML): -929.2739\n",
      "convergence dfGPdfNN Run 8/10, Epoch 16/1000, Training Loss (NLML): -930.2252\n",
      "convergence dfGPdfNN Run 8/10, Epoch 17/1000, Training Loss (NLML): -931.0984\n",
      "convergence dfGPdfNN Run 8/10, Epoch 18/1000, Training Loss (NLML): -931.9172\n",
      "convergence dfGPdfNN Run 8/10, Epoch 19/1000, Training Loss (NLML): -932.6633\n",
      "convergence dfGPdfNN Run 8/10, Epoch 20/1000, Training Loss (NLML): -933.3950\n",
      "convergence dfGPdfNN Run 8/10, Epoch 21/1000, Training Loss (NLML): -934.0946\n",
      "convergence dfGPdfNN Run 8/10, Epoch 22/1000, Training Loss (NLML): -934.6799\n",
      "convergence dfGPdfNN Run 8/10, Epoch 23/1000, Training Loss (NLML): -935.2349\n",
      "convergence dfGPdfNN Run 8/10, Epoch 24/1000, Training Loss (NLML): -936.0455\n",
      "convergence dfGPdfNN Run 8/10, Epoch 25/1000, Training Loss (NLML): -936.4840\n",
      "convergence dfGPdfNN Run 8/10, Epoch 26/1000, Training Loss (NLML): -936.9470\n",
      "convergence dfGPdfNN Run 8/10, Epoch 27/1000, Training Loss (NLML): -937.5073\n",
      "convergence dfGPdfNN Run 8/10, Epoch 28/1000, Training Loss (NLML): -938.1357\n",
      "convergence dfGPdfNN Run 8/10, Epoch 29/1000, Training Loss (NLML): -938.2432\n",
      "convergence dfGPdfNN Run 8/10, Epoch 30/1000, Training Loss (NLML): -939.0446\n",
      "convergence dfGPdfNN Run 8/10, Epoch 31/1000, Training Loss (NLML): -939.7673\n",
      "convergence dfGPdfNN Run 8/10, Epoch 32/1000, Training Loss (NLML): -940.2031\n",
      "convergence dfGPdfNN Run 8/10, Epoch 33/1000, Training Loss (NLML): -940.5603\n",
      "convergence dfGPdfNN Run 8/10, Epoch 34/1000, Training Loss (NLML): -941.0662\n",
      "convergence dfGPdfNN Run 8/10, Epoch 35/1000, Training Loss (NLML): -941.4744\n",
      "convergence dfGPdfNN Run 8/10, Epoch 36/1000, Training Loss (NLML): -941.9115\n",
      "convergence dfGPdfNN Run 8/10, Epoch 37/1000, Training Loss (NLML): -942.3325\n",
      "convergence dfGPdfNN Run 8/10, Epoch 38/1000, Training Loss (NLML): -942.7668\n",
      "convergence dfGPdfNN Run 8/10, Epoch 39/1000, Training Loss (NLML): -943.1788\n",
      "convergence dfGPdfNN Run 8/10, Epoch 40/1000, Training Loss (NLML): -943.5658\n",
      "convergence dfGPdfNN Run 8/10, Epoch 41/1000, Training Loss (NLML): -943.9316\n",
      "convergence dfGPdfNN Run 8/10, Epoch 42/1000, Training Loss (NLML): -944.2812\n",
      "convergence dfGPdfNN Run 8/10, Epoch 43/1000, Training Loss (NLML): -944.6316\n",
      "convergence dfGPdfNN Run 8/10, Epoch 44/1000, Training Loss (NLML): -944.4368\n",
      "convergence dfGPdfNN Run 8/10, Epoch 45/1000, Training Loss (NLML): -945.3156\n",
      "convergence dfGPdfNN Run 8/10, Epoch 46/1000, Training Loss (NLML): -945.6615\n",
      "convergence dfGPdfNN Run 8/10, Epoch 47/1000, Training Loss (NLML): -946.0148\n",
      "convergence dfGPdfNN Run 8/10, Epoch 48/1000, Training Loss (NLML): -946.3334\n",
      "convergence dfGPdfNN Run 8/10, Epoch 49/1000, Training Loss (NLML): -946.5898\n",
      "convergence dfGPdfNN Run 8/10, Epoch 50/1000, Training Loss (NLML): -946.8759\n",
      "convergence dfGPdfNN Run 8/10, Epoch 51/1000, Training Loss (NLML): -947.2178\n",
      "convergence dfGPdfNN Run 8/10, Epoch 52/1000, Training Loss (NLML): -947.5363\n",
      "convergence dfGPdfNN Run 8/10, Epoch 53/1000, Training Loss (NLML): -945.5184\n",
      "convergence dfGPdfNN Run 8/10, Epoch 54/1000, Training Loss (NLML): -940.5493\n",
      "convergence dfGPdfNN Run 8/10, Epoch 55/1000, Training Loss (NLML): -941.9308\n",
      "convergence dfGPdfNN Run 8/10, Epoch 56/1000, Training Loss (NLML): -948.4169\n",
      "convergence dfGPdfNN Run 8/10, Epoch 57/1000, Training Loss (NLML): -948.5485\n",
      "convergence dfGPdfNN Run 8/10, Epoch 58/1000, Training Loss (NLML): -948.6898\n",
      "convergence dfGPdfNN Run 8/10, Epoch 59/1000, Training Loss (NLML): -948.8591\n",
      "convergence dfGPdfNN Run 8/10, Epoch 60/1000, Training Loss (NLML): -949.0511\n",
      "convergence dfGPdfNN Run 8/10, Epoch 61/1000, Training Loss (NLML): -949.2079\n",
      "convergence dfGPdfNN Run 8/10, Epoch 62/1000, Training Loss (NLML): -949.4261\n",
      "convergence dfGPdfNN Run 8/10, Epoch 63/1000, Training Loss (NLML): -949.5945\n",
      "convergence dfGPdfNN Run 8/10, Epoch 64/1000, Training Loss (NLML): -949.7371\n",
      "convergence dfGPdfNN Run 8/10, Epoch 65/1000, Training Loss (NLML): -949.8999\n",
      "convergence dfGPdfNN Run 8/10, Epoch 66/1000, Training Loss (NLML): -950.0948\n",
      "convergence dfGPdfNN Run 8/10, Epoch 67/1000, Training Loss (NLML): -950.2704\n",
      "convergence dfGPdfNN Run 8/10, Epoch 68/1000, Training Loss (NLML): -950.4810\n",
      "convergence dfGPdfNN Run 8/10, Epoch 69/1000, Training Loss (NLML): -950.6655\n",
      "convergence dfGPdfNN Run 8/10, Epoch 70/1000, Training Loss (NLML): -950.8312\n",
      "convergence dfGPdfNN Run 8/10, Epoch 71/1000, Training Loss (NLML): -950.1340\n",
      "convergence dfGPdfNN Run 8/10, Epoch 72/1000, Training Loss (NLML): -951.1251\n",
      "convergence dfGPdfNN Run 8/10, Epoch 73/1000, Training Loss (NLML): -938.8378\n",
      "convergence dfGPdfNN Run 8/10, Epoch 74/1000, Training Loss (NLML): -943.6316\n",
      "convergence dfGPdfNN Run 8/10, Epoch 75/1000, Training Loss (NLML): -951.4381\n",
      "convergence dfGPdfNN Run 8/10, Epoch 76/1000, Training Loss (NLML): -951.3998\n",
      "convergence dfGPdfNN Run 8/10, Epoch 77/1000, Training Loss (NLML): -951.4135\n",
      "convergence dfGPdfNN Run 8/10, Epoch 78/1000, Training Loss (NLML): -951.5570\n",
      "convergence dfGPdfNN Run 8/10, Epoch 79/1000, Training Loss (NLML): -950.6741\n",
      "convergence dfGPdfNN Run 8/10, Epoch 80/1000, Training Loss (NLML): -951.5641\n",
      "convergence dfGPdfNN Run 8/10, Epoch 81/1000, Training Loss (NLML): -951.7295\n",
      "convergence dfGPdfNN Run 8/10, Epoch 82/1000, Training Loss (NLML): -951.7491\n",
      "convergence dfGPdfNN Run 8/10, Epoch 83/1000, Training Loss (NLML): -952.0919\n",
      "convergence dfGPdfNN Run 8/10, Epoch 84/1000, Training Loss (NLML): -952.2244\n",
      "convergence dfGPdfNN Run 8/10, Epoch 85/1000, Training Loss (NLML): -952.3434\n",
      "convergence dfGPdfNN Run 8/10, Epoch 86/1000, Training Loss (NLML): -952.4675\n",
      "convergence dfGPdfNN Run 8/10, Epoch 87/1000, Training Loss (NLML): -952.5364\n",
      "convergence dfGPdfNN Run 8/10, Epoch 88/1000, Training Loss (NLML): -952.5471\n",
      "convergence dfGPdfNN Run 8/10, Epoch 89/1000, Training Loss (NLML): -952.5094\n",
      "convergence dfGPdfNN Run 8/10, Epoch 90/1000, Training Loss (NLML): -952.7803\n",
      "convergence dfGPdfNN Run 8/10, Epoch 91/1000, Training Loss (NLML): -952.9548\n",
      "convergence dfGPdfNN Run 8/10, Epoch 92/1000, Training Loss (NLML): -953.1383\n",
      "convergence dfGPdfNN Run 8/10, Epoch 93/1000, Training Loss (NLML): -953.2001\n",
      "convergence dfGPdfNN Run 8/10, Epoch 94/1000, Training Loss (NLML): -953.2285\n",
      "convergence dfGPdfNN Run 8/10, Epoch 95/1000, Training Loss (NLML): -953.1488\n",
      "convergence dfGPdfNN Run 8/10, Epoch 96/1000, Training Loss (NLML): -952.8231\n",
      "convergence dfGPdfNN Run 8/10, Epoch 97/1000, Training Loss (NLML): -953.6246\n",
      "convergence dfGPdfNN Run 8/10, Epoch 98/1000, Training Loss (NLML): -953.0575\n",
      "convergence dfGPdfNN Run 8/10, Epoch 99/1000, Training Loss (NLML): -953.8872\n",
      "convergence dfGPdfNN Run 8/10, Epoch 100/1000, Training Loss (NLML): -954.0724\n",
      "convergence dfGPdfNN Run 8/10, Epoch 101/1000, Training Loss (NLML): -954.0610\n",
      "convergence dfGPdfNN Run 8/10, Epoch 102/1000, Training Loss (NLML): -954.2435\n",
      "convergence dfGPdfNN Run 8/10, Epoch 103/1000, Training Loss (NLML): -954.5077\n",
      "convergence dfGPdfNN Run 8/10, Epoch 104/1000, Training Loss (NLML): -954.9520\n",
      "convergence dfGPdfNN Run 8/10, Epoch 105/1000, Training Loss (NLML): -949.5989\n",
      "convergence dfGPdfNN Run 8/10, Epoch 106/1000, Training Loss (NLML): -955.2684\n",
      "convergence dfGPdfNN Run 8/10, Epoch 107/1000, Training Loss (NLML): -955.2480\n",
      "convergence dfGPdfNN Run 8/10, Epoch 108/1000, Training Loss (NLML): -955.1771\n",
      "convergence dfGPdfNN Run 8/10, Epoch 109/1000, Training Loss (NLML): -955.0592\n",
      "convergence dfGPdfNN Run 8/10, Epoch 110/1000, Training Loss (NLML): -955.2096\n",
      "convergence dfGPdfNN Run 8/10, Epoch 111/1000, Training Loss (NLML): -955.2034\n",
      "convergence dfGPdfNN Run 8/10, Epoch 112/1000, Training Loss (NLML): -955.5685\n",
      "convergence dfGPdfNN Run 8/10, Epoch 113/1000, Training Loss (NLML): -955.5801\n",
      "convergence dfGPdfNN Run 8/10, Epoch 114/1000, Training Loss (NLML): -955.6573\n",
      "convergence dfGPdfNN Run 8/10, Epoch 115/1000, Training Loss (NLML): -955.7333\n",
      "convergence dfGPdfNN Run 8/10, Epoch 116/1000, Training Loss (NLML): -955.7644\n",
      "convergence dfGPdfNN Run 8/10, Epoch 117/1000, Training Loss (NLML): -955.8271\n",
      "convergence dfGPdfNN Run 8/10, Epoch 118/1000, Training Loss (NLML): -955.7274\n",
      "convergence dfGPdfNN Run 8/10, Epoch 119/1000, Training Loss (NLML): -955.8276\n",
      "convergence dfGPdfNN Run 8/10, Epoch 120/1000, Training Loss (NLML): -956.0503\n",
      "convergence dfGPdfNN Run 8/10, Epoch 121/1000, Training Loss (NLML): -956.1450\n",
      "convergence dfGPdfNN Run 8/10, Epoch 122/1000, Training Loss (NLML): -956.1990\n",
      "convergence dfGPdfNN Run 8/10, Epoch 123/1000, Training Loss (NLML): -956.1592\n",
      "convergence dfGPdfNN Run 8/10, Epoch 124/1000, Training Loss (NLML): -955.9877\n",
      "convergence dfGPdfNN Run 8/10, Epoch 125/1000, Training Loss (NLML): -956.0072\n",
      "convergence dfGPdfNN Run 8/10, Epoch 126/1000, Training Loss (NLML): -955.9717\n",
      "convergence dfGPdfNN Run 8/10, Epoch 127/1000, Training Loss (NLML): -956.0923\n",
      "convergence dfGPdfNN Run 8/10, Epoch 128/1000, Training Loss (NLML): -956.1921\n",
      "convergence dfGPdfNN Run 8/10, Epoch 129/1000, Training Loss (NLML): -956.2683\n",
      "convergence dfGPdfNN Run 8/10, Epoch 130/1000, Training Loss (NLML): -956.2977\n",
      "convergence dfGPdfNN Run 8/10, Epoch 131/1000, Training Loss (NLML): -956.2639\n",
      "convergence dfGPdfNN Run 8/10, Epoch 132/1000, Training Loss (NLML): -956.0757\n",
      "convergence dfGPdfNN Run 8/10, Epoch 133/1000, Training Loss (NLML): -956.0896\n",
      "convergence dfGPdfNN Run 8/10, Epoch 134/1000, Training Loss (NLML): -956.3475\n",
      "convergence dfGPdfNN Run 8/10, Epoch 135/1000, Training Loss (NLML): -956.4517\n",
      "convergence dfGPdfNN Run 8/10, Epoch 136/1000, Training Loss (NLML): -956.4858\n",
      "convergence dfGPdfNN Run 8/10, Epoch 137/1000, Training Loss (NLML): -956.4277\n",
      "convergence dfGPdfNN Run 8/10, Epoch 138/1000, Training Loss (NLML): -956.4139\n",
      "convergence dfGPdfNN Run 8/10, Epoch 139/1000, Training Loss (NLML): -956.3303\n",
      "convergence dfGPdfNN Run 8/10, Epoch 140/1000, Training Loss (NLML): -956.3835\n",
      "convergence dfGPdfNN Run 8/10, Epoch 141/1000, Training Loss (NLML): -956.5065\n",
      "convergence dfGPdfNN Run 8/10, Epoch 142/1000, Training Loss (NLML): -956.5275\n",
      "convergence dfGPdfNN Run 8/10, Epoch 143/1000, Training Loss (NLML): -956.5483\n",
      "convergence dfGPdfNN Run 8/10, Epoch 144/1000, Training Loss (NLML): -956.5662\n",
      "convergence dfGPdfNN Run 8/10, Epoch 145/1000, Training Loss (NLML): -956.4956\n",
      "convergence dfGPdfNN Run 8/10, Epoch 146/1000, Training Loss (NLML): -956.5021\n",
      "convergence dfGPdfNN Run 8/10, Epoch 147/1000, Training Loss (NLML): -956.5680\n",
      "convergence dfGPdfNN Run 8/10, Epoch 148/1000, Training Loss (NLML): -956.6761\n",
      "convergence dfGPdfNN Run 8/10, Epoch 149/1000, Training Loss (NLML): -956.7050\n",
      "convergence dfGPdfNN Run 8/10, Epoch 150/1000, Training Loss (NLML): -956.7183\n",
      "convergence dfGPdfNN Run 8/10, Epoch 151/1000, Training Loss (NLML): -956.7057\n",
      "convergence dfGPdfNN Run 8/10, Epoch 152/1000, Training Loss (NLML): -956.6270\n",
      "convergence dfGPdfNN Run 8/10, Epoch 153/1000, Training Loss (NLML): -956.7056\n",
      "convergence dfGPdfNN Run 8/10, Epoch 154/1000, Training Loss (NLML): -956.7478\n",
      "convergence dfGPdfNN Run 8/10, Epoch 155/1000, Training Loss (NLML): -956.8605\n",
      "convergence dfGPdfNN Run 8/10, Epoch 156/1000, Training Loss (NLML): -956.8745\n",
      "convergence dfGPdfNN Run 8/10, Epoch 157/1000, Training Loss (NLML): -956.9724\n",
      "convergence dfGPdfNN Run 8/10, Epoch 158/1000, Training Loss (NLML): -956.9279\n",
      "convergence dfGPdfNN Run 8/10, Epoch 159/1000, Training Loss (NLML): -956.8578\n",
      "convergence dfGPdfNN Run 8/10, Epoch 160/1000, Training Loss (NLML): -956.8698\n",
      "convergence dfGPdfNN Run 8/10, Epoch 161/1000, Training Loss (NLML): -956.8678\n",
      "convergence dfGPdfNN Run 8/10, Epoch 162/1000, Training Loss (NLML): -956.9874\n",
      "convergence dfGPdfNN Run 8/10, Epoch 163/1000, Training Loss (NLML): -957.0828\n",
      "convergence dfGPdfNN Run 8/10, Epoch 164/1000, Training Loss (NLML): -957.1619\n",
      "convergence dfGPdfNN Run 8/10, Epoch 165/1000, Training Loss (NLML): -957.1412\n",
      "convergence dfGPdfNN Run 8/10, Epoch 166/1000, Training Loss (NLML): -956.9700\n",
      "convergence dfGPdfNN Run 8/10, Epoch 167/1000, Training Loss (NLML): -956.9395\n",
      "convergence dfGPdfNN Run 8/10, Epoch 168/1000, Training Loss (NLML): -957.0570\n",
      "convergence dfGPdfNN Run 8/10, Epoch 169/1000, Training Loss (NLML): -957.1670\n",
      "convergence dfGPdfNN Run 8/10, Epoch 170/1000, Training Loss (NLML): -957.2421\n",
      "convergence dfGPdfNN Run 8/10, Epoch 171/1000, Training Loss (NLML): -957.2759\n",
      "convergence dfGPdfNN Run 8/10, Epoch 172/1000, Training Loss (NLML): -957.2175\n",
      "convergence dfGPdfNN Run 8/10, Epoch 173/1000, Training Loss (NLML): -957.1238\n",
      "convergence dfGPdfNN Run 8/10, Epoch 174/1000, Training Loss (NLML): -957.1409\n",
      "convergence dfGPdfNN Run 8/10, Epoch 175/1000, Training Loss (NLML): -957.1687\n",
      "convergence dfGPdfNN Run 8/10, Epoch 176/1000, Training Loss (NLML): -957.1552\n",
      "convergence dfGPdfNN Run 8/10, Epoch 177/1000, Training Loss (NLML): -957.2854\n",
      "convergence dfGPdfNN Run 8/10, Epoch 178/1000, Training Loss (NLML): -957.3140\n",
      "convergence dfGPdfNN Run 8/10, Epoch 179/1000, Training Loss (NLML): -957.3263\n",
      "convergence dfGPdfNN Run 8/10, Epoch 180/1000, Training Loss (NLML): -957.2891\n",
      "convergence dfGPdfNN Run 8/10, Epoch 181/1000, Training Loss (NLML): -957.2994\n",
      "convergence dfGPdfNN Run 8/10, Epoch 182/1000, Training Loss (NLML): -957.3175\n",
      "convergence dfGPdfNN Run 8/10, Epoch 183/1000, Training Loss (NLML): -957.2756\n",
      "convergence dfGPdfNN Run 8/10, Epoch 184/1000, Training Loss (NLML): -957.3977\n",
      "convergence dfGPdfNN Run 8/10, Epoch 185/1000, Training Loss (NLML): -957.4716\n",
      "convergence dfGPdfNN Run 8/10, Epoch 186/1000, Training Loss (NLML): -957.4327\n",
      "convergence dfGPdfNN Run 8/10, Epoch 187/1000, Training Loss (NLML): -957.3364\n",
      "convergence dfGPdfNN Run 8/10, Epoch 188/1000, Training Loss (NLML): -957.2671\n",
      "convergence dfGPdfNN Run 8/10, Epoch 189/1000, Training Loss (NLML): -957.3092\n",
      "convergence dfGPdfNN Run 8/10, Epoch 190/1000, Training Loss (NLML): -957.4106\n",
      "convergence dfGPdfNN Run 8/10, Epoch 191/1000, Training Loss (NLML): -957.4161\n",
      "convergence dfGPdfNN Run 8/10, Epoch 192/1000, Training Loss (NLML): -957.4260\n",
      "convergence dfGPdfNN Run 8/10, Epoch 193/1000, Training Loss (NLML): -957.4122\n",
      "convergence dfGPdfNN Run 8/10, Epoch 194/1000, Training Loss (NLML): -957.4114\n",
      "convergence dfGPdfNN Run 8/10, Epoch 195/1000, Training Loss (NLML): -957.4327\n",
      "convergence dfGPdfNN Run 8/10, Epoch 196/1000, Training Loss (NLML): -957.4637\n",
      "convergence dfGPdfNN Run 8/10, Epoch 197/1000, Training Loss (NLML): -957.5696\n",
      "convergence dfGPdfNN Run 8/10, Epoch 198/1000, Training Loss (NLML): -957.6763\n",
      "convergence dfGPdfNN Run 8/10, Epoch 199/1000, Training Loss (NLML): -957.7515\n",
      "convergence dfGPdfNN Run 8/10, Epoch 200/1000, Training Loss (NLML): -957.7198\n",
      "convergence dfGPdfNN Run 8/10, Epoch 201/1000, Training Loss (NLML): -957.6085\n",
      "convergence dfGPdfNN Run 8/10, Epoch 202/1000, Training Loss (NLML): -957.6200\n",
      "convergence dfGPdfNN Run 8/10, Epoch 203/1000, Training Loss (NLML): -957.6497\n",
      "convergence dfGPdfNN Run 8/10, Epoch 204/1000, Training Loss (NLML): -957.6765\n",
      "convergence dfGPdfNN Run 8/10, Epoch 205/1000, Training Loss (NLML): -957.6849\n",
      "convergence dfGPdfNN Run 8/10, Epoch 206/1000, Training Loss (NLML): -957.7507\n",
      "convergence dfGPdfNN Run 8/10, Epoch 207/1000, Training Loss (NLML): -957.7068\n",
      "convergence dfGPdfNN Run 8/10, Epoch 208/1000, Training Loss (NLML): -957.7515\n",
      "convergence dfGPdfNN Run 8/10, Epoch 209/1000, Training Loss (NLML): -957.7109\n",
      "convergence dfGPdfNN Run 8/10, Epoch 210/1000, Training Loss (NLML): -957.7244\n",
      "convergence dfGPdfNN Run 8/10, Epoch 211/1000, Training Loss (NLML): -957.8038\n",
      "convergence dfGPdfNN Run 8/10, Epoch 212/1000, Training Loss (NLML): -957.9415\n",
      "convergence dfGPdfNN Run 8/10, Epoch 213/1000, Training Loss (NLML): -957.9622\n",
      "convergence dfGPdfNN Run 8/10, Epoch 214/1000, Training Loss (NLML): -957.9862\n",
      "convergence dfGPdfNN Run 8/10, Epoch 215/1000, Training Loss (NLML): -957.9418\n",
      "convergence dfGPdfNN Run 8/10, Epoch 216/1000, Training Loss (NLML): -957.8511\n",
      "convergence dfGPdfNN Run 8/10, Epoch 217/1000, Training Loss (NLML): -957.8784\n",
      "convergence dfGPdfNN Run 8/10, Epoch 218/1000, Training Loss (NLML): -958.0596\n",
      "convergence dfGPdfNN Run 8/10, Epoch 219/1000, Training Loss (NLML): -958.0652\n",
      "convergence dfGPdfNN Run 8/10, Epoch 220/1000, Training Loss (NLML): -958.0742\n",
      "convergence dfGPdfNN Run 8/10, Epoch 221/1000, Training Loss (NLML): -958.0474\n",
      "convergence dfGPdfNN Run 8/10, Epoch 222/1000, Training Loss (NLML): -957.9720\n",
      "convergence dfGPdfNN Run 8/10, Epoch 223/1000, Training Loss (NLML): -957.9868\n",
      "convergence dfGPdfNN Run 8/10, Epoch 224/1000, Training Loss (NLML): -958.0031\n",
      "convergence dfGPdfNN Run 8/10, Epoch 225/1000, Training Loss (NLML): -958.1030\n",
      "convergence dfGPdfNN Run 8/10, Epoch 226/1000, Training Loss (NLML): -958.1202\n",
      "convergence dfGPdfNN Run 8/10, Epoch 227/1000, Training Loss (NLML): -958.1445\n",
      "convergence dfGPdfNN Run 8/10, Epoch 228/1000, Training Loss (NLML): -958.1442\n",
      "convergence dfGPdfNN Run 8/10, Epoch 229/1000, Training Loss (NLML): -958.1382\n",
      "convergence dfGPdfNN Run 8/10, Epoch 230/1000, Training Loss (NLML): -958.1506\n",
      "convergence dfGPdfNN Run 8/10, Epoch 231/1000, Training Loss (NLML): -958.1880\n",
      "convergence dfGPdfNN Run 8/10, Epoch 232/1000, Training Loss (NLML): -958.1915\n",
      "convergence dfGPdfNN Run 8/10, Epoch 233/1000, Training Loss (NLML): -958.1952\n",
      "convergence dfGPdfNN Run 8/10, Epoch 234/1000, Training Loss (NLML): -958.2163\n",
      "convergence dfGPdfNN Run 8/10, Epoch 235/1000, Training Loss (NLML): -958.2212\n",
      "convergence dfGPdfNN Run 8/10, Epoch 236/1000, Training Loss (NLML): -958.2329\n",
      "convergence dfGPdfNN Run 8/10, Epoch 237/1000, Training Loss (NLML): -958.2428\n",
      "convergence dfGPdfNN Run 8/10, Epoch 238/1000, Training Loss (NLML): -958.2623\n",
      "convergence dfGPdfNN Run 8/10, Epoch 239/1000, Training Loss (NLML): -958.2896\n",
      "convergence dfGPdfNN Run 8/10, Epoch 240/1000, Training Loss (NLML): -958.3041\n",
      "convergence dfGPdfNN Run 8/10, Epoch 241/1000, Training Loss (NLML): -958.3282\n",
      "convergence dfGPdfNN Run 8/10, Epoch 242/1000, Training Loss (NLML): -958.3479\n",
      "convergence dfGPdfNN Run 8/10, Epoch 243/1000, Training Loss (NLML): -958.3679\n",
      "convergence dfGPdfNN Run 8/10, Epoch 244/1000, Training Loss (NLML): -958.3801\n",
      "convergence dfGPdfNN Run 8/10, Epoch 245/1000, Training Loss (NLML): -958.3954\n",
      "convergence dfGPdfNN Run 8/10, Epoch 246/1000, Training Loss (NLML): -958.3464\n",
      "convergence dfGPdfNN Run 8/10, Epoch 247/1000, Training Loss (NLML): -958.3525\n",
      "convergence dfGPdfNN Run 8/10, Epoch 248/1000, Training Loss (NLML): -958.4227\n",
      "convergence dfGPdfNN Run 8/10, Epoch 249/1000, Training Loss (NLML): -958.4321\n",
      "convergence dfGPdfNN Run 8/10, Epoch 250/1000, Training Loss (NLML): -958.4019\n",
      "convergence dfGPdfNN Run 8/10, Epoch 251/1000, Training Loss (NLML): -958.3900\n",
      "convergence dfGPdfNN Run 8/10, Epoch 252/1000, Training Loss (NLML): -958.4677\n",
      "convergence dfGPdfNN Run 8/10, Epoch 253/1000, Training Loss (NLML): -958.4368\n",
      "convergence dfGPdfNN Run 8/10, Epoch 254/1000, Training Loss (NLML): -958.4927\n",
      "convergence dfGPdfNN Run 8/10, Epoch 255/1000, Training Loss (NLML): -958.4294\n",
      "convergence dfGPdfNN Run 8/10, Epoch 256/1000, Training Loss (NLML): -958.4812\n",
      "convergence dfGPdfNN Run 8/10, Epoch 257/1000, Training Loss (NLML): -958.5452\n",
      "convergence dfGPdfNN Run 8/10, Epoch 258/1000, Training Loss (NLML): -958.5787\n",
      "convergence dfGPdfNN Run 8/10, Epoch 259/1000, Training Loss (NLML): -958.5975\n",
      "convergence dfGPdfNN Run 8/10, Epoch 260/1000, Training Loss (NLML): -958.5524\n",
      "convergence dfGPdfNN Run 8/10, Epoch 261/1000, Training Loss (NLML): -958.5704\n",
      "convergence dfGPdfNN Run 8/10, Epoch 262/1000, Training Loss (NLML): -958.5773\n",
      "convergence dfGPdfNN Run 8/10, Epoch 263/1000, Training Loss (NLML): -958.5826\n",
      "convergence dfGPdfNN Run 8/10, Epoch 264/1000, Training Loss (NLML): -958.5983\n",
      "convergence dfGPdfNN Run 8/10, Epoch 265/1000, Training Loss (NLML): -958.5538\n",
      "convergence dfGPdfNN Run 8/10, Epoch 266/1000, Training Loss (NLML): -958.5732\n",
      "convergence dfGPdfNN Run 8/10, Epoch 267/1000, Training Loss (NLML): -958.5833\n",
      "convergence dfGPdfNN Run 8/10, Epoch 268/1000, Training Loss (NLML): -958.6426\n",
      "convergence dfGPdfNN Run 8/10, Epoch 269/1000, Training Loss (NLML): -958.6531\n",
      "convergence dfGPdfNN Run 8/10, Epoch 270/1000, Training Loss (NLML): -958.7047\n",
      "convergence dfGPdfNN Run 8/10, Epoch 271/1000, Training Loss (NLML): -958.7180\n",
      "convergence dfGPdfNN Run 8/10, Epoch 272/1000, Training Loss (NLML): -958.6794\n",
      "convergence dfGPdfNN Run 8/10, Epoch 273/1000, Training Loss (NLML): -958.6925\n",
      "convergence dfGPdfNN Run 8/10, Epoch 274/1000, Training Loss (NLML): -958.7184\n",
      "convergence dfGPdfNN Run 8/10, Epoch 275/1000, Training Loss (NLML): -958.6490\n",
      "convergence dfGPdfNN Run 8/10, Epoch 276/1000, Training Loss (NLML): -958.7415\n",
      "convergence dfGPdfNN Run 8/10, Epoch 277/1000, Training Loss (NLML): -958.7474\n",
      "convergence dfGPdfNN Run 8/10, Epoch 278/1000, Training Loss (NLML): -958.6913\n",
      "convergence dfGPdfNN Run 8/10, Epoch 279/1000, Training Loss (NLML): -958.7498\n",
      "convergence dfGPdfNN Run 8/10, Epoch 280/1000, Training Loss (NLML): -958.7167\n",
      "convergence dfGPdfNN Run 8/10, Epoch 281/1000, Training Loss (NLML): -958.7268\n",
      "convergence dfGPdfNN Run 8/10, Epoch 282/1000, Training Loss (NLML): -958.8145\n",
      "convergence dfGPdfNN Run 8/10, Epoch 283/1000, Training Loss (NLML): -958.8123\n",
      "convergence dfGPdfNN Run 8/10, Epoch 284/1000, Training Loss (NLML): -958.8234\n",
      "convergence dfGPdfNN Run 8/10, Epoch 285/1000, Training Loss (NLML): -958.8319\n",
      "convergence dfGPdfNN Run 8/10, Epoch 286/1000, Training Loss (NLML): -958.8412\n",
      "convergence dfGPdfNN Run 8/10, Epoch 287/1000, Training Loss (NLML): -958.8495\n",
      "convergence dfGPdfNN Run 8/10, Epoch 288/1000, Training Loss (NLML): -958.8679\n",
      "convergence dfGPdfNN Run 8/10, Epoch 289/1000, Training Loss (NLML): -958.8790\n",
      "convergence dfGPdfNN Run 8/10, Epoch 290/1000, Training Loss (NLML): -958.9196\n",
      "convergence dfGPdfNN Run 8/10, Epoch 291/1000, Training Loss (NLML): -958.8892\n",
      "convergence dfGPdfNN Run 8/10, Epoch 292/1000, Training Loss (NLML): -958.8999\n",
      "convergence dfGPdfNN Run 8/10, Epoch 293/1000, Training Loss (NLML): -958.8434\n",
      "convergence dfGPdfNN Run 8/10, Epoch 294/1000, Training Loss (NLML): -958.8568\n",
      "convergence dfGPdfNN Run 8/10, Epoch 295/1000, Training Loss (NLML): -958.9401\n",
      "convergence dfGPdfNN Run 8/10, Epoch 296/1000, Training Loss (NLML): -958.9340\n",
      "convergence dfGPdfNN Run 8/10, Epoch 297/1000, Training Loss (NLML): -958.9427\n",
      "convergence dfGPdfNN Run 8/10, Epoch 298/1000, Training Loss (NLML): -958.9548\n",
      "convergence dfGPdfNN Run 8/10, Epoch 299/1000, Training Loss (NLML): -958.9181\n",
      "convergence dfGPdfNN Run 8/10, Epoch 300/1000, Training Loss (NLML): -958.9028\n",
      "convergence dfGPdfNN Run 8/10, Epoch 301/1000, Training Loss (NLML): -958.9836\n",
      "convergence dfGPdfNN Run 8/10, Epoch 302/1000, Training Loss (NLML): -958.9930\n",
      "convergence dfGPdfNN Run 8/10, Epoch 303/1000, Training Loss (NLML): -959.0372\n",
      "convergence dfGPdfNN Run 8/10, Epoch 304/1000, Training Loss (NLML): -959.0134\n",
      "convergence dfGPdfNN Run 8/10, Epoch 305/1000, Training Loss (NLML): -959.0214\n",
      "convergence dfGPdfNN Run 8/10, Epoch 306/1000, Training Loss (NLML): -959.0292\n",
      "convergence dfGPdfNN Run 8/10, Epoch 307/1000, Training Loss (NLML): -958.9674\n",
      "convergence dfGPdfNN Run 8/10, Epoch 308/1000, Training Loss (NLML): -959.0430\n",
      "convergence dfGPdfNN Run 8/10, Epoch 309/1000, Training Loss (NLML): -959.0490\n",
      "convergence dfGPdfNN Run 8/10, Epoch 310/1000, Training Loss (NLML): -959.0824\n",
      "convergence dfGPdfNN Run 8/10, Epoch 311/1000, Training Loss (NLML): -959.0914\n",
      "convergence dfGPdfNN Run 8/10, Epoch 312/1000, Training Loss (NLML): -959.0848\n",
      "convergence dfGPdfNN Run 8/10, Epoch 313/1000, Training Loss (NLML): -959.0936\n",
      "convergence dfGPdfNN Run 8/10, Epoch 314/1000, Training Loss (NLML): -959.0298\n",
      "convergence dfGPdfNN Run 8/10, Epoch 315/1000, Training Loss (NLML): -959.1130\n",
      "convergence dfGPdfNN Run 8/10, Epoch 316/1000, Training Loss (NLML): -959.1212\n",
      "convergence dfGPdfNN Run 8/10, Epoch 317/1000, Training Loss (NLML): -959.1289\n",
      "convergence dfGPdfNN Run 8/10, Epoch 318/1000, Training Loss (NLML): -959.1370\n",
      "convergence dfGPdfNN Run 8/10, Epoch 319/1000, Training Loss (NLML): -959.1807\n",
      "convergence dfGPdfNN Run 8/10, Epoch 320/1000, Training Loss (NLML): -959.1680\n",
      "convergence dfGPdfNN Run 8/10, Epoch 321/1000, Training Loss (NLML): -959.1766\n",
      "convergence dfGPdfNN Run 8/10, Epoch 322/1000, Training Loss (NLML): -959.1840\n",
      "convergence dfGPdfNN Run 8/10, Epoch 323/1000, Training Loss (NLML): -959.1919\n",
      "convergence dfGPdfNN Run 8/10, Epoch 324/1000, Training Loss (NLML): -959.2009\n",
      "convergence dfGPdfNN Run 8/10, Epoch 325/1000, Training Loss (NLML): -959.2463\n",
      "convergence dfGPdfNN Run 8/10, Epoch 326/1000, Training Loss (NLML): -959.2184\n",
      "convergence dfGPdfNN Run 8/10, Epoch 327/1000, Training Loss (NLML): -959.2258\n",
      "convergence dfGPdfNN Run 8/10, Epoch 328/1000, Training Loss (NLML): -959.2726\n",
      "convergence dfGPdfNN Run 8/10, Epoch 329/1000, Training Loss (NLML): -959.2406\n",
      "convergence dfGPdfNN Run 8/10, Epoch 330/1000, Training Loss (NLML): -959.2490\n",
      "convergence dfGPdfNN Run 8/10, Epoch 331/1000, Training Loss (NLML): -959.2578\n",
      "convergence dfGPdfNN Run 8/10, Epoch 332/1000, Training Loss (NLML): -959.2659\n",
      "convergence dfGPdfNN Run 8/10, Epoch 333/1000, Training Loss (NLML): -959.2744\n",
      "convergence dfGPdfNN Run 8/10, Epoch 334/1000, Training Loss (NLML): -959.3147\n",
      "convergence dfGPdfNN Run 8/10, Epoch 335/1000, Training Loss (NLML): -959.2894\n",
      "convergence dfGPdfNN Run 8/10, Epoch 336/1000, Training Loss (NLML): -959.2991\n",
      "convergence dfGPdfNN Run 8/10, Epoch 337/1000, Training Loss (NLML): -959.3058\n",
      "convergence dfGPdfNN Run 8/10, Epoch 338/1000, Training Loss (NLML): -959.3121\n",
      "convergence dfGPdfNN Run 8/10, Epoch 339/1000, Training Loss (NLML): -959.3235\n",
      "convergence dfGPdfNN Run 8/10, Epoch 340/1000, Training Loss (NLML): -959.3159\n",
      "convergence dfGPdfNN Run 8/10, Epoch 341/1000, Training Loss (NLML): -959.3252\n",
      "convergence dfGPdfNN Run 8/10, Epoch 342/1000, Training Loss (NLML): -959.3330\n",
      "convergence dfGPdfNN Run 8/10, Epoch 343/1000, Training Loss (NLML): -959.3379\n",
      "convergence dfGPdfNN Run 8/10, Epoch 344/1000, Training Loss (NLML): -959.3462\n",
      "convergence dfGPdfNN Run 8/10, Epoch 345/1000, Training Loss (NLML): -959.3558\n",
      "convergence dfGPdfNN Run 8/10, Epoch 346/1000, Training Loss (NLML): -959.3632\n",
      "convergence dfGPdfNN Run 8/10, Epoch 347/1000, Training Loss (NLML): -959.3065\n",
      "convergence dfGPdfNN Run 8/10, Epoch 348/1000, Training Loss (NLML): -959.3765\n",
      "convergence dfGPdfNN Run 8/10, Epoch 349/1000, Training Loss (NLML): -959.4026\n",
      "convergence dfGPdfNN Run 8/10, Epoch 350/1000, Training Loss (NLML): -959.4131\n",
      "convergence dfGPdfNN Run 8/10, Epoch 351/1000, Training Loss (NLML): -959.3998\n",
      "convergence dfGPdfNN Run 8/10, Epoch 352/1000, Training Loss (NLML): -959.4088\n",
      "convergence dfGPdfNN Run 8/10, Epoch 353/1000, Training Loss (NLML): -959.4154\n",
      "convergence dfGPdfNN Run 8/10, Epoch 354/1000, Training Loss (NLML): -959.4209\n",
      "convergence dfGPdfNN Run 8/10, Epoch 355/1000, Training Loss (NLML): -959.4298\n",
      "convergence dfGPdfNN Run 8/10, Epoch 356/1000, Training Loss (NLML): -959.4354\n",
      "convergence dfGPdfNN Run 8/10, Epoch 357/1000, Training Loss (NLML): -959.4436\n",
      "convergence dfGPdfNN Run 8/10, Epoch 358/1000, Training Loss (NLML): -959.4503\n",
      "convergence dfGPdfNN Run 8/10, Epoch 359/1000, Training Loss (NLML): -959.4711\n",
      "convergence dfGPdfNN Run 8/10, Epoch 360/1000, Training Loss (NLML): -959.4801\n",
      "convergence dfGPdfNN Run 8/10, Epoch 361/1000, Training Loss (NLML): -959.4858\n",
      "convergence dfGPdfNN Run 8/10, Epoch 362/1000, Training Loss (NLML): -959.4890\n",
      "convergence dfGPdfNN Run 8/10, Epoch 363/1000, Training Loss (NLML): -959.4987\n",
      "convergence dfGPdfNN Run 8/10, Epoch 364/1000, Training Loss (NLML): -959.5059\n",
      "convergence dfGPdfNN Run 8/10, Epoch 365/1000, Training Loss (NLML): -959.5117\n",
      "convergence dfGPdfNN Run 8/10, Epoch 366/1000, Training Loss (NLML): -959.5178\n",
      "convergence dfGPdfNN Run 8/10, Epoch 367/1000, Training Loss (NLML): -959.5596\n",
      "convergence dfGPdfNN Run 8/10, Epoch 368/1000, Training Loss (NLML): -959.5670\n",
      "convergence dfGPdfNN Run 8/10, Epoch 369/1000, Training Loss (NLML): -959.5382\n",
      "convergence dfGPdfNN Run 8/10, Epoch 370/1000, Training Loss (NLML): -959.5441\n",
      "convergence dfGPdfNN Run 8/10, Epoch 371/1000, Training Loss (NLML): -959.5510\n",
      "convergence dfGPdfNN Run 8/10, Epoch 372/1000, Training Loss (NLML): -959.5994\n",
      "convergence dfGPdfNN Run 8/10, Epoch 373/1000, Training Loss (NLML): -959.6035\n",
      "convergence dfGPdfNN Run 8/10, Epoch 374/1000, Training Loss (NLML): -959.5751\n",
      "convergence dfGPdfNN Run 8/10, Epoch 375/1000, Training Loss (NLML): -959.5808\n",
      "convergence dfGPdfNN Run 8/10, Epoch 376/1000, Training Loss (NLML): -959.5880\n",
      "convergence dfGPdfNN Run 8/10, Epoch 377/1000, Training Loss (NLML): -959.5815\n",
      "convergence dfGPdfNN Run 8/10, Epoch 378/1000, Training Loss (NLML): -959.6215\n",
      "convergence dfGPdfNN Run 8/10, Epoch 379/1000, Training Loss (NLML): -959.5939\n",
      "convergence dfGPdfNN Run 8/10, Epoch 380/1000, Training Loss (NLML): -959.6018\n",
      "convergence dfGPdfNN Run 8/10, Epoch 381/1000, Training Loss (NLML): -959.6171\n",
      "convergence dfGPdfNN Run 8/10, Epoch 382/1000, Training Loss (NLML): -959.6145\n",
      "convergence dfGPdfNN Run 8/10, Epoch 383/1000, Training Loss (NLML): -959.6528\n",
      "convergence dfGPdfNN Run 8/10, Epoch 384/1000, Training Loss (NLML): -959.6616\n",
      "convergence dfGPdfNN Run 8/10, Epoch 385/1000, Training Loss (NLML): -959.6310\n",
      "convergence dfGPdfNN Run 8/10, Epoch 386/1000, Training Loss (NLML): -959.6361\n",
      "convergence dfGPdfNN Run 8/10, Epoch 387/1000, Training Loss (NLML): -959.6429\n",
      "convergence dfGPdfNN Run 8/10, Epoch 388/1000, Training Loss (NLML): -959.6851\n",
      "convergence dfGPdfNN Run 8/10, Epoch 389/1000, Training Loss (NLML): -959.7247\n",
      "convergence dfGPdfNN Run 8/10, Epoch 390/1000, Training Loss (NLML): -959.6879\n",
      "convergence dfGPdfNN Run 8/10, Epoch 391/1000, Training Loss (NLML): -959.6953\n",
      "convergence dfGPdfNN Run 8/10, Epoch 392/1000, Training Loss (NLML): -959.6877\n",
      "convergence dfGPdfNN Run 8/10, Epoch 393/1000, Training Loss (NLML): -959.6948\n",
      "convergence dfGPdfNN Run 8/10, Epoch 394/1000, Training Loss (NLML): -959.7633\n",
      "convergence dfGPdfNN Run 8/10, Epoch 395/1000, Training Loss (NLML): -959.7705\n",
      "convergence dfGPdfNN Run 8/10, Epoch 396/1000, Training Loss (NLML): -959.7821\n",
      "convergence dfGPdfNN Run 8/10, Epoch 397/1000, Training Loss (NLML): -959.7911\n",
      "convergence dfGPdfNN Run 8/10, Epoch 398/1000, Training Loss (NLML): -959.7428\n",
      "convergence dfGPdfNN Run 8/10, Epoch 399/1000, Training Loss (NLML): -959.7478\n",
      "convergence dfGPdfNN Run 8/10, Epoch 400/1000, Training Loss (NLML): -959.8105\n",
      "convergence dfGPdfNN Run 8/10, Epoch 401/1000, Training Loss (NLML): -959.8158\n",
      "convergence dfGPdfNN Run 8/10, Epoch 402/1000, Training Loss (NLML): -959.8202\n",
      "convergence dfGPdfNN Run 8/10, Epoch 403/1000, Training Loss (NLML): -959.8239\n",
      "convergence dfGPdfNN Run 8/10, Epoch 404/1000, Training Loss (NLML): -959.8256\n",
      "convergence dfGPdfNN Run 8/10, Epoch 405/1000, Training Loss (NLML): -959.8328\n",
      "convergence dfGPdfNN Run 8/10, Epoch 406/1000, Training Loss (NLML): -959.7935\n",
      "convergence dfGPdfNN Run 8/10, Epoch 407/1000, Training Loss (NLML): -959.8433\n",
      "convergence dfGPdfNN Run 8/10, Epoch 408/1000, Training Loss (NLML): -959.8484\n",
      "convergence dfGPdfNN Run 8/10, Epoch 409/1000, Training Loss (NLML): -959.8544\n",
      "convergence dfGPdfNN Run 8/10, Epoch 410/1000, Training Loss (NLML): -959.8590\n",
      "convergence dfGPdfNN Run 8/10, Epoch 411/1000, Training Loss (NLML): -959.8665\n",
      "convergence dfGPdfNN Run 8/10, Epoch 412/1000, Training Loss (NLML): -959.8717\n",
      "convergence dfGPdfNN Run 8/10, Epoch 413/1000, Training Loss (NLML): -959.8867\n",
      "convergence dfGPdfNN Run 8/10, Epoch 414/1000, Training Loss (NLML): -959.8920\n",
      "convergence dfGPdfNN Run 8/10, Epoch 415/1000, Training Loss (NLML): -959.8853\n",
      "convergence dfGPdfNN Run 8/10, Epoch 416/1000, Training Loss (NLML): -959.8907\n",
      "convergence dfGPdfNN Run 8/10, Epoch 417/1000, Training Loss (NLML): -959.8967\n",
      "convergence dfGPdfNN Run 8/10, Epoch 418/1000, Training Loss (NLML): -959.9150\n",
      "convergence dfGPdfNN Run 8/10, Epoch 419/1000, Training Loss (NLML): -959.9076\n",
      "convergence dfGPdfNN Run 8/10, Epoch 420/1000, Training Loss (NLML): -959.9064\n",
      "convergence dfGPdfNN Run 8/10, Epoch 421/1000, Training Loss (NLML): -959.9125\n",
      "convergence dfGPdfNN Run 8/10, Epoch 422/1000, Training Loss (NLML): -959.9178\n",
      "convergence dfGPdfNN Run 8/10, Epoch 423/1000, Training Loss (NLML): -959.9337\n",
      "convergence dfGPdfNN Run 8/10, Epoch 424/1000, Training Loss (NLML): -959.9326\n",
      "convergence dfGPdfNN Run 8/10, Epoch 425/1000, Training Loss (NLML): -959.9465\n",
      "convergence dfGPdfNN Run 8/10, Epoch 426/1000, Training Loss (NLML): -959.9431\n",
      "convergence dfGPdfNN Run 8/10, Epoch 427/1000, Training Loss (NLML): -959.9580\n",
      "convergence dfGPdfNN Run 8/10, Epoch 428/1000, Training Loss (NLML): -959.9626\n",
      "convergence dfGPdfNN Run 8/10, Epoch 429/1000, Training Loss (NLML): -959.9602\n",
      "convergence dfGPdfNN Run 8/10, Epoch 430/1000, Training Loss (NLML): -959.9656\n",
      "convergence dfGPdfNN Run 8/10, Epoch 431/1000, Training Loss (NLML): -959.9552\n",
      "convergence dfGPdfNN Run 8/10, Epoch 432/1000, Training Loss (NLML): -959.9532\n",
      "convergence dfGPdfNN Run 8/10, Epoch 433/1000, Training Loss (NLML): -959.9309\n",
      "convergence dfGPdfNN Run 8/10, Epoch 434/1000, Training Loss (NLML): -959.9850\n",
      "convergence dfGPdfNN Run 8/10, Epoch 435/1000, Training Loss (NLML): -959.9883\n",
      "convergence dfGPdfNN Run 8/10, Epoch 436/1000, Training Loss (NLML): -959.9496\n",
      "convergence dfGPdfNN Run 8/10, Epoch 437/1000, Training Loss (NLML): -959.9485\n",
      "convergence dfGPdfNN Run 8/10, Epoch 438/1000, Training Loss (NLML): -959.9597\n",
      "convergence dfGPdfNN Run 8/10, Epoch 439/1000, Training Loss (NLML): -960.0050\n",
      "convergence dfGPdfNN Run 8/10, Epoch 440/1000, Training Loss (NLML): -959.9686\n",
      "convergence dfGPdfNN Run 8/10, Epoch 441/1000, Training Loss (NLML): -959.9746\n",
      "convergence dfGPdfNN Run 8/10, Epoch 442/1000, Training Loss (NLML): -960.0228\n",
      "convergence dfGPdfNN Run 8/10, Epoch 443/1000, Training Loss (NLML): -959.9823\n",
      "convergence dfGPdfNN Run 8/10, Epoch 444/1000, Training Loss (NLML): -960.0332\n",
      "convergence dfGPdfNN Run 8/10, Epoch 445/1000, Training Loss (NLML): -959.9928\n",
      "convergence dfGPdfNN Run 8/10, Epoch 446/1000, Training Loss (NLML): -960.0449\n",
      "convergence dfGPdfNN Run 8/10, Epoch 447/1000, Training Loss (NLML): -960.0497\n",
      "convergence dfGPdfNN Run 8/10, Epoch 448/1000, Training Loss (NLML): -960.0057\n",
      "convergence dfGPdfNN Run 8/10, Epoch 449/1000, Training Loss (NLML): -960.0580\n",
      "convergence dfGPdfNN Run 8/10, Epoch 450/1000, Training Loss (NLML): -960.0637\n",
      "convergence dfGPdfNN Run 8/10, Epoch 451/1000, Training Loss (NLML): -960.0679\n",
      "convergence dfGPdfNN Run 8/10, Epoch 452/1000, Training Loss (NLML): -960.0256\n",
      "convergence dfGPdfNN Run 8/10, Epoch 453/1000, Training Loss (NLML): -960.0782\n",
      "convergence dfGPdfNN Run 8/10, Epoch 454/1000, Training Loss (NLML): -960.0763\n",
      "convergence dfGPdfNN Run 8/10, Epoch 455/1000, Training Loss (NLML): -960.0826\n",
      "convergence dfGPdfNN Run 8/10, Epoch 456/1000, Training Loss (NLML): -960.0851\n",
      "convergence dfGPdfNN Run 8/10, Epoch 457/1000, Training Loss (NLML): -960.0277\n",
      "convergence dfGPdfNN Run 8/10, Epoch 458/1000, Training Loss (NLML): -960.0823\n",
      "convergence dfGPdfNN Run 8/10, Epoch 459/1000, Training Loss (NLML): -960.0878\n",
      "convergence dfGPdfNN Run 8/10, Epoch 460/1000, Training Loss (NLML): -960.0924\n",
      "convergence dfGPdfNN Run 8/10, Epoch 461/1000, Training Loss (NLML): -960.0978\n",
      "convergence dfGPdfNN Run 8/10, Epoch 462/1000, Training Loss (NLML): -960.1050\n",
      "convergence dfGPdfNN Run 8/10, Epoch 463/1000, Training Loss (NLML): -960.1099\n",
      "convergence dfGPdfNN Run 8/10, Epoch 464/1000, Training Loss (NLML): -960.1167\n",
      "convergence dfGPdfNN Run 8/10, Epoch 465/1000, Training Loss (NLML): -960.1431\n",
      "convergence dfGPdfNN Run 8/10, Epoch 466/1000, Training Loss (NLML): -960.1489\n",
      "convergence dfGPdfNN Run 8/10, Epoch 467/1000, Training Loss (NLML): -960.1558\n",
      "convergence dfGPdfNN Run 8/10, Epoch 468/1000, Training Loss (NLML): -960.1591\n",
      "convergence dfGPdfNN Run 8/10, Epoch 469/1000, Training Loss (NLML): -960.1637\n",
      "convergence dfGPdfNN Run 8/10, Epoch 470/1000, Training Loss (NLML): -960.1836\n",
      "convergence dfGPdfNN Run 8/10, Epoch 471/1000, Training Loss (NLML): -960.1858\n",
      "convergence dfGPdfNN Run 8/10, Epoch 472/1000, Training Loss (NLML): -960.1904\n",
      "convergence dfGPdfNN Run 8/10, Epoch 473/1000, Training Loss (NLML): -960.1561\n",
      "convergence dfGPdfNN Run 8/10, Epoch 474/1000, Training Loss (NLML): -960.1638\n",
      "convergence dfGPdfNN Run 8/10, Epoch 475/1000, Training Loss (NLML): -960.1688\n",
      "convergence dfGPdfNN Run 8/10, Epoch 476/1000, Training Loss (NLML): -960.1965\n",
      "convergence dfGPdfNN Run 8/10, Epoch 477/1000, Training Loss (NLML): -960.2024\n",
      "convergence dfGPdfNN Run 8/10, Epoch 478/1000, Training Loss (NLML): -960.1768\n",
      "convergence dfGPdfNN Run 8/10, Epoch 479/1000, Training Loss (NLML): -960.1808\n",
      "convergence dfGPdfNN Run 8/10, Epoch 480/1000, Training Loss (NLML): -960.1864\n",
      "convergence dfGPdfNN Run 8/10, Epoch 481/1000, Training Loss (NLML): -960.1908\n",
      "convergence dfGPdfNN Run 8/10, Epoch 482/1000, Training Loss (NLML): -960.2343\n",
      "convergence dfGPdfNN Run 8/10, Epoch 483/1000, Training Loss (NLML): -960.2227\n",
      "convergence dfGPdfNN Run 8/10, Epoch 484/1000, Training Loss (NLML): -960.2367\n",
      "convergence dfGPdfNN Run 8/10, Epoch 485/1000, Training Loss (NLML): -960.2535\n",
      "convergence dfGPdfNN Run 8/10, Epoch 486/1000, Training Loss (NLML): -960.2513\n",
      "convergence dfGPdfNN Run 8/10, Epoch 487/1000, Training Loss (NLML): -960.2480\n",
      "convergence dfGPdfNN Run 8/10, Epoch 488/1000, Training Loss (NLML): -960.2544\n",
      "convergence dfGPdfNN Run 8/10, Epoch 489/1000, Training Loss (NLML): -960.2588\n",
      "convergence dfGPdfNN Run 8/10, Epoch 490/1000, Training Loss (NLML): -960.2347\n",
      "convergence dfGPdfNN Run 8/10, Epoch 491/1000, Training Loss (NLML): -960.2529\n",
      "convergence dfGPdfNN Run 8/10, Epoch 492/1000, Training Loss (NLML): -960.2723\n",
      "convergence dfGPdfNN Run 8/10, Epoch 493/1000, Training Loss (NLML): -960.2689\n",
      "convergence dfGPdfNN Run 8/10, Epoch 494/1000, Training Loss (NLML): -960.2479\n",
      "convergence dfGPdfNN Run 8/10, Epoch 495/1000, Training Loss (NLML): -960.2551\n",
      "convergence dfGPdfNN Run 8/10, Epoch 496/1000, Training Loss (NLML): -960.2572\n",
      "convergence dfGPdfNN Run 8/10, Epoch 497/1000, Training Loss (NLML): -960.2676\n",
      "convergence dfGPdfNN Run 8/10, Epoch 498/1000, Training Loss (NLML): -960.2428\n",
      "convergence dfGPdfNN Run 8/10, Epoch 499/1000, Training Loss (NLML): -960.2510\n",
      "convergence dfGPdfNN Run 8/10, Epoch 500/1000, Training Loss (NLML): -960.2537\n",
      "convergence dfGPdfNN Run 8/10, Epoch 501/1000, Training Loss (NLML): -960.2632\n",
      "convergence dfGPdfNN Run 8/10, Epoch 502/1000, Training Loss (NLML): -960.2794\n",
      "convergence dfGPdfNN Run 8/10, Epoch 503/1000, Training Loss (NLML): -960.2828\n",
      "convergence dfGPdfNN Run 8/10, Epoch 504/1000, Training Loss (NLML): -960.2877\n",
      "convergence dfGPdfNN Run 8/10, Epoch 505/1000, Training Loss (NLML): -960.2922\n",
      "convergence dfGPdfNN Run 8/10, Epoch 506/1000, Training Loss (NLML): -960.2799\n",
      "convergence dfGPdfNN Run 8/10, Epoch 507/1000, Training Loss (NLML): -960.2998\n",
      "convergence dfGPdfNN Run 8/10, Epoch 508/1000, Training Loss (NLML): -960.3041\n",
      "convergence dfGPdfNN Run 8/10, Epoch 509/1000, Training Loss (NLML): -960.3090\n",
      "convergence dfGPdfNN Run 8/10, Epoch 510/1000, Training Loss (NLML): -960.3459\n",
      "convergence dfGPdfNN Run 8/10, Epoch 511/1000, Training Loss (NLML): -960.3478\n",
      "convergence dfGPdfNN Run 8/10, Epoch 512/1000, Training Loss (NLML): -960.3527\n",
      "convergence dfGPdfNN Run 8/10, Epoch 513/1000, Training Loss (NLML): -960.3232\n",
      "convergence dfGPdfNN Run 8/10, Epoch 514/1000, Training Loss (NLML): -960.3258\n",
      "convergence dfGPdfNN Run 8/10, Epoch 515/1000, Training Loss (NLML): -960.3324\n",
      "convergence dfGPdfNN Run 8/10, Epoch 516/1000, Training Loss (NLML): -960.3376\n",
      "convergence dfGPdfNN Run 8/10, Epoch 517/1000, Training Loss (NLML): -960.3689\n",
      "convergence dfGPdfNN Run 8/10, Epoch 518/1000, Training Loss (NLML): -960.3723\n",
      "convergence dfGPdfNN Run 8/10, Epoch 519/1000, Training Loss (NLML): -960.3483\n",
      "convergence dfGPdfNN Run 8/10, Epoch 520/1000, Training Loss (NLML): -960.3265\n",
      "convergence dfGPdfNN Run 8/10, Epoch 521/1000, Training Loss (NLML): -960.3328\n",
      "convergence dfGPdfNN Run 8/10, Epoch 522/1000, Training Loss (NLML): -960.3602\n",
      "convergence dfGPdfNN Run 8/10, Epoch 523/1000, Training Loss (NLML): -960.3615\n",
      "convergence dfGPdfNN Run 8/10, Epoch 524/1000, Training Loss (NLML): -960.3654\n",
      "convergence dfGPdfNN Run 8/10, Epoch 525/1000, Training Loss (NLML): -960.3701\n",
      "convergence dfGPdfNN Run 8/10, Epoch 526/1000, Training Loss (NLML): -960.3513\n",
      "convergence dfGPdfNN Run 8/10, Epoch 527/1000, Training Loss (NLML): -960.3571\n",
      "convergence dfGPdfNN Run 8/10, Epoch 528/1000, Training Loss (NLML): -960.3595\n",
      "convergence dfGPdfNN Run 8/10, Epoch 529/1000, Training Loss (NLML): -960.3651\n",
      "convergence dfGPdfNN Run 8/10, Epoch 530/1000, Training Loss (NLML): -960.3878\n",
      "convergence dfGPdfNN Run 8/10, Epoch 531/1000, Training Loss (NLML): -960.3906\n",
      "convergence dfGPdfNN Run 8/10, Epoch 532/1000, Training Loss (NLML): -960.1676\n",
      "convergence dfGPdfNN Run 8/10, Epoch 533/1000, Training Loss (NLML): -960.3771\n",
      "convergence dfGPdfNN Run 8/10, Epoch 534/1000, Training Loss (NLML): -960.3807\n",
      "convergence dfGPdfNN Run 8/10, Epoch 535/1000, Training Loss (NLML): -960.4067\n",
      "convergence dfGPdfNN Run 8/10, Epoch 536/1000, Training Loss (NLML): -960.4092\n",
      "convergence dfGPdfNN Run 8/10, Epoch 537/1000, Training Loss (NLML): -960.3918\n",
      "convergence dfGPdfNN Run 8/10, Epoch 538/1000, Training Loss (NLML): -960.4030\n",
      "convergence dfGPdfNN Run 8/10, Epoch 539/1000, Training Loss (NLML): -960.4061\n",
      "convergence dfGPdfNN Run 8/10, Epoch 540/1000, Training Loss (NLML): -960.4235\n",
      "convergence dfGPdfNN Run 8/10, Epoch 541/1000, Training Loss (NLML): -960.4264\n",
      "convergence dfGPdfNN Run 8/10, Epoch 542/1000, Training Loss (NLML): -960.4103\n",
      "convergence dfGPdfNN Run 8/10, Epoch 543/1000, Training Loss (NLML): -960.4160\n",
      "convergence dfGPdfNN Run 8/10, Epoch 544/1000, Training Loss (NLML): -960.4192\n",
      "convergence dfGPdfNN Run 8/10, Epoch 545/1000, Training Loss (NLML): -960.4216\n",
      "convergence dfGPdfNN Run 8/10, Epoch 546/1000, Training Loss (NLML): -960.4258\n",
      "convergence dfGPdfNN Run 8/10, Epoch 547/1000, Training Loss (NLML): -960.4292\n",
      "convergence dfGPdfNN Run 8/10, Epoch 548/1000, Training Loss (NLML): -960.4315\n",
      "convergence dfGPdfNN Run 8/10, Epoch 549/1000, Training Loss (NLML): -960.4358\n",
      "convergence dfGPdfNN Run 8/10, Epoch 550/1000, Training Loss (NLML): -960.4395\n",
      "convergence dfGPdfNN Run 8/10, Epoch 551/1000, Training Loss (NLML): -960.4436\n",
      "convergence dfGPdfNN Run 8/10, Epoch 552/1000, Training Loss (NLML): -960.4475\n",
      "convergence dfGPdfNN Run 8/10, Epoch 553/1000, Training Loss (NLML): -960.4503\n",
      "convergence dfGPdfNN Run 8/10, Epoch 554/1000, Training Loss (NLML): -960.4539\n",
      "convergence dfGPdfNN Run 8/10, Epoch 555/1000, Training Loss (NLML): -960.4573\n",
      "convergence dfGPdfNN Run 8/10, Epoch 556/1000, Training Loss (NLML): -960.4597\n",
      "convergence dfGPdfNN Run 8/10, Epoch 557/1000, Training Loss (NLML): -960.4811\n",
      "convergence dfGPdfNN Run 8/10, Epoch 558/1000, Training Loss (NLML): -960.4858\n",
      "convergence dfGPdfNN Run 8/10, Epoch 559/1000, Training Loss (NLML): -960.4692\n",
      "convergence dfGPdfNN Run 8/10, Epoch 560/1000, Training Loss (NLML): -960.4718\n",
      "convergence dfGPdfNN Run 8/10, Epoch 561/1000, Training Loss (NLML): -960.4746\n",
      "convergence dfGPdfNN Run 8/10, Epoch 562/1000, Training Loss (NLML): -960.4800\n",
      "convergence dfGPdfNN Run 8/10, Epoch 563/1000, Training Loss (NLML): -960.4844\n",
      "convergence dfGPdfNN Run 8/10, Epoch 564/1000, Training Loss (NLML): -960.4860\n",
      "convergence dfGPdfNN Run 8/10, Epoch 565/1000, Training Loss (NLML): -960.5049\n",
      "convergence dfGPdfNN Run 8/10, Epoch 566/1000, Training Loss (NLML): -960.4910\n",
      "convergence dfGPdfNN Run 8/10, Epoch 567/1000, Training Loss (NLML): -960.4972\n",
      "convergence dfGPdfNN Run 8/10, Epoch 568/1000, Training Loss (NLML): -960.4999\n",
      "convergence dfGPdfNN Run 8/10, Epoch 569/1000, Training Loss (NLML): -960.5026\n",
      "convergence dfGPdfNN Run 8/10, Epoch 570/1000, Training Loss (NLML): -960.5055\n",
      "convergence dfGPdfNN Run 8/10, Epoch 571/1000, Training Loss (NLML): -960.5109\n",
      "convergence dfGPdfNN Run 8/10, Epoch 572/1000, Training Loss (NLML): -960.5138\n",
      "convergence dfGPdfNN Run 8/10, Epoch 573/1000, Training Loss (NLML): -960.5148\n",
      "convergence dfGPdfNN Run 8/10, Epoch 574/1000, Training Loss (NLML): -960.5359\n",
      "convergence dfGPdfNN Run 8/10, Epoch 575/1000, Training Loss (NLML): -960.5227\n",
      "convergence dfGPdfNN Run 8/10, Epoch 576/1000, Training Loss (NLML): -960.5244\n",
      "convergence dfGPdfNN Run 8/10, Epoch 577/1000, Training Loss (NLML): -960.5308\n",
      "convergence dfGPdfNN Run 8/10, Epoch 578/1000, Training Loss (NLML): -960.5321\n",
      "convergence dfGPdfNN Run 8/10, Epoch 579/1000, Training Loss (NLML): -960.5352\n",
      "convergence dfGPdfNN Run 8/10, Epoch 580/1000, Training Loss (NLML): -960.5392\n",
      "convergence dfGPdfNN Run 8/10, Epoch 581/1000, Training Loss (NLML): -960.5609\n",
      "convergence dfGPdfNN Run 8/10, Epoch 582/1000, Training Loss (NLML): -960.5448\n",
      "convergence dfGPdfNN Run 8/10, Epoch 583/1000, Training Loss (NLML): -960.5493\n",
      "convergence dfGPdfNN Run 8/10, Epoch 584/1000, Training Loss (NLML): -960.5516\n",
      "convergence dfGPdfNN Run 8/10, Epoch 585/1000, Training Loss (NLML): -960.5551\n",
      "convergence dfGPdfNN Run 8/10, Epoch 586/1000, Training Loss (NLML): -960.5580\n",
      "convergence dfGPdfNN Run 8/10, Epoch 587/1000, Training Loss (NLML): -960.5593\n",
      "convergence dfGPdfNN Run 8/10, Epoch 588/1000, Training Loss (NLML): -960.5808\n",
      "convergence dfGPdfNN Run 8/10, Epoch 589/1000, Training Loss (NLML): -960.5652\n",
      "convergence dfGPdfNN Run 8/10, Epoch 590/1000, Training Loss (NLML): -960.5697\n",
      "convergence dfGPdfNN Run 8/10, Epoch 591/1000, Training Loss (NLML): -960.5737\n",
      "convergence dfGPdfNN Run 8/10, Epoch 592/1000, Training Loss (NLML): -960.5769\n",
      "convergence dfGPdfNN Run 8/10, Epoch 593/1000, Training Loss (NLML): -960.5781\n",
      "convergence dfGPdfNN Run 8/10, Epoch 594/1000, Training Loss (NLML): -960.5981\n",
      "convergence dfGPdfNN Run 8/10, Epoch 595/1000, Training Loss (NLML): -960.5853\n",
      "convergence dfGPdfNN Run 8/10, Epoch 596/1000, Training Loss (NLML): -960.5886\n",
      "convergence dfGPdfNN Run 8/10, Epoch 597/1000, Training Loss (NLML): -960.5924\n",
      "convergence dfGPdfNN Run 8/10, Epoch 598/1000, Training Loss (NLML): -960.5958\n",
      "convergence dfGPdfNN Run 8/10, Epoch 599/1000, Training Loss (NLML): -960.6145\n",
      "convergence dfGPdfNN Run 8/10, Epoch 600/1000, Training Loss (NLML): -960.5999\n",
      "convergence dfGPdfNN Run 8/10, Epoch 601/1000, Training Loss (NLML): -960.6045\n",
      "convergence dfGPdfNN Run 8/10, Epoch 602/1000, Training Loss (NLML): -960.6055\n",
      "convergence dfGPdfNN Run 8/10, Epoch 603/1000, Training Loss (NLML): -960.6115\n",
      "convergence dfGPdfNN Run 8/10, Epoch 604/1000, Training Loss (NLML): -960.6289\n",
      "convergence dfGPdfNN Run 8/10, Epoch 605/1000, Training Loss (NLML): -960.6167\n",
      "convergence dfGPdfNN Run 8/10, Epoch 606/1000, Training Loss (NLML): -960.6182\n",
      "convergence dfGPdfNN Run 8/10, Epoch 607/1000, Training Loss (NLML): -960.6210\n",
      "convergence dfGPdfNN Run 8/10, Epoch 608/1000, Training Loss (NLML): -960.6243\n",
      "convergence dfGPdfNN Run 8/10, Epoch 609/1000, Training Loss (NLML): -960.6437\n",
      "convergence dfGPdfNN Run 8/10, Epoch 610/1000, Training Loss (NLML): -960.6290\n",
      "convergence dfGPdfNN Run 8/10, Epoch 611/1000, Training Loss (NLML): -960.6333\n",
      "convergence dfGPdfNN Run 8/10, Epoch 612/1000, Training Loss (NLML): -960.6356\n",
      "convergence dfGPdfNN Run 8/10, Epoch 613/1000, Training Loss (NLML): -960.6377\n",
      "convergence dfGPdfNN Run 8/10, Epoch 614/1000, Training Loss (NLML): -960.6584\n",
      "convergence dfGPdfNN Run 8/10, Epoch 615/1000, Training Loss (NLML): -960.6471\n",
      "convergence dfGPdfNN Run 8/10, Epoch 616/1000, Training Loss (NLML): -960.6470\n",
      "convergence dfGPdfNN Run 8/10, Epoch 617/1000, Training Loss (NLML): -960.6521\n",
      "convergence dfGPdfNN Run 8/10, Epoch 618/1000, Training Loss (NLML): -960.6525\n",
      "convergence dfGPdfNN Run 8/10, Epoch 619/1000, Training Loss (NLML): -960.6711\n",
      "convergence dfGPdfNN Run 8/10, Epoch 620/1000, Training Loss (NLML): -960.6603\n",
      "convergence dfGPdfNN Run 8/10, Epoch 621/1000, Training Loss (NLML): -960.6615\n",
      "convergence dfGPdfNN Run 8/10, Epoch 622/1000, Training Loss (NLML): -960.6656\n",
      "convergence dfGPdfNN Run 8/10, Epoch 623/1000, Training Loss (NLML): -960.6683\n",
      "convergence dfGPdfNN Run 8/10, Epoch 624/1000, Training Loss (NLML): -960.6884\n",
      "convergence dfGPdfNN Run 8/10, Epoch 625/1000, Training Loss (NLML): -960.6740\n",
      "convergence dfGPdfNN Run 8/10, Epoch 626/1000, Training Loss (NLML): -960.6761\n",
      "convergence dfGPdfNN Run 8/10, Epoch 627/1000, Training Loss (NLML): -960.6790\n",
      "convergence dfGPdfNN Run 8/10, Epoch 628/1000, Training Loss (NLML): -960.6825\n",
      "convergence dfGPdfNN Run 8/10, Epoch 629/1000, Training Loss (NLML): -960.7012\n",
      "convergence dfGPdfNN Run 8/10, Epoch 630/1000, Training Loss (NLML): -960.6880\n",
      "convergence dfGPdfNN Run 8/10, Epoch 631/1000, Training Loss (NLML): -960.6896\n",
      "convergence dfGPdfNN Run 8/10, Epoch 632/1000, Training Loss (NLML): -960.6942\n",
      "convergence dfGPdfNN Run 8/10, Epoch 633/1000, Training Loss (NLML): -960.6964\n",
      "convergence dfGPdfNN Run 8/10, Epoch 634/1000, Training Loss (NLML): -960.7137\n",
      "convergence dfGPdfNN Run 8/10, Epoch 635/1000, Training Loss (NLML): -960.7013\n",
      "convergence dfGPdfNN Run 8/10, Epoch 636/1000, Training Loss (NLML): -960.7056\n",
      "convergence dfGPdfNN Run 8/10, Epoch 637/1000, Training Loss (NLML): -960.7083\n",
      "convergence dfGPdfNN Run 8/10, Epoch 638/1000, Training Loss (NLML): -960.7097\n",
      "convergence dfGPdfNN Run 8/10, Epoch 639/1000, Training Loss (NLML): -960.7277\n",
      "convergence dfGPdfNN Run 8/10, Epoch 640/1000, Training Loss (NLML): -960.7162\n",
      "convergence dfGPdfNN Run 8/10, Epoch 641/1000, Training Loss (NLML): -960.7185\n",
      "convergence dfGPdfNN Run 8/10, Epoch 642/1000, Training Loss (NLML): -960.7209\n",
      "convergence dfGPdfNN Run 8/10, Epoch 643/1000, Training Loss (NLML): -960.7249\n",
      "convergence dfGPdfNN Run 8/10, Epoch 644/1000, Training Loss (NLML): -960.7423\n",
      "convergence dfGPdfNN Run 8/10, Epoch 645/1000, Training Loss (NLML): -960.7269\n",
      "convergence dfGPdfNN Run 8/10, Epoch 646/1000, Training Loss (NLML): -960.7305\n",
      "convergence dfGPdfNN Run 8/10, Epoch 647/1000, Training Loss (NLML): -960.7344\n",
      "convergence dfGPdfNN Run 8/10, Epoch 648/1000, Training Loss (NLML): -960.7385\n",
      "convergence dfGPdfNN Run 8/10, Epoch 649/1000, Training Loss (NLML): -960.7548\n",
      "convergence dfGPdfNN Run 8/10, Epoch 650/1000, Training Loss (NLML): -960.7423\n",
      "convergence dfGPdfNN Run 8/10, Epoch 651/1000, Training Loss (NLML): -960.7444\n",
      "convergence dfGPdfNN Run 8/10, Epoch 652/1000, Training Loss (NLML): -960.7467\n",
      "convergence dfGPdfNN Run 8/10, Epoch 653/1000, Training Loss (NLML): -960.7520\n",
      "convergence dfGPdfNN Run 8/10, Epoch 654/1000, Training Loss (NLML): -960.7682\n",
      "convergence dfGPdfNN Run 8/10, Epoch 655/1000, Training Loss (NLML): -960.7535\n",
      "convergence dfGPdfNN Run 8/10, Epoch 656/1000, Training Loss (NLML): -960.7563\n",
      "convergence dfGPdfNN Run 8/10, Epoch 657/1000, Training Loss (NLML): -960.7600\n",
      "convergence dfGPdfNN Run 8/10, Epoch 658/1000, Training Loss (NLML): -960.7615\n",
      "convergence dfGPdfNN Run 8/10, Epoch 659/1000, Training Loss (NLML): -960.7795\n",
      "convergence dfGPdfNN Run 8/10, Epoch 660/1000, Training Loss (NLML): -960.7701\n",
      "convergence dfGPdfNN Run 8/10, Epoch 661/1000, Training Loss (NLML): -960.7728\n",
      "convergence dfGPdfNN Run 8/10, Epoch 662/1000, Training Loss (NLML): -960.7726\n",
      "convergence dfGPdfNN Run 8/10, Epoch 663/1000, Training Loss (NLML): -960.7747\n",
      "convergence dfGPdfNN Run 8/10, Epoch 664/1000, Training Loss (NLML): -960.7955\n",
      "convergence dfGPdfNN Run 8/10, Epoch 665/1000, Training Loss (NLML): -960.7798\n",
      "convergence dfGPdfNN Run 8/10, Epoch 666/1000, Training Loss (NLML): -960.7848\n",
      "convergence dfGPdfNN Run 8/10, Epoch 667/1000, Training Loss (NLML): -960.7872\n",
      "convergence dfGPdfNN Run 8/10, Epoch 668/1000, Training Loss (NLML): -960.7891\n",
      "convergence dfGPdfNN Run 8/10, Epoch 669/1000, Training Loss (NLML): -960.8052\n",
      "convergence dfGPdfNN Run 8/10, Epoch 670/1000, Training Loss (NLML): -960.7926\n",
      "convergence dfGPdfNN Run 8/10, Epoch 671/1000, Training Loss (NLML): -960.7961\n",
      "convergence dfGPdfNN Run 8/10, Epoch 672/1000, Training Loss (NLML): -960.7999\n",
      "convergence dfGPdfNN Run 8/10, Epoch 673/1000, Training Loss (NLML): -960.8020\n",
      "convergence dfGPdfNN Run 8/10, Epoch 674/1000, Training Loss (NLML): -960.8168\n",
      "convergence dfGPdfNN Run 8/10, Epoch 675/1000, Training Loss (NLML): -960.8055\n",
      "convergence dfGPdfNN Run 8/10, Epoch 676/1000, Training Loss (NLML): -960.8081\n",
      "convergence dfGPdfNN Run 8/10, Epoch 677/1000, Training Loss (NLML): -960.8104\n",
      "convergence dfGPdfNN Run 8/10, Epoch 678/1000, Training Loss (NLML): -960.8134\n",
      "convergence dfGPdfNN Run 8/10, Epoch 679/1000, Training Loss (NLML): -960.8317\n",
      "convergence dfGPdfNN Run 8/10, Epoch 680/1000, Training Loss (NLML): -960.8170\n",
      "convergence dfGPdfNN Run 8/10, Epoch 681/1000, Training Loss (NLML): -960.8192\n",
      "convergence dfGPdfNN Run 8/10, Epoch 682/1000, Training Loss (NLML): -960.8230\n",
      "convergence dfGPdfNN Run 8/10, Epoch 683/1000, Training Loss (NLML): -960.8253\n",
      "convergence dfGPdfNN Run 8/10, Epoch 684/1000, Training Loss (NLML): -960.8450\n",
      "convergence dfGPdfNN Run 8/10, Epoch 685/1000, Training Loss (NLML): -960.8309\n",
      "convergence dfGPdfNN Run 8/10, Epoch 686/1000, Training Loss (NLML): -960.8347\n",
      "convergence dfGPdfNN Run 8/10, Epoch 687/1000, Training Loss (NLML): -960.8361\n",
      "convergence dfGPdfNN Run 8/10, Epoch 688/1000, Training Loss (NLML): -960.8403\n",
      "convergence dfGPdfNN Run 8/10, Epoch 689/1000, Training Loss (NLML): -960.8529\n",
      "convergence dfGPdfNN Run 8/10, Epoch 690/1000, Training Loss (NLML): -960.8510\n",
      "convergence dfGPdfNN Run 8/10, Epoch 691/1000, Training Loss (NLML): -960.8446\n",
      "convergence dfGPdfNN Run 8/10, Epoch 692/1000, Training Loss (NLML): -960.8458\n",
      "convergence dfGPdfNN Run 8/10, Epoch 693/1000, Training Loss (NLML): -960.8645\n",
      "convergence dfGPdfNN Run 8/10, Epoch 694/1000, Training Loss (NLML): -960.8661\n",
      "convergence dfGPdfNN Run 8/10, Epoch 695/1000, Training Loss (NLML): -960.8557\n",
      "convergence dfGPdfNN Run 8/10, Epoch 696/1000, Training Loss (NLML): -960.8555\n",
      "convergence dfGPdfNN Run 8/10, Epoch 697/1000, Training Loss (NLML): -960.8601\n",
      "convergence dfGPdfNN Run 8/10, Epoch 698/1000, Training Loss (NLML): -960.8757\n",
      "convergence dfGPdfNN Run 8/10, Epoch 699/1000, Training Loss (NLML): -960.8781\n",
      "convergence dfGPdfNN Run 8/10, Epoch 700/1000, Training Loss (NLML): -960.8807\n",
      "convergence dfGPdfNN Run 8/10, Epoch 701/1000, Training Loss (NLML): -960.8668\n",
      "convergence dfGPdfNN Run 8/10, Epoch 702/1000, Training Loss (NLML): -960.8698\n",
      "convergence dfGPdfNN Run 8/10, Epoch 703/1000, Training Loss (NLML): -960.8729\n",
      "convergence dfGPdfNN Run 8/10, Epoch 704/1000, Training Loss (NLML): -960.8882\n",
      "convergence dfGPdfNN Run 8/10, Epoch 705/1000, Training Loss (NLML): -960.8906\n",
      "convergence dfGPdfNN Run 8/10, Epoch 706/1000, Training Loss (NLML): -960.8921\n",
      "convergence dfGPdfNN Run 8/10, Epoch 707/1000, Training Loss (NLML): -960.8969\n",
      "convergence dfGPdfNN Run 8/10, Epoch 708/1000, Training Loss (NLML): -960.8851\n",
      "convergence dfGPdfNN Run 8/10, Epoch 709/1000, Training Loss (NLML): -960.8865\n",
      "convergence dfGPdfNN Run 8/10, Epoch 710/1000, Training Loss (NLML): -960.9039\n",
      "convergence dfGPdfNN Run 8/10, Epoch 711/1000, Training Loss (NLML): -960.9061\n",
      "convergence dfGPdfNN Run 8/10, Epoch 712/1000, Training Loss (NLML): -960.9052\n",
      "convergence dfGPdfNN Run 8/10, Epoch 713/1000, Training Loss (NLML): -960.8975\n",
      "convergence dfGPdfNN Run 8/10, Epoch 714/1000, Training Loss (NLML): -960.9000\n",
      "convergence dfGPdfNN Run 8/10, Epoch 715/1000, Training Loss (NLML): -960.9154\n",
      "convergence dfGPdfNN Run 8/10, Epoch 716/1000, Training Loss (NLML): -960.9014\n",
      "convergence dfGPdfNN Run 8/10, Epoch 717/1000, Training Loss (NLML): -960.9036\n",
      "convergence dfGPdfNN Run 8/10, Epoch 718/1000, Training Loss (NLML): -960.9213\n",
      "convergence dfGPdfNN Run 8/10, Epoch 719/1000, Training Loss (NLML): -960.9100\n",
      "convergence dfGPdfNN Run 8/10, Epoch 720/1000, Training Loss (NLML): -960.9128\n",
      "convergence dfGPdfNN Run 8/10, Epoch 721/1000, Training Loss (NLML): -960.9286\n",
      "convergence dfGPdfNN Run 8/10, Epoch 722/1000, Training Loss (NLML): -960.9161\n",
      "convergence dfGPdfNN Run 8/10, Epoch 723/1000, Training Loss (NLML): -960.9177\n",
      "convergence dfGPdfNN Run 8/10, Epoch 724/1000, Training Loss (NLML): -960.9360\n",
      "convergence dfGPdfNN Run 8/10, Epoch 725/1000, Training Loss (NLML): -960.9385\n",
      "convergence dfGPdfNN Run 8/10, Epoch 726/1000, Training Loss (NLML): -960.9232\n",
      "convergence dfGPdfNN Run 8/10, Epoch 727/1000, Training Loss (NLML): -960.9276\n",
      "convergence dfGPdfNN Run 8/10, Epoch 728/1000, Training Loss (NLML): -960.9446\n",
      "convergence dfGPdfNN Run 8/10, Epoch 729/1000, Training Loss (NLML): -960.9457\n",
      "convergence dfGPdfNN Run 8/10, Epoch 730/1000, Training Loss (NLML): -960.9478\n",
      "convergence dfGPdfNN Run 8/10, Epoch 731/1000, Training Loss (NLML): -960.9351\n",
      "convergence dfGPdfNN Run 8/10, Epoch 732/1000, Training Loss (NLML): -960.9528\n",
      "convergence dfGPdfNN Run 8/10, Epoch 733/1000, Training Loss (NLML): -960.9470\n",
      "convergence dfGPdfNN Run 8/10, Epoch 734/1000, Training Loss (NLML): -960.9552\n",
      "convergence dfGPdfNN Run 8/10, Epoch 735/1000, Training Loss (NLML): -960.9581\n",
      "convergence dfGPdfNN Run 8/10, Epoch 736/1000, Training Loss (NLML): -960.9626\n",
      "convergence dfGPdfNN Run 8/10, Epoch 737/1000, Training Loss (NLML): -960.9658\n",
      "convergence dfGPdfNN Run 8/10, Epoch 738/1000, Training Loss (NLML): -960.9674\n",
      "convergence dfGPdfNN Run 8/10, Epoch 739/1000, Training Loss (NLML): -960.9510\n",
      "convergence dfGPdfNN Run 8/10, Epoch 740/1000, Training Loss (NLML): -960.9679\n",
      "convergence dfGPdfNN Run 8/10, Epoch 741/1000, Training Loss (NLML): -960.9634\n",
      "convergence dfGPdfNN Run 8/10, Epoch 742/1000, Training Loss (NLML): -960.9677\n",
      "convergence dfGPdfNN Run 8/10, Epoch 743/1000, Training Loss (NLML): -960.9698\n",
      "convergence dfGPdfNN Run 8/10, Epoch 744/1000, Training Loss (NLML): -960.9762\n",
      "convergence dfGPdfNN Run 8/10, Epoch 745/1000, Training Loss (NLML): -960.9801\n",
      "convergence dfGPdfNN Run 8/10, Epoch 746/1000, Training Loss (NLML): -960.9833\n",
      "convergence dfGPdfNN Run 8/10, Epoch 747/1000, Training Loss (NLML): -960.9833\n",
      "convergence dfGPdfNN Run 8/10, Epoch 748/1000, Training Loss (NLML): -960.9854\n",
      "convergence dfGPdfNN Run 8/10, Epoch 749/1000, Training Loss (NLML): -960.9894\n",
      "convergence dfGPdfNN Run 8/10, Epoch 750/1000, Training Loss (NLML): -960.9921\n",
      "convergence dfGPdfNN Run 8/10, Epoch 751/1000, Training Loss (NLML): -960.9949\n",
      "convergence dfGPdfNN Run 8/10, Epoch 752/1000, Training Loss (NLML): -960.9968\n",
      "convergence dfGPdfNN Run 8/10, Epoch 753/1000, Training Loss (NLML): -960.9979\n",
      "convergence dfGPdfNN Run 8/10, Epoch 754/1000, Training Loss (NLML): -961.0006\n",
      "convergence dfGPdfNN Run 8/10, Epoch 755/1000, Training Loss (NLML): -961.0006\n",
      "convergence dfGPdfNN Run 8/10, Epoch 756/1000, Training Loss (NLML): -960.9932\n",
      "convergence dfGPdfNN Run 8/10, Epoch 757/1000, Training Loss (NLML): -960.9974\n",
      "convergence dfGPdfNN Run 8/10, Epoch 758/1000, Training Loss (NLML): -961.0059\n",
      "convergence dfGPdfNN Run 8/10, Epoch 759/1000, Training Loss (NLML): -961.0089\n",
      "convergence dfGPdfNN Run 8/10, Epoch 760/1000, Training Loss (NLML): -961.0112\n",
      "convergence dfGPdfNN Run 8/10, Epoch 761/1000, Training Loss (NLML): -960.9993\n",
      "convergence dfGPdfNN Run 8/10, Epoch 762/1000, Training Loss (NLML): -961.0016\n",
      "convergence dfGPdfNN Run 8/10, Epoch 763/1000, Training Loss (NLML): -961.0039\n",
      "convergence dfGPdfNN Run 8/10, Epoch 764/1000, Training Loss (NLML): -961.0061\n",
      "convergence dfGPdfNN Run 8/10, Epoch 765/1000, Training Loss (NLML): -961.0084\n",
      "convergence dfGPdfNN Run 8/10, Epoch 766/1000, Training Loss (NLML): -961.0221\n",
      "convergence dfGPdfNN Run 8/10, Epoch 767/1000, Training Loss (NLML): -961.0121\n",
      "convergence dfGPdfNN Run 8/10, Epoch 768/1000, Training Loss (NLML): -961.0150\n",
      "convergence dfGPdfNN Run 8/10, Epoch 769/1000, Training Loss (NLML): -961.0302\n",
      "convergence dfGPdfNN Run 8/10, Epoch 770/1000, Training Loss (NLML): -961.0314\n",
      "convergence dfGPdfNN Run 8/10, Epoch 771/1000, Training Loss (NLML): -961.0341\n",
      "convergence dfGPdfNN Run 8/10, Epoch 772/1000, Training Loss (NLML): -961.0353\n",
      "convergence dfGPdfNN Run 8/10, Epoch 773/1000, Training Loss (NLML): -961.0393\n",
      "convergence dfGPdfNN Run 8/10, Epoch 774/1000, Training Loss (NLML): -961.0400\n",
      "convergence dfGPdfNN Run 8/10, Epoch 775/1000, Training Loss (NLML): -961.0591\n",
      "convergence dfGPdfNN Run 8/10, Epoch 776/1000, Training Loss (NLML): -961.0424\n",
      "convergence dfGPdfNN Run 8/10, Epoch 777/1000, Training Loss (NLML): -961.0476\n",
      "convergence dfGPdfNN Run 8/10, Epoch 778/1000, Training Loss (NLML): -961.0492\n",
      "convergence dfGPdfNN Run 8/10, Epoch 779/1000, Training Loss (NLML): -961.0497\n",
      "convergence dfGPdfNN Run 8/10, Epoch 780/1000, Training Loss (NLML): -961.0543\n",
      "convergence dfGPdfNN Run 8/10, Epoch 781/1000, Training Loss (NLML): -961.0521\n",
      "convergence dfGPdfNN Run 8/10, Epoch 782/1000, Training Loss (NLML): -961.0546\n",
      "convergence dfGPdfNN Run 8/10, Epoch 783/1000, Training Loss (NLML): -961.0579\n",
      "convergence dfGPdfNN Run 8/10, Epoch 784/1000, Training Loss (NLML): -961.0599\n",
      "convergence dfGPdfNN Run 8/10, Epoch 785/1000, Training Loss (NLML): -961.0607\n",
      "convergence dfGPdfNN Run 8/10, Epoch 786/1000, Training Loss (NLML): -961.0613\n",
      "convergence dfGPdfNN Run 8/10, Epoch 787/1000, Training Loss (NLML): -961.0657\n",
      "convergence dfGPdfNN Run 8/10, Epoch 788/1000, Training Loss (NLML): -961.0645\n",
      "convergence dfGPdfNN Run 8/10, Epoch 789/1000, Training Loss (NLML): -961.0665\n",
      "convergence dfGPdfNN Run 8/10, Epoch 790/1000, Training Loss (NLML): -961.0691\n",
      "convergence dfGPdfNN Run 8/10, Epoch 791/1000, Training Loss (NLML): -961.0681\n",
      "convergence dfGPdfNN Run 8/10, Epoch 792/1000, Training Loss (NLML): -961.0701\n",
      "convergence dfGPdfNN Run 8/10, Epoch 793/1000, Training Loss (NLML): -961.0906\n",
      "convergence dfGPdfNN Run 8/10, Epoch 794/1000, Training Loss (NLML): -961.0748\n",
      "convergence dfGPdfNN Run 8/10, Epoch 795/1000, Training Loss (NLML): -961.0781\n",
      "convergence dfGPdfNN Run 8/10, Epoch 796/1000, Training Loss (NLML): -961.0803\n",
      "convergence dfGPdfNN Run 8/10, Epoch 797/1000, Training Loss (NLML): -961.0834\n",
      "convergence dfGPdfNN Run 8/10, Epoch 798/1000, Training Loss (NLML): -961.0826\n",
      "convergence dfGPdfNN Run 8/10, Epoch 799/1000, Training Loss (NLML): -961.0874\n",
      "convergence dfGPdfNN Run 8/10, Epoch 800/1000, Training Loss (NLML): -961.0908\n",
      "convergence dfGPdfNN Run 8/10, Epoch 801/1000, Training Loss (NLML): -961.0894\n",
      "convergence dfGPdfNN Run 8/10, Epoch 802/1000, Training Loss (NLML): -961.0977\n",
      "convergence dfGPdfNN Run 8/10, Epoch 803/1000, Training Loss (NLML): -961.0992\n",
      "convergence dfGPdfNN Run 8/10, Epoch 804/1000, Training Loss (NLML): -961.1010\n",
      "convergence dfGPdfNN Run 8/10, Epoch 805/1000, Training Loss (NLML): -961.0985\n",
      "convergence dfGPdfNN Run 8/10, Epoch 806/1000, Training Loss (NLML): -961.1014\n",
      "convergence dfGPdfNN Run 8/10, Epoch 807/1000, Training Loss (NLML): -961.1215\n",
      "convergence dfGPdfNN Run 8/10, Epoch 808/1000, Training Loss (NLML): -961.1021\n",
      "convergence dfGPdfNN Run 8/10, Epoch 809/1000, Training Loss (NLML): -961.1064\n",
      "convergence dfGPdfNN Run 8/10, Epoch 810/1000, Training Loss (NLML): -961.1240\n",
      "convergence dfGPdfNN Run 8/10, Epoch 811/1000, Training Loss (NLML): -961.1096\n",
      "convergence dfGPdfNN Run 8/10, Epoch 812/1000, Training Loss (NLML): -961.1094\n",
      "convergence dfGPdfNN Run 8/10, Epoch 813/1000, Training Loss (NLML): -961.1116\n",
      "convergence dfGPdfNN Run 8/10, Epoch 814/1000, Training Loss (NLML): -961.1340\n",
      "convergence dfGPdfNN Run 8/10, Epoch 815/1000, Training Loss (NLML): -961.1157\n",
      "convergence dfGPdfNN Run 8/10, Epoch 816/1000, Training Loss (NLML): -961.1210\n",
      "convergence dfGPdfNN Run 8/10, Epoch 817/1000, Training Loss (NLML): -961.1184\n",
      "convergence dfGPdfNN Run 8/10, Epoch 818/1000, Training Loss (NLML): -961.1377\n",
      "convergence dfGPdfNN Run 8/10, Epoch 819/1000, Training Loss (NLML): -961.1229\n",
      "convergence dfGPdfNN Run 8/10, Epoch 820/1000, Training Loss (NLML): -961.1215\n",
      "convergence dfGPdfNN Run 8/10, Epoch 821/1000, Training Loss (NLML): -961.1420\n",
      "convergence dfGPdfNN Run 8/10, Epoch 822/1000, Training Loss (NLML): -961.1438\n",
      "convergence dfGPdfNN Run 8/10, Epoch 823/1000, Training Loss (NLML): -961.1244\n",
      "convergence dfGPdfNN Run 8/10, Epoch 824/1000, Training Loss (NLML): -961.1472\n",
      "convergence dfGPdfNN Run 8/10, Epoch 825/1000, Training Loss (NLML): -961.1467\n",
      "convergence dfGPdfNN Run 8/10, Epoch 826/1000, Training Loss (NLML): -961.1500\n",
      "convergence dfGPdfNN Run 8/10, Epoch 827/1000, Training Loss (NLML): -961.1530\n",
      "convergence dfGPdfNN Run 8/10, Epoch 828/1000, Training Loss (NLML): -961.1545\n",
      "convergence dfGPdfNN Run 8/10, Epoch 829/1000, Training Loss (NLML): -961.1554\n",
      "convergence dfGPdfNN Run 8/10, Epoch 830/1000, Training Loss (NLML): -961.1576\n",
      "convergence dfGPdfNN Run 8/10, Epoch 831/1000, Training Loss (NLML): -961.1571\n",
      "convergence dfGPdfNN Run 8/10, Epoch 832/1000, Training Loss (NLML): -961.1643\n",
      "convergence dfGPdfNN Run 8/10, Epoch 833/1000, Training Loss (NLML): -961.1710\n",
      "convergence dfGPdfNN Run 8/10, Epoch 834/1000, Training Loss (NLML): -961.1779\n",
      "convergence dfGPdfNN Run 8/10, Epoch 835/1000, Training Loss (NLML): -961.1783\n",
      "convergence dfGPdfNN Run 8/10, Epoch 836/1000, Training Loss (NLML): -961.1879\n",
      "convergence dfGPdfNN Run 8/10, Epoch 837/1000, Training Loss (NLML): -961.1919\n",
      "convergence dfGPdfNN Run 8/10, Epoch 838/1000, Training Loss (NLML): -961.1968\n",
      "convergence dfGPdfNN Run 8/10, Epoch 839/1000, Training Loss (NLML): -961.1986\n",
      "convergence dfGPdfNN Run 8/10, Epoch 840/1000, Training Loss (NLML): -961.1936\n",
      "convergence dfGPdfNN Run 8/10, Epoch 841/1000, Training Loss (NLML): -961.1945\n",
      "convergence dfGPdfNN Run 8/10, Epoch 842/1000, Training Loss (NLML): -961.1904\n",
      "convergence dfGPdfNN Run 8/10, Epoch 843/1000, Training Loss (NLML): -961.1886\n",
      "convergence dfGPdfNN Run 8/10, Epoch 844/1000, Training Loss (NLML): -961.1908\n",
      "convergence dfGPdfNN Run 8/10, Epoch 845/1000, Training Loss (NLML): -961.2054\n",
      "convergence dfGPdfNN Run 8/10, Epoch 846/1000, Training Loss (NLML): -961.2063\n",
      "convergence dfGPdfNN Run 8/10, Epoch 847/1000, Training Loss (NLML): -961.1860\n",
      "convergence dfGPdfNN Run 8/10, Epoch 848/1000, Training Loss (NLML): -961.1875\n",
      "convergence dfGPdfNN Run 8/10, Epoch 849/1000, Training Loss (NLML): -961.1779\n",
      "convergence dfGPdfNN Run 8/10, Epoch 850/1000, Training Loss (NLML): -961.1818\n",
      "convergence dfGPdfNN Run 8/10, Epoch 851/1000, Training Loss (NLML): -961.1899\n",
      "convergence dfGPdfNN Run 8/10, Epoch 852/1000, Training Loss (NLML): -961.1901\n",
      "convergence dfGPdfNN Run 8/10, Epoch 853/1000, Training Loss (NLML): -961.1870\n",
      "convergence dfGPdfNN Run 8/10, Epoch 854/1000, Training Loss (NLML): -961.1897\n",
      "convergence dfGPdfNN Run 8/10, Epoch 855/1000, Training Loss (NLML): -961.1833\n",
      "convergence dfGPdfNN Run 8/10, Epoch 856/1000, Training Loss (NLML): -961.1887\n",
      "convergence dfGPdfNN Run 8/10, Epoch 857/1000, Training Loss (NLML): -961.1848\n",
      "convergence dfGPdfNN Run 8/10, Epoch 858/1000, Training Loss (NLML): -961.2034\n",
      "convergence dfGPdfNN Run 8/10, Epoch 859/1000, Training Loss (NLML): -961.1885\n",
      "convergence dfGPdfNN Run 8/10, Epoch 860/1000, Training Loss (NLML): -961.1930\n",
      "convergence dfGPdfNN Run 8/10, Epoch 861/1000, Training Loss (NLML): -961.1982\n",
      "convergence dfGPdfNN Run 8/10, Epoch 862/1000, Training Loss (NLML): -961.2196\n",
      "convergence dfGPdfNN Run 8/10, Epoch 863/1000, Training Loss (NLML): -961.1896\n",
      "convergence dfGPdfNN Run 8/10, Epoch 864/1000, Training Loss (NLML): -961.2173\n",
      "convergence dfGPdfNN Run 8/10, Epoch 865/1000, Training Loss (NLML): -961.2170\n",
      "convergence dfGPdfNN Run 8/10, Epoch 866/1000, Training Loss (NLML): -961.2313\n",
      "convergence dfGPdfNN Run 8/10, Epoch 867/1000, Training Loss (NLML): -961.2260\n",
      "convergence dfGPdfNN Run 8/10, Epoch 868/1000, Training Loss (NLML): -961.2389\n",
      "convergence dfGPdfNN Run 8/10, Epoch 869/1000, Training Loss (NLML): -961.2446\n",
      "convergence dfGPdfNN Run 8/10, Epoch 870/1000, Training Loss (NLML): -961.2411\n",
      "convergence dfGPdfNN Run 8/10, Epoch 871/1000, Training Loss (NLML): -961.2418\n",
      "convergence dfGPdfNN Run 8/10, Epoch 872/1000, Training Loss (NLML): -961.2458\n",
      "convergence dfGPdfNN Run 8/10, Epoch 873/1000, Training Loss (NLML): -961.2377\n",
      "convergence dfGPdfNN Run 8/10, Epoch 874/1000, Training Loss (NLML): -961.2437\n",
      "convergence dfGPdfNN Run 8/10, Epoch 875/1000, Training Loss (NLML): -961.2400\n",
      "convergence dfGPdfNN Run 8/10, Epoch 876/1000, Training Loss (NLML): -961.2288\n",
      "convergence dfGPdfNN Run 8/10, Epoch 877/1000, Training Loss (NLML): -961.2335\n",
      "convergence dfGPdfNN Run 8/10, Epoch 878/1000, Training Loss (NLML): -961.2408\n",
      "convergence dfGPdfNN Run 8/10, Epoch 879/1000, Training Loss (NLML): -961.2371\n",
      "convergence dfGPdfNN Run 8/10, Epoch 880/1000, Training Loss (NLML): -961.2432\n",
      "convergence dfGPdfNN Run 8/10, Epoch 881/1000, Training Loss (NLML): -961.2356\n",
      "convergence dfGPdfNN Run 8/10, Epoch 882/1000, Training Loss (NLML): -961.2432\n",
      "convergence dfGPdfNN Run 8/10, Epoch 883/1000, Training Loss (NLML): -961.2472\n",
      "convergence dfGPdfNN Run 8/10, Epoch 884/1000, Training Loss (NLML): -961.2509\n",
      "convergence dfGPdfNN Run 8/10, Epoch 885/1000, Training Loss (NLML): -961.2513\n",
      "convergence dfGPdfNN Run 8/10, Epoch 886/1000, Training Loss (NLML): -961.2593\n",
      "convergence dfGPdfNN Run 8/10, Epoch 887/1000, Training Loss (NLML): -961.2648\n",
      "convergence dfGPdfNN Run 8/10, Epoch 888/1000, Training Loss (NLML): -961.2708\n",
      "convergence dfGPdfNN Run 8/10, Epoch 889/1000, Training Loss (NLML): -961.2692\n",
      "convergence dfGPdfNN Run 8/10, Epoch 890/1000, Training Loss (NLML): -960.1447\n",
      "convergence dfGPdfNN Run 8/10, Epoch 891/1000, Training Loss (NLML): -961.2695\n",
      "convergence dfGPdfNN Run 8/10, Epoch 892/1000, Training Loss (NLML): -961.2861\n",
      "convergence dfGPdfNN Run 8/10, Epoch 893/1000, Training Loss (NLML): -961.3182\n",
      "convergence dfGPdfNN Run 8/10, Epoch 894/1000, Training Loss (NLML): -961.2969\n",
      "convergence dfGPdfNN Run 8/10, Epoch 895/1000, Training Loss (NLML): -961.2893\n",
      "convergence dfGPdfNN Run 8/10, Epoch 896/1000, Training Loss (NLML): -961.2949\n",
      "convergence dfGPdfNN Run 8/10, Epoch 897/1000, Training Loss (NLML): -961.3088\n",
      "convergence dfGPdfNN Run 8/10, Epoch 898/1000, Training Loss (NLML): -961.2814\n",
      "convergence dfGPdfNN Run 8/10, Epoch 899/1000, Training Loss (NLML): -961.2433\n",
      "convergence dfGPdfNN Run 8/10, Epoch 900/1000, Training Loss (NLML): -961.2589\n",
      "convergence dfGPdfNN Run 8/10, Epoch 901/1000, Training Loss (NLML): -961.2429\n",
      "convergence dfGPdfNN Run 8/10, Epoch 902/1000, Training Loss (NLML): -961.2238\n",
      "convergence dfGPdfNN Run 8/10, Epoch 903/1000, Training Loss (NLML): -961.2267\n",
      "convergence dfGPdfNN Run 8/10, Epoch 904/1000, Training Loss (NLML): -961.2395\n",
      "convergence dfGPdfNN Run 8/10, Epoch 905/1000, Training Loss (NLML): -961.2793\n",
      "convergence dfGPdfNN Run 8/10, Epoch 906/1000, Training Loss (NLML): -961.2729\n",
      "convergence dfGPdfNN Run 8/10, Epoch 907/1000, Training Loss (NLML): -961.2379\n",
      "convergence dfGPdfNN Run 8/10, Epoch 908/1000, Training Loss (NLML): -961.2141\n",
      "convergence dfGPdfNN Run 8/10, Epoch 909/1000, Training Loss (NLML): -961.2034\n",
      "convergence dfGPdfNN Run 8/10, Epoch 910/1000, Training Loss (NLML): -961.2157\n",
      "convergence dfGPdfNN Run 8/10, Epoch 911/1000, Training Loss (NLML): -961.2372\n",
      "convergence dfGPdfNN Run 8/10, Epoch 912/1000, Training Loss (NLML): -961.2345\n",
      "convergence dfGPdfNN Run 8/10, Epoch 913/1000, Training Loss (NLML): -961.2234\n",
      "convergence dfGPdfNN Run 8/10, Epoch 914/1000, Training Loss (NLML): -945.2922\n",
      "convergence dfGPdfNN Run 8/10, Epoch 915/1000, Training Loss (NLML): -961.0172\n",
      "convergence dfGPdfNN Run 8/10, Epoch 916/1000, Training Loss (NLML): -960.9396\n",
      "convergence dfGPdfNN Run 8/10, Epoch 917/1000, Training Loss (NLML): -960.9733\n",
      "convergence dfGPdfNN Run 8/10, Epoch 918/1000, Training Loss (NLML): -960.9099\n",
      "convergence dfGPdfNN Run 8/10, Epoch 919/1000, Training Loss (NLML): -960.8218\n",
      "convergence dfGPdfNN Run 8/10, Epoch 920/1000, Training Loss (NLML): -960.1385\n",
      "convergence dfGPdfNN Run 8/10, Epoch 921/1000, Training Loss (NLML): -960.2765\n",
      "convergence dfGPdfNN Run 8/10, Epoch 922/1000, Training Loss (NLML): -960.3748\n",
      "convergence dfGPdfNN Run 8/10, Epoch 923/1000, Training Loss (NLML): -960.4685\n",
      "convergence dfGPdfNN Run 8/10, Epoch 924/1000, Training Loss (NLML): -960.3497\n",
      "convergence dfGPdfNN Run 8/10, Epoch 925/1000, Training Loss (NLML): -960.5566\n",
      "convergence dfGPdfNN Run 8/10, Epoch 926/1000, Training Loss (NLML): -960.5642\n",
      "convergence dfGPdfNN Run 8/10, Epoch 927/1000, Training Loss (NLML): -960.5094\n",
      "convergence dfGPdfNN Run 8/10, Epoch 928/1000, Training Loss (NLML): -960.4313\n",
      "convergence dfGPdfNN Run 8/10, Epoch 929/1000, Training Loss (NLML): -960.4271\n",
      "convergence dfGPdfNN Run 8/10, Epoch 930/1000, Training Loss (NLML): -960.4886\n",
      "convergence dfGPdfNN Run 8/10, Epoch 931/1000, Training Loss (NLML): -960.6133\n",
      "convergence dfGPdfNN Run 8/10, Epoch 932/1000, Training Loss (NLML): -960.6963\n",
      "convergence dfGPdfNN Run 8/10, Epoch 933/1000, Training Loss (NLML): -960.7596\n",
      "convergence dfGPdfNN Run 8/10, Epoch 934/1000, Training Loss (NLML): -960.7900\n",
      "convergence dfGPdfNN Run 8/10, Epoch 935/1000, Training Loss (NLML): -960.7892\n",
      "convergence dfGPdfNN Run 8/10, Epoch 936/1000, Training Loss (NLML): -960.4631\n",
      "convergence dfGPdfNN Run 8/10, Epoch 937/1000, Training Loss (NLML): -960.7904\n",
      "convergence dfGPdfNN Run 8/10, Epoch 938/1000, Training Loss (NLML): -959.8354\n",
      "convergence dfGPdfNN Run 8/10, Epoch 939/1000, Training Loss (NLML): -959.0811\n",
      "convergence dfGPdfNN Run 8/10, Epoch 940/1000, Training Loss (NLML): -958.4193\n",
      "convergence dfGPdfNN Run 8/10, Epoch 941/1000, Training Loss (NLML): -959.7782\n",
      "convergence dfGPdfNN Run 8/10, Epoch 942/1000, Training Loss (NLML): -960.6954\n",
      "convergence dfGPdfNN Run 8/10, Epoch 943/1000, Training Loss (NLML): -960.6818\n",
      "Early stopping triggered after 943 epochs.\n",
      "\n",
      "--- Training Run 9/10 ---\n",
      "\n",
      "Start Training\n",
      "convergence dfGPdfNN Run 9/10, Epoch 1/1000, Training Loss (NLML): -850.6857\n",
      "convergence dfGPdfNN Run 9/10, Epoch 2/1000, Training Loss (NLML): -860.6410\n",
      "convergence dfGPdfNN Run 9/10, Epoch 3/1000, Training Loss (NLML): -870.0913\n",
      "convergence dfGPdfNN Run 9/10, Epoch 4/1000, Training Loss (NLML): -878.2495\n",
      "convergence dfGPdfNN Run 9/10, Epoch 5/1000, Training Loss (NLML): -881.9772\n",
      "convergence dfGPdfNN Run 9/10, Epoch 6/1000, Training Loss (NLML): -885.0468\n",
      "convergence dfGPdfNN Run 9/10, Epoch 7/1000, Training Loss (NLML): -887.4518\n",
      "convergence dfGPdfNN Run 9/10, Epoch 8/1000, Training Loss (NLML): -889.3878\n",
      "convergence dfGPdfNN Run 9/10, Epoch 9/1000, Training Loss (NLML): -891.2952\n",
      "convergence dfGPdfNN Run 9/10, Epoch 10/1000, Training Loss (NLML): -893.1926\n",
      "convergence dfGPdfNN Run 9/10, Epoch 11/1000, Training Loss (NLML): -895.1665\n",
      "convergence dfGPdfNN Run 9/10, Epoch 12/1000, Training Loss (NLML): -897.2554\n",
      "convergence dfGPdfNN Run 9/10, Epoch 13/1000, Training Loss (NLML): -899.1575\n",
      "convergence dfGPdfNN Run 9/10, Epoch 14/1000, Training Loss (NLML): -901.0878\n",
      "convergence dfGPdfNN Run 9/10, Epoch 15/1000, Training Loss (NLML): -902.8147\n",
      "convergence dfGPdfNN Run 9/10, Epoch 16/1000, Training Loss (NLML): -904.4141\n",
      "convergence dfGPdfNN Run 9/10, Epoch 17/1000, Training Loss (NLML): -905.9965\n",
      "convergence dfGPdfNN Run 9/10, Epoch 18/1000, Training Loss (NLML): -907.4084\n",
      "convergence dfGPdfNN Run 9/10, Epoch 19/1000, Training Loss (NLML): -908.7676\n",
      "convergence dfGPdfNN Run 9/10, Epoch 20/1000, Training Loss (NLML): -910.0321\n",
      "convergence dfGPdfNN Run 9/10, Epoch 21/1000, Training Loss (NLML): -911.2339\n",
      "convergence dfGPdfNN Run 9/10, Epoch 22/1000, Training Loss (NLML): -912.4205\n",
      "convergence dfGPdfNN Run 9/10, Epoch 23/1000, Training Loss (NLML): -913.5645\n",
      "convergence dfGPdfNN Run 9/10, Epoch 24/1000, Training Loss (NLML): -914.6560\n",
      "convergence dfGPdfNN Run 9/10, Epoch 25/1000, Training Loss (NLML): -915.6930\n",
      "convergence dfGPdfNN Run 9/10, Epoch 26/1000, Training Loss (NLML): -916.6946\n",
      "convergence dfGPdfNN Run 9/10, Epoch 27/1000, Training Loss (NLML): -917.6946\n",
      "convergence dfGPdfNN Run 9/10, Epoch 28/1000, Training Loss (NLML): -918.6454\n",
      "convergence dfGPdfNN Run 9/10, Epoch 29/1000, Training Loss (NLML): -919.5392\n",
      "convergence dfGPdfNN Run 9/10, Epoch 30/1000, Training Loss (NLML): -920.3745\n",
      "convergence dfGPdfNN Run 9/10, Epoch 31/1000, Training Loss (NLML): -921.3036\n",
      "convergence dfGPdfNN Run 9/10, Epoch 32/1000, Training Loss (NLML): -922.0952\n",
      "convergence dfGPdfNN Run 9/10, Epoch 33/1000, Training Loss (NLML): -922.9050\n",
      "convergence dfGPdfNN Run 9/10, Epoch 34/1000, Training Loss (NLML): -923.6725\n",
      "convergence dfGPdfNN Run 9/10, Epoch 35/1000, Training Loss (NLML): -924.3905\n",
      "convergence dfGPdfNN Run 9/10, Epoch 36/1000, Training Loss (NLML): -925.0985\n",
      "convergence dfGPdfNN Run 9/10, Epoch 37/1000, Training Loss (NLML): -925.7748\n",
      "convergence dfGPdfNN Run 9/10, Epoch 38/1000, Training Loss (NLML): -926.4310\n",
      "convergence dfGPdfNN Run 9/10, Epoch 39/1000, Training Loss (NLML): -927.0811\n",
      "convergence dfGPdfNN Run 9/10, Epoch 40/1000, Training Loss (NLML): -927.7135\n",
      "convergence dfGPdfNN Run 9/10, Epoch 41/1000, Training Loss (NLML): -928.2946\n",
      "convergence dfGPdfNN Run 9/10, Epoch 42/1000, Training Loss (NLML): -928.8643\n",
      "convergence dfGPdfNN Run 9/10, Epoch 43/1000, Training Loss (NLML): -929.4240\n",
      "convergence dfGPdfNN Run 9/10, Epoch 44/1000, Training Loss (NLML): -929.9557\n",
      "convergence dfGPdfNN Run 9/10, Epoch 45/1000, Training Loss (NLML): -930.4835\n",
      "convergence dfGPdfNN Run 9/10, Epoch 46/1000, Training Loss (NLML): -930.9402\n",
      "convergence dfGPdfNN Run 9/10, Epoch 47/1000, Training Loss (NLML): -931.3898\n",
      "convergence dfGPdfNN Run 9/10, Epoch 48/1000, Training Loss (NLML): -931.9249\n",
      "convergence dfGPdfNN Run 9/10, Epoch 49/1000, Training Loss (NLML): -932.4686\n",
      "convergence dfGPdfNN Run 9/10, Epoch 50/1000, Training Loss (NLML): -932.9667\n",
      "convergence dfGPdfNN Run 9/10, Epoch 51/1000, Training Loss (NLML): -933.4347\n",
      "convergence dfGPdfNN Run 9/10, Epoch 52/1000, Training Loss (NLML): -933.8823\n",
      "convergence dfGPdfNN Run 9/10, Epoch 53/1000, Training Loss (NLML): -934.3341\n",
      "convergence dfGPdfNN Run 9/10, Epoch 54/1000, Training Loss (NLML): -934.7828\n",
      "convergence dfGPdfNN Run 9/10, Epoch 55/1000, Training Loss (NLML): -935.1658\n",
      "convergence dfGPdfNN Run 9/10, Epoch 56/1000, Training Loss (NLML): -935.5955\n",
      "convergence dfGPdfNN Run 9/10, Epoch 57/1000, Training Loss (NLML): -935.9928\n",
      "convergence dfGPdfNN Run 9/10, Epoch 58/1000, Training Loss (NLML): -936.4031\n",
      "convergence dfGPdfNN Run 9/10, Epoch 59/1000, Training Loss (NLML): -936.5995\n",
      "convergence dfGPdfNN Run 9/10, Epoch 60/1000, Training Loss (NLML): -936.8430\n",
      "convergence dfGPdfNN Run 9/10, Epoch 61/1000, Training Loss (NLML): -936.9323\n",
      "convergence dfGPdfNN Run 9/10, Epoch 62/1000, Training Loss (NLML): -937.5284\n",
      "convergence dfGPdfNN Run 9/10, Epoch 63/1000, Training Loss (NLML): -937.3508\n",
      "convergence dfGPdfNN Run 9/10, Epoch 64/1000, Training Loss (NLML): -937.9684\n",
      "convergence dfGPdfNN Run 9/10, Epoch 65/1000, Training Loss (NLML): -938.4236\n",
      "convergence dfGPdfNN Run 9/10, Epoch 66/1000, Training Loss (NLML): -938.8243\n",
      "convergence dfGPdfNN Run 9/10, Epoch 67/1000, Training Loss (NLML): -939.1493\n",
      "convergence dfGPdfNN Run 9/10, Epoch 68/1000, Training Loss (NLML): -939.5244\n",
      "convergence dfGPdfNN Run 9/10, Epoch 69/1000, Training Loss (NLML): -939.9150\n",
      "convergence dfGPdfNN Run 9/10, Epoch 70/1000, Training Loss (NLML): -940.2614\n",
      "convergence dfGPdfNN Run 9/10, Epoch 71/1000, Training Loss (NLML): -940.5912\n",
      "convergence dfGPdfNN Run 9/10, Epoch 72/1000, Training Loss (NLML): -940.8945\n",
      "convergence dfGPdfNN Run 9/10, Epoch 73/1000, Training Loss (NLML): -941.1846\n",
      "convergence dfGPdfNN Run 9/10, Epoch 74/1000, Training Loss (NLML): -941.4771\n",
      "convergence dfGPdfNN Run 9/10, Epoch 75/1000, Training Loss (NLML): -941.7664\n",
      "convergence dfGPdfNN Run 9/10, Epoch 76/1000, Training Loss (NLML): -942.0344\n",
      "convergence dfGPdfNN Run 9/10, Epoch 77/1000, Training Loss (NLML): -942.2986\n",
      "convergence dfGPdfNN Run 9/10, Epoch 78/1000, Training Loss (NLML): -942.5631\n",
      "convergence dfGPdfNN Run 9/10, Epoch 79/1000, Training Loss (NLML): -942.8350\n",
      "convergence dfGPdfNN Run 9/10, Epoch 80/1000, Training Loss (NLML): -943.0934\n",
      "convergence dfGPdfNN Run 9/10, Epoch 81/1000, Training Loss (NLML): -943.3425\n",
      "convergence dfGPdfNN Run 9/10, Epoch 82/1000, Training Loss (NLML): -943.6000\n",
      "convergence dfGPdfNN Run 9/10, Epoch 83/1000, Training Loss (NLML): -943.8353\n",
      "convergence dfGPdfNN Run 9/10, Epoch 84/1000, Training Loss (NLML): -944.0742\n",
      "convergence dfGPdfNN Run 9/10, Epoch 85/1000, Training Loss (NLML): -944.2937\n",
      "convergence dfGPdfNN Run 9/10, Epoch 86/1000, Training Loss (NLML): -944.5531\n",
      "convergence dfGPdfNN Run 9/10, Epoch 87/1000, Training Loss (NLML): -944.7876\n",
      "convergence dfGPdfNN Run 9/10, Epoch 88/1000, Training Loss (NLML): -945.0281\n",
      "convergence dfGPdfNN Run 9/10, Epoch 89/1000, Training Loss (NLML): -945.2645\n",
      "convergence dfGPdfNN Run 9/10, Epoch 90/1000, Training Loss (NLML): -945.4995\n",
      "convergence dfGPdfNN Run 9/10, Epoch 91/1000, Training Loss (NLML): -945.7318\n",
      "convergence dfGPdfNN Run 9/10, Epoch 92/1000, Training Loss (NLML): -945.9598\n",
      "convergence dfGPdfNN Run 9/10, Epoch 93/1000, Training Loss (NLML): -946.1816\n",
      "convergence dfGPdfNN Run 9/10, Epoch 94/1000, Training Loss (NLML): -946.4299\n",
      "convergence dfGPdfNN Run 9/10, Epoch 95/1000, Training Loss (NLML): -946.6479\n",
      "convergence dfGPdfNN Run 9/10, Epoch 96/1000, Training Loss (NLML): -946.8506\n",
      "convergence dfGPdfNN Run 9/10, Epoch 97/1000, Training Loss (NLML): -947.0415\n",
      "convergence dfGPdfNN Run 9/10, Epoch 98/1000, Training Loss (NLML): -947.2460\n",
      "convergence dfGPdfNN Run 9/10, Epoch 99/1000, Training Loss (NLML): -947.4701\n",
      "convergence dfGPdfNN Run 9/10, Epoch 100/1000, Training Loss (NLML): -947.6537\n",
      "convergence dfGPdfNN Run 9/10, Epoch 101/1000, Training Loss (NLML): -947.7634\n",
      "convergence dfGPdfNN Run 9/10, Epoch 102/1000, Training Loss (NLML): -947.9348\n",
      "convergence dfGPdfNN Run 9/10, Epoch 103/1000, Training Loss (NLML): -948.1499\n",
      "convergence dfGPdfNN Run 9/10, Epoch 104/1000, Training Loss (NLML): -948.3815\n",
      "convergence dfGPdfNN Run 9/10, Epoch 105/1000, Training Loss (NLML): -944.5042\n",
      "convergence dfGPdfNN Run 9/10, Epoch 106/1000, Training Loss (NLML): -945.9545\n",
      "convergence dfGPdfNN Run 9/10, Epoch 107/1000, Training Loss (NLML): -946.4425\n",
      "convergence dfGPdfNN Run 9/10, Epoch 108/1000, Training Loss (NLML): -942.5265\n",
      "convergence dfGPdfNN Run 9/10, Epoch 109/1000, Training Loss (NLML): -948.7656\n",
      "convergence dfGPdfNN Run 9/10, Epoch 110/1000, Training Loss (NLML): -948.8314\n",
      "convergence dfGPdfNN Run 9/10, Epoch 111/1000, Training Loss (NLML): -948.8103\n",
      "convergence dfGPdfNN Run 9/10, Epoch 112/1000, Training Loss (NLML): -948.8760\n",
      "convergence dfGPdfNN Run 9/10, Epoch 113/1000, Training Loss (NLML): -948.9857\n",
      "convergence dfGPdfNN Run 9/10, Epoch 114/1000, Training Loss (NLML): -949.1541\n",
      "convergence dfGPdfNN Run 9/10, Epoch 115/1000, Training Loss (NLML): -949.5374\n",
      "convergence dfGPdfNN Run 9/10, Epoch 116/1000, Training Loss (NLML): -949.6887\n",
      "convergence dfGPdfNN Run 9/10, Epoch 117/1000, Training Loss (NLML): -949.8663\n",
      "convergence dfGPdfNN Run 9/10, Epoch 118/1000, Training Loss (NLML): -950.1327\n",
      "convergence dfGPdfNN Run 9/10, Epoch 119/1000, Training Loss (NLML): -950.2354\n",
      "convergence dfGPdfNN Run 9/10, Epoch 120/1000, Training Loss (NLML): -950.0959\n",
      "convergence dfGPdfNN Run 9/10, Epoch 121/1000, Training Loss (NLML): -950.2251\n",
      "convergence dfGPdfNN Run 9/10, Epoch 122/1000, Training Loss (NLML): -950.3478\n",
      "convergence dfGPdfNN Run 9/10, Epoch 123/1000, Training Loss (NLML): -950.4946\n",
      "convergence dfGPdfNN Run 9/10, Epoch 124/1000, Training Loss (NLML): -950.6324\n",
      "convergence dfGPdfNN Run 9/10, Epoch 125/1000, Training Loss (NLML): -950.9204\n",
      "convergence dfGPdfNN Run 9/10, Epoch 126/1000, Training Loss (NLML): -951.1703\n",
      "convergence dfGPdfNN Run 9/10, Epoch 127/1000, Training Loss (NLML): -951.3806\n",
      "convergence dfGPdfNN Run 9/10, Epoch 128/1000, Training Loss (NLML): -951.5966\n",
      "convergence dfGPdfNN Run 9/10, Epoch 129/1000, Training Loss (NLML): -940.1459\n",
      "convergence dfGPdfNN Run 9/10, Epoch 130/1000, Training Loss (NLML): -951.8530\n",
      "convergence dfGPdfNN Run 9/10, Epoch 131/1000, Training Loss (NLML): -951.9501\n",
      "convergence dfGPdfNN Run 9/10, Epoch 132/1000, Training Loss (NLML): -952.1172\n",
      "convergence dfGPdfNN Run 9/10, Epoch 133/1000, Training Loss (NLML): -952.1997\n",
      "convergence dfGPdfNN Run 9/10, Epoch 134/1000, Training Loss (NLML): -952.3286\n",
      "convergence dfGPdfNN Run 9/10, Epoch 135/1000, Training Loss (NLML): -952.4633\n",
      "convergence dfGPdfNN Run 9/10, Epoch 136/1000, Training Loss (NLML): -952.5327\n",
      "convergence dfGPdfNN Run 9/10, Epoch 137/1000, Training Loss (NLML): -952.7529\n",
      "convergence dfGPdfNN Run 9/10, Epoch 138/1000, Training Loss (NLML): -952.9037\n",
      "convergence dfGPdfNN Run 9/10, Epoch 139/1000, Training Loss (NLML): -953.0250\n",
      "convergence dfGPdfNN Run 9/10, Epoch 140/1000, Training Loss (NLML): -953.0607\n",
      "convergence dfGPdfNN Run 9/10, Epoch 141/1000, Training Loss (NLML): -951.1669\n",
      "convergence dfGPdfNN Run 9/10, Epoch 142/1000, Training Loss (NLML): -953.4463\n",
      "convergence dfGPdfNN Run 9/10, Epoch 143/1000, Training Loss (NLML): -953.5563\n",
      "convergence dfGPdfNN Run 9/10, Epoch 144/1000, Training Loss (NLML): -953.5615\n",
      "convergence dfGPdfNN Run 9/10, Epoch 145/1000, Training Loss (NLML): -953.5479\n",
      "convergence dfGPdfNN Run 9/10, Epoch 146/1000, Training Loss (NLML): -950.8877\n",
      "convergence dfGPdfNN Run 9/10, Epoch 147/1000, Training Loss (NLML): -953.6888\n",
      "convergence dfGPdfNN Run 9/10, Epoch 148/1000, Training Loss (NLML): -953.7701\n",
      "convergence dfGPdfNN Run 9/10, Epoch 149/1000, Training Loss (NLML): -953.8186\n",
      "convergence dfGPdfNN Run 9/10, Epoch 150/1000, Training Loss (NLML): -953.9119\n",
      "convergence dfGPdfNN Run 9/10, Epoch 151/1000, Training Loss (NLML): -953.9974\n",
      "convergence dfGPdfNN Run 9/10, Epoch 152/1000, Training Loss (NLML): -954.1324\n",
      "convergence dfGPdfNN Run 9/10, Epoch 153/1000, Training Loss (NLML): -954.2253\n",
      "convergence dfGPdfNN Run 9/10, Epoch 154/1000, Training Loss (NLML): -954.2881\n",
      "convergence dfGPdfNN Run 9/10, Epoch 155/1000, Training Loss (NLML): -904.0782\n",
      "convergence dfGPdfNN Run 9/10, Epoch 156/1000, Training Loss (NLML): -954.3448\n",
      "convergence dfGPdfNN Run 9/10, Epoch 157/1000, Training Loss (NLML): -954.4119\n",
      "convergence dfGPdfNN Run 9/10, Epoch 158/1000, Training Loss (NLML): -954.3544\n",
      "convergence dfGPdfNN Run 9/10, Epoch 159/1000, Training Loss (NLML): -954.1349\n",
      "convergence dfGPdfNN Run 9/10, Epoch 160/1000, Training Loss (NLML): -953.9514\n",
      "convergence dfGPdfNN Run 9/10, Epoch 161/1000, Training Loss (NLML): -953.8346\n",
      "convergence dfGPdfNN Run 9/10, Epoch 162/1000, Training Loss (NLML): -953.9199\n",
      "convergence dfGPdfNN Run 9/10, Epoch 163/1000, Training Loss (NLML): -953.9586\n",
      "convergence dfGPdfNN Run 9/10, Epoch 164/1000, Training Loss (NLML): -953.9806\n",
      "convergence dfGPdfNN Run 9/10, Epoch 165/1000, Training Loss (NLML): -953.8030\n",
      "convergence dfGPdfNN Run 9/10, Epoch 166/1000, Training Loss (NLML): -953.9205\n",
      "convergence dfGPdfNN Run 9/10, Epoch 167/1000, Training Loss (NLML): -954.0505\n",
      "convergence dfGPdfNN Run 9/10, Epoch 168/1000, Training Loss (NLML): -953.7645\n",
      "convergence dfGPdfNN Run 9/10, Epoch 169/1000, Training Loss (NLML): -953.7939\n",
      "convergence dfGPdfNN Run 9/10, Epoch 170/1000, Training Loss (NLML): -954.0137\n",
      "convergence dfGPdfNN Run 9/10, Epoch 171/1000, Training Loss (NLML): -953.8586\n",
      "convergence dfGPdfNN Run 9/10, Epoch 172/1000, Training Loss (NLML): -953.7531\n",
      "convergence dfGPdfNN Run 9/10, Epoch 173/1000, Training Loss (NLML): -953.9072\n",
      "convergence dfGPdfNN Run 9/10, Epoch 174/1000, Training Loss (NLML): -953.9617\n",
      "convergence dfGPdfNN Run 9/10, Epoch 175/1000, Training Loss (NLML): -954.1725\n",
      "convergence dfGPdfNN Run 9/10, Epoch 176/1000, Training Loss (NLML): -954.2853\n",
      "convergence dfGPdfNN Run 9/10, Epoch 177/1000, Training Loss (NLML): -954.4453\n",
      "convergence dfGPdfNN Run 9/10, Epoch 178/1000, Training Loss (NLML): -954.4437\n",
      "convergence dfGPdfNN Run 9/10, Epoch 179/1000, Training Loss (NLML): -954.2516\n",
      "convergence dfGPdfNN Run 9/10, Epoch 180/1000, Training Loss (NLML): -953.7390\n",
      "convergence dfGPdfNN Run 9/10, Epoch 181/1000, Training Loss (NLML): -953.4261\n",
      "convergence dfGPdfNN Run 9/10, Epoch 182/1000, Training Loss (NLML): -954.1543\n",
      "convergence dfGPdfNN Run 9/10, Epoch 183/1000, Training Loss (NLML): -954.2911\n",
      "convergence dfGPdfNN Run 9/10, Epoch 184/1000, Training Loss (NLML): -954.5483\n",
      "convergence dfGPdfNN Run 9/10, Epoch 185/1000, Training Loss (NLML): -954.2379\n",
      "convergence dfGPdfNN Run 9/10, Epoch 186/1000, Training Loss (NLML): -954.4923\n",
      "convergence dfGPdfNN Run 9/10, Epoch 187/1000, Training Loss (NLML): -954.7410\n",
      "convergence dfGPdfNN Run 9/10, Epoch 188/1000, Training Loss (NLML): -954.6991\n",
      "convergence dfGPdfNN Run 9/10, Epoch 189/1000, Training Loss (NLML): -954.8123\n",
      "convergence dfGPdfNN Run 9/10, Epoch 190/1000, Training Loss (NLML): -954.9828\n",
      "convergence dfGPdfNN Run 9/10, Epoch 191/1000, Training Loss (NLML): -955.0542\n",
      "convergence dfGPdfNN Run 9/10, Epoch 192/1000, Training Loss (NLML): -954.6221\n",
      "convergence dfGPdfNN Run 9/10, Epoch 193/1000, Training Loss (NLML): -954.7781\n",
      "convergence dfGPdfNN Run 9/10, Epoch 194/1000, Training Loss (NLML): -954.9348\n",
      "convergence dfGPdfNN Run 9/10, Epoch 195/1000, Training Loss (NLML): -954.7853\n",
      "convergence dfGPdfNN Run 9/10, Epoch 196/1000, Training Loss (NLML): -954.9485\n",
      "convergence dfGPdfNN Run 9/10, Epoch 197/1000, Training Loss (NLML): -955.1798\n",
      "convergence dfGPdfNN Run 9/10, Epoch 198/1000, Training Loss (NLML): -955.2421\n",
      "convergence dfGPdfNN Run 9/10, Epoch 199/1000, Training Loss (NLML): -955.2805\n",
      "convergence dfGPdfNN Run 9/10, Epoch 200/1000, Training Loss (NLML): -955.0675\n",
      "convergence dfGPdfNN Run 9/10, Epoch 201/1000, Training Loss (NLML): -954.7976\n",
      "convergence dfGPdfNN Run 9/10, Epoch 202/1000, Training Loss (NLML): -955.0298\n",
      "convergence dfGPdfNN Run 9/10, Epoch 203/1000, Training Loss (NLML): -955.3765\n",
      "convergence dfGPdfNN Run 9/10, Epoch 204/1000, Training Loss (NLML): -955.6080\n",
      "convergence dfGPdfNN Run 9/10, Epoch 205/1000, Training Loss (NLML): -955.6471\n",
      "convergence dfGPdfNN Run 9/10, Epoch 206/1000, Training Loss (NLML): -954.4723\n",
      "convergence dfGPdfNN Run 9/10, Epoch 207/1000, Training Loss (NLML): -954.8762\n",
      "convergence dfGPdfNN Run 9/10, Epoch 208/1000, Training Loss (NLML): -954.7511\n",
      "convergence dfGPdfNN Run 9/10, Epoch 209/1000, Training Loss (NLML): -937.9642\n",
      "convergence dfGPdfNN Run 9/10, Epoch 210/1000, Training Loss (NLML): -942.7351\n",
      "convergence dfGPdfNN Run 9/10, Epoch 211/1000, Training Loss (NLML): -954.5840\n",
      "convergence dfGPdfNN Run 9/10, Epoch 212/1000, Training Loss (NLML): -954.8497\n",
      "convergence dfGPdfNN Run 9/10, Epoch 213/1000, Training Loss (NLML): -955.2391\n",
      "convergence dfGPdfNN Run 9/10, Epoch 214/1000, Training Loss (NLML): -955.5873\n",
      "convergence dfGPdfNN Run 9/10, Epoch 215/1000, Training Loss (NLML): -956.0186\n",
      "convergence dfGPdfNN Run 9/10, Epoch 216/1000, Training Loss (NLML): -955.9324\n",
      "convergence dfGPdfNN Run 9/10, Epoch 217/1000, Training Loss (NLML): -955.3265\n",
      "convergence dfGPdfNN Run 9/10, Epoch 218/1000, Training Loss (NLML): -954.5758\n",
      "convergence dfGPdfNN Run 9/10, Epoch 219/1000, Training Loss (NLML): -954.4747\n",
      "convergence dfGPdfNN Run 9/10, Epoch 220/1000, Training Loss (NLML): -954.6373\n",
      "convergence dfGPdfNN Run 9/10, Epoch 221/1000, Training Loss (NLML): -955.5448\n",
      "convergence dfGPdfNN Run 9/10, Epoch 222/1000, Training Loss (NLML): -954.8046\n",
      "convergence dfGPdfNN Run 9/10, Epoch 223/1000, Training Loss (NLML): -954.8835\n",
      "convergence dfGPdfNN Run 9/10, Epoch 224/1000, Training Loss (NLML): -954.7888\n",
      "convergence dfGPdfNN Run 9/10, Epoch 225/1000, Training Loss (NLML): -954.8339\n",
      "convergence dfGPdfNN Run 9/10, Epoch 226/1000, Training Loss (NLML): -954.9321\n",
      "convergence dfGPdfNN Run 9/10, Epoch 227/1000, Training Loss (NLML): -955.0685\n",
      "convergence dfGPdfNN Run 9/10, Epoch 228/1000, Training Loss (NLML): -955.1167\n",
      "convergence dfGPdfNN Run 9/10, Epoch 229/1000, Training Loss (NLML): -955.1842\n",
      "convergence dfGPdfNN Run 9/10, Epoch 230/1000, Training Loss (NLML): -955.1349\n",
      "convergence dfGPdfNN Run 9/10, Epoch 231/1000, Training Loss (NLML): -954.5153\n",
      "convergence dfGPdfNN Run 9/10, Epoch 232/1000, Training Loss (NLML): -954.5040\n",
      "convergence dfGPdfNN Run 9/10, Epoch 233/1000, Training Loss (NLML): -955.0986\n",
      "convergence dfGPdfNN Run 9/10, Epoch 234/1000, Training Loss (NLML): -955.5450\n",
      "convergence dfGPdfNN Run 9/10, Epoch 235/1000, Training Loss (NLML): -955.6692\n",
      "convergence dfGPdfNN Run 9/10, Epoch 236/1000, Training Loss (NLML): -955.3097\n",
      "convergence dfGPdfNN Run 9/10, Epoch 237/1000, Training Loss (NLML): -955.4343\n",
      "convergence dfGPdfNN Run 9/10, Epoch 238/1000, Training Loss (NLML): -954.7449\n",
      "convergence dfGPdfNN Run 9/10, Epoch 239/1000, Training Loss (NLML): -954.8488\n",
      "convergence dfGPdfNN Run 9/10, Epoch 240/1000, Training Loss (NLML): -955.8107\n",
      "convergence dfGPdfNN Run 9/10, Epoch 241/1000, Training Loss (NLML): -955.9644\n",
      "convergence dfGPdfNN Run 9/10, Epoch 242/1000, Training Loss (NLML): -956.1259\n",
      "convergence dfGPdfNN Run 9/10, Epoch 243/1000, Training Loss (NLML): -956.2137\n",
      "convergence dfGPdfNN Run 9/10, Epoch 244/1000, Training Loss (NLML): -956.1550\n",
      "convergence dfGPdfNN Run 9/10, Epoch 245/1000, Training Loss (NLML): -955.6122\n",
      "convergence dfGPdfNN Run 9/10, Epoch 246/1000, Training Loss (NLML): -955.6790\n",
      "convergence dfGPdfNN Run 9/10, Epoch 247/1000, Training Loss (NLML): -956.1964\n",
      "convergence dfGPdfNN Run 9/10, Epoch 248/1000, Training Loss (NLML): -956.2780\n",
      "convergence dfGPdfNN Run 9/10, Epoch 249/1000, Training Loss (NLML): -956.3872\n",
      "convergence dfGPdfNN Run 9/10, Epoch 250/1000, Training Loss (NLML): -956.5074\n",
      "convergence dfGPdfNN Run 9/10, Epoch 251/1000, Training Loss (NLML): -956.6409\n",
      "convergence dfGPdfNN Run 9/10, Epoch 252/1000, Training Loss (NLML): -956.4773\n",
      "convergence dfGPdfNN Run 9/10, Epoch 253/1000, Training Loss (NLML): -956.5402\n",
      "convergence dfGPdfNN Run 9/10, Epoch 254/1000, Training Loss (NLML): -956.5391\n",
      "convergence dfGPdfNN Run 9/10, Epoch 255/1000, Training Loss (NLML): -956.6685\n",
      "convergence dfGPdfNN Run 9/10, Epoch 256/1000, Training Loss (NLML): -956.6851\n",
      "convergence dfGPdfNN Run 9/10, Epoch 257/1000, Training Loss (NLML): -956.6606\n",
      "convergence dfGPdfNN Run 9/10, Epoch 258/1000, Training Loss (NLML): -956.7102\n",
      "convergence dfGPdfNN Run 9/10, Epoch 259/1000, Training Loss (NLML): -956.7382\n",
      "convergence dfGPdfNN Run 9/10, Epoch 260/1000, Training Loss (NLML): -956.8409\n",
      "convergence dfGPdfNN Run 9/10, Epoch 261/1000, Training Loss (NLML): -956.8964\n",
      "convergence dfGPdfNN Run 9/10, Epoch 262/1000, Training Loss (NLML): -956.8820\n",
      "convergence dfGPdfNN Run 9/10, Epoch 263/1000, Training Loss (NLML): -956.9202\n",
      "convergence dfGPdfNN Run 9/10, Epoch 264/1000, Training Loss (NLML): -956.9147\n",
      "convergence dfGPdfNN Run 9/10, Epoch 265/1000, Training Loss (NLML): -956.9343\n",
      "convergence dfGPdfNN Run 9/10, Epoch 266/1000, Training Loss (NLML): -957.0629\n",
      "convergence dfGPdfNN Run 9/10, Epoch 267/1000, Training Loss (NLML): -957.1606\n",
      "convergence dfGPdfNN Run 9/10, Epoch 268/1000, Training Loss (NLML): -957.1100\n",
      "convergence dfGPdfNN Run 9/10, Epoch 269/1000, Training Loss (NLML): -957.0615\n",
      "convergence dfGPdfNN Run 9/10, Epoch 270/1000, Training Loss (NLML): -957.1354\n",
      "convergence dfGPdfNN Run 9/10, Epoch 271/1000, Training Loss (NLML): -957.1211\n",
      "convergence dfGPdfNN Run 9/10, Epoch 272/1000, Training Loss (NLML): -957.1478\n",
      "convergence dfGPdfNN Run 9/10, Epoch 273/1000, Training Loss (NLML): -957.1716\n",
      "convergence dfGPdfNN Run 9/10, Epoch 274/1000, Training Loss (NLML): -957.2489\n",
      "convergence dfGPdfNN Run 9/10, Epoch 275/1000, Training Loss (NLML): -957.2850\n",
      "convergence dfGPdfNN Run 9/10, Epoch 276/1000, Training Loss (NLML): -957.3342\n",
      "convergence dfGPdfNN Run 9/10, Epoch 277/1000, Training Loss (NLML): -957.3547\n",
      "convergence dfGPdfNN Run 9/10, Epoch 278/1000, Training Loss (NLML): -957.3948\n",
      "convergence dfGPdfNN Run 9/10, Epoch 279/1000, Training Loss (NLML): -957.4255\n",
      "convergence dfGPdfNN Run 9/10, Epoch 280/1000, Training Loss (NLML): -957.4221\n",
      "convergence dfGPdfNN Run 9/10, Epoch 281/1000, Training Loss (NLML): -957.4457\n",
      "convergence dfGPdfNN Run 9/10, Epoch 282/1000, Training Loss (NLML): -957.4385\n",
      "convergence dfGPdfNN Run 9/10, Epoch 283/1000, Training Loss (NLML): -957.4598\n",
      "convergence dfGPdfNN Run 9/10, Epoch 284/1000, Training Loss (NLML): -957.4065\n",
      "convergence dfGPdfNN Run 9/10, Epoch 285/1000, Training Loss (NLML): -957.3939\n",
      "convergence dfGPdfNN Run 9/10, Epoch 286/1000, Training Loss (NLML): -957.4332\n",
      "convergence dfGPdfNN Run 9/10, Epoch 287/1000, Training Loss (NLML): -957.4497\n",
      "convergence dfGPdfNN Run 9/10, Epoch 288/1000, Training Loss (NLML): -957.4280\n",
      "convergence dfGPdfNN Run 9/10, Epoch 289/1000, Training Loss (NLML): -957.4143\n",
      "convergence dfGPdfNN Run 9/10, Epoch 290/1000, Training Loss (NLML): -957.4218\n",
      "convergence dfGPdfNN Run 9/10, Epoch 291/1000, Training Loss (NLML): -957.4867\n",
      "convergence dfGPdfNN Run 9/10, Epoch 292/1000, Training Loss (NLML): -957.4945\n",
      "convergence dfGPdfNN Run 9/10, Epoch 293/1000, Training Loss (NLML): -957.5377\n",
      "convergence dfGPdfNN Run 9/10, Epoch 294/1000, Training Loss (NLML): -957.5721\n",
      "convergence dfGPdfNN Run 9/10, Epoch 295/1000, Training Loss (NLML): -957.5104\n",
      "convergence dfGPdfNN Run 9/10, Epoch 296/1000, Training Loss (NLML): -957.5387\n",
      "convergence dfGPdfNN Run 9/10, Epoch 297/1000, Training Loss (NLML): -957.5345\n",
      "convergence dfGPdfNN Run 9/10, Epoch 298/1000, Training Loss (NLML): -957.5503\n",
      "convergence dfGPdfNN Run 9/10, Epoch 299/1000, Training Loss (NLML): -957.5834\n",
      "convergence dfGPdfNN Run 9/10, Epoch 300/1000, Training Loss (NLML): -957.3060\n",
      "convergence dfGPdfNN Run 9/10, Epoch 301/1000, Training Loss (NLML): -957.7220\n",
      "convergence dfGPdfNN Run 9/10, Epoch 302/1000, Training Loss (NLML): -957.7213\n",
      "convergence dfGPdfNN Run 9/10, Epoch 303/1000, Training Loss (NLML): -957.7235\n",
      "convergence dfGPdfNN Run 9/10, Epoch 304/1000, Training Loss (NLML): -957.7596\n",
      "convergence dfGPdfNN Run 9/10, Epoch 305/1000, Training Loss (NLML): -957.7991\n",
      "convergence dfGPdfNN Run 9/10, Epoch 306/1000, Training Loss (NLML): -957.7893\n",
      "convergence dfGPdfNN Run 9/10, Epoch 307/1000, Training Loss (NLML): -957.8223\n",
      "convergence dfGPdfNN Run 9/10, Epoch 308/1000, Training Loss (NLML): -957.8572\n",
      "convergence dfGPdfNN Run 9/10, Epoch 309/1000, Training Loss (NLML): -957.8280\n",
      "convergence dfGPdfNN Run 9/10, Epoch 310/1000, Training Loss (NLML): -957.8141\n",
      "convergence dfGPdfNN Run 9/10, Epoch 311/1000, Training Loss (NLML): -957.7955\n",
      "convergence dfGPdfNN Run 9/10, Epoch 312/1000, Training Loss (NLML): -957.7489\n",
      "convergence dfGPdfNN Run 9/10, Epoch 313/1000, Training Loss (NLML): -957.7550\n",
      "convergence dfGPdfNN Run 9/10, Epoch 314/1000, Training Loss (NLML): -957.7435\n",
      "convergence dfGPdfNN Run 9/10, Epoch 315/1000, Training Loss (NLML): -957.7817\n",
      "convergence dfGPdfNN Run 9/10, Epoch 316/1000, Training Loss (NLML): -957.8169\n",
      "convergence dfGPdfNN Run 9/10, Epoch 317/1000, Training Loss (NLML): -957.7987\n",
      "convergence dfGPdfNN Run 9/10, Epoch 318/1000, Training Loss (NLML): -957.8306\n",
      "convergence dfGPdfNN Run 9/10, Epoch 319/1000, Training Loss (NLML): -957.8147\n",
      "convergence dfGPdfNN Run 9/10, Epoch 320/1000, Training Loss (NLML): -957.8436\n",
      "convergence dfGPdfNN Run 9/10, Epoch 321/1000, Training Loss (NLML): -957.8037\n",
      "convergence dfGPdfNN Run 9/10, Epoch 322/1000, Training Loss (NLML): -957.8202\n",
      "convergence dfGPdfNN Run 9/10, Epoch 323/1000, Training Loss (NLML): -957.8376\n",
      "convergence dfGPdfNN Run 9/10, Epoch 324/1000, Training Loss (NLML): -957.8474\n",
      "convergence dfGPdfNN Run 9/10, Epoch 325/1000, Training Loss (NLML): -957.8749\n",
      "convergence dfGPdfNN Run 9/10, Epoch 326/1000, Training Loss (NLML): -957.9135\n",
      "convergence dfGPdfNN Run 9/10, Epoch 327/1000, Training Loss (NLML): -957.9763\n",
      "convergence dfGPdfNN Run 9/10, Epoch 328/1000, Training Loss (NLML): -957.9805\n",
      "convergence dfGPdfNN Run 9/10, Epoch 329/1000, Training Loss (NLML): -957.9850\n",
      "convergence dfGPdfNN Run 9/10, Epoch 330/1000, Training Loss (NLML): -958.0082\n",
      "convergence dfGPdfNN Run 9/10, Epoch 331/1000, Training Loss (NLML): -958.0927\n",
      "convergence dfGPdfNN Run 9/10, Epoch 332/1000, Training Loss (NLML): -958.1110\n",
      "convergence dfGPdfNN Run 9/10, Epoch 333/1000, Training Loss (NLML): -958.1134\n",
      "convergence dfGPdfNN Run 9/10, Epoch 334/1000, Training Loss (NLML): -958.1168\n",
      "convergence dfGPdfNN Run 9/10, Epoch 335/1000, Training Loss (NLML): -958.0785\n",
      "convergence dfGPdfNN Run 9/10, Epoch 336/1000, Training Loss (NLML): -958.1041\n",
      "convergence dfGPdfNN Run 9/10, Epoch 337/1000, Training Loss (NLML): -958.1185\n",
      "convergence dfGPdfNN Run 9/10, Epoch 338/1000, Training Loss (NLML): -958.1172\n",
      "convergence dfGPdfNN Run 9/10, Epoch 339/1000, Training Loss (NLML): -958.0332\n",
      "convergence dfGPdfNN Run 9/10, Epoch 340/1000, Training Loss (NLML): -957.9615\n",
      "convergence dfGPdfNN Run 9/10, Epoch 341/1000, Training Loss (NLML): -957.9402\n",
      "convergence dfGPdfNN Run 9/10, Epoch 342/1000, Training Loss (NLML): -957.9138\n",
      "convergence dfGPdfNN Run 9/10, Epoch 343/1000, Training Loss (NLML): -957.8672\n",
      "convergence dfGPdfNN Run 9/10, Epoch 344/1000, Training Loss (NLML): -957.9401\n",
      "convergence dfGPdfNN Run 9/10, Epoch 345/1000, Training Loss (NLML): -957.9008\n",
      "convergence dfGPdfNN Run 9/10, Epoch 346/1000, Training Loss (NLML): -957.9702\n",
      "convergence dfGPdfNN Run 9/10, Epoch 347/1000, Training Loss (NLML): -958.0204\n",
      "convergence dfGPdfNN Run 9/10, Epoch 348/1000, Training Loss (NLML): -958.0692\n",
      "convergence dfGPdfNN Run 9/10, Epoch 349/1000, Training Loss (NLML): -958.0007\n",
      "convergence dfGPdfNN Run 9/10, Epoch 350/1000, Training Loss (NLML): -958.1084\n",
      "convergence dfGPdfNN Run 9/10, Epoch 351/1000, Training Loss (NLML): -958.1505\n",
      "convergence dfGPdfNN Run 9/10, Epoch 352/1000, Training Loss (NLML): -958.1145\n",
      "convergence dfGPdfNN Run 9/10, Epoch 353/1000, Training Loss (NLML): -958.1488\n",
      "convergence dfGPdfNN Run 9/10, Epoch 354/1000, Training Loss (NLML): -958.1041\n",
      "convergence dfGPdfNN Run 9/10, Epoch 355/1000, Training Loss (NLML): -958.0845\n",
      "convergence dfGPdfNN Run 9/10, Epoch 356/1000, Training Loss (NLML): -958.0129\n",
      "convergence dfGPdfNN Run 9/10, Epoch 357/1000, Training Loss (NLML): -958.0172\n",
      "convergence dfGPdfNN Run 9/10, Epoch 358/1000, Training Loss (NLML): -957.9904\n",
      "convergence dfGPdfNN Run 9/10, Epoch 359/1000, Training Loss (NLML): -958.0562\n",
      "convergence dfGPdfNN Run 9/10, Epoch 360/1000, Training Loss (NLML): -958.1218\n",
      "convergence dfGPdfNN Run 9/10, Epoch 361/1000, Training Loss (NLML): -958.2279\n",
      "convergence dfGPdfNN Run 9/10, Epoch 362/1000, Training Loss (NLML): -958.1736\n",
      "convergence dfGPdfNN Run 9/10, Epoch 363/1000, Training Loss (NLML): -958.1353\n",
      "convergence dfGPdfNN Run 9/10, Epoch 364/1000, Training Loss (NLML): -958.2654\n",
      "convergence dfGPdfNN Run 9/10, Epoch 365/1000, Training Loss (NLML): -958.2643\n",
      "convergence dfGPdfNN Run 9/10, Epoch 366/1000, Training Loss (NLML): -958.2206\n",
      "convergence dfGPdfNN Run 9/10, Epoch 367/1000, Training Loss (NLML): -958.2128\n",
      "convergence dfGPdfNN Run 9/10, Epoch 368/1000, Training Loss (NLML): -958.0841\n",
      "convergence dfGPdfNN Run 9/10, Epoch 369/1000, Training Loss (NLML): -957.9554\n",
      "convergence dfGPdfNN Run 9/10, Epoch 370/1000, Training Loss (NLML): -957.8784\n",
      "convergence dfGPdfNN Run 9/10, Epoch 371/1000, Training Loss (NLML): -957.9318\n",
      "convergence dfGPdfNN Run 9/10, Epoch 372/1000, Training Loss (NLML): -957.9769\n",
      "convergence dfGPdfNN Run 9/10, Epoch 373/1000, Training Loss (NLML): -958.0375\n",
      "convergence dfGPdfNN Run 9/10, Epoch 374/1000, Training Loss (NLML): -958.0983\n",
      "convergence dfGPdfNN Run 9/10, Epoch 375/1000, Training Loss (NLML): -958.1194\n",
      "convergence dfGPdfNN Run 9/10, Epoch 376/1000, Training Loss (NLML): -958.1451\n",
      "convergence dfGPdfNN Run 9/10, Epoch 377/1000, Training Loss (NLML): -958.1462\n",
      "convergence dfGPdfNN Run 9/10, Epoch 378/1000, Training Loss (NLML): -958.1064\n",
      "convergence dfGPdfNN Run 9/10, Epoch 379/1000, Training Loss (NLML): -958.0707\n",
      "convergence dfGPdfNN Run 9/10, Epoch 380/1000, Training Loss (NLML): -958.0192\n",
      "convergence dfGPdfNN Run 9/10, Epoch 381/1000, Training Loss (NLML): -957.9969\n",
      "convergence dfGPdfNN Run 9/10, Epoch 382/1000, Training Loss (NLML): -957.9694\n",
      "convergence dfGPdfNN Run 9/10, Epoch 383/1000, Training Loss (NLML): -957.9891\n",
      "convergence dfGPdfNN Run 9/10, Epoch 384/1000, Training Loss (NLML): -957.9888\n",
      "convergence dfGPdfNN Run 9/10, Epoch 385/1000, Training Loss (NLML): -957.9998\n",
      "convergence dfGPdfNN Run 9/10, Epoch 386/1000, Training Loss (NLML): -958.0831\n",
      "convergence dfGPdfNN Run 9/10, Epoch 387/1000, Training Loss (NLML): -958.1278\n",
      "convergence dfGPdfNN Run 9/10, Epoch 388/1000, Training Loss (NLML): -958.1335\n",
      "convergence dfGPdfNN Run 9/10, Epoch 389/1000, Training Loss (NLML): -958.1219\n",
      "convergence dfGPdfNN Run 9/10, Epoch 390/1000, Training Loss (NLML): -958.1288\n",
      "convergence dfGPdfNN Run 9/10, Epoch 391/1000, Training Loss (NLML): -958.1289\n",
      "convergence dfGPdfNN Run 9/10, Epoch 392/1000, Training Loss (NLML): -958.1270\n",
      "convergence dfGPdfNN Run 9/10, Epoch 393/1000, Training Loss (NLML): -958.1135\n",
      "convergence dfGPdfNN Run 9/10, Epoch 394/1000, Training Loss (NLML): -958.0782\n",
      "convergence dfGPdfNN Run 9/10, Epoch 395/1000, Training Loss (NLML): -958.1252\n",
      "convergence dfGPdfNN Run 9/10, Epoch 396/1000, Training Loss (NLML): -958.1429\n",
      "convergence dfGPdfNN Run 9/10, Epoch 397/1000, Training Loss (NLML): -958.0820\n",
      "convergence dfGPdfNN Run 9/10, Epoch 398/1000, Training Loss (NLML): -958.0575\n",
      "convergence dfGPdfNN Run 9/10, Epoch 399/1000, Training Loss (NLML): -958.0316\n",
      "convergence dfGPdfNN Run 9/10, Epoch 400/1000, Training Loss (NLML): -958.0341\n",
      "convergence dfGPdfNN Run 9/10, Epoch 401/1000, Training Loss (NLML): -957.9995\n",
      "convergence dfGPdfNN Run 9/10, Epoch 402/1000, Training Loss (NLML): -957.9543\n",
      "convergence dfGPdfNN Run 9/10, Epoch 403/1000, Training Loss (NLML): -957.9814\n",
      "convergence dfGPdfNN Run 9/10, Epoch 404/1000, Training Loss (NLML): -958.0116\n",
      "convergence dfGPdfNN Run 9/10, Epoch 405/1000, Training Loss (NLML): -958.0295\n",
      "convergence dfGPdfNN Run 9/10, Epoch 406/1000, Training Loss (NLML): -958.0271\n",
      "convergence dfGPdfNN Run 9/10, Epoch 407/1000, Training Loss (NLML): -958.0010\n",
      "convergence dfGPdfNN Run 9/10, Epoch 408/1000, Training Loss (NLML): -957.9354\n",
      "convergence dfGPdfNN Run 9/10, Epoch 409/1000, Training Loss (NLML): -958.3113\n",
      "convergence dfGPdfNN Run 9/10, Epoch 410/1000, Training Loss (NLML): -958.2372\n",
      "convergence dfGPdfNN Run 9/10, Epoch 411/1000, Training Loss (NLML): -958.2136\n",
      "convergence dfGPdfNN Run 9/10, Epoch 412/1000, Training Loss (NLML): -958.2417\n",
      "convergence dfGPdfNN Run 9/10, Epoch 413/1000, Training Loss (NLML): -958.2916\n",
      "convergence dfGPdfNN Run 9/10, Epoch 414/1000, Training Loss (NLML): -958.2932\n",
      "convergence dfGPdfNN Run 9/10, Epoch 415/1000, Training Loss (NLML): -958.2853\n",
      "convergence dfGPdfNN Run 9/10, Epoch 416/1000, Training Loss (NLML): -958.2806\n",
      "convergence dfGPdfNN Run 9/10, Epoch 417/1000, Training Loss (NLML): -958.2657\n",
      "convergence dfGPdfNN Run 9/10, Epoch 418/1000, Training Loss (NLML): -958.2512\n",
      "convergence dfGPdfNN Run 9/10, Epoch 419/1000, Training Loss (NLML): -958.2628\n",
      "convergence dfGPdfNN Run 9/10, Epoch 420/1000, Training Loss (NLML): -958.2877\n",
      "convergence dfGPdfNN Run 9/10, Epoch 421/1000, Training Loss (NLML): -958.3040\n",
      "convergence dfGPdfNN Run 9/10, Epoch 422/1000, Training Loss (NLML): -957.4103\n",
      "convergence dfGPdfNN Run 9/10, Epoch 423/1000, Training Loss (NLML): -958.4001\n",
      "convergence dfGPdfNN Run 9/10, Epoch 424/1000, Training Loss (NLML): -958.4139\n",
      "convergence dfGPdfNN Run 9/10, Epoch 425/1000, Training Loss (NLML): -958.4501\n",
      "convergence dfGPdfNN Run 9/10, Epoch 426/1000, Training Loss (NLML): -958.4624\n",
      "convergence dfGPdfNN Run 9/10, Epoch 427/1000, Training Loss (NLML): -958.4613\n",
      "convergence dfGPdfNN Run 9/10, Epoch 428/1000, Training Loss (NLML): -958.4623\n",
      "convergence dfGPdfNN Run 9/10, Epoch 429/1000, Training Loss (NLML): -958.4608\n",
      "convergence dfGPdfNN Run 9/10, Epoch 430/1000, Training Loss (NLML): -958.4269\n",
      "convergence dfGPdfNN Run 9/10, Epoch 431/1000, Training Loss (NLML): -958.4252\n",
      "convergence dfGPdfNN Run 9/10, Epoch 432/1000, Training Loss (NLML): -958.4554\n",
      "convergence dfGPdfNN Run 9/10, Epoch 433/1000, Training Loss (NLML): -958.4735\n",
      "convergence dfGPdfNN Run 9/10, Epoch 434/1000, Training Loss (NLML): -958.4769\n",
      "convergence dfGPdfNN Run 9/10, Epoch 435/1000, Training Loss (NLML): -958.4861\n",
      "convergence dfGPdfNN Run 9/10, Epoch 436/1000, Training Loss (NLML): -958.5078\n",
      "convergence dfGPdfNN Run 9/10, Epoch 437/1000, Training Loss (NLML): -958.5157\n",
      "convergence dfGPdfNN Run 9/10, Epoch 438/1000, Training Loss (NLML): -958.5232\n",
      "convergence dfGPdfNN Run 9/10, Epoch 439/1000, Training Loss (NLML): -958.5165\n",
      "convergence dfGPdfNN Run 9/10, Epoch 440/1000, Training Loss (NLML): -958.5267\n",
      "convergence dfGPdfNN Run 9/10, Epoch 441/1000, Training Loss (NLML): -958.5432\n",
      "convergence dfGPdfNN Run 9/10, Epoch 442/1000, Training Loss (NLML): -958.5525\n",
      "convergence dfGPdfNN Run 9/10, Epoch 443/1000, Training Loss (NLML): -958.6348\n",
      "convergence dfGPdfNN Run 9/10, Epoch 444/1000, Training Loss (NLML): -958.6642\n",
      "convergence dfGPdfNN Run 9/10, Epoch 445/1000, Training Loss (NLML): -958.7109\n",
      "convergence dfGPdfNN Run 9/10, Epoch 446/1000, Training Loss (NLML): -958.6932\n",
      "convergence dfGPdfNN Run 9/10, Epoch 447/1000, Training Loss (NLML): -958.7006\n",
      "convergence dfGPdfNN Run 9/10, Epoch 448/1000, Training Loss (NLML): -958.6345\n",
      "convergence dfGPdfNN Run 9/10, Epoch 449/1000, Training Loss (NLML): -958.6393\n",
      "convergence dfGPdfNN Run 9/10, Epoch 450/1000, Training Loss (NLML): -958.7241\n",
      "convergence dfGPdfNN Run 9/10, Epoch 451/1000, Training Loss (NLML): -958.7789\n",
      "convergence dfGPdfNN Run 9/10, Epoch 452/1000, Training Loss (NLML): -958.8320\n",
      "convergence dfGPdfNN Run 9/10, Epoch 453/1000, Training Loss (NLML): -958.7758\n",
      "convergence dfGPdfNN Run 9/10, Epoch 454/1000, Training Loss (NLML): -958.7633\n",
      "convergence dfGPdfNN Run 9/10, Epoch 455/1000, Training Loss (NLML): -958.7719\n",
      "convergence dfGPdfNN Run 9/10, Epoch 456/1000, Training Loss (NLML): -958.7783\n",
      "convergence dfGPdfNN Run 9/10, Epoch 457/1000, Training Loss (NLML): -958.8184\n",
      "convergence dfGPdfNN Run 9/10, Epoch 458/1000, Training Loss (NLML): -958.8547\n",
      "convergence dfGPdfNN Run 9/10, Epoch 459/1000, Training Loss (NLML): -958.8657\n",
      "convergence dfGPdfNN Run 9/10, Epoch 460/1000, Training Loss (NLML): -958.9167\n",
      "convergence dfGPdfNN Run 9/10, Epoch 461/1000, Training Loss (NLML): -958.9258\n",
      "convergence dfGPdfNN Run 9/10, Epoch 462/1000, Training Loss (NLML): -958.8247\n",
      "convergence dfGPdfNN Run 9/10, Epoch 463/1000, Training Loss (NLML): -958.8553\n",
      "convergence dfGPdfNN Run 9/10, Epoch 464/1000, Training Loss (NLML): -958.8585\n",
      "convergence dfGPdfNN Run 9/10, Epoch 465/1000, Training Loss (NLML): -958.8761\n",
      "convergence dfGPdfNN Run 9/10, Epoch 466/1000, Training Loss (NLML): -958.9052\n",
      "convergence dfGPdfNN Run 9/10, Epoch 467/1000, Training Loss (NLML): -958.9568\n",
      "convergence dfGPdfNN Run 9/10, Epoch 468/1000, Training Loss (NLML): -958.9595\n",
      "convergence dfGPdfNN Run 9/10, Epoch 469/1000, Training Loss (NLML): -958.9032\n",
      "convergence dfGPdfNN Run 9/10, Epoch 470/1000, Training Loss (NLML): -958.8914\n",
      "convergence dfGPdfNN Run 9/10, Epoch 471/1000, Training Loss (NLML): -958.8751\n",
      "convergence dfGPdfNN Run 9/10, Epoch 472/1000, Training Loss (NLML): -958.9249\n",
      "convergence dfGPdfNN Run 9/10, Epoch 473/1000, Training Loss (NLML): -958.9899\n",
      "convergence dfGPdfNN Run 9/10, Epoch 474/1000, Training Loss (NLML): -959.0013\n",
      "convergence dfGPdfNN Run 9/10, Epoch 475/1000, Training Loss (NLML): -959.0023\n",
      "convergence dfGPdfNN Run 9/10, Epoch 476/1000, Training Loss (NLML): -958.9371\n",
      "convergence dfGPdfNN Run 9/10, Epoch 477/1000, Training Loss (NLML): -958.8646\n",
      "convergence dfGPdfNN Run 9/10, Epoch 478/1000, Training Loss (NLML): -958.9426\n",
      "convergence dfGPdfNN Run 9/10, Epoch 479/1000, Training Loss (NLML): -958.9880\n",
      "convergence dfGPdfNN Run 9/10, Epoch 480/1000, Training Loss (NLML): -959.0775\n",
      "convergence dfGPdfNN Run 9/10, Epoch 481/1000, Training Loss (NLML): -959.0814\n",
      "convergence dfGPdfNN Run 9/10, Epoch 482/1000, Training Loss (NLML): -959.0049\n",
      "convergence dfGPdfNN Run 9/10, Epoch 483/1000, Training Loss (NLML): -958.9391\n",
      "convergence dfGPdfNN Run 9/10, Epoch 484/1000, Training Loss (NLML): -958.9701\n",
      "convergence dfGPdfNN Run 9/10, Epoch 485/1000, Training Loss (NLML): -959.0442\n",
      "convergence dfGPdfNN Run 9/10, Epoch 486/1000, Training Loss (NLML): -959.1014\n",
      "convergence dfGPdfNN Run 9/10, Epoch 487/1000, Training Loss (NLML): -959.1167\n",
      "convergence dfGPdfNN Run 9/10, Epoch 488/1000, Training Loss (NLML): -959.1189\n",
      "convergence dfGPdfNN Run 9/10, Epoch 489/1000, Training Loss (NLML): -959.1145\n",
      "convergence dfGPdfNN Run 9/10, Epoch 490/1000, Training Loss (NLML): -959.0771\n",
      "convergence dfGPdfNN Run 9/10, Epoch 491/1000, Training Loss (NLML): -959.0823\n",
      "convergence dfGPdfNN Run 9/10, Epoch 492/1000, Training Loss (NLML): -959.2101\n",
      "convergence dfGPdfNN Run 9/10, Epoch 493/1000, Training Loss (NLML): -959.1730\n",
      "convergence dfGPdfNN Run 9/10, Epoch 494/1000, Training Loss (NLML): -959.1573\n",
      "convergence dfGPdfNN Run 9/10, Epoch 495/1000, Training Loss (NLML): -959.1426\n",
      "convergence dfGPdfNN Run 9/10, Epoch 496/1000, Training Loss (NLML): -959.1716\n",
      "convergence dfGPdfNN Run 9/10, Epoch 497/1000, Training Loss (NLML): -959.1437\n",
      "convergence dfGPdfNN Run 9/10, Epoch 498/1000, Training Loss (NLML): -959.1370\n",
      "convergence dfGPdfNN Run 9/10, Epoch 499/1000, Training Loss (NLML): -959.1736\n",
      "convergence dfGPdfNN Run 9/10, Epoch 500/1000, Training Loss (NLML): -959.1241\n",
      "convergence dfGPdfNN Run 9/10, Epoch 501/1000, Training Loss (NLML): -959.2200\n",
      "convergence dfGPdfNN Run 9/10, Epoch 502/1000, Training Loss (NLML): -959.2471\n",
      "convergence dfGPdfNN Run 9/10, Epoch 503/1000, Training Loss (NLML): -959.2588\n",
      "convergence dfGPdfNN Run 9/10, Epoch 504/1000, Training Loss (NLML): -959.2762\n",
      "convergence dfGPdfNN Run 9/10, Epoch 505/1000, Training Loss (NLML): -959.2751\n",
      "convergence dfGPdfNN Run 9/10, Epoch 506/1000, Training Loss (NLML): -959.2456\n",
      "convergence dfGPdfNN Run 9/10, Epoch 507/1000, Training Loss (NLML): -959.3136\n",
      "convergence dfGPdfNN Run 9/10, Epoch 508/1000, Training Loss (NLML): -959.3510\n",
      "convergence dfGPdfNN Run 9/10, Epoch 509/1000, Training Loss (NLML): -959.3862\n",
      "convergence dfGPdfNN Run 9/10, Epoch 510/1000, Training Loss (NLML): -959.3937\n",
      "convergence dfGPdfNN Run 9/10, Epoch 511/1000, Training Loss (NLML): -959.3428\n",
      "convergence dfGPdfNN Run 9/10, Epoch 512/1000, Training Loss (NLML): -959.2606\n",
      "convergence dfGPdfNN Run 9/10, Epoch 513/1000, Training Loss (NLML): -959.2947\n",
      "convergence dfGPdfNN Run 9/10, Epoch 514/1000, Training Loss (NLML): -959.2744\n",
      "convergence dfGPdfNN Run 9/10, Epoch 515/1000, Training Loss (NLML): -959.1556\n",
      "convergence dfGPdfNN Run 9/10, Epoch 516/1000, Training Loss (NLML): -959.1627\n",
      "convergence dfGPdfNN Run 9/10, Epoch 517/1000, Training Loss (NLML): -959.2014\n",
      "convergence dfGPdfNN Run 9/10, Epoch 518/1000, Training Loss (NLML): -959.3290\n",
      "convergence dfGPdfNN Run 9/10, Epoch 519/1000, Training Loss (NLML): -959.3624\n",
      "convergence dfGPdfNN Run 9/10, Epoch 520/1000, Training Loss (NLML): -959.3693\n",
      "convergence dfGPdfNN Run 9/10, Epoch 521/1000, Training Loss (NLML): -959.2482\n",
      "convergence dfGPdfNN Run 9/10, Epoch 522/1000, Training Loss (NLML): -959.2057\n",
      "convergence dfGPdfNN Run 9/10, Epoch 523/1000, Training Loss (NLML): -959.2882\n",
      "convergence dfGPdfNN Run 9/10, Epoch 524/1000, Training Loss (NLML): -959.3629\n",
      "convergence dfGPdfNN Run 9/10, Epoch 525/1000, Training Loss (NLML): -959.3379\n",
      "convergence dfGPdfNN Run 9/10, Epoch 526/1000, Training Loss (NLML): -959.3408\n",
      "convergence dfGPdfNN Run 9/10, Epoch 527/1000, Training Loss (NLML): -959.2958\n",
      "convergence dfGPdfNN Run 9/10, Epoch 528/1000, Training Loss (NLML): -959.3356\n",
      "convergence dfGPdfNN Run 9/10, Epoch 529/1000, Training Loss (NLML): -959.3712\n",
      "convergence dfGPdfNN Run 9/10, Epoch 530/1000, Training Loss (NLML): -959.4708\n",
      "convergence dfGPdfNN Run 9/10, Epoch 531/1000, Training Loss (NLML): -959.4806\n",
      "convergence dfGPdfNN Run 9/10, Epoch 532/1000, Training Loss (NLML): -959.4084\n",
      "convergence dfGPdfNN Run 9/10, Epoch 533/1000, Training Loss (NLML): -959.3359\n",
      "convergence dfGPdfNN Run 9/10, Epoch 534/1000, Training Loss (NLML): -959.4202\n",
      "convergence dfGPdfNN Run 9/10, Epoch 535/1000, Training Loss (NLML): -959.4485\n",
      "convergence dfGPdfNN Run 9/10, Epoch 536/1000, Training Loss (NLML): -959.4344\n",
      "convergence dfGPdfNN Run 9/10, Epoch 537/1000, Training Loss (NLML): -959.4496\n",
      "convergence dfGPdfNN Run 9/10, Epoch 538/1000, Training Loss (NLML): -959.4502\n",
      "convergence dfGPdfNN Run 9/10, Epoch 539/1000, Training Loss (NLML): -959.4563\n",
      "convergence dfGPdfNN Run 9/10, Epoch 540/1000, Training Loss (NLML): -959.4181\n",
      "convergence dfGPdfNN Run 9/10, Epoch 541/1000, Training Loss (NLML): -959.3867\n",
      "convergence dfGPdfNN Run 9/10, Epoch 542/1000, Training Loss (NLML): -959.3829\n",
      "convergence dfGPdfNN Run 9/10, Epoch 543/1000, Training Loss (NLML): -959.4286\n",
      "convergence dfGPdfNN Run 9/10, Epoch 544/1000, Training Loss (NLML): -959.4736\n",
      "convergence dfGPdfNN Run 9/10, Epoch 545/1000, Training Loss (NLML): -959.4808\n",
      "convergence dfGPdfNN Run 9/10, Epoch 546/1000, Training Loss (NLML): -959.4890\n",
      "convergence dfGPdfNN Run 9/10, Epoch 547/1000, Training Loss (NLML): -959.4640\n",
      "convergence dfGPdfNN Run 9/10, Epoch 548/1000, Training Loss (NLML): -959.3546\n",
      "convergence dfGPdfNN Run 9/10, Epoch 549/1000, Training Loss (NLML): -959.3947\n",
      "convergence dfGPdfNN Run 9/10, Epoch 550/1000, Training Loss (NLML): -959.4305\n",
      "convergence dfGPdfNN Run 9/10, Epoch 551/1000, Training Loss (NLML): -959.5078\n",
      "convergence dfGPdfNN Run 9/10, Epoch 552/1000, Training Loss (NLML): -959.5276\n",
      "convergence dfGPdfNN Run 9/10, Epoch 553/1000, Training Loss (NLML): -959.5316\n",
      "convergence dfGPdfNN Run 9/10, Epoch 554/1000, Training Loss (NLML): -959.4808\n",
      "convergence dfGPdfNN Run 9/10, Epoch 555/1000, Training Loss (NLML): -959.5226\n",
      "convergence dfGPdfNN Run 9/10, Epoch 556/1000, Training Loss (NLML): -959.5801\n",
      "convergence dfGPdfNN Run 9/10, Epoch 557/1000, Training Loss (NLML): -959.5756\n",
      "convergence dfGPdfNN Run 9/10, Epoch 558/1000, Training Loss (NLML): -959.5802\n",
      "convergence dfGPdfNN Run 9/10, Epoch 559/1000, Training Loss (NLML): -959.5221\n",
      "convergence dfGPdfNN Run 9/10, Epoch 560/1000, Training Loss (NLML): -959.5304\n",
      "convergence dfGPdfNN Run 9/10, Epoch 561/1000, Training Loss (NLML): -959.5341\n",
      "convergence dfGPdfNN Run 9/10, Epoch 562/1000, Training Loss (NLML): -959.5576\n",
      "convergence dfGPdfNN Run 9/10, Epoch 563/1000, Training Loss (NLML): -959.5005\n",
      "convergence dfGPdfNN Run 9/10, Epoch 564/1000, Training Loss (NLML): -959.5006\n",
      "convergence dfGPdfNN Run 9/10, Epoch 565/1000, Training Loss (NLML): -959.4999\n",
      "convergence dfGPdfNN Run 9/10, Epoch 566/1000, Training Loss (NLML): -959.5050\n",
      "convergence dfGPdfNN Run 9/10, Epoch 567/1000, Training Loss (NLML): -959.5372\n",
      "convergence dfGPdfNN Run 9/10, Epoch 568/1000, Training Loss (NLML): -959.5808\n",
      "convergence dfGPdfNN Run 9/10, Epoch 569/1000, Training Loss (NLML): -959.5562\n",
      "convergence dfGPdfNN Run 9/10, Epoch 570/1000, Training Loss (NLML): -959.5488\n",
      "convergence dfGPdfNN Run 9/10, Epoch 571/1000, Training Loss (NLML): -959.5146\n",
      "convergence dfGPdfNN Run 9/10, Epoch 572/1000, Training Loss (NLML): -959.5164\n",
      "convergence dfGPdfNN Run 9/10, Epoch 573/1000, Training Loss (NLML): -959.5736\n",
      "convergence dfGPdfNN Run 9/10, Epoch 574/1000, Training Loss (NLML): -959.6233\n",
      "convergence dfGPdfNN Run 9/10, Epoch 575/1000, Training Loss (NLML): -959.5820\n",
      "convergence dfGPdfNN Run 9/10, Epoch 576/1000, Training Loss (NLML): -959.5859\n",
      "convergence dfGPdfNN Run 9/10, Epoch 577/1000, Training Loss (NLML): -959.5992\n",
      "convergence dfGPdfNN Run 9/10, Epoch 578/1000, Training Loss (NLML): -959.5771\n",
      "convergence dfGPdfNN Run 9/10, Epoch 579/1000, Training Loss (NLML): -959.6149\n",
      "convergence dfGPdfNN Run 9/10, Epoch 580/1000, Training Loss (NLML): -959.6830\n",
      "convergence dfGPdfNN Run 9/10, Epoch 581/1000, Training Loss (NLML): -959.6652\n",
      "convergence dfGPdfNN Run 9/10, Epoch 582/1000, Training Loss (NLML): -959.6410\n",
      "convergence dfGPdfNN Run 9/10, Epoch 583/1000, Training Loss (NLML): -959.5468\n",
      "convergence dfGPdfNN Run 9/10, Epoch 584/1000, Training Loss (NLML): -959.5062\n",
      "convergence dfGPdfNN Run 9/10, Epoch 585/1000, Training Loss (NLML): -959.4901\n",
      "convergence dfGPdfNN Run 9/10, Epoch 586/1000, Training Loss (NLML): -959.6199\n",
      "convergence dfGPdfNN Run 9/10, Epoch 587/1000, Training Loss (NLML): -959.6537\n",
      "convergence dfGPdfNN Run 9/10, Epoch 588/1000, Training Loss (NLML): -959.6683\n",
      "convergence dfGPdfNN Run 9/10, Epoch 589/1000, Training Loss (NLML): -959.6587\n",
      "convergence dfGPdfNN Run 9/10, Epoch 590/1000, Training Loss (NLML): -959.6134\n",
      "convergence dfGPdfNN Run 9/10, Epoch 591/1000, Training Loss (NLML): -959.5251\n",
      "convergence dfGPdfNN Run 9/10, Epoch 592/1000, Training Loss (NLML): -959.6016\n",
      "convergence dfGPdfNN Run 9/10, Epoch 593/1000, Training Loss (NLML): -959.6398\n",
      "convergence dfGPdfNN Run 9/10, Epoch 594/1000, Training Loss (NLML): -959.6897\n",
      "convergence dfGPdfNN Run 9/10, Epoch 595/1000, Training Loss (NLML): -959.7137\n",
      "convergence dfGPdfNN Run 9/10, Epoch 596/1000, Training Loss (NLML): -959.7126\n",
      "convergence dfGPdfNN Run 9/10, Epoch 597/1000, Training Loss (NLML): -959.6483\n",
      "convergence dfGPdfNN Run 9/10, Epoch 598/1000, Training Loss (NLML): -959.6384\n",
      "convergence dfGPdfNN Run 9/10, Epoch 599/1000, Training Loss (NLML): -959.5992\n",
      "convergence dfGPdfNN Run 9/10, Epoch 600/1000, Training Loss (NLML): -959.5289\n",
      "convergence dfGPdfNN Run 9/10, Epoch 601/1000, Training Loss (NLML): -959.5579\n",
      "convergence dfGPdfNN Run 9/10, Epoch 602/1000, Training Loss (NLML): -959.5834\n",
      "convergence dfGPdfNN Run 9/10, Epoch 603/1000, Training Loss (NLML): -959.6875\n",
      "convergence dfGPdfNN Run 9/10, Epoch 604/1000, Training Loss (NLML): -959.7344\n",
      "convergence dfGPdfNN Run 9/10, Epoch 605/1000, Training Loss (NLML): -959.7589\n",
      "convergence dfGPdfNN Run 9/10, Epoch 606/1000, Training Loss (NLML): -959.7264\n",
      "convergence dfGPdfNN Run 9/10, Epoch 607/1000, Training Loss (NLML): -959.6798\n",
      "convergence dfGPdfNN Run 9/10, Epoch 608/1000, Training Loss (NLML): -959.6023\n",
      "convergence dfGPdfNN Run 9/10, Epoch 609/1000, Training Loss (NLML): -959.7002\n",
      "convergence dfGPdfNN Run 9/10, Epoch 610/1000, Training Loss (NLML): -959.8154\n",
      "convergence dfGPdfNN Run 9/10, Epoch 611/1000, Training Loss (NLML): -959.8177\n",
      "convergence dfGPdfNN Run 9/10, Epoch 612/1000, Training Loss (NLML): -959.7596\n",
      "convergence dfGPdfNN Run 9/10, Epoch 613/1000, Training Loss (NLML): -959.7351\n",
      "convergence dfGPdfNN Run 9/10, Epoch 614/1000, Training Loss (NLML): -959.6663\n",
      "convergence dfGPdfNN Run 9/10, Epoch 615/1000, Training Loss (NLML): -959.6761\n",
      "convergence dfGPdfNN Run 9/10, Epoch 616/1000, Training Loss (NLML): -959.7413\n",
      "convergence dfGPdfNN Run 9/10, Epoch 617/1000, Training Loss (NLML): -959.7709\n",
      "convergence dfGPdfNN Run 9/10, Epoch 618/1000, Training Loss (NLML): -959.7579\n",
      "convergence dfGPdfNN Run 9/10, Epoch 619/1000, Training Loss (NLML): -959.7754\n",
      "convergence dfGPdfNN Run 9/10, Epoch 620/1000, Training Loss (NLML): -959.8077\n",
      "convergence dfGPdfNN Run 9/10, Epoch 621/1000, Training Loss (NLML): -959.9008\n",
      "convergence dfGPdfNN Run 9/10, Epoch 622/1000, Training Loss (NLML): -959.9166\n",
      "convergence dfGPdfNN Run 9/10, Epoch 623/1000, Training Loss (NLML): -959.9066\n",
      "convergence dfGPdfNN Run 9/10, Epoch 624/1000, Training Loss (NLML): -959.7896\n",
      "convergence dfGPdfNN Run 9/10, Epoch 625/1000, Training Loss (NLML): -959.7883\n",
      "convergence dfGPdfNN Run 9/10, Epoch 626/1000, Training Loss (NLML): -959.8369\n",
      "convergence dfGPdfNN Run 9/10, Epoch 627/1000, Training Loss (NLML): -959.8375\n",
      "convergence dfGPdfNN Run 9/10, Epoch 628/1000, Training Loss (NLML): -959.7944\n",
      "convergence dfGPdfNN Run 9/10, Epoch 629/1000, Training Loss (NLML): -959.7974\n",
      "convergence dfGPdfNN Run 9/10, Epoch 630/1000, Training Loss (NLML): -959.8031\n",
      "convergence dfGPdfNN Run 9/10, Epoch 631/1000, Training Loss (NLML): -959.8363\n",
      "convergence dfGPdfNN Run 9/10, Epoch 632/1000, Training Loss (NLML): -959.9229\n",
      "convergence dfGPdfNN Run 9/10, Epoch 633/1000, Training Loss (NLML): -959.9749\n",
      "convergence dfGPdfNN Run 9/10, Epoch 634/1000, Training Loss (NLML): -959.9642\n",
      "convergence dfGPdfNN Run 9/10, Epoch 635/1000, Training Loss (NLML): -959.9193\n",
      "convergence dfGPdfNN Run 9/10, Epoch 636/1000, Training Loss (NLML): -959.8004\n",
      "convergence dfGPdfNN Run 9/10, Epoch 637/1000, Training Loss (NLML): -959.8573\n",
      "convergence dfGPdfNN Run 9/10, Epoch 638/1000, Training Loss (NLML): -959.9303\n",
      "convergence dfGPdfNN Run 9/10, Epoch 639/1000, Training Loss (NLML): -959.8911\n",
      "convergence dfGPdfNN Run 9/10, Epoch 640/1000, Training Loss (NLML): -959.8365\n",
      "convergence dfGPdfNN Run 9/10, Epoch 641/1000, Training Loss (NLML): -959.8372\n",
      "convergence dfGPdfNN Run 9/10, Epoch 642/1000, Training Loss (NLML): -959.8400\n",
      "convergence dfGPdfNN Run 9/10, Epoch 643/1000, Training Loss (NLML): -959.8787\n",
      "convergence dfGPdfNN Run 9/10, Epoch 644/1000, Training Loss (NLML): -959.9077\n",
      "convergence dfGPdfNN Run 9/10, Epoch 645/1000, Training Loss (NLML): -959.9880\n",
      "convergence dfGPdfNN Run 9/10, Epoch 646/1000, Training Loss (NLML): -960.0239\n",
      "convergence dfGPdfNN Run 9/10, Epoch 647/1000, Training Loss (NLML): -960.0052\n",
      "convergence dfGPdfNN Run 9/10, Epoch 648/1000, Training Loss (NLML): -959.9022\n",
      "convergence dfGPdfNN Run 9/10, Epoch 649/1000, Training Loss (NLML): -959.8975\n",
      "convergence dfGPdfNN Run 9/10, Epoch 650/1000, Training Loss (NLML): -959.9335\n",
      "convergence dfGPdfNN Run 9/10, Epoch 651/1000, Training Loss (NLML): -959.9602\n",
      "convergence dfGPdfNN Run 9/10, Epoch 652/1000, Training Loss (NLML): -959.9592\n",
      "convergence dfGPdfNN Run 9/10, Epoch 653/1000, Training Loss (NLML): -959.8683\n",
      "convergence dfGPdfNN Run 9/10, Epoch 654/1000, Training Loss (NLML): -959.8514\n",
      "convergence dfGPdfNN Run 9/10, Epoch 655/1000, Training Loss (NLML): -959.8336\n",
      "convergence dfGPdfNN Run 9/10, Epoch 656/1000, Training Loss (NLML): -959.8867\n",
      "convergence dfGPdfNN Run 9/10, Epoch 657/1000, Training Loss (NLML): -959.9115\n",
      "convergence dfGPdfNN Run 9/10, Epoch 658/1000, Training Loss (NLML): -959.9807\n",
      "convergence dfGPdfNN Run 9/10, Epoch 659/1000, Training Loss (NLML): -960.0319\n",
      "convergence dfGPdfNN Run 9/10, Epoch 660/1000, Training Loss (NLML): -960.0521\n",
      "convergence dfGPdfNN Run 9/10, Epoch 661/1000, Training Loss (NLML): -960.0955\n",
      "convergence dfGPdfNN Run 9/10, Epoch 662/1000, Training Loss (NLML): -960.1394\n",
      "convergence dfGPdfNN Run 9/10, Epoch 663/1000, Training Loss (NLML): -960.1189\n",
      "convergence dfGPdfNN Run 9/10, Epoch 664/1000, Training Loss (NLML): -960.0417\n",
      "convergence dfGPdfNN Run 9/10, Epoch 665/1000, Training Loss (NLML): -959.9432\n",
      "convergence dfGPdfNN Run 9/10, Epoch 666/1000, Training Loss (NLML): -959.9855\n",
      "convergence dfGPdfNN Run 9/10, Epoch 667/1000, Training Loss (NLML): -960.0442\n",
      "convergence dfGPdfNN Run 9/10, Epoch 668/1000, Training Loss (NLML): -960.0892\n",
      "convergence dfGPdfNN Run 9/10, Epoch 669/1000, Training Loss (NLML): -960.1890\n",
      "convergence dfGPdfNN Run 9/10, Epoch 670/1000, Training Loss (NLML): -960.0994\n",
      "convergence dfGPdfNN Run 9/10, Epoch 671/1000, Training Loss (NLML): -959.8679\n",
      "convergence dfGPdfNN Run 9/10, Epoch 672/1000, Training Loss (NLML): -959.8829\n",
      "convergence dfGPdfNN Run 9/10, Epoch 673/1000, Training Loss (NLML): -959.8948\n",
      "convergence dfGPdfNN Run 9/10, Epoch 674/1000, Training Loss (NLML): -959.9351\n",
      "convergence dfGPdfNN Run 9/10, Epoch 675/1000, Training Loss (NLML): -960.0295\n",
      "convergence dfGPdfNN Run 9/10, Epoch 676/1000, Training Loss (NLML): -959.9977\n",
      "convergence dfGPdfNN Run 9/10, Epoch 677/1000, Training Loss (NLML): -960.1121\n",
      "convergence dfGPdfNN Run 9/10, Epoch 678/1000, Training Loss (NLML): -960.0988\n",
      "convergence dfGPdfNN Run 9/10, Epoch 679/1000, Training Loss (NLML): -960.0542\n",
      "convergence dfGPdfNN Run 9/10, Epoch 680/1000, Training Loss (NLML): -960.0725\n",
      "convergence dfGPdfNN Run 9/10, Epoch 681/1000, Training Loss (NLML): -960.1023\n",
      "convergence dfGPdfNN Run 9/10, Epoch 682/1000, Training Loss (NLML): -960.1251\n",
      "convergence dfGPdfNN Run 9/10, Epoch 683/1000, Training Loss (NLML): -960.1744\n",
      "convergence dfGPdfNN Run 9/10, Epoch 684/1000, Training Loss (NLML): -960.1704\n",
      "convergence dfGPdfNN Run 9/10, Epoch 685/1000, Training Loss (NLML): -960.1584\n",
      "convergence dfGPdfNN Run 9/10, Epoch 686/1000, Training Loss (NLML): -960.1454\n",
      "convergence dfGPdfNN Run 9/10, Epoch 687/1000, Training Loss (NLML): -960.1517\n",
      "convergence dfGPdfNN Run 9/10, Epoch 688/1000, Training Loss (NLML): -960.1671\n",
      "convergence dfGPdfNN Run 9/10, Epoch 689/1000, Training Loss (NLML): -960.2100\n",
      "convergence dfGPdfNN Run 9/10, Epoch 690/1000, Training Loss (NLML): -960.2177\n",
      "convergence dfGPdfNN Run 9/10, Epoch 691/1000, Training Loss (NLML): -960.2208\n",
      "convergence dfGPdfNN Run 9/10, Epoch 692/1000, Training Loss (NLML): -960.2050\n",
      "convergence dfGPdfNN Run 9/10, Epoch 693/1000, Training Loss (NLML): -960.2015\n",
      "convergence dfGPdfNN Run 9/10, Epoch 694/1000, Training Loss (NLML): -960.2129\n",
      "convergence dfGPdfNN Run 9/10, Epoch 695/1000, Training Loss (NLML): -960.2236\n",
      "convergence dfGPdfNN Run 9/10, Epoch 696/1000, Training Loss (NLML): -960.2318\n",
      "convergence dfGPdfNN Run 9/10, Epoch 697/1000, Training Loss (NLML): -960.2378\n",
      "convergence dfGPdfNN Run 9/10, Epoch 698/1000, Training Loss (NLML): -960.2343\n",
      "convergence dfGPdfNN Run 9/10, Epoch 699/1000, Training Loss (NLML): -960.2166\n",
      "convergence dfGPdfNN Run 9/10, Epoch 700/1000, Training Loss (NLML): -960.2172\n",
      "convergence dfGPdfNN Run 9/10, Epoch 701/1000, Training Loss (NLML): -960.2230\n",
      "convergence dfGPdfNN Run 9/10, Epoch 702/1000, Training Loss (NLML): -960.2284\n",
      "convergence dfGPdfNN Run 9/10, Epoch 703/1000, Training Loss (NLML): -960.2314\n",
      "convergence dfGPdfNN Run 9/10, Epoch 704/1000, Training Loss (NLML): -960.2130\n",
      "convergence dfGPdfNN Run 9/10, Epoch 705/1000, Training Loss (NLML): -960.1877\n",
      "convergence dfGPdfNN Run 9/10, Epoch 706/1000, Training Loss (NLML): -960.1772\n",
      "convergence dfGPdfNN Run 9/10, Epoch 707/1000, Training Loss (NLML): -960.1814\n",
      "convergence dfGPdfNN Run 9/10, Epoch 708/1000, Training Loss (NLML): -960.2106\n",
      "convergence dfGPdfNN Run 9/10, Epoch 709/1000, Training Loss (NLML): -960.1951\n",
      "convergence dfGPdfNN Run 9/10, Epoch 710/1000, Training Loss (NLML): -960.1993\n",
      "convergence dfGPdfNN Run 9/10, Epoch 711/1000, Training Loss (NLML): -960.2074\n",
      "convergence dfGPdfNN Run 9/10, Epoch 712/1000, Training Loss (NLML): -960.1477\n",
      "convergence dfGPdfNN Run 9/10, Epoch 713/1000, Training Loss (NLML): -960.1313\n",
      "convergence dfGPdfNN Run 9/10, Epoch 714/1000, Training Loss (NLML): -960.1288\n",
      "convergence dfGPdfNN Run 9/10, Epoch 715/1000, Training Loss (NLML): -960.1515\n",
      "convergence dfGPdfNN Run 9/10, Epoch 716/1000, Training Loss (NLML): -960.1061\n",
      "convergence dfGPdfNN Run 9/10, Epoch 717/1000, Training Loss (NLML): -960.1108\n",
      "convergence dfGPdfNN Run 9/10, Epoch 718/1000, Training Loss (NLML): -960.1162\n",
      "convergence dfGPdfNN Run 9/10, Epoch 719/1000, Training Loss (NLML): -960.1244\n",
      "convergence dfGPdfNN Run 9/10, Epoch 720/1000, Training Loss (NLML): -960.1292\n",
      "convergence dfGPdfNN Run 9/10, Epoch 721/1000, Training Loss (NLML): -959.9821\n",
      "convergence dfGPdfNN Run 9/10, Epoch 722/1000, Training Loss (NLML): -959.9637\n",
      "convergence dfGPdfNN Run 9/10, Epoch 723/1000, Training Loss (NLML): -959.9840\n",
      "convergence dfGPdfNN Run 9/10, Epoch 724/1000, Training Loss (NLML): -960.0099\n",
      "convergence dfGPdfNN Run 9/10, Epoch 725/1000, Training Loss (NLML): -960.0358\n",
      "convergence dfGPdfNN Run 9/10, Epoch 726/1000, Training Loss (NLML): -960.0530\n",
      "convergence dfGPdfNN Run 9/10, Epoch 727/1000, Training Loss (NLML): -960.0869\n",
      "convergence dfGPdfNN Run 9/10, Epoch 728/1000, Training Loss (NLML): -960.0946\n",
      "convergence dfGPdfNN Run 9/10, Epoch 729/1000, Training Loss (NLML): -960.0974\n",
      "convergence dfGPdfNN Run 9/10, Epoch 730/1000, Training Loss (NLML): -960.0760\n",
      "convergence dfGPdfNN Run 9/10, Epoch 731/1000, Training Loss (NLML): -960.0664\n",
      "convergence dfGPdfNN Run 9/10, Epoch 732/1000, Training Loss (NLML): -960.0743\n",
      "convergence dfGPdfNN Run 9/10, Epoch 733/1000, Training Loss (NLML): -960.0884\n",
      "convergence dfGPdfNN Run 9/10, Epoch 734/1000, Training Loss (NLML): -960.0708\n",
      "convergence dfGPdfNN Run 9/10, Epoch 735/1000, Training Loss (NLML): -960.0743\n",
      "convergence dfGPdfNN Run 9/10, Epoch 736/1000, Training Loss (NLML): -960.0776\n",
      "convergence dfGPdfNN Run 9/10, Epoch 737/1000, Training Loss (NLML): -960.0822\n",
      "convergence dfGPdfNN Run 9/10, Epoch 738/1000, Training Loss (NLML): -960.0851\n",
      "convergence dfGPdfNN Run 9/10, Epoch 739/1000, Training Loss (NLML): -960.0956\n",
      "convergence dfGPdfNN Run 9/10, Epoch 740/1000, Training Loss (NLML): -960.0887\n",
      "convergence dfGPdfNN Run 9/10, Epoch 741/1000, Training Loss (NLML): -960.0940\n",
      "convergence dfGPdfNN Run 9/10, Epoch 742/1000, Training Loss (NLML): -960.1040\n",
      "convergence dfGPdfNN Run 9/10, Epoch 743/1000, Training Loss (NLML): -960.1071\n",
      "convergence dfGPdfNN Run 9/10, Epoch 744/1000, Training Loss (NLML): -960.1227\n",
      "convergence dfGPdfNN Run 9/10, Epoch 745/1000, Training Loss (NLML): -960.1414\n",
      "convergence dfGPdfNN Run 9/10, Epoch 746/1000, Training Loss (NLML): -960.1605\n",
      "convergence dfGPdfNN Run 9/10, Epoch 747/1000, Training Loss (NLML): -960.1731\n",
      "Early stopping triggered after 747 epochs.\n",
      "\n",
      "--- Training Run 10/10 ---\n",
      "\n",
      "Start Training\n",
      "convergence dfGPdfNN Run 10/10, Epoch 1/1000, Training Loss (NLML): -863.7656\n",
      "convergence dfGPdfNN Run 10/10, Epoch 2/1000, Training Loss (NLML): -873.5072\n",
      "convergence dfGPdfNN Run 10/10, Epoch 3/1000, Training Loss (NLML): -879.6776\n",
      "convergence dfGPdfNN Run 10/10, Epoch 4/1000, Training Loss (NLML): -883.2446\n",
      "convergence dfGPdfNN Run 10/10, Epoch 5/1000, Training Loss (NLML): -886.3529\n",
      "convergence dfGPdfNN Run 10/10, Epoch 6/1000, Training Loss (NLML): -888.3539\n",
      "convergence dfGPdfNN Run 10/10, Epoch 7/1000, Training Loss (NLML): -891.1956\n",
      "convergence dfGPdfNN Run 10/10, Epoch 8/1000, Training Loss (NLML): -893.8679\n",
      "convergence dfGPdfNN Run 10/10, Epoch 9/1000, Training Loss (NLML): -895.9878\n",
      "convergence dfGPdfNN Run 10/10, Epoch 10/1000, Training Loss (NLML): -898.3131\n",
      "convergence dfGPdfNN Run 10/10, Epoch 11/1000, Training Loss (NLML): -900.1975\n",
      "convergence dfGPdfNN Run 10/10, Epoch 12/1000, Training Loss (NLML): -902.0211\n",
      "convergence dfGPdfNN Run 10/10, Epoch 13/1000, Training Loss (NLML): -903.8209\n",
      "convergence dfGPdfNN Run 10/10, Epoch 14/1000, Training Loss (NLML): -905.4860\n",
      "convergence dfGPdfNN Run 10/10, Epoch 15/1000, Training Loss (NLML): -906.9609\n",
      "convergence dfGPdfNN Run 10/10, Epoch 16/1000, Training Loss (NLML): -908.2709\n",
      "convergence dfGPdfNN Run 10/10, Epoch 17/1000, Training Loss (NLML): -909.7222\n",
      "convergence dfGPdfNN Run 10/10, Epoch 18/1000, Training Loss (NLML): -910.9832\n",
      "convergence dfGPdfNN Run 10/10, Epoch 19/1000, Training Loss (NLML): -912.3777\n",
      "convergence dfGPdfNN Run 10/10, Epoch 20/1000, Training Loss (NLML): -913.7439\n",
      "convergence dfGPdfNN Run 10/10, Epoch 21/1000, Training Loss (NLML): -914.9447\n",
      "convergence dfGPdfNN Run 10/10, Epoch 22/1000, Training Loss (NLML): -915.8541\n",
      "convergence dfGPdfNN Run 10/10, Epoch 23/1000, Training Loss (NLML): -916.9781\n",
      "convergence dfGPdfNN Run 10/10, Epoch 24/1000, Training Loss (NLML): -918.0626\n",
      "convergence dfGPdfNN Run 10/10, Epoch 25/1000, Training Loss (NLML): -919.0985\n",
      "convergence dfGPdfNN Run 10/10, Epoch 26/1000, Training Loss (NLML): -920.0940\n",
      "convergence dfGPdfNN Run 10/10, Epoch 27/1000, Training Loss (NLML): -921.0411\n",
      "convergence dfGPdfNN Run 10/10, Epoch 28/1000, Training Loss (NLML): -921.9946\n",
      "convergence dfGPdfNN Run 10/10, Epoch 29/1000, Training Loss (NLML): -922.9135\n",
      "convergence dfGPdfNN Run 10/10, Epoch 30/1000, Training Loss (NLML): -923.7518\n",
      "convergence dfGPdfNN Run 10/10, Epoch 31/1000, Training Loss (NLML): -924.5542\n",
      "convergence dfGPdfNN Run 10/10, Epoch 32/1000, Training Loss (NLML): -925.3232\n",
      "convergence dfGPdfNN Run 10/10, Epoch 33/1000, Training Loss (NLML): -926.0785\n",
      "convergence dfGPdfNN Run 10/10, Epoch 34/1000, Training Loss (NLML): -926.7753\n",
      "convergence dfGPdfNN Run 10/10, Epoch 35/1000, Training Loss (NLML): -927.4740\n",
      "convergence dfGPdfNN Run 10/10, Epoch 36/1000, Training Loss (NLML): -928.1323\n",
      "convergence dfGPdfNN Run 10/10, Epoch 37/1000, Training Loss (NLML): -928.7977\n",
      "convergence dfGPdfNN Run 10/10, Epoch 38/1000, Training Loss (NLML): -929.4542\n",
      "convergence dfGPdfNN Run 10/10, Epoch 39/1000, Training Loss (NLML): -929.4094\n",
      "convergence dfGPdfNN Run 10/10, Epoch 40/1000, Training Loss (NLML): -930.6852\n",
      "convergence dfGPdfNN Run 10/10, Epoch 41/1000, Training Loss (NLML): -931.2411\n",
      "convergence dfGPdfNN Run 10/10, Epoch 42/1000, Training Loss (NLML): -931.8016\n",
      "convergence dfGPdfNN Run 10/10, Epoch 43/1000, Training Loss (NLML): -932.3350\n",
      "convergence dfGPdfNN Run 10/10, Epoch 44/1000, Training Loss (NLML): -932.8572\n",
      "convergence dfGPdfNN Run 10/10, Epoch 45/1000, Training Loss (NLML): -933.3630\n",
      "convergence dfGPdfNN Run 10/10, Epoch 46/1000, Training Loss (NLML): -933.8713\n",
      "convergence dfGPdfNN Run 10/10, Epoch 47/1000, Training Loss (NLML): -934.3696\n",
      "convergence dfGPdfNN Run 10/10, Epoch 48/1000, Training Loss (NLML): -934.8500\n",
      "convergence dfGPdfNN Run 10/10, Epoch 49/1000, Training Loss (NLML): -935.3181\n",
      "convergence dfGPdfNN Run 10/10, Epoch 50/1000, Training Loss (NLML): -935.2140\n",
      "convergence dfGPdfNN Run 10/10, Epoch 51/1000, Training Loss (NLML): -935.6967\n",
      "convergence dfGPdfNN Run 10/10, Epoch 52/1000, Training Loss (NLML): -936.6471\n",
      "convergence dfGPdfNN Run 10/10, Epoch 53/1000, Training Loss (NLML): -936.1927\n",
      "convergence dfGPdfNN Run 10/10, Epoch 54/1000, Training Loss (NLML): -937.4700\n",
      "convergence dfGPdfNN Run 10/10, Epoch 55/1000, Training Loss (NLML): -937.8749\n",
      "convergence dfGPdfNN Run 10/10, Epoch 56/1000, Training Loss (NLML): -938.2632\n",
      "convergence dfGPdfNN Run 10/10, Epoch 57/1000, Training Loss (NLML): -938.6417\n",
      "convergence dfGPdfNN Run 10/10, Epoch 58/1000, Training Loss (NLML): -939.0231\n",
      "convergence dfGPdfNN Run 10/10, Epoch 59/1000, Training Loss (NLML): -939.3944\n",
      "convergence dfGPdfNN Run 10/10, Epoch 60/1000, Training Loss (NLML): -939.7715\n",
      "convergence dfGPdfNN Run 10/10, Epoch 61/1000, Training Loss (NLML): -940.1351\n",
      "convergence dfGPdfNN Run 10/10, Epoch 62/1000, Training Loss (NLML): -940.4921\n",
      "convergence dfGPdfNN Run 10/10, Epoch 63/1000, Training Loss (NLML): -940.8413\n",
      "convergence dfGPdfNN Run 10/10, Epoch 64/1000, Training Loss (NLML): -940.5608\n",
      "convergence dfGPdfNN Run 10/10, Epoch 65/1000, Training Loss (NLML): -941.4849\n",
      "convergence dfGPdfNN Run 10/10, Epoch 66/1000, Training Loss (NLML): -941.8168\n",
      "convergence dfGPdfNN Run 10/10, Epoch 67/1000, Training Loss (NLML): -942.1051\n",
      "convergence dfGPdfNN Run 10/10, Epoch 68/1000, Training Loss (NLML): -942.3687\n",
      "convergence dfGPdfNN Run 10/10, Epoch 69/1000, Training Loss (NLML): -942.6598\n",
      "convergence dfGPdfNN Run 10/10, Epoch 70/1000, Training Loss (NLML): -942.9916\n",
      "convergence dfGPdfNN Run 10/10, Epoch 71/1000, Training Loss (NLML): -943.3210\n",
      "convergence dfGPdfNN Run 10/10, Epoch 72/1000, Training Loss (NLML): -943.5498\n",
      "convergence dfGPdfNN Run 10/10, Epoch 73/1000, Training Loss (NLML): -943.8563\n",
      "convergence dfGPdfNN Run 10/10, Epoch 74/1000, Training Loss (NLML): -944.1165\n",
      "convergence dfGPdfNN Run 10/10, Epoch 75/1000, Training Loss (NLML): -944.3568\n",
      "convergence dfGPdfNN Run 10/10, Epoch 76/1000, Training Loss (NLML): -944.5851\n",
      "convergence dfGPdfNN Run 10/10, Epoch 77/1000, Training Loss (NLML): -944.8268\n",
      "convergence dfGPdfNN Run 10/10, Epoch 78/1000, Training Loss (NLML): -945.1135\n",
      "convergence dfGPdfNN Run 10/10, Epoch 79/1000, Training Loss (NLML): -945.4152\n",
      "convergence dfGPdfNN Run 10/10, Epoch 80/1000, Training Loss (NLML): -945.7179\n",
      "convergence dfGPdfNN Run 10/10, Epoch 81/1000, Training Loss (NLML): -945.9360\n",
      "convergence dfGPdfNN Run 10/10, Epoch 82/1000, Training Loss (NLML): -946.1537\n",
      "convergence dfGPdfNN Run 10/10, Epoch 83/1000, Training Loss (NLML): -946.3784\n",
      "convergence dfGPdfNN Run 10/10, Epoch 84/1000, Training Loss (NLML): -946.5878\n",
      "convergence dfGPdfNN Run 10/10, Epoch 85/1000, Training Loss (NLML): -946.8138\n",
      "convergence dfGPdfNN Run 10/10, Epoch 86/1000, Training Loss (NLML): -947.0468\n",
      "convergence dfGPdfNN Run 10/10, Epoch 87/1000, Training Loss (NLML): -947.2406\n",
      "convergence dfGPdfNN Run 10/10, Epoch 88/1000, Training Loss (NLML): -947.4553\n",
      "convergence dfGPdfNN Run 10/10, Epoch 89/1000, Training Loss (NLML): -947.6759\n",
      "convergence dfGPdfNN Run 10/10, Epoch 90/1000, Training Loss (NLML): -947.9028\n",
      "convergence dfGPdfNN Run 10/10, Epoch 91/1000, Training Loss (NLML): -948.1111\n",
      "convergence dfGPdfNN Run 10/10, Epoch 92/1000, Training Loss (NLML): -948.3041\n",
      "convergence dfGPdfNN Run 10/10, Epoch 93/1000, Training Loss (NLML): -948.5005\n",
      "convergence dfGPdfNN Run 10/10, Epoch 94/1000, Training Loss (NLML): -948.7045\n",
      "convergence dfGPdfNN Run 10/10, Epoch 95/1000, Training Loss (NLML): -948.8950\n",
      "convergence dfGPdfNN Run 10/10, Epoch 96/1000, Training Loss (NLML): -949.0803\n",
      "convergence dfGPdfNN Run 10/10, Epoch 97/1000, Training Loss (NLML): -949.2659\n",
      "convergence dfGPdfNN Run 10/10, Epoch 98/1000, Training Loss (NLML): -949.4406\n",
      "convergence dfGPdfNN Run 10/10, Epoch 99/1000, Training Loss (NLML): -949.6163\n",
      "convergence dfGPdfNN Run 10/10, Epoch 100/1000, Training Loss (NLML): -949.7858\n",
      "convergence dfGPdfNN Run 10/10, Epoch 101/1000, Training Loss (NLML): -949.9476\n",
      "convergence dfGPdfNN Run 10/10, Epoch 102/1000, Training Loss (NLML): -950.1199\n",
      "convergence dfGPdfNN Run 10/10, Epoch 103/1000, Training Loss (NLML): -950.2897\n",
      "convergence dfGPdfNN Run 10/10, Epoch 104/1000, Training Loss (NLML): -950.4478\n",
      "convergence dfGPdfNN Run 10/10, Epoch 105/1000, Training Loss (NLML): -950.5833\n",
      "convergence dfGPdfNN Run 10/10, Epoch 106/1000, Training Loss (NLML): -950.7317\n",
      "convergence dfGPdfNN Run 10/10, Epoch 107/1000, Training Loss (NLML): -950.8649\n",
      "convergence dfGPdfNN Run 10/10, Epoch 108/1000, Training Loss (NLML): -951.0072\n",
      "convergence dfGPdfNN Run 10/10, Epoch 109/1000, Training Loss (NLML): -951.1470\n",
      "convergence dfGPdfNN Run 10/10, Epoch 110/1000, Training Loss (NLML): -951.2927\n",
      "convergence dfGPdfNN Run 10/10, Epoch 111/1000, Training Loss (NLML): -951.4220\n",
      "convergence dfGPdfNN Run 10/10, Epoch 112/1000, Training Loss (NLML): -951.5430\n",
      "convergence dfGPdfNN Run 10/10, Epoch 113/1000, Training Loss (NLML): -951.6716\n",
      "convergence dfGPdfNN Run 10/10, Epoch 114/1000, Training Loss (NLML): -951.7938\n",
      "convergence dfGPdfNN Run 10/10, Epoch 115/1000, Training Loss (NLML): -951.9126\n",
      "convergence dfGPdfNN Run 10/10, Epoch 116/1000, Training Loss (NLML): -952.0265\n",
      "convergence dfGPdfNN Run 10/10, Epoch 117/1000, Training Loss (NLML): -952.1499\n",
      "convergence dfGPdfNN Run 10/10, Epoch 118/1000, Training Loss (NLML): -952.2627\n",
      "convergence dfGPdfNN Run 10/10, Epoch 119/1000, Training Loss (NLML): -952.3608\n",
      "convergence dfGPdfNN Run 10/10, Epoch 120/1000, Training Loss (NLML): -952.4548\n",
      "convergence dfGPdfNN Run 10/10, Epoch 121/1000, Training Loss (NLML): -952.5378\n",
      "convergence dfGPdfNN Run 10/10, Epoch 122/1000, Training Loss (NLML): -952.5725\n",
      "convergence dfGPdfNN Run 10/10, Epoch 123/1000, Training Loss (NLML): -952.6196\n",
      "convergence dfGPdfNN Run 10/10, Epoch 124/1000, Training Loss (NLML): -952.6359\n",
      "convergence dfGPdfNN Run 10/10, Epoch 125/1000, Training Loss (NLML): -952.5961\n",
      "convergence dfGPdfNN Run 10/10, Epoch 126/1000, Training Loss (NLML): -952.6691\n",
      "convergence dfGPdfNN Run 10/10, Epoch 127/1000, Training Loss (NLML): -952.7479\n",
      "convergence dfGPdfNN Run 10/10, Epoch 128/1000, Training Loss (NLML): -952.7982\n",
      "convergence dfGPdfNN Run 10/10, Epoch 129/1000, Training Loss (NLML): -952.8535\n",
      "convergence dfGPdfNN Run 10/10, Epoch 130/1000, Training Loss (NLML): -952.8914\n",
      "convergence dfGPdfNN Run 10/10, Epoch 131/1000, Training Loss (NLML): -952.9244\n",
      "convergence dfGPdfNN Run 10/10, Epoch 132/1000, Training Loss (NLML): -952.9969\n",
      "convergence dfGPdfNN Run 10/10, Epoch 133/1000, Training Loss (NLML): -953.0486\n",
      "convergence dfGPdfNN Run 10/10, Epoch 134/1000, Training Loss (NLML): -953.1423\n",
      "convergence dfGPdfNN Run 10/10, Epoch 135/1000, Training Loss (NLML): -953.2095\n",
      "convergence dfGPdfNN Run 10/10, Epoch 136/1000, Training Loss (NLML): -953.2528\n",
      "convergence dfGPdfNN Run 10/10, Epoch 137/1000, Training Loss (NLML): -953.3451\n",
      "convergence dfGPdfNN Run 10/10, Epoch 138/1000, Training Loss (NLML): -953.4155\n",
      "convergence dfGPdfNN Run 10/10, Epoch 139/1000, Training Loss (NLML): -953.5074\n",
      "convergence dfGPdfNN Run 10/10, Epoch 140/1000, Training Loss (NLML): -953.5889\n",
      "convergence dfGPdfNN Run 10/10, Epoch 141/1000, Training Loss (NLML): -953.6464\n",
      "convergence dfGPdfNN Run 10/10, Epoch 142/1000, Training Loss (NLML): -953.6844\n",
      "convergence dfGPdfNN Run 10/10, Epoch 143/1000, Training Loss (NLML): -953.6566\n",
      "convergence dfGPdfNN Run 10/10, Epoch 144/1000, Training Loss (NLML): -953.6589\n",
      "convergence dfGPdfNN Run 10/10, Epoch 145/1000, Training Loss (NLML): -953.6472\n",
      "convergence dfGPdfNN Run 10/10, Epoch 146/1000, Training Loss (NLML): -953.5760\n",
      "convergence dfGPdfNN Run 10/10, Epoch 147/1000, Training Loss (NLML): -953.5846\n",
      "convergence dfGPdfNN Run 10/10, Epoch 148/1000, Training Loss (NLML): -953.5974\n",
      "convergence dfGPdfNN Run 10/10, Epoch 149/1000, Training Loss (NLML): -953.6238\n",
      "convergence dfGPdfNN Run 10/10, Epoch 150/1000, Training Loss (NLML): -953.6956\n",
      "convergence dfGPdfNN Run 10/10, Epoch 151/1000, Training Loss (NLML): -953.7802\n",
      "convergence dfGPdfNN Run 10/10, Epoch 152/1000, Training Loss (NLML): -953.8522\n",
      "convergence dfGPdfNN Run 10/10, Epoch 153/1000, Training Loss (NLML): -953.9423\n",
      "convergence dfGPdfNN Run 10/10, Epoch 154/1000, Training Loss (NLML): -954.0421\n",
      "convergence dfGPdfNN Run 10/10, Epoch 155/1000, Training Loss (NLML): -954.2058\n",
      "convergence dfGPdfNN Run 10/10, Epoch 156/1000, Training Loss (NLML): -954.2156\n",
      "convergence dfGPdfNN Run 10/10, Epoch 157/1000, Training Loss (NLML): -954.2939\n",
      "convergence dfGPdfNN Run 10/10, Epoch 158/1000, Training Loss (NLML): -954.3735\n",
      "convergence dfGPdfNN Run 10/10, Epoch 159/1000, Training Loss (NLML): -954.4001\n",
      "convergence dfGPdfNN Run 10/10, Epoch 160/1000, Training Loss (NLML): -954.4945\n",
      "convergence dfGPdfNN Run 10/10, Epoch 161/1000, Training Loss (NLML): -954.4558\n",
      "convergence dfGPdfNN Run 10/10, Epoch 162/1000, Training Loss (NLML): -954.5365\n",
      "convergence dfGPdfNN Run 10/10, Epoch 163/1000, Training Loss (NLML): -954.5724\n",
      "convergence dfGPdfNN Run 10/10, Epoch 164/1000, Training Loss (NLML): -954.4056\n",
      "convergence dfGPdfNN Run 10/10, Epoch 165/1000, Training Loss (NLML): -954.2163\n",
      "convergence dfGPdfNN Run 10/10, Epoch 166/1000, Training Loss (NLML): -954.0654\n",
      "convergence dfGPdfNN Run 10/10, Epoch 167/1000, Training Loss (NLML): -954.0596\n",
      "convergence dfGPdfNN Run 10/10, Epoch 168/1000, Training Loss (NLML): -953.9827\n",
      "convergence dfGPdfNN Run 10/10, Epoch 169/1000, Training Loss (NLML): -954.1053\n",
      "convergence dfGPdfNN Run 10/10, Epoch 170/1000, Training Loss (NLML): -954.1100\n",
      "convergence dfGPdfNN Run 10/10, Epoch 171/1000, Training Loss (NLML): -918.8597\n",
      "convergence dfGPdfNN Run 10/10, Epoch 172/1000, Training Loss (NLML): -954.1677\n",
      "convergence dfGPdfNN Run 10/10, Epoch 173/1000, Training Loss (NLML): -954.0903\n",
      "convergence dfGPdfNN Run 10/10, Epoch 174/1000, Training Loss (NLML): -954.0261\n",
      "convergence dfGPdfNN Run 10/10, Epoch 175/1000, Training Loss (NLML): -953.8811\n",
      "convergence dfGPdfNN Run 10/10, Epoch 176/1000, Training Loss (NLML): -953.7418\n",
      "convergence dfGPdfNN Run 10/10, Epoch 177/1000, Training Loss (NLML): -953.8483\n",
      "convergence dfGPdfNN Run 10/10, Epoch 178/1000, Training Loss (NLML): -953.9185\n",
      "convergence dfGPdfNN Run 10/10, Epoch 179/1000, Training Loss (NLML): -954.0664\n",
      "convergence dfGPdfNN Run 10/10, Epoch 180/1000, Training Loss (NLML): -954.1854\n",
      "convergence dfGPdfNN Run 10/10, Epoch 181/1000, Training Loss (NLML): -954.2938\n",
      "convergence dfGPdfNN Run 10/10, Epoch 182/1000, Training Loss (NLML): -954.2931\n",
      "convergence dfGPdfNN Run 10/10, Epoch 183/1000, Training Loss (NLML): -954.2129\n",
      "convergence dfGPdfNN Run 10/10, Epoch 184/1000, Training Loss (NLML): -954.0662\n",
      "convergence dfGPdfNN Run 10/10, Epoch 185/1000, Training Loss (NLML): -954.1354\n",
      "convergence dfGPdfNN Run 10/10, Epoch 186/1000, Training Loss (NLML): -954.2220\n",
      "convergence dfGPdfNN Run 10/10, Epoch 187/1000, Training Loss (NLML): -954.2643\n",
      "convergence dfGPdfNN Run 10/10, Epoch 188/1000, Training Loss (NLML): -954.4442\n",
      "convergence dfGPdfNN Run 10/10, Epoch 189/1000, Training Loss (NLML): -954.6676\n",
      "convergence dfGPdfNN Run 10/10, Epoch 190/1000, Training Loss (NLML): -954.5228\n",
      "convergence dfGPdfNN Run 10/10, Epoch 191/1000, Training Loss (NLML): -954.4935\n",
      "convergence dfGPdfNN Run 10/10, Epoch 192/1000, Training Loss (NLML): -954.3898\n",
      "convergence dfGPdfNN Run 10/10, Epoch 193/1000, Training Loss (NLML): -954.2944\n",
      "convergence dfGPdfNN Run 10/10, Epoch 194/1000, Training Loss (NLML): -954.2714\n",
      "convergence dfGPdfNN Run 10/10, Epoch 195/1000, Training Loss (NLML): -954.3491\n",
      "convergence dfGPdfNN Run 10/10, Epoch 196/1000, Training Loss (NLML): -954.5015\n",
      "convergence dfGPdfNN Run 10/10, Epoch 197/1000, Training Loss (NLML): -954.8309\n",
      "convergence dfGPdfNN Run 10/10, Epoch 198/1000, Training Loss (NLML): -954.9785\n",
      "convergence dfGPdfNN Run 10/10, Epoch 199/1000, Training Loss (NLML): -955.0264\n",
      "convergence dfGPdfNN Run 10/10, Epoch 200/1000, Training Loss (NLML): -955.1714\n",
      "convergence dfGPdfNN Run 10/10, Epoch 201/1000, Training Loss (NLML): -955.1846\n",
      "convergence dfGPdfNN Run 10/10, Epoch 202/1000, Training Loss (NLML): -955.2686\n",
      "convergence dfGPdfNN Run 10/10, Epoch 203/1000, Training Loss (NLML): -955.3029\n",
      "convergence dfGPdfNN Run 10/10, Epoch 204/1000, Training Loss (NLML): -955.1270\n",
      "convergence dfGPdfNN Run 10/10, Epoch 205/1000, Training Loss (NLML): -955.1469\n",
      "convergence dfGPdfNN Run 10/10, Epoch 206/1000, Training Loss (NLML): -955.3661\n",
      "convergence dfGPdfNN Run 10/10, Epoch 207/1000, Training Loss (NLML): -955.5150\n",
      "convergence dfGPdfNN Run 10/10, Epoch 208/1000, Training Loss (NLML): -955.5033\n",
      "convergence dfGPdfNN Run 10/10, Epoch 209/1000, Training Loss (NLML): -955.4750\n",
      "convergence dfGPdfNN Run 10/10, Epoch 210/1000, Training Loss (NLML): -955.5789\n",
      "convergence dfGPdfNN Run 10/10, Epoch 211/1000, Training Loss (NLML): -955.5682\n",
      "convergence dfGPdfNN Run 10/10, Epoch 212/1000, Training Loss (NLML): -955.4442\n",
      "convergence dfGPdfNN Run 10/10, Epoch 213/1000, Training Loss (NLML): -955.4158\n",
      "convergence dfGPdfNN Run 10/10, Epoch 214/1000, Training Loss (NLML): -955.5265\n",
      "convergence dfGPdfNN Run 10/10, Epoch 215/1000, Training Loss (NLML): -955.4194\n",
      "convergence dfGPdfNN Run 10/10, Epoch 216/1000, Training Loss (NLML): -955.2717\n",
      "convergence dfGPdfNN Run 10/10, Epoch 217/1000, Training Loss (NLML): -955.1953\n",
      "convergence dfGPdfNN Run 10/10, Epoch 218/1000, Training Loss (NLML): -955.0918\n",
      "convergence dfGPdfNN Run 10/10, Epoch 219/1000, Training Loss (NLML): -955.0332\n",
      "convergence dfGPdfNN Run 10/10, Epoch 220/1000, Training Loss (NLML): -954.9849\n",
      "convergence dfGPdfNN Run 10/10, Epoch 221/1000, Training Loss (NLML): -955.0251\n",
      "convergence dfGPdfNN Run 10/10, Epoch 222/1000, Training Loss (NLML): -955.0538\n",
      "convergence dfGPdfNN Run 10/10, Epoch 223/1000, Training Loss (NLML): -955.0039\n",
      "convergence dfGPdfNN Run 10/10, Epoch 224/1000, Training Loss (NLML): -955.0211\n",
      "convergence dfGPdfNN Run 10/10, Epoch 225/1000, Training Loss (NLML): -955.0488\n",
      "convergence dfGPdfNN Run 10/10, Epoch 226/1000, Training Loss (NLML): -955.0483\n",
      "convergence dfGPdfNN Run 10/10, Epoch 227/1000, Training Loss (NLML): -955.0851\n",
      "convergence dfGPdfNN Run 10/10, Epoch 228/1000, Training Loss (NLML): -955.1110\n",
      "convergence dfGPdfNN Run 10/10, Epoch 229/1000, Training Loss (NLML): -955.0841\n",
      "convergence dfGPdfNN Run 10/10, Epoch 230/1000, Training Loss (NLML): -955.0680\n",
      "convergence dfGPdfNN Run 10/10, Epoch 231/1000, Training Loss (NLML): -955.0876\n",
      "convergence dfGPdfNN Run 10/10, Epoch 232/1000, Training Loss (NLML): -955.0986\n",
      "convergence dfGPdfNN Run 10/10, Epoch 233/1000, Training Loss (NLML): -955.1133\n",
      "convergence dfGPdfNN Run 10/10, Epoch 234/1000, Training Loss (NLML): -955.1371\n",
      "convergence dfGPdfNN Run 10/10, Epoch 235/1000, Training Loss (NLML): -955.1705\n",
      "convergence dfGPdfNN Run 10/10, Epoch 236/1000, Training Loss (NLML): -955.2411\n",
      "convergence dfGPdfNN Run 10/10, Epoch 237/1000, Training Loss (NLML): -955.2673\n",
      "convergence dfGPdfNN Run 10/10, Epoch 238/1000, Training Loss (NLML): -955.3032\n",
      "convergence dfGPdfNN Run 10/10, Epoch 239/1000, Training Loss (NLML): -955.4050\n",
      "convergence dfGPdfNN Run 10/10, Epoch 240/1000, Training Loss (NLML): -955.4388\n",
      "convergence dfGPdfNN Run 10/10, Epoch 241/1000, Training Loss (NLML): -955.4741\n",
      "convergence dfGPdfNN Run 10/10, Epoch 242/1000, Training Loss (NLML): -955.4948\n",
      "convergence dfGPdfNN Run 10/10, Epoch 243/1000, Training Loss (NLML): -955.5516\n",
      "convergence dfGPdfNN Run 10/10, Epoch 244/1000, Training Loss (NLML): -955.5784\n",
      "convergence dfGPdfNN Run 10/10, Epoch 245/1000, Training Loss (NLML): -955.5717\n",
      "convergence dfGPdfNN Run 10/10, Epoch 246/1000, Training Loss (NLML): -955.5254\n",
      "convergence dfGPdfNN Run 10/10, Epoch 247/1000, Training Loss (NLML): -955.4574\n",
      "convergence dfGPdfNN Run 10/10, Epoch 248/1000, Training Loss (NLML): -955.5570\n",
      "convergence dfGPdfNN Run 10/10, Epoch 249/1000, Training Loss (NLML): -955.5834\n",
      "convergence dfGPdfNN Run 10/10, Epoch 250/1000, Training Loss (NLML): -955.5997\n",
      "convergence dfGPdfNN Run 10/10, Epoch 251/1000, Training Loss (NLML): -955.6482\n",
      "convergence dfGPdfNN Run 10/10, Epoch 252/1000, Training Loss (NLML): -955.6796\n",
      "convergence dfGPdfNN Run 10/10, Epoch 253/1000, Training Loss (NLML): -955.7295\n",
      "convergence dfGPdfNN Run 10/10, Epoch 254/1000, Training Loss (NLML): -955.7380\n",
      "convergence dfGPdfNN Run 10/10, Epoch 255/1000, Training Loss (NLML): -955.7427\n",
      "convergence dfGPdfNN Run 10/10, Epoch 256/1000, Training Loss (NLML): -955.7191\n",
      "convergence dfGPdfNN Run 10/10, Epoch 257/1000, Training Loss (NLML): -955.6696\n",
      "convergence dfGPdfNN Run 10/10, Epoch 258/1000, Training Loss (NLML): -955.6383\n",
      "convergence dfGPdfNN Run 10/10, Epoch 259/1000, Training Loss (NLML): -955.6252\n",
      "convergence dfGPdfNN Run 10/10, Epoch 260/1000, Training Loss (NLML): -955.5812\n",
      "convergence dfGPdfNN Run 10/10, Epoch 261/1000, Training Loss (NLML): -955.6237\n",
      "convergence dfGPdfNN Run 10/10, Epoch 262/1000, Training Loss (NLML): -955.6854\n",
      "convergence dfGPdfNN Run 10/10, Epoch 263/1000, Training Loss (NLML): -955.8143\n",
      "convergence dfGPdfNN Run 10/10, Epoch 264/1000, Training Loss (NLML): -955.8474\n",
      "convergence dfGPdfNN Run 10/10, Epoch 265/1000, Training Loss (NLML): -955.8317\n",
      "convergence dfGPdfNN Run 10/10, Epoch 266/1000, Training Loss (NLML): -955.7598\n",
      "convergence dfGPdfNN Run 10/10, Epoch 267/1000, Training Loss (NLML): -955.7375\n",
      "convergence dfGPdfNN Run 10/10, Epoch 268/1000, Training Loss (NLML): -955.8158\n",
      "convergence dfGPdfNN Run 10/10, Epoch 269/1000, Training Loss (NLML): -955.8364\n",
      "convergence dfGPdfNN Run 10/10, Epoch 270/1000, Training Loss (NLML): -955.8765\n",
      "convergence dfGPdfNN Run 10/10, Epoch 271/1000, Training Loss (NLML): -955.9318\n",
      "convergence dfGPdfNN Run 10/10, Epoch 272/1000, Training Loss (NLML): -955.9493\n",
      "convergence dfGPdfNN Run 10/10, Epoch 273/1000, Training Loss (NLML): -955.9567\n",
      "convergence dfGPdfNN Run 10/10, Epoch 274/1000, Training Loss (NLML): -955.9668\n",
      "convergence dfGPdfNN Run 10/10, Epoch 275/1000, Training Loss (NLML): -955.9958\n",
      "convergence dfGPdfNN Run 10/10, Epoch 276/1000, Training Loss (NLML): -956.0341\n",
      "convergence dfGPdfNN Run 10/10, Epoch 277/1000, Training Loss (NLML): -956.0538\n",
      "convergence dfGPdfNN Run 10/10, Epoch 278/1000, Training Loss (NLML): -956.0596\n",
      "convergence dfGPdfNN Run 10/10, Epoch 279/1000, Training Loss (NLML): -956.0839\n",
      "convergence dfGPdfNN Run 10/10, Epoch 280/1000, Training Loss (NLML): -956.1001\n",
      "convergence dfGPdfNN Run 10/10, Epoch 281/1000, Training Loss (NLML): -956.1244\n",
      "convergence dfGPdfNN Run 10/10, Epoch 282/1000, Training Loss (NLML): -956.1648\n",
      "convergence dfGPdfNN Run 10/10, Epoch 283/1000, Training Loss (NLML): -956.1803\n",
      "convergence dfGPdfNN Run 10/10, Epoch 284/1000, Training Loss (NLML): -956.2000\n",
      "convergence dfGPdfNN Run 10/10, Epoch 285/1000, Training Loss (NLML): -956.1798\n",
      "convergence dfGPdfNN Run 10/10, Epoch 286/1000, Training Loss (NLML): -956.1984\n",
      "convergence dfGPdfNN Run 10/10, Epoch 287/1000, Training Loss (NLML): -956.2198\n",
      "convergence dfGPdfNN Run 10/10, Epoch 288/1000, Training Loss (NLML): -956.2378\n",
      "convergence dfGPdfNN Run 10/10, Epoch 289/1000, Training Loss (NLML): -956.2539\n",
      "convergence dfGPdfNN Run 10/10, Epoch 290/1000, Training Loss (NLML): -956.3029\n",
      "convergence dfGPdfNN Run 10/10, Epoch 291/1000, Training Loss (NLML): -956.3545\n",
      "convergence dfGPdfNN Run 10/10, Epoch 292/1000, Training Loss (NLML): -956.3723\n",
      "convergence dfGPdfNN Run 10/10, Epoch 293/1000, Training Loss (NLML): -956.3926\n",
      "convergence dfGPdfNN Run 10/10, Epoch 294/1000, Training Loss (NLML): -956.4207\n",
      "convergence dfGPdfNN Run 10/10, Epoch 295/1000, Training Loss (NLML): -956.4392\n",
      "convergence dfGPdfNN Run 10/10, Epoch 296/1000, Training Loss (NLML): -956.4515\n",
      "convergence dfGPdfNN Run 10/10, Epoch 297/1000, Training Loss (NLML): -956.4678\n",
      "convergence dfGPdfNN Run 10/10, Epoch 298/1000, Training Loss (NLML): -956.4845\n",
      "convergence dfGPdfNN Run 10/10, Epoch 299/1000, Training Loss (NLML): -956.4991\n",
      "convergence dfGPdfNN Run 10/10, Epoch 300/1000, Training Loss (NLML): -956.5044\n",
      "convergence dfGPdfNN Run 10/10, Epoch 301/1000, Training Loss (NLML): -956.5535\n",
      "convergence dfGPdfNN Run 10/10, Epoch 302/1000, Training Loss (NLML): -956.5598\n",
      "convergence dfGPdfNN Run 10/10, Epoch 303/1000, Training Loss (NLML): -956.5983\n",
      "convergence dfGPdfNN Run 10/10, Epoch 304/1000, Training Loss (NLML): -956.6157\n",
      "convergence dfGPdfNN Run 10/10, Epoch 305/1000, Training Loss (NLML): -956.6307\n",
      "convergence dfGPdfNN Run 10/10, Epoch 306/1000, Training Loss (NLML): -956.6481\n",
      "convergence dfGPdfNN Run 10/10, Epoch 307/1000, Training Loss (NLML): -956.6643\n",
      "convergence dfGPdfNN Run 10/10, Epoch 308/1000, Training Loss (NLML): -956.6881\n",
      "convergence dfGPdfNN Run 10/10, Epoch 309/1000, Training Loss (NLML): -956.6774\n",
      "convergence dfGPdfNN Run 10/10, Epoch 310/1000, Training Loss (NLML): -956.6830\n",
      "convergence dfGPdfNN Run 10/10, Epoch 311/1000, Training Loss (NLML): -956.7064\n",
      "convergence dfGPdfNN Run 10/10, Epoch 312/1000, Training Loss (NLML): -956.7235\n",
      "convergence dfGPdfNN Run 10/10, Epoch 313/1000, Training Loss (NLML): -956.7744\n",
      "convergence dfGPdfNN Run 10/10, Epoch 314/1000, Training Loss (NLML): -956.7900\n",
      "convergence dfGPdfNN Run 10/10, Epoch 315/1000, Training Loss (NLML): -956.8046\n",
      "convergence dfGPdfNN Run 10/10, Epoch 316/1000, Training Loss (NLML): -956.8143\n",
      "convergence dfGPdfNN Run 10/10, Epoch 317/1000, Training Loss (NLML): -956.8281\n",
      "convergence dfGPdfNN Run 10/10, Epoch 318/1000, Training Loss (NLML): -956.8439\n",
      "convergence dfGPdfNN Run 10/10, Epoch 319/1000, Training Loss (NLML): -956.8546\n",
      "convergence dfGPdfNN Run 10/10, Epoch 320/1000, Training Loss (NLML): -956.8684\n",
      "convergence dfGPdfNN Run 10/10, Epoch 321/1000, Training Loss (NLML): -956.8807\n",
      "convergence dfGPdfNN Run 10/10, Epoch 322/1000, Training Loss (NLML): -956.8949\n",
      "convergence dfGPdfNN Run 10/10, Epoch 323/1000, Training Loss (NLML): -956.9091\n",
      "convergence dfGPdfNN Run 10/10, Epoch 324/1000, Training Loss (NLML): -956.9382\n",
      "convergence dfGPdfNN Run 10/10, Epoch 325/1000, Training Loss (NLML): -956.9526\n",
      "convergence dfGPdfNN Run 10/10, Epoch 326/1000, Training Loss (NLML): -956.9656\n",
      "convergence dfGPdfNN Run 10/10, Epoch 327/1000, Training Loss (NLML): -956.9807\n",
      "convergence dfGPdfNN Run 10/10, Epoch 328/1000, Training Loss (NLML): -956.9943\n",
      "convergence dfGPdfNN Run 10/10, Epoch 329/1000, Training Loss (NLML): -956.9741\n",
      "convergence dfGPdfNN Run 10/10, Epoch 330/1000, Training Loss (NLML): -956.9534\n",
      "convergence dfGPdfNN Run 10/10, Epoch 331/1000, Training Loss (NLML): -956.9576\n",
      "convergence dfGPdfNN Run 10/10, Epoch 332/1000, Training Loss (NLML): -956.9741\n",
      "convergence dfGPdfNN Run 10/10, Epoch 333/1000, Training Loss (NLML): -956.9921\n",
      "convergence dfGPdfNN Run 10/10, Epoch 334/1000, Training Loss (NLML): -957.0062\n",
      "convergence dfGPdfNN Run 10/10, Epoch 335/1000, Training Loss (NLML): -956.9783\n",
      "convergence dfGPdfNN Run 10/10, Epoch 336/1000, Training Loss (NLML): -956.9601\n",
      "convergence dfGPdfNN Run 10/10, Epoch 337/1000, Training Loss (NLML): -956.9751\n",
      "convergence dfGPdfNN Run 10/10, Epoch 338/1000, Training Loss (NLML): -956.9910\n",
      "convergence dfGPdfNN Run 10/10, Epoch 339/1000, Training Loss (NLML): -956.9996\n",
      "convergence dfGPdfNN Run 10/10, Epoch 340/1000, Training Loss (NLML): -957.0122\n",
      "convergence dfGPdfNN Run 10/10, Epoch 341/1000, Training Loss (NLML): -957.0551\n",
      "convergence dfGPdfNN Run 10/10, Epoch 342/1000, Training Loss (NLML): -957.0697\n",
      "convergence dfGPdfNN Run 10/10, Epoch 343/1000, Training Loss (NLML): -957.0836\n",
      "convergence dfGPdfNN Run 10/10, Epoch 344/1000, Training Loss (NLML): -957.0865\n",
      "convergence dfGPdfNN Run 10/10, Epoch 345/1000, Training Loss (NLML): -957.0719\n",
      "convergence dfGPdfNN Run 10/10, Epoch 346/1000, Training Loss (NLML): -957.0864\n",
      "convergence dfGPdfNN Run 10/10, Epoch 347/1000, Training Loss (NLML): -957.0951\n",
      "convergence dfGPdfNN Run 10/10, Epoch 348/1000, Training Loss (NLML): -957.1014\n",
      "convergence dfGPdfNN Run 10/10, Epoch 349/1000, Training Loss (NLML): -957.1097\n",
      "convergence dfGPdfNN Run 10/10, Epoch 350/1000, Training Loss (NLML): -957.1252\n",
      "convergence dfGPdfNN Run 10/10, Epoch 351/1000, Training Loss (NLML): -957.1389\n",
      "convergence dfGPdfNN Run 10/10, Epoch 352/1000, Training Loss (NLML): -957.1575\n",
      "convergence dfGPdfNN Run 10/10, Epoch 353/1000, Training Loss (NLML): -957.1750\n",
      "convergence dfGPdfNN Run 10/10, Epoch 354/1000, Training Loss (NLML): -957.1967\n",
      "convergence dfGPdfNN Run 10/10, Epoch 355/1000, Training Loss (NLML): -957.2233\n",
      "convergence dfGPdfNN Run 10/10, Epoch 356/1000, Training Loss (NLML): -957.2609\n",
      "convergence dfGPdfNN Run 10/10, Epoch 357/1000, Training Loss (NLML): -957.2917\n",
      "convergence dfGPdfNN Run 10/10, Epoch 358/1000, Training Loss (NLML): -957.3337\n",
      "convergence dfGPdfNN Run 10/10, Epoch 359/1000, Training Loss (NLML): -957.3531\n",
      "convergence dfGPdfNN Run 10/10, Epoch 360/1000, Training Loss (NLML): -957.3334\n",
      "convergence dfGPdfNN Run 10/10, Epoch 361/1000, Training Loss (NLML): -957.3076\n",
      "convergence dfGPdfNN Run 10/10, Epoch 362/1000, Training Loss (NLML): -957.2570\n",
      "convergence dfGPdfNN Run 10/10, Epoch 363/1000, Training Loss (NLML): -957.3042\n",
      "convergence dfGPdfNN Run 10/10, Epoch 364/1000, Training Loss (NLML): -957.3040\n",
      "convergence dfGPdfNN Run 10/10, Epoch 365/1000, Training Loss (NLML): -957.2601\n",
      "convergence dfGPdfNN Run 10/10, Epoch 366/1000, Training Loss (NLML): -957.1542\n",
      "convergence dfGPdfNN Run 10/10, Epoch 367/1000, Training Loss (NLML): -957.2798\n",
      "convergence dfGPdfNN Run 10/10, Epoch 368/1000, Training Loss (NLML): -957.4921\n",
      "convergence dfGPdfNN Run 10/10, Epoch 369/1000, Training Loss (NLML): -957.5422\n",
      "convergence dfGPdfNN Run 10/10, Epoch 370/1000, Training Loss (NLML): -957.5643\n",
      "convergence dfGPdfNN Run 10/10, Epoch 371/1000, Training Loss (NLML): -957.5729\n",
      "convergence dfGPdfNN Run 10/10, Epoch 372/1000, Training Loss (NLML): -957.5753\n",
      "convergence dfGPdfNN Run 10/10, Epoch 373/1000, Training Loss (NLML): -957.4969\n",
      "convergence dfGPdfNN Run 10/10, Epoch 374/1000, Training Loss (NLML): -957.4252\n",
      "convergence dfGPdfNN Run 10/10, Epoch 375/1000, Training Loss (NLML): -957.3971\n",
      "convergence dfGPdfNN Run 10/10, Epoch 376/1000, Training Loss (NLML): -957.3846\n",
      "convergence dfGPdfNN Run 10/10, Epoch 377/1000, Training Loss (NLML): -957.3871\n",
      "convergence dfGPdfNN Run 10/10, Epoch 378/1000, Training Loss (NLML): -957.4402\n",
      "convergence dfGPdfNN Run 10/10, Epoch 379/1000, Training Loss (NLML): -957.4780\n",
      "convergence dfGPdfNN Run 10/10, Epoch 380/1000, Training Loss (NLML): -957.5278\n",
      "convergence dfGPdfNN Run 10/10, Epoch 381/1000, Training Loss (NLML): -957.5565\n",
      "convergence dfGPdfNN Run 10/10, Epoch 382/1000, Training Loss (NLML): -957.5684\n",
      "convergence dfGPdfNN Run 10/10, Epoch 383/1000, Training Loss (NLML): -957.5845\n",
      "convergence dfGPdfNN Run 10/10, Epoch 384/1000, Training Loss (NLML): -957.5929\n",
      "convergence dfGPdfNN Run 10/10, Epoch 385/1000, Training Loss (NLML): -957.6124\n",
      "convergence dfGPdfNN Run 10/10, Epoch 386/1000, Training Loss (NLML): -957.6777\n",
      "convergence dfGPdfNN Run 10/10, Epoch 387/1000, Training Loss (NLML): -957.6902\n",
      "convergence dfGPdfNN Run 10/10, Epoch 388/1000, Training Loss (NLML): -957.7042\n",
      "convergence dfGPdfNN Run 10/10, Epoch 389/1000, Training Loss (NLML): -957.7184\n",
      "convergence dfGPdfNN Run 10/10, Epoch 390/1000, Training Loss (NLML): -957.6693\n",
      "convergence dfGPdfNN Run 10/10, Epoch 391/1000, Training Loss (NLML): -957.6753\n",
      "convergence dfGPdfNN Run 10/10, Epoch 392/1000, Training Loss (NLML): -957.6827\n",
      "convergence dfGPdfNN Run 10/10, Epoch 393/1000, Training Loss (NLML): -957.6919\n",
      "convergence dfGPdfNN Run 10/10, Epoch 394/1000, Training Loss (NLML): -957.6663\n",
      "convergence dfGPdfNN Run 10/10, Epoch 395/1000, Training Loss (NLML): -957.6787\n",
      "convergence dfGPdfNN Run 10/10, Epoch 396/1000, Training Loss (NLML): -957.6512\n",
      "convergence dfGPdfNN Run 10/10, Epoch 397/1000, Training Loss (NLML): -957.6539\n",
      "convergence dfGPdfNN Run 10/10, Epoch 398/1000, Training Loss (NLML): -957.6685\n",
      "convergence dfGPdfNN Run 10/10, Epoch 399/1000, Training Loss (NLML): -957.7144\n",
      "convergence dfGPdfNN Run 10/10, Epoch 400/1000, Training Loss (NLML): -957.7357\n",
      "convergence dfGPdfNN Run 10/10, Epoch 401/1000, Training Loss (NLML): -957.7461\n",
      "convergence dfGPdfNN Run 10/10, Epoch 402/1000, Training Loss (NLML): -957.7571\n",
      "convergence dfGPdfNN Run 10/10, Epoch 403/1000, Training Loss (NLML): -957.7722\n",
      "convergence dfGPdfNN Run 10/10, Epoch 404/1000, Training Loss (NLML): -957.7825\n",
      "convergence dfGPdfNN Run 10/10, Epoch 405/1000, Training Loss (NLML): -957.7948\n",
      "convergence dfGPdfNN Run 10/10, Epoch 406/1000, Training Loss (NLML): -957.8099\n",
      "convergence dfGPdfNN Run 10/10, Epoch 407/1000, Training Loss (NLML): -957.8214\n",
      "convergence dfGPdfNN Run 10/10, Epoch 408/1000, Training Loss (NLML): -957.8326\n",
      "convergence dfGPdfNN Run 10/10, Epoch 409/1000, Training Loss (NLML): -957.8422\n",
      "convergence dfGPdfNN Run 10/10, Epoch 410/1000, Training Loss (NLML): -957.8528\n",
      "convergence dfGPdfNN Run 10/10, Epoch 411/1000, Training Loss (NLML): -957.8585\n",
      "convergence dfGPdfNN Run 10/10, Epoch 412/1000, Training Loss (NLML): -957.8706\n",
      "convergence dfGPdfNN Run 10/10, Epoch 413/1000, Training Loss (NLML): -957.8802\n",
      "convergence dfGPdfNN Run 10/10, Epoch 414/1000, Training Loss (NLML): -957.8917\n",
      "convergence dfGPdfNN Run 10/10, Epoch 415/1000, Training Loss (NLML): -957.9020\n",
      "convergence dfGPdfNN Run 10/10, Epoch 416/1000, Training Loss (NLML): -957.9066\n",
      "convergence dfGPdfNN Run 10/10, Epoch 417/1000, Training Loss (NLML): -957.9478\n",
      "convergence dfGPdfNN Run 10/10, Epoch 418/1000, Training Loss (NLML): -957.9565\n",
      "convergence dfGPdfNN Run 10/10, Epoch 419/1000, Training Loss (NLML): -957.9657\n",
      "convergence dfGPdfNN Run 10/10, Epoch 420/1000, Training Loss (NLML): -957.9771\n",
      "convergence dfGPdfNN Run 10/10, Epoch 421/1000, Training Loss (NLML): -957.9879\n",
      "convergence dfGPdfNN Run 10/10, Epoch 422/1000, Training Loss (NLML): -958.0013\n",
      "convergence dfGPdfNN Run 10/10, Epoch 423/1000, Training Loss (NLML): -958.0125\n",
      "convergence dfGPdfNN Run 10/10, Epoch 424/1000, Training Loss (NLML): -958.0214\n",
      "convergence dfGPdfNN Run 10/10, Epoch 425/1000, Training Loss (NLML): -958.0327\n",
      "convergence dfGPdfNN Run 10/10, Epoch 426/1000, Training Loss (NLML): -958.0436\n",
      "convergence dfGPdfNN Run 10/10, Epoch 427/1000, Training Loss (NLML): -958.0518\n",
      "convergence dfGPdfNN Run 10/10, Epoch 428/1000, Training Loss (NLML): -958.0619\n",
      "convergence dfGPdfNN Run 10/10, Epoch 429/1000, Training Loss (NLML): -958.0717\n",
      "convergence dfGPdfNN Run 10/10, Epoch 430/1000, Training Loss (NLML): -958.0817\n",
      "convergence dfGPdfNN Run 10/10, Epoch 431/1000, Training Loss (NLML): -958.0920\n",
      "convergence dfGPdfNN Run 10/10, Epoch 432/1000, Training Loss (NLML): -958.1016\n",
      "convergence dfGPdfNN Run 10/10, Epoch 433/1000, Training Loss (NLML): -958.1115\n",
      "convergence dfGPdfNN Run 10/10, Epoch 434/1000, Training Loss (NLML): -958.1212\n",
      "convergence dfGPdfNN Run 10/10, Epoch 435/1000, Training Loss (NLML): -958.1318\n",
      "convergence dfGPdfNN Run 10/10, Epoch 436/1000, Training Loss (NLML): -958.1393\n",
      "convergence dfGPdfNN Run 10/10, Epoch 437/1000, Training Loss (NLML): -958.1487\n",
      "convergence dfGPdfNN Run 10/10, Epoch 438/1000, Training Loss (NLML): -958.1592\n",
      "convergence dfGPdfNN Run 10/10, Epoch 439/1000, Training Loss (NLML): -958.1688\n",
      "convergence dfGPdfNN Run 10/10, Epoch 440/1000, Training Loss (NLML): -958.1770\n",
      "convergence dfGPdfNN Run 10/10, Epoch 441/1000, Training Loss (NLML): -958.1868\n",
      "convergence dfGPdfNN Run 10/10, Epoch 442/1000, Training Loss (NLML): -958.1951\n",
      "convergence dfGPdfNN Run 10/10, Epoch 443/1000, Training Loss (NLML): -958.2051\n",
      "convergence dfGPdfNN Run 10/10, Epoch 444/1000, Training Loss (NLML): -958.2152\n",
      "convergence dfGPdfNN Run 10/10, Epoch 445/1000, Training Loss (NLML): -958.2225\n",
      "convergence dfGPdfNN Run 10/10, Epoch 446/1000, Training Loss (NLML): -958.2325\n",
      "convergence dfGPdfNN Run 10/10, Epoch 447/1000, Training Loss (NLML): -958.2399\n",
      "convergence dfGPdfNN Run 10/10, Epoch 448/1000, Training Loss (NLML): -958.2493\n",
      "convergence dfGPdfNN Run 10/10, Epoch 449/1000, Training Loss (NLML): -958.2609\n",
      "convergence dfGPdfNN Run 10/10, Epoch 450/1000, Training Loss (NLML): -958.2677\n",
      "convergence dfGPdfNN Run 10/10, Epoch 451/1000, Training Loss (NLML): -958.2769\n",
      "convergence dfGPdfNN Run 10/10, Epoch 452/1000, Training Loss (NLML): -958.2850\n",
      "convergence dfGPdfNN Run 10/10, Epoch 453/1000, Training Loss (NLML): -958.2948\n",
      "convergence dfGPdfNN Run 10/10, Epoch 454/1000, Training Loss (NLML): -958.3031\n",
      "convergence dfGPdfNN Run 10/10, Epoch 455/1000, Training Loss (NLML): -958.3124\n",
      "convergence dfGPdfNN Run 10/10, Epoch 456/1000, Training Loss (NLML): -958.3213\n",
      "convergence dfGPdfNN Run 10/10, Epoch 457/1000, Training Loss (NLML): -958.3291\n",
      "convergence dfGPdfNN Run 10/10, Epoch 458/1000, Training Loss (NLML): -958.3373\n",
      "convergence dfGPdfNN Run 10/10, Epoch 459/1000, Training Loss (NLML): -958.3457\n",
      "convergence dfGPdfNN Run 10/10, Epoch 460/1000, Training Loss (NLML): -958.3561\n",
      "convergence dfGPdfNN Run 10/10, Epoch 461/1000, Training Loss (NLML): -958.3595\n",
      "convergence dfGPdfNN Run 10/10, Epoch 462/1000, Training Loss (NLML): -958.3673\n",
      "convergence dfGPdfNN Run 10/10, Epoch 463/1000, Training Loss (NLML): -958.3767\n",
      "convergence dfGPdfNN Run 10/10, Epoch 464/1000, Training Loss (NLML): -958.3851\n",
      "convergence dfGPdfNN Run 10/10, Epoch 465/1000, Training Loss (NLML): -958.3950\n",
      "convergence dfGPdfNN Run 10/10, Epoch 466/1000, Training Loss (NLML): -958.3977\n",
      "convergence dfGPdfNN Run 10/10, Epoch 467/1000, Training Loss (NLML): -958.4094\n",
      "convergence dfGPdfNN Run 10/10, Epoch 468/1000, Training Loss (NLML): -958.4183\n",
      "convergence dfGPdfNN Run 10/10, Epoch 469/1000, Training Loss (NLML): -958.4274\n",
      "convergence dfGPdfNN Run 10/10, Epoch 470/1000, Training Loss (NLML): -958.4365\n",
      "convergence dfGPdfNN Run 10/10, Epoch 471/1000, Training Loss (NLML): -958.4432\n",
      "convergence dfGPdfNN Run 10/10, Epoch 472/1000, Training Loss (NLML): -958.4529\n",
      "convergence dfGPdfNN Run 10/10, Epoch 473/1000, Training Loss (NLML): -958.4620\n",
      "convergence dfGPdfNN Run 10/10, Epoch 474/1000, Training Loss (NLML): -958.4694\n",
      "convergence dfGPdfNN Run 10/10, Epoch 475/1000, Training Loss (NLML): -958.4772\n",
      "convergence dfGPdfNN Run 10/10, Epoch 476/1000, Training Loss (NLML): -958.4865\n",
      "convergence dfGPdfNN Run 10/10, Epoch 477/1000, Training Loss (NLML): -958.4922\n",
      "convergence dfGPdfNN Run 10/10, Epoch 478/1000, Training Loss (NLML): -958.5010\n",
      "convergence dfGPdfNN Run 10/10, Epoch 479/1000, Training Loss (NLML): -958.5088\n",
      "convergence dfGPdfNN Run 10/10, Epoch 480/1000, Training Loss (NLML): -958.5186\n",
      "convergence dfGPdfNN Run 10/10, Epoch 481/1000, Training Loss (NLML): -958.5254\n",
      "convergence dfGPdfNN Run 10/10, Epoch 482/1000, Training Loss (NLML): -958.5333\n",
      "convergence dfGPdfNN Run 10/10, Epoch 483/1000, Training Loss (NLML): -958.5411\n",
      "convergence dfGPdfNN Run 10/10, Epoch 484/1000, Training Loss (NLML): -958.5503\n",
      "convergence dfGPdfNN Run 10/10, Epoch 485/1000, Training Loss (NLML): -958.5583\n",
      "convergence dfGPdfNN Run 10/10, Epoch 486/1000, Training Loss (NLML): -958.5647\n",
      "convergence dfGPdfNN Run 10/10, Epoch 487/1000, Training Loss (NLML): -958.5720\n",
      "convergence dfGPdfNN Run 10/10, Epoch 488/1000, Training Loss (NLML): -958.5809\n",
      "convergence dfGPdfNN Run 10/10, Epoch 489/1000, Training Loss (NLML): -958.5879\n",
      "convergence dfGPdfNN Run 10/10, Epoch 490/1000, Training Loss (NLML): -958.5962\n",
      "convergence dfGPdfNN Run 10/10, Epoch 491/1000, Training Loss (NLML): -958.6027\n",
      "convergence dfGPdfNN Run 10/10, Epoch 492/1000, Training Loss (NLML): -958.6105\n",
      "convergence dfGPdfNN Run 10/10, Epoch 493/1000, Training Loss (NLML): -958.6188\n",
      "convergence dfGPdfNN Run 10/10, Epoch 494/1000, Training Loss (NLML): -958.6270\n",
      "convergence dfGPdfNN Run 10/10, Epoch 495/1000, Training Loss (NLML): -958.6337\n",
      "convergence dfGPdfNN Run 10/10, Epoch 496/1000, Training Loss (NLML): -958.6405\n",
      "convergence dfGPdfNN Run 10/10, Epoch 497/1000, Training Loss (NLML): -958.6492\n",
      "convergence dfGPdfNN Run 10/10, Epoch 498/1000, Training Loss (NLML): -958.6552\n",
      "convergence dfGPdfNN Run 10/10, Epoch 499/1000, Training Loss (NLML): -958.6650\n",
      "convergence dfGPdfNN Run 10/10, Epoch 500/1000, Training Loss (NLML): -958.6707\n",
      "convergence dfGPdfNN Run 10/10, Epoch 501/1000, Training Loss (NLML): -958.6781\n",
      "convergence dfGPdfNN Run 10/10, Epoch 502/1000, Training Loss (NLML): -958.6854\n",
      "convergence dfGPdfNN Run 10/10, Epoch 503/1000, Training Loss (NLML): -958.6923\n",
      "convergence dfGPdfNN Run 10/10, Epoch 504/1000, Training Loss (NLML): -958.7008\n",
      "convergence dfGPdfNN Run 10/10, Epoch 505/1000, Training Loss (NLML): -958.7090\n",
      "convergence dfGPdfNN Run 10/10, Epoch 506/1000, Training Loss (NLML): -958.7140\n",
      "convergence dfGPdfNN Run 10/10, Epoch 507/1000, Training Loss (NLML): -958.7224\n",
      "convergence dfGPdfNN Run 10/10, Epoch 508/1000, Training Loss (NLML): -958.7292\n",
      "convergence dfGPdfNN Run 10/10, Epoch 509/1000, Training Loss (NLML): -958.7369\n",
      "convergence dfGPdfNN Run 10/10, Epoch 510/1000, Training Loss (NLML): -958.7429\n",
      "convergence dfGPdfNN Run 10/10, Epoch 511/1000, Training Loss (NLML): -958.7506\n",
      "convergence dfGPdfNN Run 10/10, Epoch 512/1000, Training Loss (NLML): -958.7576\n",
      "convergence dfGPdfNN Run 10/10, Epoch 513/1000, Training Loss (NLML): -958.7645\n",
      "convergence dfGPdfNN Run 10/10, Epoch 514/1000, Training Loss (NLML): -958.7703\n",
      "convergence dfGPdfNN Run 10/10, Epoch 515/1000, Training Loss (NLML): -958.7781\n",
      "convergence dfGPdfNN Run 10/10, Epoch 516/1000, Training Loss (NLML): -958.7849\n",
      "convergence dfGPdfNN Run 10/10, Epoch 517/1000, Training Loss (NLML): -958.7931\n",
      "convergence dfGPdfNN Run 10/10, Epoch 518/1000, Training Loss (NLML): -958.7983\n",
      "convergence dfGPdfNN Run 10/10, Epoch 519/1000, Training Loss (NLML): -958.8062\n",
      "convergence dfGPdfNN Run 10/10, Epoch 520/1000, Training Loss (NLML): -958.8136\n",
      "convergence dfGPdfNN Run 10/10, Epoch 521/1000, Training Loss (NLML): -958.8199\n",
      "convergence dfGPdfNN Run 10/10, Epoch 522/1000, Training Loss (NLML): -958.8275\n",
      "convergence dfGPdfNN Run 10/10, Epoch 523/1000, Training Loss (NLML): -958.8347\n",
      "convergence dfGPdfNN Run 10/10, Epoch 524/1000, Training Loss (NLML): -958.8413\n",
      "convergence dfGPdfNN Run 10/10, Epoch 525/1000, Training Loss (NLML): -958.8483\n",
      "convergence dfGPdfNN Run 10/10, Epoch 526/1000, Training Loss (NLML): -958.8536\n",
      "convergence dfGPdfNN Run 10/10, Epoch 527/1000, Training Loss (NLML): -958.8616\n",
      "convergence dfGPdfNN Run 10/10, Epoch 528/1000, Training Loss (NLML): -958.8676\n",
      "convergence dfGPdfNN Run 10/10, Epoch 529/1000, Training Loss (NLML): -958.8750\n",
      "convergence dfGPdfNN Run 10/10, Epoch 530/1000, Training Loss (NLML): -958.8813\n",
      "convergence dfGPdfNN Run 10/10, Epoch 531/1000, Training Loss (NLML): -958.8868\n",
      "convergence dfGPdfNN Run 10/10, Epoch 532/1000, Training Loss (NLML): -958.8943\n",
      "convergence dfGPdfNN Run 10/10, Epoch 533/1000, Training Loss (NLML): -958.9010\n",
      "convergence dfGPdfNN Run 10/10, Epoch 534/1000, Training Loss (NLML): -958.9125\n",
      "convergence dfGPdfNN Run 10/10, Epoch 535/1000, Training Loss (NLML): -958.9137\n",
      "convergence dfGPdfNN Run 10/10, Epoch 536/1000, Training Loss (NLML): -958.9208\n",
      "convergence dfGPdfNN Run 10/10, Epoch 537/1000, Training Loss (NLML): -958.9260\n",
      "convergence dfGPdfNN Run 10/10, Epoch 538/1000, Training Loss (NLML): -958.9324\n",
      "convergence dfGPdfNN Run 10/10, Epoch 539/1000, Training Loss (NLML): -958.9393\n",
      "convergence dfGPdfNN Run 10/10, Epoch 540/1000, Training Loss (NLML): -958.9485\n",
      "convergence dfGPdfNN Run 10/10, Epoch 541/1000, Training Loss (NLML): -958.9526\n",
      "convergence dfGPdfNN Run 10/10, Epoch 542/1000, Training Loss (NLML): -958.9608\n",
      "convergence dfGPdfNN Run 10/10, Epoch 543/1000, Training Loss (NLML): -958.9657\n",
      "convergence dfGPdfNN Run 10/10, Epoch 544/1000, Training Loss (NLML): -958.9708\n",
      "convergence dfGPdfNN Run 10/10, Epoch 545/1000, Training Loss (NLML): -958.9773\n",
      "convergence dfGPdfNN Run 10/10, Epoch 546/1000, Training Loss (NLML): -958.9841\n",
      "convergence dfGPdfNN Run 10/10, Epoch 547/1000, Training Loss (NLML): -958.9913\n",
      "convergence dfGPdfNN Run 10/10, Epoch 548/1000, Training Loss (NLML): -958.9963\n",
      "convergence dfGPdfNN Run 10/10, Epoch 549/1000, Training Loss (NLML): -959.0035\n",
      "convergence dfGPdfNN Run 10/10, Epoch 550/1000, Training Loss (NLML): -959.0095\n",
      "convergence dfGPdfNN Run 10/10, Epoch 551/1000, Training Loss (NLML): -959.0159\n",
      "convergence dfGPdfNN Run 10/10, Epoch 552/1000, Training Loss (NLML): -959.0220\n",
      "convergence dfGPdfNN Run 10/10, Epoch 553/1000, Training Loss (NLML): -959.0278\n",
      "convergence dfGPdfNN Run 10/10, Epoch 554/1000, Training Loss (NLML): -959.0344\n",
      "convergence dfGPdfNN Run 10/10, Epoch 555/1000, Training Loss (NLML): -959.0465\n",
      "convergence dfGPdfNN Run 10/10, Epoch 556/1000, Training Loss (NLML): -959.0481\n",
      "convergence dfGPdfNN Run 10/10, Epoch 557/1000, Training Loss (NLML): -959.0553\n",
      "convergence dfGPdfNN Run 10/10, Epoch 558/1000, Training Loss (NLML): -959.0603\n",
      "convergence dfGPdfNN Run 10/10, Epoch 559/1000, Training Loss (NLML): -959.0660\n",
      "convergence dfGPdfNN Run 10/10, Epoch 560/1000, Training Loss (NLML): -959.0732\n",
      "convergence dfGPdfNN Run 10/10, Epoch 561/1000, Training Loss (NLML): -959.0784\n",
      "convergence dfGPdfNN Run 10/10, Epoch 562/1000, Training Loss (NLML): -959.0840\n",
      "convergence dfGPdfNN Run 10/10, Epoch 563/1000, Training Loss (NLML): -959.0898\n",
      "convergence dfGPdfNN Run 10/10, Epoch 564/1000, Training Loss (NLML): -959.0963\n",
      "convergence dfGPdfNN Run 10/10, Epoch 565/1000, Training Loss (NLML): -959.1017\n",
      "convergence dfGPdfNN Run 10/10, Epoch 566/1000, Training Loss (NLML): -959.1125\n",
      "convergence dfGPdfNN Run 10/10, Epoch 567/1000, Training Loss (NLML): -959.1144\n",
      "convergence dfGPdfNN Run 10/10, Epoch 568/1000, Training Loss (NLML): -959.1256\n",
      "convergence dfGPdfNN Run 10/10, Epoch 569/1000, Training Loss (NLML): -959.1289\n",
      "convergence dfGPdfNN Run 10/10, Epoch 570/1000, Training Loss (NLML): -959.1354\n",
      "convergence dfGPdfNN Run 10/10, Epoch 571/1000, Training Loss (NLML): -959.1422\n",
      "convergence dfGPdfNN Run 10/10, Epoch 572/1000, Training Loss (NLML): -959.1266\n",
      "convergence dfGPdfNN Run 10/10, Epoch 573/1000, Training Loss (NLML): -959.1335\n",
      "convergence dfGPdfNN Run 10/10, Epoch 574/1000, Training Loss (NLML): -959.1191\n",
      "convergence dfGPdfNN Run 10/10, Epoch 575/1000, Training Loss (NLML): -959.1244\n",
      "convergence dfGPdfNN Run 10/10, Epoch 576/1000, Training Loss (NLML): -959.1331\n",
      "convergence dfGPdfNN Run 10/10, Epoch 577/1000, Training Loss (NLML): -959.1368\n",
      "convergence dfGPdfNN Run 10/10, Epoch 578/1000, Training Loss (NLML): -959.1428\n",
      "convergence dfGPdfNN Run 10/10, Epoch 579/1000, Training Loss (NLML): -959.1477\n",
      "convergence dfGPdfNN Run 10/10, Epoch 580/1000, Training Loss (NLML): -959.1544\n",
      "convergence dfGPdfNN Run 10/10, Epoch 581/1000, Training Loss (NLML): -959.1605\n",
      "convergence dfGPdfNN Run 10/10, Epoch 582/1000, Training Loss (NLML): -959.1680\n",
      "convergence dfGPdfNN Run 10/10, Epoch 583/1000, Training Loss (NLML): -959.1725\n",
      "convergence dfGPdfNN Run 10/10, Epoch 584/1000, Training Loss (NLML): -959.1650\n",
      "convergence dfGPdfNN Run 10/10, Epoch 585/1000, Training Loss (NLML): -959.1880\n",
      "convergence dfGPdfNN Run 10/10, Epoch 586/1000, Training Loss (NLML): -959.1954\n",
      "convergence dfGPdfNN Run 10/10, Epoch 587/1000, Training Loss (NLML): -959.2020\n",
      "convergence dfGPdfNN Run 10/10, Epoch 588/1000, Training Loss (NLML): -959.2078\n",
      "convergence dfGPdfNN Run 10/10, Epoch 589/1000, Training Loss (NLML): -959.2142\n",
      "convergence dfGPdfNN Run 10/10, Epoch 590/1000, Training Loss (NLML): -959.2197\n",
      "convergence dfGPdfNN Run 10/10, Epoch 591/1000, Training Loss (NLML): -959.2212\n",
      "convergence dfGPdfNN Run 10/10, Epoch 592/1000, Training Loss (NLML): -959.2281\n",
      "convergence dfGPdfNN Run 10/10, Epoch 593/1000, Training Loss (NLML): -959.2327\n",
      "convergence dfGPdfNN Run 10/10, Epoch 594/1000, Training Loss (NLML): -959.2389\n",
      "convergence dfGPdfNN Run 10/10, Epoch 595/1000, Training Loss (NLML): -959.2437\n",
      "convergence dfGPdfNN Run 10/10, Epoch 596/1000, Training Loss (NLML): -959.2496\n",
      "convergence dfGPdfNN Run 10/10, Epoch 597/1000, Training Loss (NLML): -959.2550\n",
      "convergence dfGPdfNN Run 10/10, Epoch 598/1000, Training Loss (NLML): -959.2616\n",
      "convergence dfGPdfNN Run 10/10, Epoch 599/1000, Training Loss (NLML): -959.2688\n",
      "convergence dfGPdfNN Run 10/10, Epoch 600/1000, Training Loss (NLML): -959.2734\n",
      "convergence dfGPdfNN Run 10/10, Epoch 601/1000, Training Loss (NLML): -959.2780\n",
      "convergence dfGPdfNN Run 10/10, Epoch 602/1000, Training Loss (NLML): -959.2826\n",
      "convergence dfGPdfNN Run 10/10, Epoch 603/1000, Training Loss (NLML): -959.2875\n",
      "convergence dfGPdfNN Run 10/10, Epoch 604/1000, Training Loss (NLML): -959.2944\n",
      "convergence dfGPdfNN Run 10/10, Epoch 605/1000, Training Loss (NLML): -959.2990\n",
      "convergence dfGPdfNN Run 10/10, Epoch 606/1000, Training Loss (NLML): -959.3055\n",
      "convergence dfGPdfNN Run 10/10, Epoch 607/1000, Training Loss (NLML): -959.2946\n",
      "convergence dfGPdfNN Run 10/10, Epoch 608/1000, Training Loss (NLML): -959.2976\n",
      "convergence dfGPdfNN Run 10/10, Epoch 609/1000, Training Loss (NLML): -959.3082\n",
      "convergence dfGPdfNN Run 10/10, Epoch 610/1000, Training Loss (NLML): -959.3307\n",
      "convergence dfGPdfNN Run 10/10, Epoch 611/1000, Training Loss (NLML): -959.3301\n",
      "convergence dfGPdfNN Run 10/10, Epoch 612/1000, Training Loss (NLML): -959.3361\n",
      "convergence dfGPdfNN Run 10/10, Epoch 613/1000, Training Loss (NLML): -959.3424\n",
      "convergence dfGPdfNN Run 10/10, Epoch 614/1000, Training Loss (NLML): -959.3467\n",
      "convergence dfGPdfNN Run 10/10, Epoch 615/1000, Training Loss (NLML): -959.3507\n",
      "convergence dfGPdfNN Run 10/10, Epoch 616/1000, Training Loss (NLML): -959.3555\n",
      "convergence dfGPdfNN Run 10/10, Epoch 617/1000, Training Loss (NLML): -959.3597\n",
      "convergence dfGPdfNN Run 10/10, Epoch 618/1000, Training Loss (NLML): -959.3655\n",
      "convergence dfGPdfNN Run 10/10, Epoch 619/1000, Training Loss (NLML): -959.3711\n",
      "convergence dfGPdfNN Run 10/10, Epoch 620/1000, Training Loss (NLML): -959.3811\n",
      "convergence dfGPdfNN Run 10/10, Epoch 621/1000, Training Loss (NLML): -959.3849\n",
      "convergence dfGPdfNN Run 10/10, Epoch 622/1000, Training Loss (NLML): -959.3865\n",
      "convergence dfGPdfNN Run 10/10, Epoch 623/1000, Training Loss (NLML): -959.3918\n",
      "convergence dfGPdfNN Run 10/10, Epoch 624/1000, Training Loss (NLML): -959.3966\n",
      "convergence dfGPdfNN Run 10/10, Epoch 625/1000, Training Loss (NLML): -959.4031\n",
      "convergence dfGPdfNN Run 10/10, Epoch 626/1000, Training Loss (NLML): -959.4066\n",
      "convergence dfGPdfNN Run 10/10, Epoch 627/1000, Training Loss (NLML): -959.4122\n",
      "convergence dfGPdfNN Run 10/10, Epoch 628/1000, Training Loss (NLML): -959.4171\n",
      "convergence dfGPdfNN Run 10/10, Epoch 629/1000, Training Loss (NLML): -959.4198\n",
      "convergence dfGPdfNN Run 10/10, Epoch 630/1000, Training Loss (NLML): -959.4297\n",
      "convergence dfGPdfNN Run 10/10, Epoch 631/1000, Training Loss (NLML): -959.4358\n",
      "convergence dfGPdfNN Run 10/10, Epoch 632/1000, Training Loss (NLML): -959.4351\n",
      "convergence dfGPdfNN Run 10/10, Epoch 633/1000, Training Loss (NLML): -959.4413\n",
      "convergence dfGPdfNN Run 10/10, Epoch 634/1000, Training Loss (NLML): -959.4437\n",
      "convergence dfGPdfNN Run 10/10, Epoch 635/1000, Training Loss (NLML): -959.4484\n",
      "convergence dfGPdfNN Run 10/10, Epoch 636/1000, Training Loss (NLML): -959.4547\n",
      "convergence dfGPdfNN Run 10/10, Epoch 637/1000, Training Loss (NLML): -959.4594\n",
      "convergence dfGPdfNN Run 10/10, Epoch 638/1000, Training Loss (NLML): -959.4630\n",
      "convergence dfGPdfNN Run 10/10, Epoch 639/1000, Training Loss (NLML): -959.4678\n",
      "convergence dfGPdfNN Run 10/10, Epoch 640/1000, Training Loss (NLML): -959.4722\n",
      "convergence dfGPdfNN Run 10/10, Epoch 641/1000, Training Loss (NLML): -959.4773\n",
      "convergence dfGPdfNN Run 10/10, Epoch 642/1000, Training Loss (NLML): -959.4823\n",
      "convergence dfGPdfNN Run 10/10, Epoch 643/1000, Training Loss (NLML): -959.4871\n",
      "convergence dfGPdfNN Run 10/10, Epoch 644/1000, Training Loss (NLML): -959.4924\n",
      "convergence dfGPdfNN Run 10/10, Epoch 645/1000, Training Loss (NLML): -959.4960\n",
      "convergence dfGPdfNN Run 10/10, Epoch 646/1000, Training Loss (NLML): -959.5012\n",
      "convergence dfGPdfNN Run 10/10, Epoch 647/1000, Training Loss (NLML): -959.5068\n",
      "convergence dfGPdfNN Run 10/10, Epoch 648/1000, Training Loss (NLML): -959.5099\n",
      "convergence dfGPdfNN Run 10/10, Epoch 649/1000, Training Loss (NLML): -959.5126\n",
      "convergence dfGPdfNN Run 10/10, Epoch 650/1000, Training Loss (NLML): -959.5201\n",
      "convergence dfGPdfNN Run 10/10, Epoch 651/1000, Training Loss (NLML): -959.5227\n",
      "convergence dfGPdfNN Run 10/10, Epoch 652/1000, Training Loss (NLML): -959.5283\n",
      "convergence dfGPdfNN Run 10/10, Epoch 653/1000, Training Loss (NLML): -959.5339\n",
      "convergence dfGPdfNN Run 10/10, Epoch 654/1000, Training Loss (NLML): -959.5392\n",
      "convergence dfGPdfNN Run 10/10, Epoch 655/1000, Training Loss (NLML): -959.5414\n",
      "convergence dfGPdfNN Run 10/10, Epoch 656/1000, Training Loss (NLML): -959.5476\n",
      "convergence dfGPdfNN Run 10/10, Epoch 657/1000, Training Loss (NLML): -959.5519\n",
      "convergence dfGPdfNN Run 10/10, Epoch 658/1000, Training Loss (NLML): -959.5564\n",
      "convergence dfGPdfNN Run 10/10, Epoch 659/1000, Training Loss (NLML): -959.5599\n",
      "convergence dfGPdfNN Run 10/10, Epoch 660/1000, Training Loss (NLML): -959.5677\n",
      "convergence dfGPdfNN Run 10/10, Epoch 661/1000, Training Loss (NLML): -959.5707\n",
      "convergence dfGPdfNN Run 10/10, Epoch 662/1000, Training Loss (NLML): -959.5742\n",
      "convergence dfGPdfNN Run 10/10, Epoch 663/1000, Training Loss (NLML): -959.5790\n",
      "convergence dfGPdfNN Run 10/10, Epoch 664/1000, Training Loss (NLML): -959.5834\n",
      "convergence dfGPdfNN Run 10/10, Epoch 665/1000, Training Loss (NLML): -959.5869\n",
      "convergence dfGPdfNN Run 10/10, Epoch 666/1000, Training Loss (NLML): -959.5914\n",
      "convergence dfGPdfNN Run 10/10, Epoch 667/1000, Training Loss (NLML): -959.5970\n",
      "convergence dfGPdfNN Run 10/10, Epoch 668/1000, Training Loss (NLML): -959.6018\n",
      "convergence dfGPdfNN Run 10/10, Epoch 669/1000, Training Loss (NLML): -959.6063\n",
      "convergence dfGPdfNN Run 10/10, Epoch 670/1000, Training Loss (NLML): -959.6097\n",
      "convergence dfGPdfNN Run 10/10, Epoch 671/1000, Training Loss (NLML): -959.6163\n",
      "convergence dfGPdfNN Run 10/10, Epoch 672/1000, Training Loss (NLML): -959.6188\n",
      "convergence dfGPdfNN Run 10/10, Epoch 673/1000, Training Loss (NLML): -959.6237\n",
      "convergence dfGPdfNN Run 10/10, Epoch 674/1000, Training Loss (NLML): -959.6272\n",
      "convergence dfGPdfNN Run 10/10, Epoch 675/1000, Training Loss (NLML): -959.6328\n",
      "convergence dfGPdfNN Run 10/10, Epoch 676/1000, Training Loss (NLML): -959.6367\n",
      "convergence dfGPdfNN Run 10/10, Epoch 677/1000, Training Loss (NLML): -959.6405\n",
      "convergence dfGPdfNN Run 10/10, Epoch 678/1000, Training Loss (NLML): -959.6462\n",
      "convergence dfGPdfNN Run 10/10, Epoch 679/1000, Training Loss (NLML): -959.6497\n",
      "convergence dfGPdfNN Run 10/10, Epoch 680/1000, Training Loss (NLML): -959.6536\n",
      "convergence dfGPdfNN Run 10/10, Epoch 681/1000, Training Loss (NLML): -959.6588\n",
      "convergence dfGPdfNN Run 10/10, Epoch 682/1000, Training Loss (NLML): -959.6622\n",
      "convergence dfGPdfNN Run 10/10, Epoch 683/1000, Training Loss (NLML): -959.6650\n",
      "convergence dfGPdfNN Run 10/10, Epoch 684/1000, Training Loss (NLML): -959.6705\n",
      "convergence dfGPdfNN Run 10/10, Epoch 685/1000, Training Loss (NLML): -959.6761\n",
      "convergence dfGPdfNN Run 10/10, Epoch 686/1000, Training Loss (NLML): -959.6808\n",
      "convergence dfGPdfNN Run 10/10, Epoch 687/1000, Training Loss (NLML): -959.6841\n",
      "convergence dfGPdfNN Run 10/10, Epoch 688/1000, Training Loss (NLML): -959.6881\n",
      "convergence dfGPdfNN Run 10/10, Epoch 689/1000, Training Loss (NLML): -959.6903\n",
      "convergence dfGPdfNN Run 10/10, Epoch 690/1000, Training Loss (NLML): -959.6951\n",
      "convergence dfGPdfNN Run 10/10, Epoch 691/1000, Training Loss (NLML): -959.7006\n",
      "convergence dfGPdfNN Run 10/10, Epoch 692/1000, Training Loss (NLML): -959.7050\n",
      "convergence dfGPdfNN Run 10/10, Epoch 693/1000, Training Loss (NLML): -959.7079\n",
      "convergence dfGPdfNN Run 10/10, Epoch 694/1000, Training Loss (NLML): -959.7135\n",
      "convergence dfGPdfNN Run 10/10, Epoch 695/1000, Training Loss (NLML): -959.7177\n",
      "convergence dfGPdfNN Run 10/10, Epoch 696/1000, Training Loss (NLML): -959.7218\n",
      "convergence dfGPdfNN Run 10/10, Epoch 697/1000, Training Loss (NLML): -959.7258\n",
      "convergence dfGPdfNN Run 10/10, Epoch 698/1000, Training Loss (NLML): -959.7292\n",
      "convergence dfGPdfNN Run 10/10, Epoch 699/1000, Training Loss (NLML): -959.7343\n",
      "convergence dfGPdfNN Run 10/10, Epoch 700/1000, Training Loss (NLML): -959.7358\n",
      "convergence dfGPdfNN Run 10/10, Epoch 701/1000, Training Loss (NLML): -959.7432\n",
      "convergence dfGPdfNN Run 10/10, Epoch 702/1000, Training Loss (NLML): -959.7446\n",
      "convergence dfGPdfNN Run 10/10, Epoch 703/1000, Training Loss (NLML): -959.7496\n",
      "convergence dfGPdfNN Run 10/10, Epoch 704/1000, Training Loss (NLML): -959.7532\n",
      "convergence dfGPdfNN Run 10/10, Epoch 705/1000, Training Loss (NLML): -959.7588\n",
      "convergence dfGPdfNN Run 10/10, Epoch 706/1000, Training Loss (NLML): -959.7610\n",
      "convergence dfGPdfNN Run 10/10, Epoch 707/1000, Training Loss (NLML): -959.7654\n",
      "convergence dfGPdfNN Run 10/10, Epoch 708/1000, Training Loss (NLML): -959.7683\n",
      "convergence dfGPdfNN Run 10/10, Epoch 709/1000, Training Loss (NLML): -959.7686\n",
      "convergence dfGPdfNN Run 10/10, Epoch 710/1000, Training Loss (NLML): -959.7743\n",
      "convergence dfGPdfNN Run 10/10, Epoch 711/1000, Training Loss (NLML): -959.7783\n",
      "convergence dfGPdfNN Run 10/10, Epoch 712/1000, Training Loss (NLML): -959.7812\n",
      "convergence dfGPdfNN Run 10/10, Epoch 713/1000, Training Loss (NLML): -959.7859\n",
      "convergence dfGPdfNN Run 10/10, Epoch 714/1000, Training Loss (NLML): -959.7881\n",
      "convergence dfGPdfNN Run 10/10, Epoch 715/1000, Training Loss (NLML): -959.7947\n",
      "convergence dfGPdfNN Run 10/10, Epoch 716/1000, Training Loss (NLML): -959.7991\n",
      "convergence dfGPdfNN Run 10/10, Epoch 717/1000, Training Loss (NLML): -959.8011\n",
      "convergence dfGPdfNN Run 10/10, Epoch 718/1000, Training Loss (NLML): -959.8053\n",
      "convergence dfGPdfNN Run 10/10, Epoch 719/1000, Training Loss (NLML): -959.8087\n",
      "convergence dfGPdfNN Run 10/10, Epoch 720/1000, Training Loss (NLML): -959.8123\n",
      "convergence dfGPdfNN Run 10/10, Epoch 721/1000, Training Loss (NLML): -959.8169\n",
      "convergence dfGPdfNN Run 10/10, Epoch 722/1000, Training Loss (NLML): -959.8217\n",
      "convergence dfGPdfNN Run 10/10, Epoch 723/1000, Training Loss (NLML): -959.8245\n",
      "convergence dfGPdfNN Run 10/10, Epoch 724/1000, Training Loss (NLML): -959.8274\n",
      "convergence dfGPdfNN Run 10/10, Epoch 725/1000, Training Loss (NLML): -959.8323\n",
      "convergence dfGPdfNN Run 10/10, Epoch 726/1000, Training Loss (NLML): -959.8368\n",
      "convergence dfGPdfNN Run 10/10, Epoch 727/1000, Training Loss (NLML): -959.8403\n",
      "convergence dfGPdfNN Run 10/10, Epoch 728/1000, Training Loss (NLML): -959.8457\n",
      "convergence dfGPdfNN Run 10/10, Epoch 729/1000, Training Loss (NLML): -959.8466\n",
      "convergence dfGPdfNN Run 10/10, Epoch 730/1000, Training Loss (NLML): -959.8522\n",
      "convergence dfGPdfNN Run 10/10, Epoch 731/1000, Training Loss (NLML): -959.8546\n",
      "convergence dfGPdfNN Run 10/10, Epoch 732/1000, Training Loss (NLML): -959.8579\n",
      "convergence dfGPdfNN Run 10/10, Epoch 733/1000, Training Loss (NLML): -959.8613\n",
      "convergence dfGPdfNN Run 10/10, Epoch 734/1000, Training Loss (NLML): -959.8656\n",
      "convergence dfGPdfNN Run 10/10, Epoch 735/1000, Training Loss (NLML): -959.8694\n",
      "convergence dfGPdfNN Run 10/10, Epoch 736/1000, Training Loss (NLML): -959.8732\n",
      "convergence dfGPdfNN Run 10/10, Epoch 737/1000, Training Loss (NLML): -959.8794\n",
      "convergence dfGPdfNN Run 10/10, Epoch 738/1000, Training Loss (NLML): -959.8813\n",
      "convergence dfGPdfNN Run 10/10, Epoch 739/1000, Training Loss (NLML): -959.8853\n",
      "convergence dfGPdfNN Run 10/10, Epoch 740/1000, Training Loss (NLML): -959.8889\n",
      "convergence dfGPdfNN Run 10/10, Epoch 741/1000, Training Loss (NLML): -959.8933\n",
      "convergence dfGPdfNN Run 10/10, Epoch 742/1000, Training Loss (NLML): -959.8964\n",
      "convergence dfGPdfNN Run 10/10, Epoch 743/1000, Training Loss (NLML): -959.9009\n",
      "convergence dfGPdfNN Run 10/10, Epoch 744/1000, Training Loss (NLML): -959.9032\n",
      "convergence dfGPdfNN Run 10/10, Epoch 745/1000, Training Loss (NLML): -959.9064\n",
      "convergence dfGPdfNN Run 10/10, Epoch 746/1000, Training Loss (NLML): -959.9119\n",
      "convergence dfGPdfNN Run 10/10, Epoch 747/1000, Training Loss (NLML): -959.9153\n",
      "convergence dfGPdfNN Run 10/10, Epoch 748/1000, Training Loss (NLML): -959.9193\n",
      "convergence dfGPdfNN Run 10/10, Epoch 749/1000, Training Loss (NLML): -959.9216\n",
      "convergence dfGPdfNN Run 10/10, Epoch 750/1000, Training Loss (NLML): -959.9270\n",
      "convergence dfGPdfNN Run 10/10, Epoch 751/1000, Training Loss (NLML): -959.9279\n",
      "convergence dfGPdfNN Run 10/10, Epoch 752/1000, Training Loss (NLML): -959.9335\n",
      "convergence dfGPdfNN Run 10/10, Epoch 753/1000, Training Loss (NLML): -959.9352\n",
      "convergence dfGPdfNN Run 10/10, Epoch 754/1000, Training Loss (NLML): -959.9397\n",
      "convergence dfGPdfNN Run 10/10, Epoch 755/1000, Training Loss (NLML): -959.9424\n",
      "convergence dfGPdfNN Run 10/10, Epoch 756/1000, Training Loss (NLML): -959.9478\n",
      "convergence dfGPdfNN Run 10/10, Epoch 757/1000, Training Loss (NLML): -959.9513\n",
      "convergence dfGPdfNN Run 10/10, Epoch 758/1000, Training Loss (NLML): -959.9540\n",
      "convergence dfGPdfNN Run 10/10, Epoch 759/1000, Training Loss (NLML): -959.9578\n",
      "convergence dfGPdfNN Run 10/10, Epoch 760/1000, Training Loss (NLML): -959.9607\n",
      "convergence dfGPdfNN Run 10/10, Epoch 761/1000, Training Loss (NLML): -959.9630\n",
      "convergence dfGPdfNN Run 10/10, Epoch 762/1000, Training Loss (NLML): -959.9673\n",
      "convergence dfGPdfNN Run 10/10, Epoch 763/1000, Training Loss (NLML): -959.9720\n",
      "convergence dfGPdfNN Run 10/10, Epoch 764/1000, Training Loss (NLML): -959.9735\n",
      "convergence dfGPdfNN Run 10/10, Epoch 765/1000, Training Loss (NLML): -959.9784\n",
      "convergence dfGPdfNN Run 10/10, Epoch 766/1000, Training Loss (NLML): -959.9816\n",
      "convergence dfGPdfNN Run 10/10, Epoch 767/1000, Training Loss (NLML): -959.9857\n",
      "convergence dfGPdfNN Run 10/10, Epoch 768/1000, Training Loss (NLML): -959.9897\n",
      "convergence dfGPdfNN Run 10/10, Epoch 769/1000, Training Loss (NLML): -959.9928\n",
      "convergence dfGPdfNN Run 10/10, Epoch 770/1000, Training Loss (NLML): -959.9971\n",
      "convergence dfGPdfNN Run 10/10, Epoch 771/1000, Training Loss (NLML): -959.9989\n",
      "convergence dfGPdfNN Run 10/10, Epoch 772/1000, Training Loss (NLML): -960.0022\n",
      "convergence dfGPdfNN Run 10/10, Epoch 773/1000, Training Loss (NLML): -960.0046\n",
      "convergence dfGPdfNN Run 10/10, Epoch 774/1000, Training Loss (NLML): -960.0090\n",
      "convergence dfGPdfNN Run 10/10, Epoch 775/1000, Training Loss (NLML): -960.0122\n",
      "convergence dfGPdfNN Run 10/10, Epoch 776/1000, Training Loss (NLML): -960.0167\n",
      "convergence dfGPdfNN Run 10/10, Epoch 777/1000, Training Loss (NLML): -960.0200\n",
      "convergence dfGPdfNN Run 10/10, Epoch 778/1000, Training Loss (NLML): -960.0228\n",
      "convergence dfGPdfNN Run 10/10, Epoch 779/1000, Training Loss (NLML): -960.0258\n",
      "convergence dfGPdfNN Run 10/10, Epoch 780/1000, Training Loss (NLML): -960.0297\n",
      "convergence dfGPdfNN Run 10/10, Epoch 781/1000, Training Loss (NLML): -960.0319\n",
      "convergence dfGPdfNN Run 10/10, Epoch 782/1000, Training Loss (NLML): -960.0360\n",
      "convergence dfGPdfNN Run 10/10, Epoch 783/1000, Training Loss (NLML): -960.0400\n",
      "convergence dfGPdfNN Run 10/10, Epoch 784/1000, Training Loss (NLML): -960.0421\n",
      "convergence dfGPdfNN Run 10/10, Epoch 785/1000, Training Loss (NLML): -960.0459\n",
      "convergence dfGPdfNN Run 10/10, Epoch 786/1000, Training Loss (NLML): -960.0494\n",
      "convergence dfGPdfNN Run 10/10, Epoch 787/1000, Training Loss (NLML): -960.0531\n",
      "convergence dfGPdfNN Run 10/10, Epoch 788/1000, Training Loss (NLML): -960.0575\n",
      "convergence dfGPdfNN Run 10/10, Epoch 789/1000, Training Loss (NLML): -960.0597\n",
      "convergence dfGPdfNN Run 10/10, Epoch 790/1000, Training Loss (NLML): -960.0618\n",
      "convergence dfGPdfNN Run 10/10, Epoch 791/1000, Training Loss (NLML): -960.0645\n",
      "convergence dfGPdfNN Run 10/10, Epoch 792/1000, Training Loss (NLML): -960.0682\n",
      "convergence dfGPdfNN Run 10/10, Epoch 793/1000, Training Loss (NLML): -960.0706\n",
      "convergence dfGPdfNN Run 10/10, Epoch 794/1000, Training Loss (NLML): -960.0736\n",
      "convergence dfGPdfNN Run 10/10, Epoch 795/1000, Training Loss (NLML): -960.0781\n",
      "convergence dfGPdfNN Run 10/10, Epoch 796/1000, Training Loss (NLML): -960.0814\n",
      "convergence dfGPdfNN Run 10/10, Epoch 797/1000, Training Loss (NLML): -960.0845\n",
      "convergence dfGPdfNN Run 10/10, Epoch 798/1000, Training Loss (NLML): -960.0886\n",
      "convergence dfGPdfNN Run 10/10, Epoch 799/1000, Training Loss (NLML): -960.0916\n",
      "convergence dfGPdfNN Run 10/10, Epoch 800/1000, Training Loss (NLML): -960.0958\n",
      "convergence dfGPdfNN Run 10/10, Epoch 801/1000, Training Loss (NLML): -960.0979\n",
      "convergence dfGPdfNN Run 10/10, Epoch 802/1000, Training Loss (NLML): -960.0992\n",
      "convergence dfGPdfNN Run 10/10, Epoch 803/1000, Training Loss (NLML): -960.1051\n",
      "convergence dfGPdfNN Run 10/10, Epoch 804/1000, Training Loss (NLML): -960.1086\n",
      "convergence dfGPdfNN Run 10/10, Epoch 805/1000, Training Loss (NLML): -960.1122\n",
      "convergence dfGPdfNN Run 10/10, Epoch 806/1000, Training Loss (NLML): -960.1135\n",
      "convergence dfGPdfNN Run 10/10, Epoch 807/1000, Training Loss (NLML): -960.1168\n",
      "convergence dfGPdfNN Run 10/10, Epoch 808/1000, Training Loss (NLML): -960.1208\n",
      "convergence dfGPdfNN Run 10/10, Epoch 809/1000, Training Loss (NLML): -960.1229\n",
      "convergence dfGPdfNN Run 10/10, Epoch 810/1000, Training Loss (NLML): -960.1256\n",
      "convergence dfGPdfNN Run 10/10, Epoch 811/1000, Training Loss (NLML): -960.1302\n",
      "convergence dfGPdfNN Run 10/10, Epoch 812/1000, Training Loss (NLML): -960.1326\n",
      "convergence dfGPdfNN Run 10/10, Epoch 813/1000, Training Loss (NLML): -960.1344\n",
      "convergence dfGPdfNN Run 10/10, Epoch 814/1000, Training Loss (NLML): -960.1376\n",
      "convergence dfGPdfNN Run 10/10, Epoch 815/1000, Training Loss (NLML): -960.1403\n",
      "convergence dfGPdfNN Run 10/10, Epoch 816/1000, Training Loss (NLML): -960.1434\n",
      "convergence dfGPdfNN Run 10/10, Epoch 817/1000, Training Loss (NLML): -960.1487\n",
      "convergence dfGPdfNN Run 10/10, Epoch 818/1000, Training Loss (NLML): -960.1515\n",
      "convergence dfGPdfNN Run 10/10, Epoch 819/1000, Training Loss (NLML): -960.1553\n",
      "convergence dfGPdfNN Run 10/10, Epoch 820/1000, Training Loss (NLML): -960.1559\n",
      "convergence dfGPdfNN Run 10/10, Epoch 821/1000, Training Loss (NLML): -960.1603\n",
      "convergence dfGPdfNN Run 10/10, Epoch 822/1000, Training Loss (NLML): -960.1625\n",
      "convergence dfGPdfNN Run 10/10, Epoch 823/1000, Training Loss (NLML): -960.1660\n",
      "convergence dfGPdfNN Run 10/10, Epoch 824/1000, Training Loss (NLML): -960.1700\n",
      "convergence dfGPdfNN Run 10/10, Epoch 825/1000, Training Loss (NLML): -960.1707\n",
      "convergence dfGPdfNN Run 10/10, Epoch 826/1000, Training Loss (NLML): -960.1741\n",
      "convergence dfGPdfNN Run 10/10, Epoch 827/1000, Training Loss (NLML): -960.1777\n",
      "convergence dfGPdfNN Run 10/10, Epoch 828/1000, Training Loss (NLML): -960.1813\n",
      "convergence dfGPdfNN Run 10/10, Epoch 829/1000, Training Loss (NLML): -960.1818\n",
      "convergence dfGPdfNN Run 10/10, Epoch 830/1000, Training Loss (NLML): -960.1876\n",
      "convergence dfGPdfNN Run 10/10, Epoch 831/1000, Training Loss (NLML): -960.1880\n",
      "convergence dfGPdfNN Run 10/10, Epoch 832/1000, Training Loss (NLML): -960.1920\n",
      "convergence dfGPdfNN Run 10/10, Epoch 833/1000, Training Loss (NLML): -960.1945\n",
      "convergence dfGPdfNN Run 10/10, Epoch 834/1000, Training Loss (NLML): -960.1976\n",
      "convergence dfGPdfNN Run 10/10, Epoch 835/1000, Training Loss (NLML): -960.2010\n",
      "convergence dfGPdfNN Run 10/10, Epoch 836/1000, Training Loss (NLML): -960.2053\n",
      "convergence dfGPdfNN Run 10/10, Epoch 837/1000, Training Loss (NLML): -960.2081\n",
      "convergence dfGPdfNN Run 10/10, Epoch 838/1000, Training Loss (NLML): -960.2096\n",
      "convergence dfGPdfNN Run 10/10, Epoch 839/1000, Training Loss (NLML): -960.2147\n",
      "convergence dfGPdfNN Run 10/10, Epoch 840/1000, Training Loss (NLML): -960.2174\n",
      "convergence dfGPdfNN Run 10/10, Epoch 841/1000, Training Loss (NLML): -960.2208\n",
      "convergence dfGPdfNN Run 10/10, Epoch 842/1000, Training Loss (NLML): -960.2217\n",
      "convergence dfGPdfNN Run 10/10, Epoch 843/1000, Training Loss (NLML): -960.2257\n",
      "convergence dfGPdfNN Run 10/10, Epoch 844/1000, Training Loss (NLML): -960.2284\n",
      "convergence dfGPdfNN Run 10/10, Epoch 845/1000, Training Loss (NLML): -960.2316\n",
      "convergence dfGPdfNN Run 10/10, Epoch 846/1000, Training Loss (NLML): -960.2350\n",
      "convergence dfGPdfNN Run 10/10, Epoch 847/1000, Training Loss (NLML): -960.2362\n",
      "convergence dfGPdfNN Run 10/10, Epoch 848/1000, Training Loss (NLML): -960.2385\n",
      "convergence dfGPdfNN Run 10/10, Epoch 849/1000, Training Loss (NLML): -960.2443\n",
      "convergence dfGPdfNN Run 10/10, Epoch 850/1000, Training Loss (NLML): -960.2443\n",
      "convergence dfGPdfNN Run 10/10, Epoch 851/1000, Training Loss (NLML): -960.2479\n",
      "convergence dfGPdfNN Run 10/10, Epoch 852/1000, Training Loss (NLML): -960.2513\n",
      "convergence dfGPdfNN Run 10/10, Epoch 853/1000, Training Loss (NLML): -960.2534\n",
      "convergence dfGPdfNN Run 10/10, Epoch 854/1000, Training Loss (NLML): -960.2554\n",
      "convergence dfGPdfNN Run 10/10, Epoch 855/1000, Training Loss (NLML): -960.2598\n",
      "convergence dfGPdfNN Run 10/10, Epoch 856/1000, Training Loss (NLML): -960.2635\n",
      "convergence dfGPdfNN Run 10/10, Epoch 857/1000, Training Loss (NLML): -960.2646\n",
      "convergence dfGPdfNN Run 10/10, Epoch 858/1000, Training Loss (NLML): -960.2673\n",
      "convergence dfGPdfNN Run 10/10, Epoch 859/1000, Training Loss (NLML): -960.2688\n",
      "convergence dfGPdfNN Run 10/10, Epoch 860/1000, Training Loss (NLML): -960.2748\n",
      "convergence dfGPdfNN Run 10/10, Epoch 861/1000, Training Loss (NLML): -960.2766\n",
      "convergence dfGPdfNN Run 10/10, Epoch 862/1000, Training Loss (NLML): -960.2792\n",
      "convergence dfGPdfNN Run 10/10, Epoch 863/1000, Training Loss (NLML): -960.2804\n",
      "convergence dfGPdfNN Run 10/10, Epoch 864/1000, Training Loss (NLML): -960.2847\n",
      "convergence dfGPdfNN Run 10/10, Epoch 865/1000, Training Loss (NLML): -960.2877\n",
      "convergence dfGPdfNN Run 10/10, Epoch 866/1000, Training Loss (NLML): -960.2905\n",
      "convergence dfGPdfNN Run 10/10, Epoch 867/1000, Training Loss (NLML): -960.2933\n",
      "convergence dfGPdfNN Run 10/10, Epoch 868/1000, Training Loss (NLML): -960.2975\n",
      "convergence dfGPdfNN Run 10/10, Epoch 869/1000, Training Loss (NLML): -960.2971\n",
      "convergence dfGPdfNN Run 10/10, Epoch 870/1000, Training Loss (NLML): -960.3018\n",
      "convergence dfGPdfNN Run 10/10, Epoch 871/1000, Training Loss (NLML): -960.3035\n",
      "convergence dfGPdfNN Run 10/10, Epoch 872/1000, Training Loss (NLML): -960.3057\n",
      "convergence dfGPdfNN Run 10/10, Epoch 873/1000, Training Loss (NLML): -960.3105\n",
      "convergence dfGPdfNN Run 10/10, Epoch 874/1000, Training Loss (NLML): -960.3123\n",
      "convergence dfGPdfNN Run 10/10, Epoch 875/1000, Training Loss (NLML): -960.3152\n",
      "convergence dfGPdfNN Run 10/10, Epoch 876/1000, Training Loss (NLML): -960.3180\n",
      "convergence dfGPdfNN Run 10/10, Epoch 877/1000, Training Loss (NLML): -960.3197\n",
      "convergence dfGPdfNN Run 10/10, Epoch 878/1000, Training Loss (NLML): -960.3221\n",
      "convergence dfGPdfNN Run 10/10, Epoch 879/1000, Training Loss (NLML): -960.3257\n",
      "convergence dfGPdfNN Run 10/10, Epoch 880/1000, Training Loss (NLML): -960.3292\n",
      "convergence dfGPdfNN Run 10/10, Epoch 881/1000, Training Loss (NLML): -960.3307\n",
      "convergence dfGPdfNN Run 10/10, Epoch 882/1000, Training Loss (NLML): -960.3345\n",
      "convergence dfGPdfNN Run 10/10, Epoch 883/1000, Training Loss (NLML): -960.3353\n",
      "convergence dfGPdfNN Run 10/10, Epoch 884/1000, Training Loss (NLML): -960.3391\n",
      "convergence dfGPdfNN Run 10/10, Epoch 885/1000, Training Loss (NLML): -960.3412\n",
      "convergence dfGPdfNN Run 10/10, Epoch 886/1000, Training Loss (NLML): -960.3440\n",
      "convergence dfGPdfNN Run 10/10, Epoch 887/1000, Training Loss (NLML): -960.3473\n",
      "convergence dfGPdfNN Run 10/10, Epoch 888/1000, Training Loss (NLML): -960.3494\n",
      "convergence dfGPdfNN Run 10/10, Epoch 889/1000, Training Loss (NLML): -960.3525\n",
      "convergence dfGPdfNN Run 10/10, Epoch 890/1000, Training Loss (NLML): -960.3547\n",
      "convergence dfGPdfNN Run 10/10, Epoch 891/1000, Training Loss (NLML): -960.3580\n",
      "convergence dfGPdfNN Run 10/10, Epoch 892/1000, Training Loss (NLML): -960.3600\n",
      "convergence dfGPdfNN Run 10/10, Epoch 893/1000, Training Loss (NLML): -960.3613\n",
      "convergence dfGPdfNN Run 10/10, Epoch 894/1000, Training Loss (NLML): -960.3651\n",
      "convergence dfGPdfNN Run 10/10, Epoch 895/1000, Training Loss (NLML): -960.3685\n",
      "convergence dfGPdfNN Run 10/10, Epoch 896/1000, Training Loss (NLML): -960.3695\n",
      "convergence dfGPdfNN Run 10/10, Epoch 897/1000, Training Loss (NLML): -960.3723\n",
      "convergence dfGPdfNN Run 10/10, Epoch 898/1000, Training Loss (NLML): -960.3750\n",
      "convergence dfGPdfNN Run 10/10, Epoch 899/1000, Training Loss (NLML): -960.3782\n",
      "convergence dfGPdfNN Run 10/10, Epoch 900/1000, Training Loss (NLML): -960.3820\n",
      "convergence dfGPdfNN Run 10/10, Epoch 901/1000, Training Loss (NLML): -960.3827\n",
      "convergence dfGPdfNN Run 10/10, Epoch 902/1000, Training Loss (NLML): -960.3845\n",
      "convergence dfGPdfNN Run 10/10, Epoch 903/1000, Training Loss (NLML): -960.3876\n",
      "convergence dfGPdfNN Run 10/10, Epoch 904/1000, Training Loss (NLML): -960.3892\n",
      "convergence dfGPdfNN Run 10/10, Epoch 905/1000, Training Loss (NLML): -960.3940\n",
      "convergence dfGPdfNN Run 10/10, Epoch 906/1000, Training Loss (NLML): -960.3942\n",
      "convergence dfGPdfNN Run 10/10, Epoch 907/1000, Training Loss (NLML): -960.4044\n",
      "convergence dfGPdfNN Run 10/10, Epoch 908/1000, Training Loss (NLML): -960.4014\n",
      "convergence dfGPdfNN Run 10/10, Epoch 909/1000, Training Loss (NLML): -960.4110\n",
      "convergence dfGPdfNN Run 10/10, Epoch 910/1000, Training Loss (NLML): -960.4121\n",
      "convergence dfGPdfNN Run 10/10, Epoch 911/1000, Training Loss (NLML): -960.4144\n",
      "convergence dfGPdfNN Run 10/10, Epoch 912/1000, Training Loss (NLML): -960.4187\n",
      "convergence dfGPdfNN Run 10/10, Epoch 913/1000, Training Loss (NLML): -960.4186\n",
      "convergence dfGPdfNN Run 10/10, Epoch 914/1000, Training Loss (NLML): -960.4192\n",
      "convergence dfGPdfNN Run 10/10, Epoch 915/1000, Training Loss (NLML): -960.4231\n",
      "convergence dfGPdfNN Run 10/10, Epoch 916/1000, Training Loss (NLML): -960.4265\n",
      "convergence dfGPdfNN Run 10/10, Epoch 917/1000, Training Loss (NLML): -960.4287\n",
      "convergence dfGPdfNN Run 10/10, Epoch 918/1000, Training Loss (NLML): -960.4299\n",
      "convergence dfGPdfNN Run 10/10, Epoch 919/1000, Training Loss (NLML): -960.4340\n",
      "convergence dfGPdfNN Run 10/10, Epoch 920/1000, Training Loss (NLML): -960.4355\n",
      "convergence dfGPdfNN Run 10/10, Epoch 921/1000, Training Loss (NLML): -960.4371\n",
      "convergence dfGPdfNN Run 10/10, Epoch 922/1000, Training Loss (NLML): -960.4294\n",
      "convergence dfGPdfNN Run 10/10, Epoch 923/1000, Training Loss (NLML): -960.4338\n",
      "convergence dfGPdfNN Run 10/10, Epoch 924/1000, Training Loss (NLML): -960.4344\n",
      "convergence dfGPdfNN Run 10/10, Epoch 925/1000, Training Loss (NLML): -960.4297\n",
      "convergence dfGPdfNN Run 10/10, Epoch 926/1000, Training Loss (NLML): -960.4323\n",
      "convergence dfGPdfNN Run 10/10, Epoch 927/1000, Training Loss (NLML): -960.4344\n",
      "convergence dfGPdfNN Run 10/10, Epoch 928/1000, Training Loss (NLML): -960.4354\n",
      "convergence dfGPdfNN Run 10/10, Epoch 929/1000, Training Loss (NLML): -960.4395\n",
      "convergence dfGPdfNN Run 10/10, Epoch 930/1000, Training Loss (NLML): -960.4418\n",
      "convergence dfGPdfNN Run 10/10, Epoch 931/1000, Training Loss (NLML): -960.4420\n",
      "convergence dfGPdfNN Run 10/10, Epoch 932/1000, Training Loss (NLML): -960.4474\n",
      "convergence dfGPdfNN Run 10/10, Epoch 933/1000, Training Loss (NLML): -960.4524\n",
      "convergence dfGPdfNN Run 10/10, Epoch 934/1000, Training Loss (NLML): -960.4564\n",
      "convergence dfGPdfNN Run 10/10, Epoch 935/1000, Training Loss (NLML): -960.4583\n",
      "convergence dfGPdfNN Run 10/10, Epoch 936/1000, Training Loss (NLML): -960.4567\n",
      "convergence dfGPdfNN Run 10/10, Epoch 937/1000, Training Loss (NLML): -960.4587\n",
      "convergence dfGPdfNN Run 10/10, Epoch 938/1000, Training Loss (NLML): -960.4524\n",
      "convergence dfGPdfNN Run 10/10, Epoch 939/1000, Training Loss (NLML): -960.4556\n",
      "convergence dfGPdfNN Run 10/10, Epoch 940/1000, Training Loss (NLML): -960.4580\n",
      "convergence dfGPdfNN Run 10/10, Epoch 941/1000, Training Loss (NLML): -960.4672\n",
      "convergence dfGPdfNN Run 10/10, Epoch 942/1000, Training Loss (NLML): -960.4725\n",
      "convergence dfGPdfNN Run 10/10, Epoch 943/1000, Training Loss (NLML): -960.4742\n",
      "convergence dfGPdfNN Run 10/10, Epoch 944/1000, Training Loss (NLML): -960.4672\n",
      "convergence dfGPdfNN Run 10/10, Epoch 945/1000, Training Loss (NLML): -960.4680\n",
      "convergence dfGPdfNN Run 10/10, Epoch 946/1000, Training Loss (NLML): -960.4686\n",
      "convergence dfGPdfNN Run 10/10, Epoch 947/1000, Training Loss (NLML): -960.4720\n",
      "convergence dfGPdfNN Run 10/10, Epoch 948/1000, Training Loss (NLML): -960.4768\n",
      "convergence dfGPdfNN Run 10/10, Epoch 949/1000, Training Loss (NLML): -960.4781\n",
      "convergence dfGPdfNN Run 10/10, Epoch 950/1000, Training Loss (NLML): -960.4819\n",
      "convergence dfGPdfNN Run 10/10, Epoch 951/1000, Training Loss (NLML): -960.4821\n",
      "convergence dfGPdfNN Run 10/10, Epoch 952/1000, Training Loss (NLML): -960.4868\n",
      "convergence dfGPdfNN Run 10/10, Epoch 953/1000, Training Loss (NLML): -960.4874\n",
      "convergence dfGPdfNN Run 10/10, Epoch 954/1000, Training Loss (NLML): -960.4916\n",
      "convergence dfGPdfNN Run 10/10, Epoch 955/1000, Training Loss (NLML): -960.4951\n",
      "convergence dfGPdfNN Run 10/10, Epoch 956/1000, Training Loss (NLML): -960.4939\n",
      "convergence dfGPdfNN Run 10/10, Epoch 957/1000, Training Loss (NLML): -960.5065\n",
      "convergence dfGPdfNN Run 10/10, Epoch 958/1000, Training Loss (NLML): -960.5063\n",
      "convergence dfGPdfNN Run 10/10, Epoch 959/1000, Training Loss (NLML): -960.5092\n",
      "convergence dfGPdfNN Run 10/10, Epoch 960/1000, Training Loss (NLML): -960.5121\n",
      "convergence dfGPdfNN Run 10/10, Epoch 961/1000, Training Loss (NLML): -960.5142\n",
      "convergence dfGPdfNN Run 10/10, Epoch 962/1000, Training Loss (NLML): -960.5173\n",
      "convergence dfGPdfNN Run 10/10, Epoch 963/1000, Training Loss (NLML): -960.5236\n",
      "convergence dfGPdfNN Run 10/10, Epoch 964/1000, Training Loss (NLML): -960.5220\n",
      "convergence dfGPdfNN Run 10/10, Epoch 965/1000, Training Loss (NLML): -960.5258\n",
      "convergence dfGPdfNN Run 10/10, Epoch 966/1000, Training Loss (NLML): -960.5286\n",
      "convergence dfGPdfNN Run 10/10, Epoch 967/1000, Training Loss (NLML): -960.5293\n",
      "convergence dfGPdfNN Run 10/10, Epoch 968/1000, Training Loss (NLML): -960.5316\n",
      "convergence dfGPdfNN Run 10/10, Epoch 969/1000, Training Loss (NLML): -960.5336\n",
      "convergence dfGPdfNN Run 10/10, Epoch 970/1000, Training Loss (NLML): -960.5350\n",
      "convergence dfGPdfNN Run 10/10, Epoch 971/1000, Training Loss (NLML): -960.5475\n",
      "convergence dfGPdfNN Run 10/10, Epoch 972/1000, Training Loss (NLML): -960.5505\n",
      "convergence dfGPdfNN Run 10/10, Epoch 973/1000, Training Loss (NLML): -960.5436\n",
      "convergence dfGPdfNN Run 10/10, Epoch 974/1000, Training Loss (NLML): -960.5444\n",
      "convergence dfGPdfNN Run 10/10, Epoch 975/1000, Training Loss (NLML): -960.5455\n",
      "convergence dfGPdfNN Run 10/10, Epoch 976/1000, Training Loss (NLML): -960.5496\n",
      "convergence dfGPdfNN Run 10/10, Epoch 977/1000, Training Loss (NLML): -960.5526\n",
      "convergence dfGPdfNN Run 10/10, Epoch 978/1000, Training Loss (NLML): -960.5549\n",
      "convergence dfGPdfNN Run 10/10, Epoch 979/1000, Training Loss (NLML): -960.5581\n",
      "convergence dfGPdfNN Run 10/10, Epoch 980/1000, Training Loss (NLML): -960.5569\n",
      "convergence dfGPdfNN Run 10/10, Epoch 981/1000, Training Loss (NLML): -960.5619\n",
      "convergence dfGPdfNN Run 10/10, Epoch 982/1000, Training Loss (NLML): -960.5641\n",
      "convergence dfGPdfNN Run 10/10, Epoch 983/1000, Training Loss (NLML): -960.5652\n",
      "convergence dfGPdfNN Run 10/10, Epoch 984/1000, Training Loss (NLML): -960.5663\n",
      "convergence dfGPdfNN Run 10/10, Epoch 985/1000, Training Loss (NLML): -960.5702\n",
      "convergence dfGPdfNN Run 10/10, Epoch 986/1000, Training Loss (NLML): -960.5725\n",
      "convergence dfGPdfNN Run 10/10, Epoch 987/1000, Training Loss (NLML): -960.5741\n",
      "convergence dfGPdfNN Run 10/10, Epoch 988/1000, Training Loss (NLML): -960.5756\n",
      "convergence dfGPdfNN Run 10/10, Epoch 989/1000, Training Loss (NLML): -960.5778\n",
      "convergence dfGPdfNN Run 10/10, Epoch 990/1000, Training Loss (NLML): -960.5824\n",
      "convergence dfGPdfNN Run 10/10, Epoch 991/1000, Training Loss (NLML): -960.5828\n",
      "convergence dfGPdfNN Run 10/10, Epoch 992/1000, Training Loss (NLML): -960.5873\n",
      "convergence dfGPdfNN Run 10/10, Epoch 993/1000, Training Loss (NLML): -960.5847\n",
      "convergence dfGPdfNN Run 10/10, Epoch 994/1000, Training Loss (NLML): -960.5886\n",
      "convergence dfGPdfNN Run 10/10, Epoch 995/1000, Training Loss (NLML): -960.5901\n",
      "convergence dfGPdfNN Run 10/10, Epoch 996/1000, Training Loss (NLML): -960.5941\n",
      "convergence dfGPdfNN Run 10/10, Epoch 997/1000, Training Loss (NLML): -960.5955\n",
      "convergence dfGPdfNN Run 10/10, Epoch 998/1000, Training Loss (NLML): -960.5995\n",
      "convergence dfGPdfNN Run 10/10, Epoch 999/1000, Training Loss (NLML): -960.6000\n",
      "convergence dfGPdfNN Run 10/10, Epoch 1000/1000, Training Loss (NLML): -960.6033\n",
      "\n",
      "Results saved to results/dfGPdfNN/convergence_dfGPdfNN_metrics_per_run.csv\n",
      "\n",
      "Mean & Std saved to results/dfGPdfNN/convergence_dfGPdfNN_metrics_summary.csv\n"
     ]
    }
   ],
   "source": [
    "from GP_models import GP_predict\n",
    "from NN_models import dfNN_for_vmap\n",
    "from simulate import simulate_convergence, simulate_branching, simulate_ridge, simulate_merge, simulate_deflection\n",
    "from metrics import compute_RMSE, compute_MAE, compute_NLL, compute_NLL_full\n",
    "from utils import set_seed\n",
    "\n",
    "# Global file for training configs\n",
    "from configs import PATIENCE, GP_MAX_NUM_EPOCHS, NUM_RUNS, GP_LEARNING_RATE, WEIGHT_DECAY, N_SIDE, DFGPDFNN_RESULTS_DIR, SIGMA_F_RANGE, L_RANGE\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "### TIMING ###\n",
    "import time\n",
    "start_time = time.time()  # Start timing after imports\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu' # for now\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "# stick to covention to keep df lower case\n",
    "model_name = \"dfGPdfNN\"\n",
    "\n",
    "#########################\n",
    "### x_train & y_train ###\n",
    "#########################\n",
    "\n",
    "# Import all simulation functions\n",
    "from simulate import (\n",
    "    simulate_convergence,\n",
    "    simulate_branching,\n",
    "    simulate_merge,\n",
    "    simulate_deflection,\n",
    "    simulate_ridge,\n",
    ")\n",
    "\n",
    "# Define simulations as a dictionary with names as keys to function objects\n",
    "simulations = {\n",
    "    \"convergence\": simulate_convergence,\n",
    "    \"branching\": simulate_branching,\n",
    "    \"merge\": simulate_merge,\n",
    "    \"deflection\": simulate_deflection,\n",
    "    \"ridge\": simulate_ridge,\n",
    "}\n",
    "\n",
    "# Load training inputs\n",
    "x_train = torch.load(\"data/sim_data/x_train_lines_discretised_0to1.pt\", weights_only = False).float()\n",
    "\n",
    "# Storage dictionaries\n",
    "y_train_dict = {}\n",
    "\n",
    "# Make y_train_dict: Iterate over all simulation functions\n",
    "for sim_name, sim_func in simulations.items():\n",
    "\n",
    "    # Generate training observations\n",
    "    y_train = sim_func(x_train)\n",
    "    y_train_dict[sim_name] = y_train  # Store training outputs\n",
    "\n",
    "    # Print details\n",
    "    print(f\"=== {sim_name.upper()} ===\")\n",
    "    print(f\"Training inputs shape: {x_train.shape}\")\n",
    "    print(f\"Training observations shape: {y_train.shape}\")\n",
    "    print(f\"Training inputs dtype: {x_train.dtype}\")\n",
    "    print()\n",
    "\n",
    "#######################\n",
    "### x_test & y_test ###\n",
    "#######################\n",
    "\n",
    "print(\"=== Generating test data ===\")\n",
    "\n",
    "# Choose discretisation that is good for simulations and also for quiver plotting\n",
    "N_SIDE = N_SIDE\n",
    "\n",
    "side_array = torch.linspace(start = 0.0, end = 1.0, steps = N_SIDE)\n",
    "XX, YY = torch.meshgrid(side_array, side_array, indexing = \"xy\")\n",
    "x_test_grid = torch.cat([XX.unsqueeze(-1), YY.unsqueeze(-1)], dim = -1)\n",
    "# long format\n",
    "x_test = x_test_grid.reshape(-1, 2)\n",
    "\n",
    "# Storage dictionaries\n",
    "y_test_dict = {}\n",
    "\n",
    "# Make y_test_dict: Iterate over all simulation functions\n",
    "for sim_name, sim_func in simulations.items():\n",
    "\n",
    "    # Generate test observations\n",
    "    y_test = sim_func(x_test)\n",
    "    y_test_dict[sim_name] = y_test  # Store test outputs\n",
    "\n",
    "    # Print details\n",
    "    print(f\"=== {sim_name.upper()} ===\")\n",
    "    print(f\"Test inputs shape: {x_test.shape}\")\n",
    "    print(f\"Test observations shape: {y_test.shape}\")\n",
    "    print(f\"Test inputs dtype: {x_test.dtype}\")\n",
    "    print()\n",
    "\n",
    "    # visualise_v_quiver(y_test, x_test, title_string = name)\n",
    "\n",
    "#####################\n",
    "### Training loop ###\n",
    "#####################\n",
    "\n",
    "# Early stopping parameters\n",
    "PATIENCE = PATIENCE\n",
    "MAX_NUM_EPOCHS = GP_MAX_NUM_EPOCHS\n",
    "\n",
    "# Number of training runs for mean and std of metrics\n",
    "NUM_RUNS = NUM_RUNS\n",
    "LEARNING_RATE = GP_LEARNING_RATE\n",
    "WEIGHT_DECAY = WEIGHT_DECAY\n",
    "\n",
    "# Pass in all the training data\n",
    "# BATCH_SIZE = BATCH_SIZE\n",
    "\n",
    "# Ensure the results folder exists\n",
    "RESULTS_DIR = DFGPDFNN_RESULTS_DIR\n",
    "os.makedirs(RESULTS_DIR, exist_ok = True)\n",
    "\n",
    "### LOOP OVER SIMULATIONS ###\n",
    "for sim_name, sim_func in simulations.items():\n",
    "    print(f\"\\nTraining for {sim_name.upper()}...\")\n",
    "\n",
    "    # Store metrics for the current simulation\n",
    "    simulation_results = []\n",
    "\n",
    "    # x_train is the same, select y_train\n",
    "    x_train = x_train.to(device)\n",
    "\n",
    "    y_train = y_train_dict[sim_name].to(device)\n",
    "    # select the correct y_test (PREVIOUS ERROR)\n",
    "    y_test = y_test_dict[sim_name].to(device)\n",
    "\n",
    "    ### LOOP OVER RUNS ###\n",
    "    for run in range(NUM_RUNS):\n",
    "        print(f\"\\n--- Training Run {run + 1}/{NUM_RUNS} ---\")\n",
    "\n",
    "        # Sample from uniform distributions to initialise hyperparameters\n",
    "        sigma_n = torch.tensor([0.05], requires_grad = False).to(device) # no optimisation for noise, no sampling\n",
    "        # nn.Parameter avoids the leaf problem\n",
    "        sigma_f = nn.Parameter(torch.empty(1, device = device).uniform_( * SIGMA_F_RANGE)) # Trainable\n",
    "        l = nn.Parameter(torch.empty(2, device = device).uniform_( * L_RANGE)) # Trainable\n",
    "        \n",
    "        # We do not need to \"initialse\" the GP model but the mean model\n",
    "        # Initialise fresh model\n",
    "        dfNN_mean_model = dfNN_for_vmap()\n",
    "        dfNN_mean_model.to(device)\n",
    "        # We don't need a criterion either\n",
    "\n",
    "        # Define optimizer (e.g., AdamW)\n",
    "        optimizer = optim.AdamW(list(dfNN_mean_model.parameters()) + [sigma_f, l], lr = LEARNING_RATE, weight_decay = WEIGHT_DECAY)\n",
    "\n",
    "        # Initialise tensors to store losses over epochs (for convergence plot)\n",
    "        epoch_train_NLML_losses = torch.zeros(MAX_NUM_EPOCHS)\n",
    "        epoch_train_RMSE_losses = torch.zeros(MAX_NUM_EPOCHS)\n",
    "        epoch_test_RMSE_losses = torch.zeros(MAX_NUM_EPOCHS)\n",
    "\n",
    "        epoch_sigma_f = torch.zeros(MAX_NUM_EPOCHS)\n",
    "        epoch_l1 = torch.zeros(MAX_NUM_EPOCHS)\n",
    "        epoch_l2 = torch.zeros(MAX_NUM_EPOCHS)\n",
    "\n",
    "        # Early stopping variables\n",
    "        best_loss = float('inf')\n",
    "        epochs_no_improve = 0\n",
    "\n",
    "        ### LOOP OVER EPOCHS ###\n",
    "        print(\"\\nStart Training\")\n",
    "        for epoch in range(MAX_NUM_EPOCHS):\n",
    "            \n",
    "            dfNN_mean_model.train()\n",
    "\n",
    "            # No batching - full epoch pass in one\n",
    "            if run == 0:\n",
    "                mean_pred_train, _, lml_train = GP_predict(\n",
    "                        x_train,\n",
    "                        y_train,\n",
    "                        x_train, # have predictions for training data again\n",
    "                        [sigma_n, sigma_f, l], # initial hyperparameters\n",
    "                        dfNN_mean_model, # vmap?\n",
    "                        divergence_free_bool = True)\n",
    "                \n",
    "                loss = - lml_train\n",
    "                # Backpropagation\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Compute test loss for loss convergence plot\n",
    "                mean_pred_test, _, _ = GP_predict(\n",
    "                        x_train,\n",
    "                        y_train,\n",
    "                        x_test.to(device), # have predictions for training data again\n",
    "                        [sigma_n, sigma_f, l], # initial hyperparameters\n",
    "                        dfNN_mean_model,\n",
    "                        divergence_free_bool = True)\n",
    "                \n",
    "                train_RMSE = compute_RMSE(y_train, mean_pred_train)\n",
    "                test_RMSE = compute_RMSE(y_test, mean_pred_test)\n",
    "\n",
    "                epoch_train_NLML_losses[epoch] = - lml_train\n",
    "                epoch_train_RMSE_losses[epoch] = train_RMSE\n",
    "                # epoch_test_NLML_losses[epoch] =  # train NLML\n",
    "                epoch_test_RMSE_losses[epoch] = test_RMSE\n",
    "\n",
    "                epoch_sigma_f[epoch] = sigma_f\n",
    "                epoch_l1[epoch] = l[0]\n",
    "                epoch_l2[epoch] = l[1]\n",
    "\n",
    "                print(f\"{sim_name} {model_name} Run {run + 1}/{NUM_RUNS}, Epoch {epoch + 1}/{MAX_NUM_EPOCHS}, Training Loss (NLML): {loss:.4f}, (RMSE): {train_RMSE:.4f}\")\n",
    "            \n",
    "            else:\n",
    "                # Save compute after run 1\n",
    "                _, _, lml_train = GP_predict(\n",
    "                        x_train,\n",
    "                        y_train,\n",
    "                        x_train[0:2], # have predictions for training data again\n",
    "                        [sigma_n, sigma_f, l], # initial hyperparameters\n",
    "                        dfNN_mean_model,\n",
    "                        divergence_free_bool = True)\n",
    "                \n",
    "                loss = - lml_train\n",
    "                # Backpropagation\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                print(f\"{sim_name} {model_name} Run {run + 1}/{NUM_RUNS}, Epoch {epoch + 1}/{MAX_NUM_EPOCHS}, Training Loss (NLML): {loss:.4f}\")\n",
    "\n",
    "            # Early stopping check\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                epochs_no_improve = 0  # Reset counter\n",
    "                # best_model_state = dfGP_model.state_dict()  # Save best model\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "\n",
    "            if epochs_no_improve >= PATIENCE:\n",
    "                print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
    "                break\n",
    "\n",
    "        ################\n",
    "        ### EVALUATE ###\n",
    "        ################\n",
    "\n",
    "        dfNN_mean_model.eval()\n",
    "\n",
    "        # Now HPs should be tuned\n",
    "        # Evaluate the trained model after all epochs are finished/early stopping\n",
    "\n",
    "        mean_pred_test, covar_pred_test, _ = GP_predict(\n",
    "                     x_train,\n",
    "                     y_train,\n",
    "                     x_test.to(device),\n",
    "                     [sigma_n, sigma_f, l], # optimal hypers\n",
    "                     dfNN_mean_model,\n",
    "                     divergence_free_bool = True)\n",
    "\n",
    "        # Only save things for one run\n",
    "        if run == 0:\n",
    "            #(1) Save predictions from first run so we can visualise them later\n",
    "            torch.save(mean_pred_test, f\"{RESULTS_DIR}/{sim_name}_{model_name}_test_mean_predictions.pt\")\n",
    "            torch.save(covar_pred_test, f\"{RESULTS_DIR}/{sim_name}_{model_name}_test_covar_predictions.pt\")\n",
    "\n",
    "            #(2) Save best hyperparameters for run 1\n",
    "            # Stack tensors into a single tensor\n",
    "            best_hypers_tensor = torch.cat([\n",
    "                sigma_n.reshape(-1),  # Ensure 1D shape\n",
    "                sigma_f.reshape(-1),\n",
    "                l.reshape(-1)\n",
    "            ])\n",
    "\n",
    "            # Save the tensor\n",
    "            torch.save(best_hypers_tensor, f\"{RESULTS_DIR}/{sim_name}_{model_name}_best_hypers.pt\")\n",
    "\n",
    "            #(3) Save loss over epochs\n",
    "            df_losses = pd.DataFrame({\n",
    "                'Epoch': list(range(epoch_train_NLML_losses.shape[0])), # pythonic\n",
    "                'Train Loss NLML': epoch_train_NLML_losses.tolist(),\n",
    "                'Train Loss RMSE': epoch_train_RMSE_losses.tolist(),\n",
    "                'Test Loss RMSE': epoch_test_RMSE_losses.tolist(),\n",
    "                'Sigma_f': epoch_sigma_f.tolist(),\n",
    "                'l1': epoch_l1.tolist(),\n",
    "                'l2': epoch_l2.tolist()\n",
    "                })\n",
    "            \n",
    "            df_losses.to_csv(f\"{RESULTS_DIR}/{sim_name}_{model_name}_losses_over_epochs.csv\", index = False)\n",
    "\n",
    "        mean_pred_train, covar_pred_train, _ = GP_predict(\n",
    "                     x_train,\n",
    "                     y_train,\n",
    "                     x_train,\n",
    "                     [sigma_n, sigma_f, l], # optimal hypers\n",
    "                     dfNN_mean_model,\n",
    "                     divergence_free_bool = True)\n",
    "\n",
    "        ### Divergence\n",
    "        # Need wrapper function for functional divergence\n",
    "        def apply_GP(input):\n",
    "            mean, _, _ = GP_predict(\n",
    "                x_train,\n",
    "                y_train,\n",
    "                input,\n",
    "                [sigma_n, sigma_f, l], # optimal hypers\n",
    "                dfNN_mean_model,\n",
    "                divergence_free_bool = True)\n",
    "            return mean\n",
    "        \n",
    "\n",
    "        # functional div test\n",
    "        jac_autograd_test = torch.autograd.functional.jacobian(apply_GP, \n",
    "                                        x_test.to(device))\n",
    "        jac_autograd_test = torch.einsum(\"bobi -> boi\", jac_autograd_test) # batch out batch in\n",
    "        dfGPdfNN_test_div = torch.diagonal(jac_autograd_test, dim1 = -2, dim2 = -1).sum().item()\n",
    "\n",
    "        # functional div train\n",
    "        jac_autograd_train = torch.autograd.functional.jacobian(apply_GP, \n",
    "                                        x_train.to(device))\n",
    "        jac_autograd_train = torch.einsum(\"bobi -> boi\", jac_autograd_train) # batch out batch in\n",
    "        dfGPdfNN_train_div = torch.diagonal(jac_autograd_train, dim1 = -2, dim2 = -1).sum().item()\n",
    "\n",
    "        # Compute metrics (convert tensors to float) for every run's tuned model\n",
    "        dfGPdfNN_train_RMSE = compute_RMSE(y_train, mean_pred_train).item()\n",
    "        dfGPdfNN_train_MAE = compute_MAE(y_train, mean_pred_train).item()\n",
    "        dfGPdfNN_train_NLL = compute_NLL_full(y_train, mean_pred_train, covar_pred_train).item()\n",
    "\n",
    "        dfGPdfNN_test_RMSE = compute_RMSE(y_test, mean_pred_test).item()\n",
    "        dfGPdfNN_test_MAE = compute_MAE(y_test, mean_pred_test).item()\n",
    "        # full has cuased issues\n",
    "        dfGPdfNN_test_NLL = compute_NLL(y_test, mean_pred_test, covar_pred_test).item()\n",
    "\n",
    "        simulation_results.append([\n",
    "            run + 1,\n",
    "            dfGPdfNN_train_RMSE, dfGPdfNN_train_MAE, dfGPdfNN_train_NLL, dfGPdfNN_train_div,\n",
    "            dfGPdfNN_test_RMSE, dfGPdfNN_test_MAE, dfGPdfNN_test_NLL, dfGPdfNN_test_div\n",
    "        ])\n",
    "\n",
    "    ### FINISH LOOP OVER RUNS ###\n",
    "    # Convert results to a Pandas DataFrame\n",
    "    df = pd.DataFrame(\n",
    "        simulation_results, \n",
    "        columns = [\"Run\", \n",
    "                   \"Train RMSE\", \"Train MAE\", \"Train NLL\", \"Train Divergence\",\n",
    "                   \"Test RMSE\", \"Test MAE\", \"Test NLL\", \"Test Divergence\"])\n",
    "\n",
    "    # Compute mean and standard deviation for each metric\n",
    "    mean_std_df = df.iloc[:, 1:].agg([\"mean\", \"std\"])  # Exclude \"Run\" column\n",
    "\n",
    "    # Save results to CSV\n",
    "    results_file = os.path.join(RESULTS_DIR, f\"{sim_name}_{model_name}_metrics_per_run.csv\")\n",
    "    df.to_csv(results_file, index = False)\n",
    "    print(f\"\\nResults saved to {results_file}\")\n",
    "\n",
    "    # Save mean and standard deviation to CSV\n",
    "    mean_std_file = os.path.join(RESULTS_DIR, f\"{sim_name}_{model_name}_metrics_summary.csv\")\n",
    "    mean_std_df.to_csv(mean_std_file)\n",
    "    print(f\"\\nMean & Std saved to {mean_std_file}\")\n",
    "    # Only train for one simulation for now\n",
    "\n",
    "### End timing ###\n",
    "end_time = time.time()  # End timing\n",
    "elapsed_time = end_time - start_time  # Compute elapsed time\n",
    "\n",
    "print(f\"Elapsed wall time: {elapsed_time:.4f} seconds\")\n",
    "\n",
    "# Define full path for the file\n",
    "wall_time_path = os.path.join(RESULTS_DIR, \"wall_time.txt\")\n",
    "\n",
    "# Save to the correct folder\n",
    "with open(wall_time_path, \"w\") as f:\n",
    "    f.write(f\"Elapsed wall time: {elapsed_time:.4f} seconds\\n\")\n",
    "\n",
    "print(f\"Wall time saved to {wall_time_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 4090'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wall_time_path = os.path.join(RESULTS_DIR, model_name + \"_run_\" \"wall_time.txt\")\n",
    "gpu_name = torch.cuda.get_device_name(0)\n",
    "gpu_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  2.6897, -0.4457,  0.8486],\n",
       "        [-2.6897,  0.0000, -1.0157, -0.8175],\n",
       "        [ 0.4457,  1.0157,  0.0000, -1.0308],\n",
       "        [-0.8486,  0.8175,  1.0308,  0.0000]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "A = torch.randn(4, 4)\n",
    "A - A.T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
