{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c19fe915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpytorch\n",
    "import torch\n",
    "from GPyTorch_models import DivergenceFreeSEKernel\n",
    "from NN_models import dfNN\n",
    "\n",
    "class dfNGP(gpytorch.models.ExactGP):\n",
    "    # dfGP model with constant mean\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        # Inherit from ExactGP with 3 inputs + self = 4 inputs\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "\n",
    "        self.mean_module = dfNN()\n",
    "        self.covar_module = DivergenceFreeSEKernel()\n",
    "        \n",
    "        # initialize hyperparameters by sampling from a uniform distribution over predefined ranges\n",
    "        self.likelihood.noise = torch.zeros(1, device = device) + 0.005\n",
    "        self.covar_module.outputscale = torch.ones(1, device = device) * 0.001\n",
    "        self.covar_module.lengthscale = torch.ones(2, device = device)\n",
    "\n",
    "        self.likelihood.noise_covar.register_constraint(\n",
    "            \"raw_noise\", gpytorch.constraints.GreaterThan(1e-4)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Make 2D for dfNN\n",
    "        x_for_dfNN = x.reshape(2, -1).T\n",
    "        mean_x_from_dfNN = self.mean_module(x_for_dfNN)\n",
    "        # HACK: Reshape to interleaved format. This is counterintuitive but necessary\n",
    "        mean_x = mean_x_from_dfNN.reshape(-1)\n",
    "\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f85ee702",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MLPMean(gpytorch.means.Mean):\n",
    "    def __init__(self, dim):\n",
    "        super(MLPMean, self).__init__()\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, 32), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        m = self.mlp(x)\n",
    "        print(\"MLPMean output shape:\", m.shape)\n",
    "        return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb0b78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpytorch\n",
    "import torch\n",
    "from GPyTorch_models import DivergenceFreeSEKernel\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLPMean(gpytorch.means.Mean):\n",
    "    def __init__(self, dim):\n",
    "        super(MLPMean, self).__init__()\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, 32), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        m = self.mlp(x)\n",
    "        print(\"MLPMean output shape:\", m.shape)\n",
    "        return m\n",
    "\n",
    "class dfRBFKernel(gpytorch.kernel.Kernel):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(dfRBFKernel, self).__init__(**kwargs)\n",
    "\n",
    "        # Register the outputscale / variance (sigma_f) as a learnable parameter\n",
    "        self.register_parameter(name = \"raw_outputscale\", \n",
    "                                parameter = torch.nn.Parameter(torch.tensor(1.0)))\n",
    "        \n",
    "        self.register_parameter(name = \"raw_lengthscale\",\n",
    "                                parameter = torch.nn.Parameter(torch.tensor([1.0, 1.0])))\n",
    "\n",
    "        # Register transform for positivity (softplus)\n",
    "        self.register_constraint(\"raw_outputscale\", gpytorch.constraints.Positive())\n",
    "\n",
    "        self.register_constraint(\"raw_lengthscale\", gpytorch.constraints.Positive())\n",
    "\n",
    "    @property\n",
    "    def outputscale(self):\n",
    "        return self.raw_outputscale_constraint.transform(self.raw_outputscale)\n",
    "    \n",
    "    @property\n",
    "    def lengthscale(self):\n",
    "        return self.raw_lengthscale_constraint.transform(self.raw_lengthscale)\n",
    "\n",
    "    @outputscale.setter\n",
    "    def outputscale(self, value):\n",
    "        self.initialize(raw_outputscale = self.raw_outputscale_constraint.inverse_transform(value))\n",
    "\n",
    "    @lengthscale.setter\n",
    "    def lengthscale(self, value):\n",
    "        self.initialize(raw_lengthscale = self.raw_lengthscale_constraint.inverse_transform(value))\n",
    "\n",
    "    def forward(self, x1, x2, diag = False, **params):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x1: torch.Size([2N, 1]) flattened, second explicit dim is automatic\n",
    "            x2: torch.Size([2M, 1])\n",
    "        Returns:\n",
    "            K: torch.Size([2N, 2M])\n",
    "        \"\"\"\n",
    "        # Transform long/flat format into 2D\n",
    "        mid_x1 = x1.shape[0] // 2\n",
    "        mid_x2 = x2.shape[0] // 2 \n",
    "\n",
    "        # torch.Size([N, 2])\n",
    "        x1 = torch.cat((x1[:mid_x1], x1[mid_x1:]), dim = 1).to(x1.device)\n",
    "        # torch.Size([M, 2])\n",
    "        x2 = torch.cat((x2[:mid_x2], x2[mid_x2:]), dim = 1).to(x2.device)\n",
    "\n",
    "        l = self.lengthscale.squeeze().to(x1.device)  # Shape (2,)\n",
    "\n",
    "        lx1, lx2 = l[0].to(x1.device), l[1].to(x1.device)\n",
    "\n",
    "        sigma_f = self.outputscale\n",
    "\n",
    "        # Broadcast pairwise differences: shape [N, M, 2]\n",
    "        diff = (x1[:, None, :] - x2[None, :, :]).to(x1.device)\n",
    "\n",
    "        ### 2x2 block components ###\n",
    "        upper_left = (1 - diff[:, :, 1].square() / lx2.square()) / lx2.square()\n",
    "        lower_right = (1 - diff[:, :, 0].square() / lx1.square()) / lx1.square()\n",
    "        upper_right = (diff[:, :, 0] * diff[:, :, 1]) / (lx1.square() * lx2.square())\n",
    "        lower_left = upper_right\n",
    "\n",
    "        # Block matrix assembly\n",
    "        top = torch.cat((upper_left, upper_right), dim = 1)\n",
    "        bottom = torch.cat((lower_left, lower_right), dim = 1)\n",
    "        blocks = torch.cat((top, bottom), dim = 0)\n",
    "\n",
    "        # RBF/SE envelope (elementwise)\n",
    "        exp_term = torch.exp(-0.5 * (diff.square() / l.square()).sum(dim = -1))\n",
    "        # .tile(2, 2) forms (N, M) -> (2N, 2M) for the 2D vector field\n",
    "        K = sigma_f.square() * blocks * exp_term.tile(2, 2)\n",
    "\n",
    "        # Add this for Quantile Coverage Error (QCE) calculation\n",
    "        if diag:\n",
    "        # Return only the diagonal as a 1D tensor\n",
    "            return K.diag()\n",
    "\n",
    "        return K\n",
    "\n",
    "class dfNGP(gpytorch.models.ExactGP):\n",
    "    # dfGP model with constant mean\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        # Inherit from ExactGP with 3 inputs + self = 4 inputs\n",
    "        # train_x is in flat block shape: [x1_1, x1_2, ..., x1_n, x2_1, x2_2, ..., x2_n]\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "\n",
    "        self.mean_module = MLPMean(dim = 2)\n",
    "        self.base_module = DivergenceFreeSEKernel()\n",
    "        \n",
    "        # initialize hyperparameters by sampling from a uniform distribution over predefined ranges\n",
    "        self.likelihood.noise = torch.zeros(1, device = device) + 0.0001\n",
    "        self.covar_module.outputscale = torch.ones(1, device = device) * 0.001\n",
    "        self.covar_module.lengthscale = torch.ones(2, device = device)\n",
    "\n",
    "        self.likelihood.noise_covar.register_constraint(\n",
    "            \"raw_noise\", gpytorch.constraints.GreaterThan(1e-4)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Make 2D for dfNN\n",
    "        x_for_dfNN = x.reshape(2, -1).T\n",
    "        mean_x = self.mean_module(x_for_dfNN).T.reshape(-1)\n",
    "        # print(\"x:\", x_for_dfNN.shape)\n",
    "        # print(\"mean_x shape:\", mean_x.shape)\n",
    "\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a5c0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simulate import (simulate_detailed_curve,)\n",
    "simulations = {\"curve\": simulate_detailed_curve}\n",
    "x_train = torch.load(\"data/sim_data/x_train_lines_discretised_0to1.pt\", weights_only = False).float()\n",
    "from utils import make_grid\n",
    "N_SIDE = 20\n",
    "_, x_test = make_grid(N_SIDE)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "y_train = simulate_detailed_curve(x_train.cpu()).to(device)\n",
    "y_test = simulate_detailed_curve(x_test.cpu()).to(device)\n",
    "x_test = x_test.to(device)\n",
    "x_train = x_train.to(device)\n",
    "\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood().to(device)\n",
    "model = dfNGP(\n",
    "            x_train.T.reshape(-1),\n",
    "            y_train.T.reshape(-1), \n",
    "            likelihood,\n",
    "            ).to(device)\n",
    "model.train()\n",
    "likelihood.train()\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "optimizer = torch.optim.AdamW([\n",
    "            {\"params\": model.mean_module.parameters(), \"weight_decay\": 0.0001, \"lr\": (0.1)},\n",
    "            # {\"params\": list(model.covar_module.parameters()) + list(model.likelihood.parameters()), \"weight_decay\": # WEIGHT_DECAY, \"lr\": MODEL_LEARNING_RATE},\n",
    "            ])\n",
    "\n",
    "for i in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    train_pred_dist = model(x_train.T.reshape(-1).to(device))\n",
    "    print(\"HERE\")\n",
    "    # Train on noisy or true targets?\n",
    "    loss = - mll(train_pred_dist, y_train.T.reshape(-1).to(device))  # negative marginal log likelihood\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "\n",
    "    print(\"HERE 2\")\n",
    "    train_pred_dist = model(x_train.T.reshape(-1).to(device))\n",
    "    print(\"HERE 3\")\n",
    "    test_pred_dist = model(x_test.T.reshape(-1).to(device))\n",
    "    print(\"HERE 4\")\n",
    "\n",
    "    train_RMSE = torch.sqrt(gpytorch.metrics.mean_squared_error(train_pred_dist, y_train.T.reshape(-1).to(device)))\n",
    "    test_RMSE = torch.sqrt(gpytorch.metrics.mean_squared_error(test_pred_dist, y_test.T.reshape(-1).to(device)))\n",
    "    test_mean_module_RMSE = (model.mean_module(x_test.to(device)).detach() - y_test.to(device)).norm(dim = 1).mean()\n",
    "\n",
    "    print(f\"Epoch {i + 1}, Training Loss (NLML): {loss:.4f}, (RMSE): {train_RMSE:.4f}, Test RMSE: {test_RMSE:.4f}, Test Mean Module RMSE: {test_mean_module_RMSE:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "446e107d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First column: 80 values from 1 to 8\n",
    "col1 = torch.linspace(1, 8, steps = 80)\n",
    "\n",
    "# Second column: 80 values from 10 to 80\n",
    "col2 = torch.linspace(10, 80, steps = 80)\n",
    "\n",
    "# Stack into a tensor of shape [80, 2]\n",
    "x_tensor = torch.stack([col1, col2], dim = 1)\n",
    "\n",
    "# shuffle formats\n",
    "x_flat_block = x_tensor.T.reshape(-1)\n",
    "x_two_column = x_flat_block.reshape(2, -1).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d160527",
   "metadata": {},
   "source": [
    "# Build in 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7201f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpytorch\n",
    "import torch\n",
    "\n",
    "class MLPMean_2D(gpytorch.means.Mean):\n",
    "    def __init__(self, dim):\n",
    "        super(MLPMean_2D, self).__init__()\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            torch.nn.Linear(dim, 32), \n",
    "            torch.nn.ReLU(), \n",
    "            torch.nn.Linear(32, 32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(32, dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        m = self.mlp(x)\n",
    "        return m\n",
    "\n",
    "class dfNN(gpytorch.means.Mean):\n",
    "    # NOTE: This needs to be initialised of class gpytorch.means.Mean\n",
    "    def __init__(self, input_dim = 2, hidden_dim = 32):\n",
    "        # NOTE: we use the same default dimensionalities as the dfNN NN model\n",
    "        super(dfNN, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = 1  # Scalar potential (corresponds to H in HNNs)\n",
    "        \n",
    "        # HACK: SiLu() worked much better than ReLU() for this gradient-based model\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, self.output_dim),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        H = self.net(x)\n",
    "\n",
    "        partials = torch.autograd.grad(\n",
    "                outputs = H.sum(), # we can sum here because every H row only depend on every x row\n",
    "                inputs = x,\n",
    "                create_graph = True\n",
    "            )[0]\n",
    "\n",
    "        # Symplectic gradient\n",
    "        # flip columns (last dim) for x2, x1 order. Multiply x2 by -1\n",
    "        mean_symp = partials.flip(-1) * torch.tensor([1, -1], dtype = torch.float32, device = x.device)\n",
    "\n",
    "        # return symp, H # NOTE: return H as well if we want to see what is going on\n",
    "        return mean_symp\n",
    "\n",
    "class dfNGP(gpytorch.models.ExactGP):\n",
    "    # dfGP model with constant mean\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(dfNGP, self).__init__(train_x, train_y, likelihood)\n",
    "\n",
    "        self.mean_module = dfNN(input_dim = 2) # default hidden_dim = 32\n",
    "        self.base_kernel = gpytorch.kernels.MultitaskKernel(\n",
    "            gpytorch.kernels.RBFKernel(), num_tasks=2, rank=1\n",
    "        )\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(self.base_kernel)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        # print(\"mean_x shape:\", mean_x.shape)\n",
    "        covar_x = self.covar_module(x)\n",
    "        # print(\"covar_x shape:\", covar_x.shape)\n",
    "        \n",
    "        return gpytorch.distributions.MultitaskMultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4478f0e0",
   "metadata": {},
   "source": [
    "# Add dfRBFKernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "dee07776",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from gpytorch.lazy import KroneckerProductLazyTensor, lazify\n",
    "from gpytorch.kernels import Kernel\n",
    "\n",
    "class MultitaskKernelWrapper(Kernel):\n",
    "    r\"\"\"\n",
    "    Kernel supporting Kronecker style multitask Gaussian processes (where every data point is evaluated at every\n",
    "    task) using :class:`gpytorch.kernels.IndexKernel` as a basic multitask kernel.\n",
    "\n",
    "    Given a base covariance module to be used for the data, :math:`K_{XX}`, this kernel computes a task kernel of\n",
    "    specified size :math:`K_{TT}` and returns :math:`K = K_{TT} \\otimes K_{XX}`. as an\n",
    "    :obj:`gpytorch.lazy.KroneckerProductLazyTensor`.\n",
    "\n",
    "    :param ~gpytorch.kernels.Kernel data_covar_module: Kernel to use as the data kernel.\n",
    "    :param int num_tasks: Number of tasks\n",
    "    :param int rank: (default 1) Rank of index kernel to use for task covariance matrix.\n",
    "    :param ~gpytorch.priors.Prior task_covar_prior: (default None) Prior to use for task kernel.\n",
    "        See :class:`gpytorch.kernels.IndexKernel` for details.\n",
    "    :param dict kwargs: Additional arguments to pass to the kernel.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_covar_module: Kernel,\n",
    "        num_tasks: int,\n",
    "        rank: Optional[int] = 1,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\"\"\"\n",
    "        super(MultitaskKernelWrapper, self).__init__(**kwargs)\n",
    "        self.data_covar_module = data_covar_module\n",
    "        self.num_tasks = num_tasks\n",
    "\n",
    "    def forward(self, x1, x2, diag=False, last_dim_is_batch=False, **params):\n",
    "        if last_dim_is_batch:\n",
    "            raise RuntimeError(\"MultitaskKernel does not accept the last_dim_is_batch argument.\")\n",
    "        covar_x = lazify(self.data_covar_module.forward(x1, x2, **params))\n",
    "        return covar_x.diag() if diag else covar_x\n",
    "\n",
    "    def num_outputs_per_input(self, x1, x2):\n",
    "        \"\"\"\n",
    "        Given `n` data points `x1` and `m` datapoints `x2`, this multitask\n",
    "        kernel returns an `(n*num_tasks) x (m*num_tasks)` covariance matrix.\n",
    "        \"\"\"\n",
    "        return self.num_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "16d8cf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dfRBFKernel(gpytorch.kernels.Kernel):\n",
    "    def __init__(self, num_tasks = 2, **kwargs):\n",
    "        super().__init__(num_tasks = num_tasks, **kwargs)\n",
    "        self.num_tasks = num_tasks\n",
    "        \n",
    "        self.register_parameter(name = \"raw_lengthscale\",\n",
    "                                parameter = torch.nn.Parameter(torch.tensor([1.0, 1.0])))\n",
    "\n",
    "        self.register_constraint(\"raw_lengthscale\", gpytorch.constraints.Positive())\n",
    "\n",
    "    @property\n",
    "    def outputscale(self):\n",
    "        return self.raw_outputscale_constraint.transform(self.raw_outputscale)\n",
    "    \n",
    "    @property\n",
    "    def lengthscale(self):\n",
    "        return self.raw_lengthscale_constraint.transform(self.raw_lengthscale)\n",
    "\n",
    "    @outputscale.setter\n",
    "    def outputscale(self, value):\n",
    "        self.initialize(raw_outputscale = self.raw_outputscale_constraint.inverse_transform(value))\n",
    "\n",
    "    @lengthscale.setter\n",
    "    def lengthscale(self, value):\n",
    "        self.initialize(raw_lengthscale = self.raw_lengthscale_constraint.inverse_transform(value))\n",
    "\n",
    "    def forward(self, row_tensor, column_tensor, diag = False, **params):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            row_tensor: torch.Size([N, 2]) first input will correspond to rows in returned K\n",
    "            column_tensor: torch.Size([M, 2]) second input will correspond to columns in returned K\n",
    "            diag: bool, if True, return only the diagonal of the covariance matrix\n",
    "        Returns:\n",
    "            K: torch.Size([2N, 2M])\n",
    "        \"\"\"\n",
    "        # Extract the chosen device\n",
    "        device = row_tensor.device\n",
    "\n",
    "        # Remove second explicit dim to get Shape: (2,)\n",
    "        l = self.lengthscale.squeeze().to(device)\n",
    "        # Extract both lengthscales\n",
    "        l1, l2 = l[0], l[1]\n",
    "\n",
    "        # STEP 1: Pairwise differences of shape [N, M, 2]\n",
    "        # Expand row_tensor [N, 2] -> [N, 1, 2] and column_tensor [M, 2] -> [1, M, 2]\n",
    "        diff = (row_tensor[:, None, :] - column_tensor[None, :, :]).to(device)\n",
    "        # Extract the components (columns) for convenience, matching paper notation\n",
    "        r1 = diff[:, :, 0]\n",
    "        r2 = diff[:, :, 1]\n",
    "\n",
    "        # STEP 2: Block matrix\n",
    "        # Compute the 4 (2x2) block components\n",
    "        upper_left = l2.square() - r2.square()\n",
    "        lower_right = l1.square() - r1.square()\n",
    "        upper_right = r1 * r2\n",
    "        lower_left = upper_right # symmetric\n",
    "\n",
    "        # Assemble the 2x2 block matrix\n",
    "        top = torch.cat((upper_left, upper_right), dim = 1) # Shape: [N, 2M]\n",
    "        bottom = torch.cat((lower_left, lower_right), dim = 1) # Shape: [N, 2M]\n",
    "        blocks = torch.cat((top, bottom), dim = 0) # Shape: [2N, 2M]\n",
    "\n",
    "        # STEP 3: RBF/SE envelope (elementwise)\n",
    "        exponent_term = torch.exp(-0.5 * ((r1 / l1) ** 2 + (r2 / l2) ** 2))  # Shape: [N, M]\n",
    "        \n",
    "        # .tile(2, 2) forms (N, M) -> (2N, 2M) for the 2D vector field\n",
    "        K = (1 / (l1**2 * l2**2)) * blocks * exponent_term.tile(2, 2)\n",
    "\n",
    "        # Add this for Quantile Coverage Error (QCE) calculation\n",
    "        if diag:\n",
    "            # Return only the diagonal as a 1D tensor\n",
    "            return K.diag()\n",
    "\n",
    "        return K\n",
    "\n",
    "class dfNN(gpytorch.means.Mean):\n",
    "    # NOTE: This needs to be initialised of class gpytorch.means.Mean\n",
    "    def __init__(self, input_dim = 2, hidden_dim = 32):\n",
    "        # NOTE: we use the same default dimensionalities as the dfNN NN model\n",
    "        super(dfNN, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = 1  # Scalar potential (corresponds to H in HNNs)\n",
    "        \n",
    "        # HACK: SiLu() worked much better than ReLU() for this gradient-based model\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, self.output_dim),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        H = self.net(x)\n",
    "\n",
    "        partials = torch.autograd.grad(\n",
    "                outputs = H.sum(), # we can sum here because every H row only depend on every x row\n",
    "                inputs = x,\n",
    "                create_graph = True\n",
    "            )[0]\n",
    "\n",
    "        # Symplectic gradient\n",
    "        # flip columns (last dim) for x2, x1 order. Multiply x2 by -1\n",
    "        mean_symp = partials.flip(-1) * torch.tensor([1, -1], dtype = torch.float32, device = x.device)\n",
    "\n",
    "        # return symp, H # NOTE: return H as well if we want to see what is going on\n",
    "        return mean_symp\n",
    "\n",
    "class dfNGP(gpytorch.models.ExactGP):\n",
    "    # dfGP model with constant mean\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(dfNGP, self).__init__(train_x, train_y, likelihood)\n",
    "\n",
    "        # Custom mean module\n",
    "        self.mean_module = dfNN(input_dim = 2) # default hidden_dim = 32\n",
    "        # Custom kernel module\n",
    "        self.base_kernel = MultitaskKernelWrapper(\n",
    "            dfRBFKernel(num_tasks = 2), \n",
    "            num_tasks = 2,\n",
    "            )\n",
    "\n",
    "        # self.base_kernel = dfRBFKernel(num_tasks = 2)\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(self.base_kernel)\n",
    "\n",
    "        self.covar_module.outputscale = torch.ones(1, device = device) * 0.001\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        # print(\"mean_x shape:\", mean_x.shape)\n",
    "        covar_x = self.covar_module(x)\n",
    "        # print(\"covar_x shape:\", covar_x.shape)\n",
    "        \n",
    "        return gpytorch.distributions.MultitaskMultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "c6d5ed67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_task_noises torch.Size([2])\n",
      "raw_noise torch.Size([1])\n",
      "likelihood.raw_task_noises torch.Size([2])\n",
      "likelihood.raw_noise torch.Size([1])\n",
      "mean_module.net.0.weight torch.Size([32, 2])\n",
      "mean_module.net.0.bias torch.Size([32])\n",
      "mean_module.net.2.weight torch.Size([32, 32])\n",
      "mean_module.net.2.bias torch.Size([32])\n",
      "mean_module.net.4.weight torch.Size([1, 32])\n",
      "mean_module.net.4.bias torch.Size([1])\n",
      "base_kernel.data_covar_module.raw_lengthscale torch.Size([2])\n",
      "covar_module.raw_outputscale torch.Size([])\n",
      "Epoch 1 Training Loss (NLML): 1.4876 Train RMSE: 0.6641 Test RMSE: 0.7849, Test Mean Module RMSE: 0.8084\n",
      "Epoch 2 Training Loss (NLML): 1.2366 Train RMSE: 0.4072 Test RMSE: 0.4399, Test Mean Module RMSE: 0.4355\n",
      "Epoch 3 Training Loss (NLML): 1.1417 Train RMSE: 0.6143 Test RMSE: 0.5901, Test Mean Module RMSE: 0.5700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kim/anaconda3/envs/ice_thickness_gpytorch/lib/python3.9/site-packages/gpytorch/lazy/non_lazy_tensor.py:13: UserWarning: gpytorch.lazy.lazify is deprecated in favor of linear_operator.to_linear_operator\n",
      "  warnings.warn(\"gpytorch.lazy.lazify is deprecated in favor of linear_operator.to_linear_operator\")\n",
      "/home/kim/anaconda3/envs/ice_thickness_gpytorch/lib/python3.9/site-packages/gpytorch/models/exact_gp.py:284: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Training Loss (NLML): 1.2141 Train RMSE: 0.3435 Test RMSE: 0.4031, Test Mean Module RMSE: 0.4064\n",
      "Epoch 5 Training Loss (NLML): 1.1250 Train RMSE: 0.4438 Test RMSE: 0.5582, Test Mean Module RMSE: 0.5784\n",
      "Epoch 6 Training Loss (NLML): 1.1521 Train RMSE: 0.4983 Test RMSE: 0.6132, Test Mean Module RMSE: 0.6353\n",
      "Epoch 7 Training Loss (NLML): 1.1697 Train RMSE: 0.4057 Test RMSE: 0.5120, Test Mean Module RMSE: 0.5320\n",
      "Epoch 8 Training Loss (NLML): 1.1407 Train RMSE: 0.2850 Test RMSE: 0.3605, Test Mean Module RMSE: 0.3702\n",
      "Epoch 9 Training Loss (NLML): 1.1118 Train RMSE: 0.3635 Test RMSE: 0.3791, Test Mean Module RMSE: 0.3685\n",
      "Epoch 10 Training Loss (NLML): 1.1293 Train RMSE: 0.4100 Test RMSE: 0.4187, Test Mean Module RMSE: 0.4046\n",
      "Epoch 11 Training Loss (NLML): 1.1417 Train RMSE: 0.3098 Test RMSE: 0.3453, Test Mean Module RMSE: 0.3404\n",
      "Epoch 12 Training Loss (NLML): 1.1167 Train RMSE: 0.2769 Test RMSE: 0.3564, Test Mean Module RMSE: 0.3672\n",
      "Epoch 13 Training Loss (NLML): 1.1101 Train RMSE: 0.3331 Test RMSE: 0.4285, Test Mean Module RMSE: 0.4462\n",
      "Epoch 14 Training Loss (NLML): 1.1219 Train RMSE: 0.3570 Test RMSE: 0.4557, Test Mean Module RMSE: 0.4749\n",
      "Epoch 15 Training Loss (NLML): 1.1275 Train RMSE: 0.3272 Test RMSE: 0.4253, Test Mean Module RMSE: 0.4432\n",
      "Epoch 16 Training Loss (NLML): 1.1204 Train RMSE: 0.2770 Test RMSE: 0.3691, Test Mean Module RMSE: 0.3816\n",
      "Epoch 17 Training Loss (NLML): 1.1099 Train RMSE: 0.2703 Test RMSE: 0.3442, Test Mean Module RMSE: 0.3461\n",
      "Epoch 18 Training Loss (NLML): 1.1087 Train RMSE: 0.3090 Test RMSE: 0.3654, Test Mean Module RMSE: 0.3576\n",
      "Epoch 19 Training Loss (NLML): 1.1163 Train RMSE: 0.3167 Test RMSE: 0.3725, Test Mean Module RMSE: 0.3623\n",
      "Epoch 20 Training Loss (NLML): 1.1179 Train RMSE: 0.2838 Test RMSE: 0.3551, Test Mean Module RMSE: 0.3500\n",
      "Epoch 21 Training Loss (NLML): 1.1111 Train RMSE: 0.2651 Test RMSE: 0.3565, Test Mean Module RMSE: 0.3605\n",
      "Epoch 22 Training Loss (NLML): 1.1077 Train RMSE: 0.2803 Test RMSE: 0.3816, Test Mean Module RMSE: 0.3926\n",
      "Epoch 23 Training Loss (NLML): 1.1106 Train RMSE: 0.2961 Test RMSE: 0.3991, Test Mean Module RMSE: 0.4133\n",
      "Epoch 24 Training Loss (NLML): 1.1137 Train RMSE: 0.2909 Test RMSE: 0.3921, Test Mean Module RMSE: 0.4063\n",
      "Epoch 25 Training Loss (NLML): 1.1126 Train RMSE: 0.2700 Test RMSE: 0.3661, Test Mean Module RMSE: 0.3773\n",
      "Epoch 26 Training Loss (NLML): 1.1085 Train RMSE: 0.2568 Test RMSE: 0.3417, Test Mean Module RMSE: 0.3467\n",
      "Epoch 27 Training Loss (NLML): 1.1061 Train RMSE: 0.2659 Test RMSE: 0.3355, Test Mean Module RMSE: 0.3337\n",
      "Epoch 28 Training Loss (NLML): 1.1077 Train RMSE: 0.2758 Test RMSE: 0.3361, Test Mean Module RMSE: 0.3311\n",
      "Epoch 29 Training Loss (NLML): 1.1096 Train RMSE: 0.2660 Test RMSE: 0.3287, Test Mean Module RMSE: 0.3261\n",
      "Epoch 30 Training Loss (NLML): 1.1078 Train RMSE: 0.2518 Test RMSE: 0.3252, Test Mean Module RMSE: 0.3288\n",
      "Epoch 31 Training Loss (NLML): 1.1053 Train RMSE: 0.2528 Test RMSE: 0.3357, Test Mean Module RMSE: 0.3454\n",
      "Epoch 32 Training Loss (NLML): 1.1055 Train RMSE: 0.2611 Test RMSE: 0.3475, Test Mean Module RMSE: 0.3606\n",
      "Epoch 33 Training Loss (NLML): 1.1069 Train RMSE: 0.2614 Test RMSE: 0.3467, Test Mean Module RMSE: 0.3603\n",
      "Epoch 34 Training Loss (NLML): 1.1069 Train RMSE: 0.2528 Test RMSE: 0.3337, Test Mean Module RMSE: 0.3451\n",
      "Epoch 35 Training Loss (NLML): 1.1054 Train RMSE: 0.2458 Test RMSE: 0.3192, Test Mean Module RMSE: 0.3262\n",
      "Epoch 36 Training Loss (NLML): 1.1042 Train RMSE: 0.2470 Test RMSE: 0.3123, Test Mean Module RMSE: 0.3145\n",
      "Epoch 37 Training Loss (NLML): 1.1044 Train RMSE: 0.2500 Test RMSE: 0.3112, Test Mean Module RMSE: 0.3110\n",
      "Epoch 38 Training Loss (NLML): 1.1049 Train RMSE: 0.2465 Test RMSE: 0.3110, Test Mean Module RMSE: 0.3119\n",
      "Epoch 39 Training Loss (NLML): 1.1043 Train RMSE: 0.2403 Test RMSE: 0.3133, Test Mean Module RMSE: 0.3178\n",
      "Epoch 40 Training Loss (NLML): 1.1033 Train RMSE: 0.2390 Test RMSE: 0.3198, Test Mean Module RMSE: 0.3279\n",
      "Epoch 41 Training Loss (NLML): 1.1030 Train RMSE: 0.2415 Test RMSE: 0.3260, Test Mean Module RMSE: 0.3362\n",
      "Epoch 42 Training Loss (NLML): 1.1034 Train RMSE: 0.2421 Test RMSE: 0.3261, Test Mean Module RMSE: 0.3364\n",
      "Epoch 43 Training Loss (NLML): 1.1035 Train RMSE: 0.2387 Test RMSE: 0.3194, Test Mean Module RMSE: 0.3280\n",
      "Epoch 44 Training Loss (NLML): 1.1030 Train RMSE: 0.2353 Test RMSE: 0.3106, Test Mean Module RMSE: 0.3164\n",
      "Epoch 45 Training Loss (NLML): 1.1024 Train RMSE: 0.2358 Test RMSE: 0.3050, Test Mean Module RMSE: 0.3084\n",
      "Epoch 46 Training Loss (NLML): 1.1025 Train RMSE: 0.2375 Test RMSE: 0.3029, Test Mean Module RMSE: 0.3056\n",
      "Epoch 47 Training Loss (NLML): 1.1028 Train RMSE: 0.2360 Test RMSE: 0.3023, Test Mean Module RMSE: 0.3065\n",
      "Epoch 48 Training Loss (NLML): 1.1026 Train RMSE: 0.2332 Test RMSE: 0.3036, Test Mean Module RMSE: 0.3102\n",
      "Epoch 49 Training Loss (NLML): 1.1021 Train RMSE: 0.2329 Test RMSE: 0.3070, Test Mean Module RMSE: 0.3156\n",
      "Epoch 50 Training Loss (NLML): 1.1020 Train RMSE: 0.2340 Test RMSE: 0.3098, Test Mean Module RMSE: 0.3191\n",
      "Epoch 51 Training Loss (NLML): 1.1022 Train RMSE: 0.2333 Test RMSE: 0.3089, Test Mean Module RMSE: 0.3177\n",
      "Epoch 52 Training Loss (NLML): 1.1021 Train RMSE: 0.2308 Test RMSE: 0.3050, Test Mean Module RMSE: 0.3124\n",
      "Epoch 53 Training Loss (NLML): 1.1017 Train RMSE: 0.2298 Test RMSE: 0.3015, Test Mean Module RMSE: 0.3071\n",
      "Epoch 54 Training Loss (NLML): 1.1015 Train RMSE: 0.2305 Test RMSE: 0.3001, Test Mean Module RMSE: 0.3045\n",
      "Epoch 55 Training Loss (NLML): 1.1016 Train RMSE: 0.2305 Test RMSE: 0.2999, Test Mean Module RMSE: 0.3043\n",
      "Epoch 56 Training Loss (NLML): 1.1016 Train RMSE: 0.2288 Test RMSE: 0.3004, Test Mean Module RMSE: 0.3056\n",
      "Epoch 57 Training Loss (NLML): 1.1014 Train RMSE: 0.2275 Test RMSE: 0.3018, Test Mean Module RMSE: 0.3083\n",
      "Epoch 58 Training Loss (NLML): 1.1012 Train RMSE: 0.2275 Test RMSE: 0.3035, Test Mean Module RMSE: 0.3111\n",
      "Epoch 59 Training Loss (NLML): 1.1012 Train RMSE: 0.2274 Test RMSE: 0.3037, Test Mean Module RMSE: 0.3116\n",
      "Epoch 60 Training Loss (NLML): 1.1011 Train RMSE: 0.2262 Test RMSE: 0.3016, Test Mean Module RMSE: 0.3092\n",
      "Epoch 61 Training Loss (NLML): 1.1009 Train RMSE: 0.2250 Test RMSE: 0.2986, Test Mean Module RMSE: 0.3054\n",
      "Epoch 62 Training Loss (NLML): 1.1008 Train RMSE: 0.2248 Test RMSE: 0.2964, Test Mean Module RMSE: 0.3022\n",
      "Epoch 63 Training Loss (NLML): 1.1007 Train RMSE: 0.2247 Test RMSE: 0.2955, Test Mean Module RMSE: 0.3006\n",
      "Epoch 64 Training Loss (NLML): 1.1007 Train RMSE: 0.2239 Test RMSE: 0.2957, Test Mean Module RMSE: 0.3009\n",
      "Epoch 65 Training Loss (NLML): 1.1006 Train RMSE: 0.2230 Test RMSE: 0.2971, Test Mean Module RMSE: 0.3030\n",
      "Epoch 66 Training Loss (NLML): 1.1004 Train RMSE: 0.2227 Test RMSE: 0.2992, Test Mean Module RMSE: 0.3060\n",
      "Epoch 67 Training Loss (NLML): 1.1004 Train RMSE: 0.2226 Test RMSE: 0.3007, Test Mean Module RMSE: 0.3080\n",
      "Epoch 68 Training Loss (NLML): 1.1004 Train RMSE: 0.2220 Test RMSE: 0.3006, Test Mean Module RMSE: 0.3079\n",
      "Epoch 69 Training Loss (NLML): 1.1003 Train RMSE: 0.2213 Test RMSE: 0.2990, Test Mean Module RMSE: 0.3057\n",
      "Epoch 70 Training Loss (NLML): 1.1002 Train RMSE: 0.2209 Test RMSE: 0.2971, Test Mean Module RMSE: 0.3029\n",
      "Epoch 71 Training Loss (NLML): 1.1001 Train RMSE: 0.2207 Test RMSE: 0.2959, Test Mean Module RMSE: 0.3010\n",
      "Epoch 72 Training Loss (NLML): 1.1001 Train RMSE: 0.2203 Test RMSE: 0.2955, Test Mean Module RMSE: 0.3007\n",
      "Epoch 73 Training Loss (NLML): 1.1000 Train RMSE: 0.2198 Test RMSE: 0.2962, Test Mean Module RMSE: 0.3020\n",
      "Epoch 74 Training Loss (NLML): 1.0999 Train RMSE: 0.2195 Test RMSE: 0.2975, Test Mean Module RMSE: 0.3041\n",
      "Epoch 75 Training Loss (NLML): 1.0999 Train RMSE: 0.2194 Test RMSE: 0.2984, Test Mean Module RMSE: 0.3056\n",
      "Epoch 76 Training Loss (NLML): 1.0999 Train RMSE: 0.2191 Test RMSE: 0.2984, Test Mean Module RMSE: 0.3054\n",
      "Epoch 77 Training Loss (NLML): 1.0998 Train RMSE: 0.2187 Test RMSE: 0.2976, Test Mean Module RMSE: 0.3040\n",
      "Epoch 78 Training Loss (NLML): 1.0997 Train RMSE: 0.2184 Test RMSE: 0.2967, Test Mean Module RMSE: 0.3024\n",
      "Epoch 79 Training Loss (NLML): 1.0997 Train RMSE: 0.2182 Test RMSE: 0.2965, Test Mean Module RMSE: 0.3018\n",
      "Epoch 80 Training Loss (NLML): 1.0997 Train RMSE: 0.2180 Test RMSE: 0.2971, Test Mean Module RMSE: 0.3024\n",
      "Epoch 81 Training Loss (NLML): 1.0996 Train RMSE: 0.2177 Test RMSE: 0.2981, Test Mean Module RMSE: 0.3040\n",
      "Epoch 82 Training Loss (NLML): 1.0996 Train RMSE: 0.2175 Test RMSE: 0.2993, Test Mean Module RMSE: 0.3056\n",
      "Epoch 83 Training Loss (NLML): 1.0996 Train RMSE: 0.2174 Test RMSE: 0.2999, Test Mean Module RMSE: 0.3063\n",
      "Epoch 84 Training Loss (NLML): 1.0995 Train RMSE: 0.2172 Test RMSE: 0.2998, Test Mean Module RMSE: 0.3060\n",
      "Epoch 85 Training Loss (NLML): 1.0995 Train RMSE: 0.2169 Test RMSE: 0.2992, Test Mean Module RMSE: 0.3050\n",
      "Epoch 86 Training Loss (NLML): 1.0995 Train RMSE: 0.2168 Test RMSE: 0.2986, Test Mean Module RMSE: 0.3042\n",
      "Epoch 87 Training Loss (NLML): 1.0995 Train RMSE: 0.2167 Test RMSE: 0.2986, Test Mean Module RMSE: 0.3040\n",
      "Epoch 88 Training Loss (NLML): 1.0994 Train RMSE: 0.2165 Test RMSE: 0.2990, Test Mean Module RMSE: 0.3045\n",
      "Epoch 89 Training Loss (NLML): 1.0994 Train RMSE: 0.2164 Test RMSE: 0.2999, Test Mean Module RMSE: 0.3055\n",
      "Epoch 90 Training Loss (NLML): 1.0994 Train RMSE: 0.2163 Test RMSE: 0.3008, Test Mean Module RMSE: 0.3066\n",
      "Epoch 91 Training Loss (NLML): 1.0994 Train RMSE: 0.2162 Test RMSE: 0.3014, Test Mean Module RMSE: 0.3073\n",
      "Epoch 92 Training Loss (NLML): 1.0994 Train RMSE: 0.2160 Test RMSE: 0.3016, Test Mean Module RMSE: 0.3074\n",
      "Epoch 93 Training Loss (NLML): 1.0993 Train RMSE: 0.2159 Test RMSE: 0.3015, Test Mean Module RMSE: 0.3070\n",
      "Epoch 94 Training Loss (NLML): 1.0993 Train RMSE: 0.2159 Test RMSE: 0.3014, Test Mean Module RMSE: 0.3067\n",
      "Epoch 95 Training Loss (NLML): 1.0993 Train RMSE: 0.2158 Test RMSE: 0.3015, Test Mean Module RMSE: 0.3066\n",
      "Epoch 96 Training Loss (NLML): 1.0993 Train RMSE: 0.2157 Test RMSE: 0.3020, Test Mean Module RMSE: 0.3071\n",
      "Epoch 97 Training Loss (NLML): 1.0993 Train RMSE: 0.2156 Test RMSE: 0.3026, Test Mean Module RMSE: 0.3079\n",
      "Epoch 98 Training Loss (NLML): 1.0993 Train RMSE: 0.2156 Test RMSE: 0.3030, Test Mean Module RMSE: 0.3086\n",
      "Epoch 99 Training Loss (NLML): 1.0993 Train RMSE: 0.2155 Test RMSE: 0.3032, Test Mean Module RMSE: 0.3088\n",
      "Epoch 100 Training Loss (NLML): 1.0992 Train RMSE: 0.2154 Test RMSE: 0.3031, Test Mean Module RMSE: 0.3084\n",
      "Epoch 101 Training Loss (NLML): 1.0992 Train RMSE: 0.2153 Test RMSE: 0.3029, Test Mean Module RMSE: 0.3080\n",
      "Epoch 102 Training Loss (NLML): 1.0992 Train RMSE: 0.2153 Test RMSE: 0.3029, Test Mean Module RMSE: 0.3079\n",
      "Epoch 103 Training Loss (NLML): 1.0992 Train RMSE: 0.2152 Test RMSE: 0.3031, Test Mean Module RMSE: 0.3082\n",
      "Epoch 104 Training Loss (NLML): 1.0992 Train RMSE: 0.2151 Test RMSE: 0.3034, Test Mean Module RMSE: 0.3086\n",
      "Epoch 105 Training Loss (NLML): 1.0992 Train RMSE: 0.2150 Test RMSE: 0.3036, Test Mean Module RMSE: 0.3089\n",
      "Epoch 106 Training Loss (NLML): 1.0992 Train RMSE: 0.2150 Test RMSE: 0.3036, Test Mean Module RMSE: 0.3089\n",
      "Epoch 107 Training Loss (NLML): 1.0992 Train RMSE: 0.2149 Test RMSE: 0.3034, Test Mean Module RMSE: 0.3086\n",
      "Epoch 108 Training Loss (NLML): 1.0992 Train RMSE: 0.2148 Test RMSE: 0.3031, Test Mean Module RMSE: 0.3082\n",
      "Epoch 109 Training Loss (NLML): 1.0991 Train RMSE: 0.2147 Test RMSE: 0.3029, Test Mean Module RMSE: 0.3079\n",
      "Epoch 110 Training Loss (NLML): 1.0991 Train RMSE: 0.2146 Test RMSE: 0.3028, Test Mean Module RMSE: 0.3078\n",
      "Epoch 111 Training Loss (NLML): 1.0991 Train RMSE: 0.2145 Test RMSE: 0.3028, Test Mean Module RMSE: 0.3079\n",
      "Epoch 112 Training Loss (NLML): 1.0991 Train RMSE: 0.2144 Test RMSE: 0.3028, Test Mean Module RMSE: 0.3080\n",
      "Epoch 113 Training Loss (NLML): 1.0991 Train RMSE: 0.2143 Test RMSE: 0.3027, Test Mean Module RMSE: 0.3079\n",
      "Epoch 114 Training Loss (NLML): 1.0991 Train RMSE: 0.2142 Test RMSE: 0.3024, Test Mean Module RMSE: 0.3076\n",
      "Epoch 115 Training Loss (NLML): 1.0991 Train RMSE: 0.2141 Test RMSE: 0.3019, Test Mean Module RMSE: 0.3071\n",
      "Epoch 116 Training Loss (NLML): 1.0990 Train RMSE: 0.2140 Test RMSE: 0.3015, Test Mean Module RMSE: 0.3066\n",
      "Epoch 117 Training Loss (NLML): 1.0990 Train RMSE: 0.2139 Test RMSE: 0.3013, Test Mean Module RMSE: 0.3063\n",
      "Epoch 118 Training Loss (NLML): 1.0990 Train RMSE: 0.2137 Test RMSE: 0.3011, Test Mean Module RMSE: 0.3062\n",
      "Epoch 119 Training Loss (NLML): 1.0990 Train RMSE: 0.2136 Test RMSE: 0.3010, Test Mean Module RMSE: 0.3062\n",
      "Epoch 120 Training Loss (NLML): 1.0990 Train RMSE: 0.2135 Test RMSE: 0.3008, Test Mean Module RMSE: 0.3060\n",
      "Epoch 121 Training Loss (NLML): 1.0989 Train RMSE: 0.2134 Test RMSE: 0.3005, Test Mean Module RMSE: 0.3057\n",
      "Epoch 122 Training Loss (NLML): 1.0989 Train RMSE: 0.2132 Test RMSE: 0.3002, Test Mean Module RMSE: 0.3052\n",
      "Epoch 123 Training Loss (NLML): 1.0989 Train RMSE: 0.2131 Test RMSE: 0.2998, Test Mean Module RMSE: 0.3049\n",
      "Epoch 124 Training Loss (NLML): 1.0989 Train RMSE: 0.2129 Test RMSE: 0.2995, Test Mean Module RMSE: 0.3046\n",
      "Epoch 125 Training Loss (NLML): 1.0989 Train RMSE: 0.2128 Test RMSE: 0.2993, Test Mean Module RMSE: 0.3044\n",
      "Epoch 126 Training Loss (NLML): 1.0988 Train RMSE: 0.2126 Test RMSE: 0.2990, Test Mean Module RMSE: 0.3042\n",
      "Epoch 127 Training Loss (NLML): 1.0988 Train RMSE: 0.2125 Test RMSE: 0.2988, Test Mean Module RMSE: 0.3039\n",
      "Epoch 128 Training Loss (NLML): 1.0988 Train RMSE: 0.2123 Test RMSE: 0.2985, Test Mean Module RMSE: 0.3036\n",
      "Epoch 129 Training Loss (NLML): 1.0988 Train RMSE: 0.2122 Test RMSE: 0.2981, Test Mean Module RMSE: 0.3032\n",
      "Epoch 130 Training Loss (NLML): 1.0987 Train RMSE: 0.2120 Test RMSE: 0.2978, Test Mean Module RMSE: 0.3028\n",
      "Epoch 131 Training Loss (NLML): 1.0987 Train RMSE: 0.2118 Test RMSE: 0.2975, Test Mean Module RMSE: 0.3025\n",
      "Epoch 132 Training Loss (NLML): 1.0987 Train RMSE: 0.2116 Test RMSE: 0.2973, Test Mean Module RMSE: 0.3023\n",
      "Epoch 133 Training Loss (NLML): 1.0987 Train RMSE: 0.2114 Test RMSE: 0.2971, Test Mean Module RMSE: 0.3021\n",
      "Epoch 134 Training Loss (NLML): 1.0986 Train RMSE: 0.2112 Test RMSE: 0.2968, Test Mean Module RMSE: 0.3018\n",
      "Epoch 135 Training Loss (NLML): 1.0986 Train RMSE: 0.2111 Test RMSE: 0.2965, Test Mean Module RMSE: 0.3014\n",
      "Epoch 136 Training Loss (NLML): 1.0986 Train RMSE: 0.2109 Test RMSE: 0.2962, Test Mean Module RMSE: 0.3011\n",
      "Epoch 137 Training Loss (NLML): 1.0985 Train RMSE: 0.2107 Test RMSE: 0.2959, Test Mean Module RMSE: 0.3007\n",
      "Epoch 138 Training Loss (NLML): 1.0985 Train RMSE: 0.2104 Test RMSE: 0.2957, Test Mean Module RMSE: 0.3005\n",
      "Epoch 139 Training Loss (NLML): 1.0985 Train RMSE: 0.2102 Test RMSE: 0.2955, Test Mean Module RMSE: 0.3003\n",
      "Epoch 140 Training Loss (NLML): 1.0984 Train RMSE: 0.2100 Test RMSE: 0.2953, Test Mean Module RMSE: 0.3002\n",
      "Epoch 141 Training Loss (NLML): 1.0984 Train RMSE: 0.2098 Test RMSE: 0.2952, Test Mean Module RMSE: 0.3000\n",
      "Epoch 142 Training Loss (NLML): 1.0984 Train RMSE: 0.2096 Test RMSE: 0.2950, Test Mean Module RMSE: 0.2997\n",
      "Epoch 143 Training Loss (NLML): 1.0984 Train RMSE: 0.2094 Test RMSE: 0.2948, Test Mean Module RMSE: 0.2995\n",
      "Epoch 144 Training Loss (NLML): 1.0983 Train RMSE: 0.2092 Test RMSE: 0.2946, Test Mean Module RMSE: 0.2993\n",
      "Epoch 145 Training Loss (NLML): 1.0983 Train RMSE: 0.2090 Test RMSE: 0.2945, Test Mean Module RMSE: 0.2992\n",
      "Epoch 146 Training Loss (NLML): 1.0983 Train RMSE: 0.2088 Test RMSE: 0.2945, Test Mean Module RMSE: 0.2991\n",
      "Epoch 147 Training Loss (NLML): 1.0982 Train RMSE: 0.2086 Test RMSE: 0.2944, Test Mean Module RMSE: 0.2991\n",
      "Epoch 148 Training Loss (NLML): 1.0982 Train RMSE: 0.2084 Test RMSE: 0.2944, Test Mean Module RMSE: 0.2990\n",
      "Epoch 149 Training Loss (NLML): 1.0982 Train RMSE: 0.2082 Test RMSE: 0.2944, Test Mean Module RMSE: 0.2990\n",
      "Epoch 150 Training Loss (NLML): 1.0981 Train RMSE: 0.2080 Test RMSE: 0.2945, Test Mean Module RMSE: 0.2990\n",
      "Epoch 151 Training Loss (NLML): 1.0981 Train RMSE: 0.2078 Test RMSE: 0.2946, Test Mean Module RMSE: 0.2990\n",
      "Epoch 152 Training Loss (NLML): 1.0981 Train RMSE: 0.2076 Test RMSE: 0.2948, Test Mean Module RMSE: 0.2992\n",
      "Epoch 153 Training Loss (NLML): 1.0981 Train RMSE: 0.2075 Test RMSE: 0.2950, Test Mean Module RMSE: 0.2993\n",
      "Epoch 154 Training Loss (NLML): 1.0980 Train RMSE: 0.2073 Test RMSE: 0.2953, Test Mean Module RMSE: 0.2996\n",
      "Epoch 155 Training Loss (NLML): 1.0980 Train RMSE: 0.2072 Test RMSE: 0.2955, Test Mean Module RMSE: 0.2998\n",
      "Epoch 156 Training Loss (NLML): 1.0980 Train RMSE: 0.2070 Test RMSE: 0.2959, Test Mean Module RMSE: 0.3001\n",
      "Epoch 157 Training Loss (NLML): 1.0980 Train RMSE: 0.2069 Test RMSE: 0.2962, Test Mean Module RMSE: 0.3004\n",
      "Epoch 158 Training Loss (NLML): 1.0979 Train RMSE: 0.2067 Test RMSE: 0.2966, Test Mean Module RMSE: 0.3007\n",
      "Epoch 159 Training Loss (NLML): 1.0979 Train RMSE: 0.2066 Test RMSE: 0.2970, Test Mean Module RMSE: 0.3011\n",
      "Epoch 160 Training Loss (NLML): 1.0979 Train RMSE: 0.2064 Test RMSE: 0.2973, Test Mean Module RMSE: 0.3015\n",
      "Epoch 161 Training Loss (NLML): 1.0979 Train RMSE: 0.2063 Test RMSE: 0.2977, Test Mean Module RMSE: 0.3018\n",
      "Epoch 162 Training Loss (NLML): 1.0979 Train RMSE: 0.2062 Test RMSE: 0.2980, Test Mean Module RMSE: 0.3021\n",
      "Epoch 163 Training Loss (NLML): 1.0978 Train RMSE: 0.2060 Test RMSE: 0.2984, Test Mean Module RMSE: 0.3024\n",
      "Epoch 164 Training Loss (NLML): 1.0978 Train RMSE: 0.2059 Test RMSE: 0.2987, Test Mean Module RMSE: 0.3027\n",
      "Epoch 165 Training Loss (NLML): 1.0978 Train RMSE: 0.2058 Test RMSE: 0.2990, Test Mean Module RMSE: 0.3029\n",
      "Epoch 166 Training Loss (NLML): 1.0978 Train RMSE: 0.2056 Test RMSE: 0.2992, Test Mean Module RMSE: 0.3032\n",
      "Epoch 167 Training Loss (NLML): 1.0978 Train RMSE: 0.2055 Test RMSE: 0.2994, Test Mean Module RMSE: 0.3033\n",
      "Epoch 168 Training Loss (NLML): 1.0977 Train RMSE: 0.2054 Test RMSE: 0.2995, Test Mean Module RMSE: 0.3034\n",
      "Epoch 169 Training Loss (NLML): 1.0977 Train RMSE: 0.2052 Test RMSE: 0.2996, Test Mean Module RMSE: 0.3035\n",
      "Epoch 170 Training Loss (NLML): 1.0977 Train RMSE: 0.2051 Test RMSE: 0.2996, Test Mean Module RMSE: 0.3035\n",
      "Epoch 171 Training Loss (NLML): 1.0977 Train RMSE: 0.2049 Test RMSE: 0.2996, Test Mean Module RMSE: 0.3035\n",
      "Epoch 172 Training Loss (NLML): 1.0977 Train RMSE: 0.2048 Test RMSE: 0.2996, Test Mean Module RMSE: 0.3034\n",
      "Epoch 173 Training Loss (NLML): 1.0976 Train RMSE: 0.2046 Test RMSE: 0.2995, Test Mean Module RMSE: 0.3034\n",
      "Epoch 174 Training Loss (NLML): 1.0976 Train RMSE: 0.2044 Test RMSE: 0.2994, Test Mean Module RMSE: 0.3032\n",
      "Epoch 175 Training Loss (NLML): 1.0976 Train RMSE: 0.2043 Test RMSE: 0.2992, Test Mean Module RMSE: 0.3031\n",
      "Epoch 176 Training Loss (NLML): 1.0976 Train RMSE: 0.2041 Test RMSE: 0.2990, Test Mean Module RMSE: 0.3029\n",
      "Epoch 177 Training Loss (NLML): 1.0975 Train RMSE: 0.2040 Test RMSE: 0.2988, Test Mean Module RMSE: 0.3027\n",
      "Epoch 178 Training Loss (NLML): 1.0975 Train RMSE: 0.2038 Test RMSE: 0.2986, Test Mean Module RMSE: 0.3025\n",
      "Epoch 179 Training Loss (NLML): 1.0975 Train RMSE: 0.2036 Test RMSE: 0.2983, Test Mean Module RMSE: 0.3022\n",
      "Epoch 180 Training Loss (NLML): 1.0975 Train RMSE: 0.2034 Test RMSE: 0.2979, Test Mean Module RMSE: 0.3018\n",
      "Epoch 181 Training Loss (NLML): 1.0974 Train RMSE: 0.2032 Test RMSE: 0.2975, Test Mean Module RMSE: 0.3015\n",
      "Epoch 182 Training Loss (NLML): 1.0974 Train RMSE: 0.2031 Test RMSE: 0.2972, Test Mean Module RMSE: 0.3011\n",
      "Epoch 183 Training Loss (NLML): 1.0974 Train RMSE: 0.2029 Test RMSE: 0.2967, Test Mean Module RMSE: 0.3007\n",
      "Epoch 184 Training Loss (NLML): 1.0974 Train RMSE: 0.2027 Test RMSE: 0.2963, Test Mean Module RMSE: 0.3003\n",
      "Epoch 185 Training Loss (NLML): 1.0973 Train RMSE: 0.2025 Test RMSE: 0.2959, Test Mean Module RMSE: 0.2999\n",
      "Epoch 186 Training Loss (NLML): 1.0973 Train RMSE: 0.2023 Test RMSE: 0.2955, Test Mean Module RMSE: 0.2995\n",
      "Epoch 187 Training Loss (NLML): 1.0973 Train RMSE: 0.2021 Test RMSE: 0.2950, Test Mean Module RMSE: 0.2991\n",
      "Epoch 188 Training Loss (NLML): 1.0972 Train RMSE: 0.2019 Test RMSE: 0.2946, Test Mean Module RMSE: 0.2986\n",
      "Epoch 189 Training Loss (NLML): 1.0972 Train RMSE: 0.2017 Test RMSE: 0.2942, Test Mean Module RMSE: 0.2982\n",
      "Epoch 190 Training Loss (NLML): 1.0972 Train RMSE: 0.2015 Test RMSE: 0.2937, Test Mean Module RMSE: 0.2978\n",
      "Epoch 191 Training Loss (NLML): 1.0971 Train RMSE: 0.2013 Test RMSE: 0.2933, Test Mean Module RMSE: 0.2974\n",
      "Epoch 192 Training Loss (NLML): 1.0971 Train RMSE: 0.2010 Test RMSE: 0.2929, Test Mean Module RMSE: 0.2970\n",
      "Epoch 193 Training Loss (NLML): 1.0971 Train RMSE: 0.2008 Test RMSE: 0.2925, Test Mean Module RMSE: 0.2966\n",
      "Epoch 194 Training Loss (NLML): 1.0971 Train RMSE: 0.2006 Test RMSE: 0.2921, Test Mean Module RMSE: 0.2962\n",
      "Epoch 195 Training Loss (NLML): 1.0970 Train RMSE: 0.2004 Test RMSE: 0.2917, Test Mean Module RMSE: 0.2959\n",
      "Epoch 196 Training Loss (NLML): 1.0970 Train RMSE: 0.2001 Test RMSE: 0.2913, Test Mean Module RMSE: 0.2955\n",
      "Epoch 197 Training Loss (NLML): 1.0970 Train RMSE: 0.1999 Test RMSE: 0.2909, Test Mean Module RMSE: 0.2951\n",
      "Epoch 198 Training Loss (NLML): 1.0969 Train RMSE: 0.1996 Test RMSE: 0.2905, Test Mean Module RMSE: 0.2948\n",
      "Epoch 199 Training Loss (NLML): 1.0969 Train RMSE: 0.1994 Test RMSE: 0.2901, Test Mean Module RMSE: 0.2944\n",
      "Epoch 200 Training Loss (NLML): 1.0968 Train RMSE: 0.1992 Test RMSE: 0.2898, Test Mean Module RMSE: 0.2940\n",
      "Epoch 201 Training Loss (NLML): 1.0968 Train RMSE: 0.1989 Test RMSE: 0.2894, Test Mean Module RMSE: 0.2937\n",
      "Epoch 202 Training Loss (NLML): 1.0968 Train RMSE: 0.1986 Test RMSE: 0.2890, Test Mean Module RMSE: 0.2933\n",
      "Epoch 203 Training Loss (NLML): 1.0967 Train RMSE: 0.1984 Test RMSE: 0.2887, Test Mean Module RMSE: 0.2930\n",
      "Epoch 204 Training Loss (NLML): 1.0967 Train RMSE: 0.1981 Test RMSE: 0.2883, Test Mean Module RMSE: 0.2926\n",
      "Epoch 205 Training Loss (NLML): 1.0967 Train RMSE: 0.1978 Test RMSE: 0.2879, Test Mean Module RMSE: 0.2923\n",
      "Epoch 206 Training Loss (NLML): 1.0966 Train RMSE: 0.1976 Test RMSE: 0.2876, Test Mean Module RMSE: 0.2919\n",
      "Epoch 207 Training Loss (NLML): 1.0966 Train RMSE: 0.1973 Test RMSE: 0.2871, Test Mean Module RMSE: 0.2915\n",
      "Epoch 208 Training Loss (NLML): 1.0965 Train RMSE: 0.1970 Test RMSE: 0.2867, Test Mean Module RMSE: 0.2911\n",
      "Epoch 209 Training Loss (NLML): 1.0965 Train RMSE: 0.1967 Test RMSE: 0.2862, Test Mean Module RMSE: 0.2906\n",
      "Epoch 210 Training Loss (NLML): 1.0965 Train RMSE: 0.1964 Test RMSE: 0.2858, Test Mean Module RMSE: 0.2902\n",
      "Epoch 211 Training Loss (NLML): 1.0964 Train RMSE: 0.1961 Test RMSE: 0.2853, Test Mean Module RMSE: 0.2897\n",
      "Epoch 212 Training Loss (NLML): 1.0964 Train RMSE: 0.1958 Test RMSE: 0.2847, Test Mean Module RMSE: 0.2892\n",
      "Epoch 213 Training Loss (NLML): 1.0963 Train RMSE: 0.1955 Test RMSE: 0.2842, Test Mean Module RMSE: 0.2887\n",
      "Epoch 214 Training Loss (NLML): 1.0963 Train RMSE: 0.1952 Test RMSE: 0.2837, Test Mean Module RMSE: 0.2882\n",
      "Epoch 215 Training Loss (NLML): 1.0963 Train RMSE: 0.1949 Test RMSE: 0.2831, Test Mean Module RMSE: 0.2877\n",
      "Epoch 216 Training Loss (NLML): 1.0962 Train RMSE: 0.1946 Test RMSE: 0.2825, Test Mean Module RMSE: 0.2871\n",
      "Epoch 217 Training Loss (NLML): 1.0962 Train RMSE: 0.1943 Test RMSE: 0.2819, Test Mean Module RMSE: 0.2865\n",
      "Epoch 218 Training Loss (NLML): 1.0961 Train RMSE: 0.1940 Test RMSE: 0.2813, Test Mean Module RMSE: 0.2859\n",
      "Epoch 219 Training Loss (NLML): 1.0961 Train RMSE: 0.1936 Test RMSE: 0.2806, Test Mean Module RMSE: 0.2853\n",
      "Epoch 220 Training Loss (NLML): 1.0960 Train RMSE: 0.1933 Test RMSE: 0.2799, Test Mean Module RMSE: 0.2846\n",
      "Epoch 221 Training Loss (NLML): 1.0960 Train RMSE: 0.1930 Test RMSE: 0.2791, Test Mean Module RMSE: 0.2839\n",
      "Epoch 222 Training Loss (NLML): 1.0959 Train RMSE: 0.1927 Test RMSE: 0.2783, Test Mean Module RMSE: 0.2831\n",
      "Epoch 223 Training Loss (NLML): 1.0959 Train RMSE: 0.1923 Test RMSE: 0.2774, Test Mean Module RMSE: 0.2823\n",
      "Epoch 224 Training Loss (NLML): 1.0959 Train RMSE: 0.1920 Test RMSE: 0.2766, Test Mean Module RMSE: 0.2815\n",
      "Epoch 225 Training Loss (NLML): 1.0958 Train RMSE: 0.1917 Test RMSE: 0.2757, Test Mean Module RMSE: 0.2806\n",
      "Epoch 226 Training Loss (NLML): 1.0958 Train RMSE: 0.1913 Test RMSE: 0.2748, Test Mean Module RMSE: 0.2798\n",
      "Epoch 227 Training Loss (NLML): 1.0957 Train RMSE: 0.1910 Test RMSE: 0.2738, Test Mean Module RMSE: 0.2789\n",
      "Epoch 228 Training Loss (NLML): 1.0957 Train RMSE: 0.1907 Test RMSE: 0.2728, Test Mean Module RMSE: 0.2780\n",
      "Epoch 229 Training Loss (NLML): 1.0956 Train RMSE: 0.1903 Test RMSE: 0.2718, Test Mean Module RMSE: 0.2770\n",
      "Epoch 230 Training Loss (NLML): 1.0956 Train RMSE: 0.1900 Test RMSE: 0.2708, Test Mean Module RMSE: 0.2760\n",
      "Epoch 231 Training Loss (NLML): 1.0955 Train RMSE: 0.1896 Test RMSE: 0.2697, Test Mean Module RMSE: 0.2750\n",
      "Epoch 232 Training Loss (NLML): 1.0955 Train RMSE: 0.1893 Test RMSE: 0.2686, Test Mean Module RMSE: 0.2739\n",
      "Epoch 233 Training Loss (NLML): 1.0954 Train RMSE: 0.1889 Test RMSE: 0.2674, Test Mean Module RMSE: 0.2728\n",
      "Epoch 234 Training Loss (NLML): 1.0954 Train RMSE: 0.1886 Test RMSE: 0.2663, Test Mean Module RMSE: 0.2718\n",
      "Epoch 235 Training Loss (NLML): 1.0953 Train RMSE: 0.1882 Test RMSE: 0.2652, Test Mean Module RMSE: 0.2707\n",
      "Epoch 236 Training Loss (NLML): 1.0953 Train RMSE: 0.1879 Test RMSE: 0.2641, Test Mean Module RMSE: 0.2697\n",
      "Epoch 237 Training Loss (NLML): 1.0952 Train RMSE: 0.1875 Test RMSE: 0.2630, Test Mean Module RMSE: 0.2686\n",
      "Epoch 238 Training Loss (NLML): 1.0952 Train RMSE: 0.1871 Test RMSE: 0.2619, Test Mean Module RMSE: 0.2676\n",
      "Epoch 239 Training Loss (NLML): 1.0951 Train RMSE: 0.1868 Test RMSE: 0.2608, Test Mean Module RMSE: 0.2666\n",
      "Epoch 240 Training Loss (NLML): 1.0951 Train RMSE: 0.1864 Test RMSE: 0.2598, Test Mean Module RMSE: 0.2656\n",
      "Epoch 241 Training Loss (NLML): 1.0950 Train RMSE: 0.1860 Test RMSE: 0.2588, Test Mean Module RMSE: 0.2647\n",
      "Epoch 242 Training Loss (NLML): 1.0950 Train RMSE: 0.1856 Test RMSE: 0.2579, Test Mean Module RMSE: 0.2638\n",
      "Epoch 243 Training Loss (NLML): 1.0949 Train RMSE: 0.1852 Test RMSE: 0.2569, Test Mean Module RMSE: 0.2630\n",
      "Epoch 244 Training Loss (NLML): 1.0949 Train RMSE: 0.1848 Test RMSE: 0.2561, Test Mean Module RMSE: 0.2622\n",
      "Epoch 245 Training Loss (NLML): 1.0948 Train RMSE: 0.1844 Test RMSE: 0.2553, Test Mean Module RMSE: 0.2614\n",
      "Epoch 246 Training Loss (NLML): 1.0948 Train RMSE: 0.1840 Test RMSE: 0.2545, Test Mean Module RMSE: 0.2607\n",
      "Epoch 247 Training Loss (NLML): 1.0947 Train RMSE: 0.1836 Test RMSE: 0.2537, Test Mean Module RMSE: 0.2600\n",
      "Epoch 248 Training Loss (NLML): 1.0947 Train RMSE: 0.1832 Test RMSE: 0.2530, Test Mean Module RMSE: 0.2593\n",
      "Epoch 249 Training Loss (NLML): 1.0946 Train RMSE: 0.1828 Test RMSE: 0.2523, Test Mean Module RMSE: 0.2586\n",
      "Epoch 250 Training Loss (NLML): 1.0946 Train RMSE: 0.1824 Test RMSE: 0.2518, Test Mean Module RMSE: 0.2582\n",
      "Epoch 251 Training Loss (NLML): 1.0945 Train RMSE: 0.1820 Test RMSE: 0.2509, Test Mean Module RMSE: 0.2573\n",
      "Epoch 252 Training Loss (NLML): 1.0945 Train RMSE: 0.1816 Test RMSE: 0.2508, Test Mean Module RMSE: 0.2574\n",
      "Epoch 253 Training Loss (NLML): 1.0944 Train RMSE: 0.1814 Test RMSE: 0.2490, Test Mean Module RMSE: 0.2555\n",
      "Epoch 254 Training Loss (NLML): 1.0944 Train RMSE: 0.1819 Test RMSE: 0.2523, Test Mean Module RMSE: 0.2590\n",
      "Epoch 255 Training Loss (NLML): 1.0944 Train RMSE: 0.1868 Test RMSE: 0.2476, Test Mean Module RMSE: 0.2538\n",
      "Epoch 256 Training Loss (NLML): 1.0951 Train RMSE: 0.2098 Test RMSE: 0.2822, Test Mean Module RMSE: 0.2898\n",
      "Epoch 257 Training Loss (NLML): 1.0984 Train RMSE: 0.2427 Test RMSE: 0.2745, Test Mean Module RMSE: 0.2772\n",
      "Epoch 258 Training Loss (NLML): 1.1037 Train RMSE: 0.2217 Test RMSE: 0.3025, Test Mean Module RMSE: 0.3122\n",
      "Epoch 259 Training Loss (NLML): 1.1002 Train RMSE: 0.1903 Test RMSE: 0.2675, Test Mean Module RMSE: 0.2760\n",
      "Epoch 260 Training Loss (NLML): 1.0956 Train RMSE: 0.2266 Test RMSE: 0.2597, Test Mean Module RMSE: 0.2628\n",
      "Epoch 261 Training Loss (NLML): 1.1010 Train RMSE: 0.1858 Test RMSE: 0.2518, Test Mean Module RMSE: 0.2614\n",
      "Epoch 262 Training Loss (NLML): 1.0949 Train RMSE: 0.2088 Test RMSE: 0.2763, Test Mean Module RMSE: 0.2864\n",
      "Epoch 263 Training Loss (NLML): 1.0982 Train RMSE: 0.1926 Test RMSE: 0.2423, Test Mean Module RMSE: 0.2448\n",
      "Epoch 264 Training Loss (NLML): 1.0958 Train RMSE: 0.1973 Test RMSE: 0.2456, Test Mean Module RMSE: 0.2493\n",
      "Epoch 265 Training Loss (NLML): 1.0965 Train RMSE: 0.1938 Test RMSE: 0.2672, Test Mean Module RMSE: 0.2787\n",
      "Epoch 266 Training Loss (NLML): 1.0960 Train RMSE: 0.1936 Test RMSE: 0.2659, Test Mean Module RMSE: 0.2768\n",
      "Epoch 267 Training Loss (NLML): 1.0960 Train RMSE: 0.1895 Test RMSE: 0.2400, Test Mean Module RMSE: 0.2433\n",
      "Epoch 268 Training Loss (NLML): 1.0954 Train RMSE: 0.1896 Test RMSE: 0.2389, Test Mean Module RMSE: 0.2419\n",
      "Epoch 269 Training Loss (NLML): 1.0954 Train RMSE: 0.1884 Test RMSE: 0.2548, Test Mean Module RMSE: 0.2647\n",
      "Epoch 270 Training Loss (NLML): 1.0953 Train RMSE: 0.1854 Test RMSE: 0.2516, Test Mean Module RMSE: 0.2614\n",
      "Epoch 271 Training Loss (NLML): 1.0949 Train RMSE: 0.1863 Test RMSE: 0.2400, Test Mean Module RMSE: 0.2443\n",
      "Epoch 272 Training Loss (NLML): 1.0950 Train RMSE: 0.1835 Test RMSE: 0.2449, Test Mean Module RMSE: 0.2500\n",
      "Epoch 273 Training Loss (NLML): 1.0946 Train RMSE: 0.1852 Test RMSE: 0.2604, Test Mean Module RMSE: 0.2700\n",
      "Epoch 274 Training Loss (NLML): 1.0949 Train RMSE: 0.1812 Test RMSE: 0.2493, Test Mean Module RMSE: 0.2574\n",
      "Epoch 275 Training Loss (NLML): 1.0944 Train RMSE: 0.1849 Test RMSE: 0.2381, Test Mean Module RMSE: 0.2416\n",
      "Epoch 276 Training Loss (NLML): 1.0948 Train RMSE: 0.1794 Test RMSE: 0.2424, Test Mean Module RMSE: 0.2481\n",
      "Epoch 277 Training Loss (NLML): 1.0941 Train RMSE: 0.1834 Test RMSE: 0.2568, Test Mean Module RMSE: 0.2662\n",
      "Epoch 278 Training Loss (NLML): 1.0946 Train RMSE: 0.1779 Test RMSE: 0.2440, Test Mean Module RMSE: 0.2514\n",
      "Epoch 279 Training Loss (NLML): 1.0939 Train RMSE: 0.1816 Test RMSE: 0.2371, Test Mean Module RMSE: 0.2416\n",
      "Epoch 280 Training Loss (NLML): 1.0944 Train RMSE: 0.1770 Test RMSE: 0.2438, Test Mean Module RMSE: 0.2509\n",
      "Epoch 281 Training Loss (NLML): 1.0938 Train RMSE: 0.1800 Test RMSE: 0.2510, Test Mean Module RMSE: 0.2601\n",
      "Epoch 282 Training Loss (NLML): 1.0942 Train RMSE: 0.1765 Test RMSE: 0.2357, Test Mean Module RMSE: 0.2418\n",
      "Epoch 283 Training Loss (NLML): 1.0937 Train RMSE: 0.1782 Test RMSE: 0.2334, Test Mean Module RMSE: 0.2379\n",
      "Epoch 284 Training Loss (NLML): 1.0939 Train RMSE: 0.1763 Test RMSE: 0.2463, Test Mean Module RMSE: 0.2543\n",
      "Epoch 285 Training Loss (NLML): 1.0937 Train RMSE: 0.1769 Test RMSE: 0.2491, Test Mean Module RMSE: 0.2581\n",
      "Epoch 286 Training Loss (NLML): 1.0938 Train RMSE: 0.1753 Test RMSE: 0.2348, Test Mean Module RMSE: 0.2410\n",
      "Epoch 287 Training Loss (NLML): 1.0936 Train RMSE: 0.1748 Test RMSE: 0.2331, Test Mean Module RMSE: 0.2384\n",
      "Epoch 288 Training Loss (NLML): 1.0935 Train RMSE: 0.1746 Test RMSE: 0.2437, Test Mean Module RMSE: 0.2514\n",
      "Epoch 289 Training Loss (NLML): 1.0935 Train RMSE: 0.1729 Test RMSE: 0.2407, Test Mean Module RMSE: 0.2478\n",
      "Epoch 290 Training Loss (NLML): 1.0933 Train RMSE: 0.1735 Test RMSE: 0.2323, Test Mean Module RMSE: 0.2376\n",
      "Epoch 291 Training Loss (NLML): 1.0934 Train RMSE: 0.1712 Test RMSE: 0.2349, Test Mean Module RMSE: 0.2415\n",
      "Epoch 292 Training Loss (NLML): 1.0931 Train RMSE: 0.1721 Test RMSE: 0.2401, Test Mean Module RMSE: 0.2476\n",
      "Epoch 293 Training Loss (NLML): 1.0932 Train RMSE: 0.1701 Test RMSE: 0.2304, Test Mean Module RMSE: 0.2354\n",
      "Epoch 294 Training Loss (NLML): 1.0929 Train RMSE: 0.1700 Test RMSE: 0.2277, Test Mean Module RMSE: 0.2325\n",
      "Epoch 295 Training Loss (NLML): 1.0929 Train RMSE: 0.1692 Test RMSE: 0.2352, Test Mean Module RMSE: 0.2428\n",
      "Epoch 296 Training Loss (NLML): 1.0928 Train RMSE: 0.1677 Test RMSE: 0.2315, Test Mean Module RMSE: 0.2384\n",
      "Epoch 297 Training Loss (NLML): 1.0927 Train RMSE: 0.1679 Test RMSE: 0.2243, Test Mean Module RMSE: 0.2286\n",
      "Epoch 298 Training Loss (NLML): 1.0927 Train RMSE: 0.1660 Test RMSE: 0.2276, Test Mean Module RMSE: 0.2334\n",
      "Epoch 299 Training Loss (NLML): 1.0924 Train RMSE: 0.1657 Test RMSE: 0.2293, Test Mean Module RMSE: 0.2359\n",
      "Epoch 300 Training Loss (NLML): 1.0924 Train RMSE: 0.1646 Test RMSE: 0.2226, Test Mean Module RMSE: 0.2272\n",
      "Epoch 301 Training Loss (NLML): 1.0923 Train RMSE: 0.1632 Test RMSE: 0.2227, Test Mean Module RMSE: 0.2279\n",
      "Epoch 302 Training Loss (NLML): 1.0921 Train RMSE: 0.1627 Test RMSE: 0.2250, Test Mean Module RMSE: 0.2314\n",
      "Epoch 303 Training Loss (NLML): 1.0921 Train RMSE: 0.1613 Test RMSE: 0.2190, Test Mean Module RMSE: 0.2237\n",
      "Epoch 304 Training Loss (NLML): 1.0919 Train RMSE: 0.1601 Test RMSE: 0.2181, Test Mean Module RMSE: 0.2226\n",
      "Epoch 305 Training Loss (NLML): 1.0918 Train RMSE: 0.1592 Test RMSE: 0.2205, Test Mean Module RMSE: 0.2265\n",
      "Epoch 306 Training Loss (NLML): 1.0916 Train RMSE: 0.1578 Test RMSE: 0.2171, Test Mean Module RMSE: 0.2222\n",
      "Epoch 307 Training Loss (NLML): 1.0915 Train RMSE: 0.1561 Test RMSE: 0.2171, Test Mean Module RMSE: 0.2220\n",
      "Epoch 308 Training Loss (NLML): 1.0913 Train RMSE: 0.1550 Test RMSE: 0.2175, Test Mean Module RMSE: 0.2222\n",
      "Epoch 309 Training Loss (NLML): 1.0912 Train RMSE: 0.1538 Test RMSE: 0.2163, Test Mean Module RMSE: 0.2200\n",
      "Epoch 310 Training Loss (NLML): 1.0910 Train RMSE: 0.1529 Test RMSE: 0.2182, Test Mean Module RMSE: 0.2240\n",
      "Epoch 311 Training Loss (NLML): 1.0909 Train RMSE: 0.1515 Test RMSE: 0.2162, Test Mean Module RMSE: 0.2194\n",
      "Epoch 312 Training Loss (NLML): 1.0908 Train RMSE: 0.1508 Test RMSE: 0.2175, Test Mean Module RMSE: 0.2228\n",
      "Epoch 313 Training Loss (NLML): 1.0907 Train RMSE: 0.1494 Test RMSE: 0.2182, Test Mean Module RMSE: 0.2207\n",
      "Epoch 314 Training Loss (NLML): 1.0906 Train RMSE: 0.1498 Test RMSE: 0.2178, Test Mean Module RMSE: 0.2237\n",
      "Epoch 315 Training Loss (NLML): 1.0906 Train RMSE: 0.1480 Test RMSE: 0.2182, Test Mean Module RMSE: 0.2201\n",
      "Epoch 316 Training Loss (NLML): 1.0904 Train RMSE: 0.1476 Test RMSE: 0.2168, Test Mean Module RMSE: 0.2226\n",
      "Epoch 317 Training Loss (NLML): 1.0904 Train RMSE: 0.1449 Test RMSE: 0.2188, Test Mean Module RMSE: 0.2210\n",
      "Epoch 318 Training Loss (NLML): 1.0901 Train RMSE: 0.1434 Test RMSE: 0.2150, Test Mean Module RMSE: 0.2195\n",
      "Epoch 319 Training Loss (NLML): 1.0899 Train RMSE: 0.1419 Test RMSE: 0.2166, Test Mean Module RMSE: 0.2195\n",
      "Epoch 320 Training Loss (NLML): 1.0898 Train RMSE: 0.1408 Test RMSE: 0.2156, Test Mean Module RMSE: 0.2195\n",
      "Epoch 321 Training Loss (NLML): 1.0897 Train RMSE: 0.1401 Test RMSE: 0.2156, Test Mean Module RMSE: 0.2194\n",
      "Epoch 322 Training Loss (NLML): 1.0896 Train RMSE: 0.1396 Test RMSE: 0.2158, Test Mean Module RMSE: 0.2187\n",
      "Epoch 323 Training Loss (NLML): 1.0895 Train RMSE: 0.1395 Test RMSE: 0.2121, Test Mean Module RMSE: 0.2164\n",
      "Epoch 324 Training Loss (NLML): 1.0895 Train RMSE: 0.1401 Test RMSE: 0.2193, Test Mean Module RMSE: 0.2213\n",
      "Epoch 325 Training Loss (NLML): 1.0896 Train RMSE: 0.1444 Test RMSE: 0.2124, Test Mean Module RMSE: 0.2187\n",
      "Epoch 326 Training Loss (NLML): 1.0900 Train RMSE: 0.1449 Test RMSE: 0.2228, Test Mean Module RMSE: 0.2226\n",
      "Epoch 327 Training Loss (NLML): 1.0901 Train RMSE: 0.1487 Test RMSE: 0.2153, Test Mean Module RMSE: 0.2226\n",
      "Epoch 328 Training Loss (NLML): 1.0905 Train RMSE: 0.1392 Test RMSE: 0.2112, Test Mean Module RMSE: 0.2121\n",
      "Epoch 329 Training Loss (NLML): 1.0895 Train RMSE: 0.1368 Test RMSE: 0.2136, Test Mean Module RMSE: 0.2163\n",
      "Epoch 330 Training Loss (NLML): 1.0893 Train RMSE: 0.1434 Test RMSE: 0.2140, Test Mean Module RMSE: 0.2212\n",
      "Epoch 331 Training Loss (NLML): 1.0899 Train RMSE: 0.1451 Test RMSE: 0.2172, Test Mean Module RMSE: 0.2160\n",
      "Epoch 332 Training Loss (NLML): 1.0901 Train RMSE: 0.1430 Test RMSE: 0.2078, Test Mean Module RMSE: 0.2152\n",
      "Epoch 333 Training Loss (NLML): 1.0899 Train RMSE: 0.1344 Test RMSE: 0.2006, Test Mean Module RMSE: 0.2045\n",
      "Epoch 334 Training Loss (NLML): 1.0890 Train RMSE: 0.1394 Test RMSE: 0.2083, Test Mean Module RMSE: 0.2093\n",
      "Epoch 335 Training Loss (NLML): 1.0895 Train RMSE: 0.1496 Test RMSE: 0.2123, Test Mean Module RMSE: 0.2208\n",
      "Epoch 336 Training Loss (NLML): 1.0906 Train RMSE: 0.1392 Test RMSE: 0.1977, Test Mean Module RMSE: 0.1978\n",
      "Epoch 337 Training Loss (NLML): 1.0895 Train RMSE: 0.1334 Test RMSE: 0.2018, Test Mean Module RMSE: 0.2053\n",
      "Epoch 338 Training Loss (NLML): 1.0889 Train RMSE: 0.1441 Test RMSE: 0.2106, Test Mean Module RMSE: 0.2197\n",
      "Epoch 339 Training Loss (NLML): 1.0900 Train RMSE: 0.1422 Test RMSE: 0.1986, Test Mean Module RMSE: 0.1959\n",
      "Epoch 340 Training Loss (NLML): 1.0898 Train RMSE: 0.1325 Test RMSE: 0.1917, Test Mean Module RMSE: 0.1977\n",
      "Epoch 341 Training Loss (NLML): 1.0888 Train RMSE: 0.1334 Test RMSE: 0.1960, Test Mean Module RMSE: 0.2043\n",
      "Epoch 342 Training Loss (NLML): 1.0889 Train RMSE: 0.1406 Test RMSE: 0.1995, Test Mean Module RMSE: 0.1982\n",
      "Epoch 343 Training Loss (NLML): 1.0896 Train RMSE: 0.1431 Test RMSE: 0.1991, Test Mean Module RMSE: 0.2066\n",
      "Epoch 344 Training Loss (NLML): 1.0899 Train RMSE: 0.1309 Test RMSE: 0.1827, Test Mean Module RMSE: 0.1871\n",
      "Epoch 345 Training Loss (NLML): 1.0887 Train RMSE: 0.1421 Test RMSE: 0.1985, Test Mean Module RMSE: 0.1989\n",
      "Epoch 346 Training Loss (NLML): 1.0898 Train RMSE: 0.1506 Test RMSE: 0.2091, Test Mean Module RMSE: 0.2196\n",
      "Epoch 347 Training Loss (NLML): 1.0906 Train RMSE: 0.1343 Test RMSE: 0.1802, Test Mean Module RMSE: 0.1786\n",
      "Epoch 348 Training Loss (NLML): 1.0890 Train RMSE: 0.1348 Test RMSE: 0.1867, Test Mean Module RMSE: 0.1873\n",
      "Epoch 349 Training Loss (NLML): 1.0891 Train RMSE: 0.1511 Test RMSE: 0.2082, Test Mean Module RMSE: 0.2211\n",
      "Epoch 350 Training Loss (NLML): 1.0906 Train RMSE: 0.1287 Test RMSE: 0.1786, Test Mean Module RMSE: 0.1778\n",
      "Epoch 351 Training Loss (NLML): 1.0885 Train RMSE: 0.1333 Test RMSE: 0.1757, Test Mean Module RMSE: 0.1742\n",
      "Epoch 352 Training Loss (NLML): 1.0889 Train RMSE: 0.1394 Test RMSE: 0.1935, Test Mean Module RMSE: 0.2057\n",
      "Epoch 353 Training Loss (NLML): 1.0895 Train RMSE: 0.1264 Test RMSE: 0.1797, Test Mean Module RMSE: 0.1833\n",
      "Epoch 354 Training Loss (NLML): 1.0883 Train RMSE: 0.1275 Test RMSE: 0.1745, Test Mean Module RMSE: 0.1732\n",
      "Epoch 355 Training Loss (NLML): 1.0884 Train RMSE: 0.1335 Test RMSE: 0.1834, Test Mean Module RMSE: 0.1919\n",
      "Epoch 356 Training Loss (NLML): 1.0889 Train RMSE: 0.1225 Test RMSE: 0.1709, Test Mean Module RMSE: 0.1760\n",
      "Epoch 357 Training Loss (NLML): 1.0879 Train RMSE: 0.1293 Test RMSE: 0.1791, Test Mean Module RMSE: 0.1792\n",
      "Epoch 358 Training Loss (NLML): 1.0885 Train RMSE: 0.1359 Test RMSE: 0.1863, Test Mean Module RMSE: 0.1946\n",
      "Epoch 359 Training Loss (NLML): 1.0892 Train RMSE: 0.1226 Test RMSE: 0.1658, Test Mean Module RMSE: 0.1681\n",
      "Epoch 360 Training Loss (NLML): 1.0879 Train RMSE: 0.1301 Test RMSE: 0.1759, Test Mean Module RMSE: 0.1766\n",
      "Epoch 361 Training Loss (NLML): 1.0886 Train RMSE: 0.1379 Test RMSE: 0.1892, Test Mean Module RMSE: 0.1997\n",
      "Epoch 362 Training Loss (NLML): 1.0893 Train RMSE: 0.1206 Test RMSE: 0.1667, Test Mean Module RMSE: 0.1676\n",
      "Epoch 363 Training Loss (NLML): 1.0878 Train RMSE: 0.1302 Test RMSE: 0.1724, Test Mean Module RMSE: 0.1718\n",
      "Epoch 364 Training Loss (NLML): 1.0886 Train RMSE: 0.1386 Test RMSE: 0.1893, Test Mean Module RMSE: 0.2017\n",
      "Epoch 365 Training Loss (NLML): 1.0894 Train RMSE: 0.1184 Test RMSE: 0.1655, Test Mean Module RMSE: 0.1679\n",
      "Epoch 366 Training Loss (NLML): 1.0876 Train RMSE: 0.1322 Test RMSE: 0.1743, Test Mean Module RMSE: 0.1711\n",
      "Epoch 367 Training Loss (NLML): 1.0888 Train RMSE: 0.1417 Test RMSE: 0.1911, Test Mean Module RMSE: 0.2036\n",
      "Epoch 368 Training Loss (NLML): 1.0897 Train RMSE: 0.1184 Test RMSE: 0.1608, Test Mean Module RMSE: 0.1659\n",
      "Epoch 369 Training Loss (NLML): 1.0876 Train RMSE: 0.1423 Test RMSE: 0.1835, Test Mean Module RMSE: 0.1780\n",
      "Epoch 370 Training Loss (NLML): 1.0898 Train RMSE: 0.1447 Test RMSE: 0.1937, Test Mean Module RMSE: 0.2051\n",
      "Epoch 371 Training Loss (NLML): 1.0900 Train RMSE: 0.1217 Test RMSE: 0.1644, Test Mean Module RMSE: 0.1713\n",
      "Epoch 372 Training Loss (NLML): 1.0879 Train RMSE: 0.1509 Test RMSE: 0.1863, Test Mean Module RMSE: 0.1805\n",
      "Epoch 373 Training Loss (NLML): 1.0907 Train RMSE: 0.1307 Test RMSE: 0.1796, Test Mean Module RMSE: 0.1898\n",
      "Epoch 374 Training Loss (NLML): 1.0887 Train RMSE: 0.1298 Test RMSE: 0.1802, Test Mean Module RMSE: 0.1891\n",
      "Epoch 375 Training Loss (NLML): 1.0886 Train RMSE: 0.1396 Test RMSE: 0.1767, Test Mean Module RMSE: 0.1713\n",
      "Epoch 376 Training Loss (NLML): 1.0895 Train RMSE: 0.1174 Test RMSE: 0.1617, Test Mean Module RMSE: 0.1688\n",
      "Epoch 377 Training Loss (NLML): 1.0875 Train RMSE: 0.1308 Test RMSE: 0.1824, Test Mean Module RMSE: 0.1937\n",
      "Epoch 378 Training Loss (NLML): 1.0887 Train RMSE: 0.1199 Test RMSE: 0.1666, Test Mean Module RMSE: 0.1659\n",
      "Epoch 379 Training Loss (NLML): 1.0877 Train RMSE: 0.1208 Test RMSE: 0.1646, Test Mean Module RMSE: 0.1641\n",
      "Epoch 380 Training Loss (NLML): 1.0878 Train RMSE: 0.1260 Test RMSE: 0.1749, Test Mean Module RMSE: 0.1862\n",
      "Epoch 381 Training Loss (NLML): 1.0882 Train RMSE: 0.1156 Test RMSE: 0.1627, Test Mean Module RMSE: 0.1690\n",
      "Epoch 382 Training Loss (NLML): 1.0873 Train RMSE: 0.1275 Test RMSE: 0.1713, Test Mean Module RMSE: 0.1674\n",
      "Epoch 383 Training Loss (NLML): 1.0883 Train RMSE: 0.1170 Test RMSE: 0.1661, Test Mean Module RMSE: 0.1734\n",
      "Epoch 384 Training Loss (NLML): 1.0875 Train RMSE: 0.1195 Test RMSE: 0.1667, Test Mean Module RMSE: 0.1761\n",
      "Epoch 385 Training Loss (NLML): 1.0877 Train RMSE: 0.1201 Test RMSE: 0.1620, Test Mean Module RMSE: 0.1612\n",
      "Epoch 386 Training Loss (NLML): 1.0877 Train RMSE: 0.1133 Test RMSE: 0.1599, Test Mean Module RMSE: 0.1625\n",
      "Epoch 387 Training Loss (NLML): 1.0871 Train RMSE: 0.1194 Test RMSE: 0.1700, Test Mean Module RMSE: 0.1794\n",
      "Epoch 388 Training Loss (NLML): 1.0876 Train RMSE: 0.1130 Test RMSE: 0.1599, Test Mean Module RMSE: 0.1640\n",
      "Epoch 389 Training Loss (NLML): 1.0871 Train RMSE: 0.1157 Test RMSE: 0.1596, Test Mean Module RMSE: 0.1607\n",
      "Epoch 390 Training Loss (NLML): 1.0873 Train RMSE: 0.1162 Test RMSE: 0.1665, Test Mean Module RMSE: 0.1745\n",
      "Epoch 391 Training Loss (NLML): 1.0874 Train RMSE: 0.1118 Test RMSE: 0.1612, Test Mean Module RMSE: 0.1676\n",
      "Epoch 392 Training Loss (NLML): 1.0870 Train RMSE: 0.1166 Test RMSE: 0.1613, Test Mean Module RMSE: 0.1625\n",
      "Epoch 393 Training Loss (NLML): 1.0874 Train RMSE: 0.1137 Test RMSE: 0.1611, Test Mean Module RMSE: 0.1682\n",
      "Epoch 394 Training Loss (NLML): 1.0872 Train RMSE: 0.1117 Test RMSE: 0.1582, Test Mean Module RMSE: 0.1645\n",
      "Epoch 395 Training Loss (NLML): 1.0870 Train RMSE: 0.1155 Test RMSE: 0.1592, Test Mean Module RMSE: 0.1605\n",
      "Epoch 396 Training Loss (NLML): 1.0873 Train RMSE: 0.1113 Test RMSE: 0.1588, Test Mean Module RMSE: 0.1656\n",
      "Epoch 397 Training Loss (NLML): 1.0870 Train RMSE: 0.1112 Test RMSE: 0.1579, Test Mean Module RMSE: 0.1642\n",
      "Epoch 398 Training Loss (NLML): 1.0870 Train RMSE: 0.1131 Test RMSE: 0.1560, Test Mean Module RMSE: 0.1574\n",
      "Epoch 399 Training Loss (NLML): 1.0871 Train RMSE: 0.1097 Test RMSE: 0.1581, Test Mean Module RMSE: 0.1652\n",
      "Epoch 400 Training Loss (NLML): 1.0869 Train RMSE: 0.1098 Test RMSE: 0.1594, Test Mean Module RMSE: 0.1661\n",
      "Epoch 401 Training Loss (NLML): 1.0869 Train RMSE: 0.1115 Test RMSE: 0.1558, Test Mean Module RMSE: 0.1574\n",
      "Epoch 402 Training Loss (NLML): 1.0870 Train RMSE: 0.1093 Test RMSE: 0.1574, Test Mean Module RMSE: 0.1649\n",
      "Epoch 403 Training Loss (NLML): 1.0868 Train RMSE: 0.1080 Test RMSE: 0.1561, Test Mean Module RMSE: 0.1625\n",
      "Epoch 404 Training Loss (NLML): 1.0867 Train RMSE: 0.1099 Test RMSE: 0.1551, Test Mean Module RMSE: 0.1571\n",
      "Epoch 405 Training Loss (NLML): 1.0869 Train RMSE: 0.1094 Test RMSE: 0.1577, Test Mean Module RMSE: 0.1654\n",
      "Epoch 406 Training Loss (NLML): 1.0868 Train RMSE: 0.1070 Test RMSE: 0.1520, Test Mean Module RMSE: 0.1571\n",
      "Epoch 407 Training Loss (NLML): 1.0866 Train RMSE: 0.1075 Test RMSE: 0.1528, Test Mean Module RMSE: 0.1562\n",
      "Epoch 408 Training Loss (NLML): 1.0867 Train RMSE: 0.1090 Test RMSE: 0.1589, Test Mean Module RMSE: 0.1668\n",
      "Epoch 409 Training Loss (NLML): 1.0868 Train RMSE: 0.1076 Test RMSE: 0.1520, Test Mean Module RMSE: 0.1551\n",
      "Epoch 410 Training Loss (NLML): 1.0867 Train RMSE: 0.1058 Test RMSE: 0.1537, Test Mean Module RMSE: 0.1602\n",
      "Epoch 411 Training Loss (NLML): 1.0866 Train RMSE: 0.1054 Test RMSE: 0.1547, Test Mean Module RMSE: 0.1607\n",
      "Epoch 412 Training Loss (NLML): 1.0865 Train RMSE: 0.1063 Test RMSE: 0.1528, Test Mean Module RMSE: 0.1563\n",
      "Epoch 413 Training Loss (NLML): 1.0866 Train RMSE: 0.1076 Test RMSE: 0.1579, Test Mean Module RMSE: 0.1664\n",
      "Epoch 414 Training Loss (NLML): 1.0867 Train RMSE: 0.1078 Test RMSE: 0.1516, Test Mean Module RMSE: 0.1535\n",
      "Epoch 415 Training Loss (NLML): 1.0867 Train RMSE: 0.1079 Test RMSE: 0.1588, Test Mean Module RMSE: 0.1676\n",
      "Epoch 416 Training Loss (NLML): 1.0867 Train RMSE: 0.1069 Test RMSE: 0.1507, Test Mean Module RMSE: 0.1529\n",
      "Epoch 417 Training Loss (NLML): 1.0866 Train RMSE: 0.1062 Test RMSE: 0.1557, Test Mean Module RMSE: 0.1640\n",
      "Epoch 418 Training Loss (NLML): 1.0866 Train RMSE: 0.1053 Test RMSE: 0.1503, Test Mean Module RMSE: 0.1529\n",
      "Epoch 419 Training Loss (NLML): 1.0865 Train RMSE: 0.1051 Test RMSE: 0.1564, Test Mean Module RMSE: 0.1647\n",
      "Epoch 420 Training Loss (NLML): 1.0865 Train RMSE: 0.1053 Test RMSE: 0.1503, Test Mean Module RMSE: 0.1525\n",
      "Epoch 421 Training Loss (NLML): 1.0865 Train RMSE: 0.1071 Test RMSE: 0.1598, Test Mean Module RMSE: 0.1690\n",
      "Epoch 422 Training Loss (NLML): 1.0867 Train RMSE: 0.1113 Test RMSE: 0.1537, Test Mean Module RMSE: 0.1533\n",
      "Epoch 423 Training Loss (NLML): 1.0870 Train RMSE: 0.1235 Test RMSE: 0.1776, Test Mean Module RMSE: 0.1901\n",
      "Epoch 424 Training Loss (NLML): 1.0880 Train RMSE: 0.1371 Test RMSE: 0.1686, Test Mean Module RMSE: 0.1613\n",
      "Epoch 425 Training Loss (NLML): 1.0892 Train RMSE: 0.1555 Test RMSE: 0.2053, Test Mean Module RMSE: 0.2218\n",
      "Epoch 426 Training Loss (NLML): 1.0911 Train RMSE: 0.1256 Test RMSE: 0.1548, Test Mean Module RMSE: 0.1469\n",
      "Epoch 427 Training Loss (NLML): 1.0881 Train RMSE: 0.1115 Test RMSE: 0.1490, Test Mean Module RMSE: 0.1559\n",
      "Epoch 428 Training Loss (NLML): 1.0870 Train RMSE: 0.1287 Test RMSE: 0.1789, Test Mean Module RMSE: 0.1923\n",
      "Epoch 429 Training Loss (NLML): 1.0884 Train RMSE: 0.1242 Test RMSE: 0.1594, Test Mean Module RMSE: 0.1520\n",
      "Epoch 430 Training Loss (NLML): 1.0880 Train RMSE: 0.1082 Test RMSE: 0.1564, Test Mean Module RMSE: 0.1669\n",
      "Epoch 431 Training Loss (NLML): 1.0867 Train RMSE: 0.1140 Test RMSE: 0.1732, Test Mean Module RMSE: 0.1841\n",
      "Epoch 432 Training Loss (NLML): 1.0872 Train RMSE: 0.1194 Test RMSE: 0.1631, Test Mean Module RMSE: 0.1572\n",
      "Epoch 433 Training Loss (NLML): 1.0876 Train RMSE: 0.1061 Test RMSE: 0.1580, Test Mean Module RMSE: 0.1682\n",
      "Epoch 434 Training Loss (NLML): 1.0866 Train RMSE: 0.1075 Test RMSE: 0.1621, Test Mean Module RMSE: 0.1725\n",
      "Epoch 435 Training Loss (NLML): 1.0867 Train RMSE: 0.1129 Test RMSE: 0.1636, Test Mean Module RMSE: 0.1592\n",
      "Epoch 436 Training Loss (NLML): 1.0871 Train RMSE: 0.1066 Test RMSE: 0.1565, Test Mean Module RMSE: 0.1628\n",
      "Epoch 437 Training Loss (NLML): 1.0866 Train RMSE: 0.1067 Test RMSE: 0.1523, Test Mean Module RMSE: 0.1626\n",
      "Epoch 438 Training Loss (NLML): 1.0866 Train RMSE: 0.1082 Test RMSE: 0.1556, Test Mean Module RMSE: 0.1552\n",
      "Epoch 439 Training Loss (NLML): 1.0867 Train RMSE: 0.1086 Test RMSE: 0.1650, Test Mean Module RMSE: 0.1691\n",
      "Epoch 440 Training Loss (NLML): 1.0868 Train RMSE: 0.1020 Test RMSE: 0.1526, Test Mean Module RMSE: 0.1602\n",
      "Epoch 441 Training Loss (NLML): 1.0863 Train RMSE: 0.1081 Test RMSE: 0.1535, Test Mean Module RMSE: 0.1583\n",
      "Epoch 442 Training Loss (NLML): 1.0867 Train RMSE: 0.1084 Test RMSE: 0.1641, Test Mean Module RMSE: 0.1701\n",
      "Epoch 443 Training Loss (NLML): 1.0868 Train RMSE: 0.1019 Test RMSE: 0.1528, Test Mean Module RMSE: 0.1560\n",
      "Epoch 444 Training Loss (NLML): 1.0863 Train RMSE: 0.1075 Test RMSE: 0.1510, Test Mean Module RMSE: 0.1561\n",
      "Epoch 445 Training Loss (NLML): 1.0867 Train RMSE: 0.1067 Test RMSE: 0.1600, Test Mean Module RMSE: 0.1697\n",
      "Epoch 446 Training Loss (NLML): 1.0866 Train RMSE: 0.1016 Test RMSE: 0.1546, Test Mean Module RMSE: 0.1561\n",
      "Epoch 447 Training Loss (NLML): 1.0862 Train RMSE: 0.1048 Test RMSE: 0.1525, Test Mean Module RMSE: 0.1545\n",
      "Epoch 448 Training Loss (NLML): 1.0865 Train RMSE: 0.1081 Test RMSE: 0.1598, Test Mean Module RMSE: 0.1709\n",
      "Epoch 449 Training Loss (NLML): 1.0867 Train RMSE: 0.1008 Test RMSE: 0.1466, Test Mean Module RMSE: 0.1482\n",
      "Epoch 450 Training Loss (NLML): 1.0862 Train RMSE: 0.1024 Test RMSE: 0.1498, Test Mean Module RMSE: 0.1505\n",
      "Epoch 451 Training Loss (NLML): 1.0863 Train RMSE: 0.1079 Test RMSE: 0.1630, Test Mean Module RMSE: 0.1739\n",
      "Epoch 452 Training Loss (NLML): 1.0867 Train RMSE: 0.1008 Test RMSE: 0.1471, Test Mean Module RMSE: 0.1495\n",
      "Epoch 453 Training Loss (NLML): 1.0862 Train RMSE: 0.0985 Test RMSE: 0.1475, Test Mean Module RMSE: 0.1502\n",
      "Epoch 454 Training Loss (NLML): 1.0860 Train RMSE: 0.1029 Test RMSE: 0.1584, Test Mean Module RMSE: 0.1680\n",
      "Epoch 455 Training Loss (NLML): 1.0863 Train RMSE: 0.1009 Test RMSE: 0.1487, Test Mean Module RMSE: 0.1514\n",
      "Epoch 456 Training Loss (NLML): 1.0862 Train RMSE: 0.0980 Test RMSE: 0.1500, Test Mean Module RMSE: 0.1551\n",
      "Epoch 457 Training Loss (NLML): 1.0860 Train RMSE: 0.0970 Test RMSE: 0.1512, Test Mean Module RMSE: 0.1587\n",
      "Epoch 458 Training Loss (NLML): 1.0859 Train RMSE: 0.1001 Test RMSE: 0.1495, Test Mean Module RMSE: 0.1527\n",
      "Epoch 459 Training Loss (NLML): 1.0861 Train RMSE: 0.1010 Test RMSE: 0.1566, Test Mean Module RMSE: 0.1634\n",
      "Epoch 460 Training Loss (NLML): 1.0862 Train RMSE: 0.0975 Test RMSE: 0.1441, Test Mean Module RMSE: 0.1469\n",
      "Epoch 461 Training Loss (NLML): 1.0859 Train RMSE: 0.0953 Test RMSE: 0.1474, Test Mean Module RMSE: 0.1543\n",
      "Epoch 462 Training Loss (NLML): 1.0858 Train RMSE: 0.0957 Test RMSE: 0.1521, Test Mean Module RMSE: 0.1576\n",
      "Epoch 463 Training Loss (NLML): 1.0858 Train RMSE: 0.0979 Test RMSE: 0.1463, Test Mean Module RMSE: 0.1480\n",
      "Epoch 464 Training Loss (NLML): 1.0860 Train RMSE: 0.1005 Test RMSE: 0.1560, Test Mean Module RMSE: 0.1666\n",
      "Epoch 465 Training Loss (NLML): 1.0861 Train RMSE: 0.1000 Test RMSE: 0.1467, Test Mean Module RMSE: 0.1469\n",
      "Epoch 466 Training Loss (NLML): 1.0861 Train RMSE: 0.0984 Test RMSE: 0.1546, Test Mean Module RMSE: 0.1625\n",
      "Epoch 467 Training Loss (NLML): 1.0860 Train RMSE: 0.0953 Test RMSE: 0.1431, Test Mean Module RMSE: 0.1474\n",
      "Epoch 468 Training Loss (NLML): 1.0858 Train RMSE: 0.0929 Test RMSE: 0.1491, Test Mean Module RMSE: 0.1556\n",
      "Epoch 469 Training Loss (NLML): 1.0856 Train RMSE: 0.0918 Test RMSE: 0.1465, Test Mean Module RMSE: 0.1504\n",
      "Epoch 470 Training Loss (NLML): 1.0856 Train RMSE: 0.0914 Test RMSE: 0.1446, Test Mean Module RMSE: 0.1507\n",
      "Epoch 471 Training Loss (NLML): 1.0855 Train RMSE: 0.0911 Test RMSE: 0.1468, Test Mean Module RMSE: 0.1525\n",
      "Epoch 472 Training Loss (NLML): 1.0855 Train RMSE: 0.0917 Test RMSE: 0.1450, Test Mean Module RMSE: 0.1476\n",
      "Epoch 473 Training Loss (NLML): 1.0856 Train RMSE: 0.0950 Test RMSE: 0.1527, Test Mean Module RMSE: 0.1620\n",
      "Epoch 474 Training Loss (NLML): 1.0858 Train RMSE: 0.1053 Test RMSE: 0.1480, Test Mean Module RMSE: 0.1461\n",
      "Epoch 475 Training Loss (NLML): 1.0865 Train RMSE: 0.1385 Test RMSE: 0.1953, Test Mean Module RMSE: 0.2081\n",
      "Epoch 476 Training Loss (NLML): 1.0894 Train RMSE: 0.1750 Test RMSE: 0.1984, Test Mean Module RMSE: 0.1878\n",
      "Epoch 477 Training Loss (NLML): 1.0934 Train RMSE: 0.2038 Test RMSE: 0.2563, Test Mean Module RMSE: 0.2735\n",
      "Epoch 478 Training Loss (NLML): 1.0972 Train RMSE: 0.1349 Test RMSE: 0.1590, Test Mean Module RMSE: 0.1552\n",
      "Epoch 479 Training Loss (NLML): 1.0890 Train RMSE: 0.1800 Test RMSE: 0.2049, Test Mean Module RMSE: 0.2033\n",
      "Epoch 480 Training Loss (NLML): 1.0942 Train RMSE: 0.1484 Test RMSE: 0.1967, Test Mean Module RMSE: 0.2117\n",
      "Epoch 481 Training Loss (NLML): 1.0904 Train RMSE: 0.1513 Test RMSE: 0.1926, Test Mean Module RMSE: 0.1964\n",
      "Epoch 482 Training Loss (NLML): 1.0908 Train RMSE: 0.1387 Test RMSE: 0.1660, Test Mean Module RMSE: 0.1658\n",
      "Epoch 483 Training Loss (NLML): 1.0894 Train RMSE: 0.1271 Test RMSE: 0.1692, Test Mean Module RMSE: 0.1745\n",
      "Epoch 484 Training Loss (NLML): 1.0883 Train RMSE: 0.1382 Test RMSE: 0.2026, Test Mean Module RMSE: 0.2047\n",
      "Epoch 485 Training Loss (NLML): 1.0894 Train RMSE: 0.1111 Test RMSE: 0.1679, Test Mean Module RMSE: 0.1757\n",
      "Epoch 486 Training Loss (NLML): 1.0870 Train RMSE: 0.1415 Test RMSE: 0.1838, Test Mean Module RMSE: 0.1893\n",
      "Epoch 487 Training Loss (NLML): 1.0897 Train RMSE: 0.1074 Test RMSE: 0.1749, Test Mean Module RMSE: 0.1792\n",
      "Epoch 488 Training Loss (NLML): 1.0867 Train RMSE: 0.1296 Test RMSE: 0.1908, Test Mean Module RMSE: 0.1925\n",
      "Epoch 489 Training Loss (NLML): 1.0886 Train RMSE: 0.1040 Test RMSE: 0.1521, Test Mean Module RMSE: 0.1580\n",
      "Epoch 490 Training Loss (NLML): 1.0864 Train RMSE: 0.1245 Test RMSE: 0.1652, Test Mean Module RMSE: 0.1710\n",
      "Epoch 491 Training Loss (NLML): 1.0881 Train RMSE: 0.1037 Test RMSE: 0.1612, Test Mean Module RMSE: 0.1653\n",
      "Epoch 492 Training Loss (NLML): 1.0864 Train RMSE: 0.1198 Test RMSE: 0.1785, Test Mean Module RMSE: 0.1808\n",
      "Epoch 493 Training Loss (NLML): 1.0877 Train RMSE: 0.1027 Test RMSE: 0.1576, Test Mean Module RMSE: 0.1604\n",
      "Epoch 494 Training Loss (NLML): 1.0863 Train RMSE: 0.1126 Test RMSE: 0.1543, Test Mean Module RMSE: 0.1596\n",
      "Epoch 495 Training Loss (NLML): 1.0871 Train RMSE: 0.1042 Test RMSE: 0.1518, Test Mean Module RMSE: 0.1580\n",
      "Epoch 496 Training Loss (NLML): 1.0864 Train RMSE: 0.1090 Test RMSE: 0.1619, Test Mean Module RMSE: 0.1649\n",
      "Epoch 497 Training Loss (NLML): 1.0868 Train RMSE: 0.0989 Test RMSE: 0.1559, Test Mean Module RMSE: 0.1583\n",
      "Epoch 498 Training Loss (NLML): 1.0860 Train RMSE: 0.1088 Test RMSE: 0.1625, Test Mean Module RMSE: 0.1671\n",
      "Epoch 499 Training Loss (NLML): 1.0868 Train RMSE: 0.1005 Test RMSE: 0.1596, Test Mean Module RMSE: 0.1671\n",
      "Epoch 500 Training Loss (NLML): 1.0862 Train RMSE: 0.1050 Test RMSE: 0.1600, Test Mean Module RMSE: 0.1641\n",
      "Epoch 501 Training Loss (NLML): 1.0865 Train RMSE: 0.0985 Test RMSE: 0.1481, Test Mean Module RMSE: 0.1498\n",
      "Epoch 502 Training Loss (NLML): 1.0860 Train RMSE: 0.1026 Test RMSE: 0.1513, Test Mean Module RMSE: 0.1566\n",
      "Epoch 503 Training Loss (NLML): 1.0863 Train RMSE: 0.0982 Test RMSE: 0.1556, Test Mean Module RMSE: 0.1630\n",
      "Epoch 504 Training Loss (NLML): 1.0860 Train RMSE: 0.0991 Test RMSE: 0.1541, Test Mean Module RMSE: 0.1570\n",
      "Epoch 505 Training Loss (NLML): 1.0861 Train RMSE: 0.0982 Test RMSE: 0.1465, Test Mean Module RMSE: 0.1476\n",
      "Epoch 506 Training Loss (NLML): 1.0860 Train RMSE: 0.0959 Test RMSE: 0.1459, Test Mean Module RMSE: 0.1530\n",
      "Epoch 507 Training Loss (NLML): 1.0858 Train RMSE: 0.0967 Test RMSE: 0.1506, Test Mean Module RMSE: 0.1588\n",
      "Epoch 508 Training Loss (NLML): 1.0859 Train RMSE: 0.0951 Test RMSE: 0.1483, Test Mean Module RMSE: 0.1499\n",
      "Epoch 509 Training Loss (NLML): 1.0858 Train RMSE: 0.0946 Test RMSE: 0.1433, Test Mean Module RMSE: 0.1442\n",
      "Epoch 510 Training Loss (NLML): 1.0857 Train RMSE: 0.0933 Test RMSE: 0.1453, Test Mean Module RMSE: 0.1542\n",
      "Epoch 511 Training Loss (NLML): 1.0857 Train RMSE: 0.0932 Test RMSE: 0.1505, Test Mean Module RMSE: 0.1592\n",
      "Epoch 512 Training Loss (NLML): 1.0856 Train RMSE: 0.0923 Test RMSE: 0.1501, Test Mean Module RMSE: 0.1509\n",
      "Epoch 513 Training Loss (NLML): 1.0856 Train RMSE: 0.0911 Test RMSE: 0.1473, Test Mean Module RMSE: 0.1490\n",
      "Epoch 514 Training Loss (NLML): 1.0855 Train RMSE: 0.0916 Test RMSE: 0.1471, Test Mean Module RMSE: 0.1562\n",
      "Epoch 515 Training Loss (NLML): 1.0855 Train RMSE: 0.0891 Test RMSE: 0.1431, Test Mean Module RMSE: 0.1503\n",
      "Epoch 516 Training Loss (NLML): 1.0854 Train RMSE: 0.0899 Test RMSE: 0.1434, Test Mean Module RMSE: 0.1444\n",
      "Epoch 517 Training Loss (NLML): 1.0854 Train RMSE: 0.0880 Test RMSE: 0.1456, Test Mean Module RMSE: 0.1503\n",
      "Epoch 518 Training Loss (NLML): 1.0853 Train RMSE: 0.0883 Test RMSE: 0.1432, Test Mean Module RMSE: 0.1516\n",
      "Epoch 519 Training Loss (NLML): 1.0853 Train RMSE: 0.0872 Test RMSE: 0.1365, Test Mean Module RMSE: 0.1412\n",
      "Epoch 520 Training Loss (NLML): 1.0853 Train RMSE: 0.0866 Test RMSE: 0.1374, Test Mean Module RMSE: 0.1410\n",
      "Epoch 521 Training Loss (NLML): 1.0852 Train RMSE: 0.0860 Test RMSE: 0.1414, Test Mean Module RMSE: 0.1480\n",
      "Epoch 522 Training Loss (NLML): 1.0852 Train RMSE: 0.0855 Test RMSE: 0.1397, Test Mean Module RMSE: 0.1453\n",
      "Epoch 523 Training Loss (NLML): 1.0852 Train RMSE: 0.0843 Test RMSE: 0.1391, Test Mean Module RMSE: 0.1436\n",
      "Epoch 524 Training Loss (NLML): 1.0851 Train RMSE: 0.0843 Test RMSE: 0.1407, Test Mean Module RMSE: 0.1466\n",
      "Epoch 525 Training Loss (NLML): 1.0851 Train RMSE: 0.0835 Test RMSE: 0.1377, Test Mean Module RMSE: 0.1433\n",
      "Epoch 526 Training Loss (NLML): 1.0850 Train RMSE: 0.0824 Test RMSE: 0.1378, Test Mean Module RMSE: 0.1433\n",
      "Epoch 527 Training Loss (NLML): 1.0850 Train RMSE: 0.0824 Test RMSE: 0.1378, Test Mean Module RMSE: 0.1431\n",
      "Epoch 528 Training Loss (NLML): 1.0850 Train RMSE: 0.0816 Test RMSE: 0.1347, Test Mean Module RMSE: 0.1398\n",
      "Epoch 529 Training Loss (NLML): 1.0849 Train RMSE: 0.0809 Test RMSE: 0.1370, Test Mean Module RMSE: 0.1440\n",
      "Epoch 530 Training Loss (NLML): 1.0849 Train RMSE: 0.0801 Test RMSE: 0.1367, Test Mean Module RMSE: 0.1414\n",
      "Epoch 531 Training Loss (NLML): 1.0848 Train RMSE: 0.0799 Test RMSE: 0.1335, Test Mean Module RMSE: 0.1370\n",
      "Epoch 532 Training Loss (NLML): 1.0848 Train RMSE: 0.0796 Test RMSE: 0.1348, Test Mean Module RMSE: 0.1423\n",
      "Epoch 533 Training Loss (NLML): 1.0848 Train RMSE: 0.0785 Test RMSE: 0.1333, Test Mean Module RMSE: 0.1379\n",
      "Epoch 534 Training Loss (NLML): 1.0847 Train RMSE: 0.0780 Test RMSE: 0.1354, Test Mean Module RMSE: 0.1397\n",
      "Epoch 535 Training Loss (NLML): 1.0847 Train RMSE: 0.0771 Test RMSE: 0.1334, Test Mean Module RMSE: 0.1393\n",
      "Epoch 536 Training Loss (NLML): 1.0847 Train RMSE: 0.0768 Test RMSE: 0.1317, Test Mean Module RMSE: 0.1377\n",
      "Epoch 537 Training Loss (NLML): 1.0846 Train RMSE: 0.0764 Test RMSE: 0.1337, Test Mean Module RMSE: 0.1390\n",
      "Epoch 538 Training Loss (NLML): 1.0846 Train RMSE: 0.0762 Test RMSE: 0.1321, Test Mean Module RMSE: 0.1358\n",
      "Epoch 539 Training Loss (NLML): 1.0846 Train RMSE: 0.0766 Test RMSE: 0.1345, Test Mean Module RMSE: 0.1422\n",
      "Epoch 540 Training Loss (NLML): 1.0846 Train RMSE: 0.0779 Test RMSE: 0.1297, Test Mean Module RMSE: 0.1322\n",
      "Epoch 541 Training Loss (NLML): 1.0847 Train RMSE: 0.0829 Test RMSE: 0.1400, Test Mean Module RMSE: 0.1485\n",
      "Epoch 542 Training Loss (NLML): 1.0850 Train RMSE: 0.0956 Test RMSE: 0.1381, Test Mean Module RMSE: 0.1367\n",
      "Epoch 543 Training Loss (NLML): 1.0858 Train RMSE: 0.1288 Test RMSE: 0.1779, Test Mean Module RMSE: 0.1906\n",
      "Epoch 544 Training Loss (NLML): 1.0884 Train RMSE: 0.1636 Test RMSE: 0.1885, Test Mean Module RMSE: 0.1794\n",
      "Epoch 545 Training Loss (NLML): 1.0920 Train RMSE: 0.1953 Test RMSE: 0.2409, Test Mean Module RMSE: 0.2561\n",
      "Epoch 546 Training Loss (NLML): 1.0961 Train RMSE: 0.1143 Test RMSE: 0.1447, Test Mean Module RMSE: 0.1411\n",
      "Epoch 547 Training Loss (NLML): 1.0872 Train RMSE: 0.1471 Test RMSE: 0.1752, Test Mean Module RMSE: 0.1735\n",
      "Epoch 548 Training Loss (NLML): 1.0903 Train RMSE: 0.1720 Test RMSE: 0.2142, Test Mean Module RMSE: 0.2296\n",
      "Epoch 549 Training Loss (NLML): 1.0930 Train RMSE: 0.0923 Test RMSE: 0.1393, Test Mean Module RMSE: 0.1384\n",
      "Epoch 550 Training Loss (NLML): 1.0856 Train RMSE: 0.1705 Test RMSE: 0.1938, Test Mean Module RMSE: 0.1866\n",
      "Epoch 551 Training Loss (NLML): 1.0929 Train RMSE: 0.1498 Test RMSE: 0.1968, Test Mean Module RMSE: 0.2068\n",
      "Epoch 552 Training Loss (NLML): 1.0906 Train RMSE: 0.1314 Test RMSE: 0.1852, Test Mean Module RMSE: 0.1978\n",
      "Epoch 553 Training Loss (NLML): 1.0887 Train RMSE: 0.1375 Test RMSE: 0.1681, Test Mean Module RMSE: 0.1649\n",
      "Epoch 554 Training Loss (NLML): 1.0893 Train RMSE: 0.1096 Test RMSE: 0.1480, Test Mean Module RMSE: 0.1476\n",
      "Epoch 555 Training Loss (NLML): 1.0868 Train RMSE: 0.1339 Test RMSE: 0.1837, Test Mean Module RMSE: 0.1969\n",
      "Epoch 556 Training Loss (NLML): 1.0889 Train RMSE: 0.1032 Test RMSE: 0.1549, Test Mean Module RMSE: 0.1647\n",
      "Epoch 557 Training Loss (NLML): 1.0863 Train RMSE: 0.1265 Test RMSE: 0.1591, Test Mean Module RMSE: 0.1524\n",
      "Epoch 558 Training Loss (NLML): 1.0882 Train RMSE: 0.0932 Test RMSE: 0.1376, Test Mean Module RMSE: 0.1377\n",
      "Epoch 559 Training Loss (NLML): 1.0856 Train RMSE: 0.1171 Test RMSE: 0.1647, Test Mean Module RMSE: 0.1751\n",
      "Epoch 560 Training Loss (NLML): 1.0874 Train RMSE: 0.0946 Test RMSE: 0.1414, Test Mean Module RMSE: 0.1505\n",
      "Epoch 561 Training Loss (NLML): 1.0857 Train RMSE: 0.1072 Test RMSE: 0.1448, Test Mean Module RMSE: 0.1436\n",
      "Epoch 562 Training Loss (NLML): 1.0866 Train RMSE: 0.0939 Test RMSE: 0.1403, Test Mean Module RMSE: 0.1426\n",
      "Epoch 563 Training Loss (NLML): 1.0857 Train RMSE: 0.1011 Test RMSE: 0.1533, Test Mean Module RMSE: 0.1622\n",
      "Epoch 564 Training Loss (NLML): 1.0862 Train RMSE: 0.0961 Test RMSE: 0.1421, Test Mean Module RMSE: 0.1507\n",
      "Epoch 565 Training Loss (NLML): 1.0858 Train RMSE: 0.0927 Test RMSE: 0.1330, Test Mean Module RMSE: 0.1367\n",
      "Epoch 566 Training Loss (NLML): 1.0856 Train RMSE: 0.0962 Test RMSE: 0.1433, Test Mean Module RMSE: 0.1460\n",
      "Epoch 567 Training Loss (NLML): 1.0859 Train RMSE: 0.0898 Test RMSE: 0.1501, Test Mean Module RMSE: 0.1553\n",
      "Epoch 568 Training Loss (NLML): 1.0854 Train RMSE: 0.0946 Test RMSE: 0.1501, Test Mean Module RMSE: 0.1556\n",
      "Epoch 569 Training Loss (NLML): 1.0857 Train RMSE: 0.0888 Test RMSE: 0.1354, Test Mean Module RMSE: 0.1416\n",
      "Epoch 570 Training Loss (NLML): 1.0854 Train RMSE: 0.0901 Test RMSE: 0.1334, Test Mean Module RMSE: 0.1373\n",
      "Epoch 571 Training Loss (NLML): 1.0854 Train RMSE: 0.0872 Test RMSE: 0.1392, Test Mean Module RMSE: 0.1413\n",
      "Epoch 572 Training Loss (NLML): 1.0853 Train RMSE: 0.0888 Test RMSE: 0.1406, Test Mean Module RMSE: 0.1456\n",
      "Epoch 573 Training Loss (NLML): 1.0854 Train RMSE: 0.0864 Test RMSE: 0.1341, Test Mean Module RMSE: 0.1434\n",
      "Epoch 574 Training Loss (NLML): 1.0852 Train RMSE: 0.0851 Test RMSE: 0.1330, Test Mean Module RMSE: 0.1395\n",
      "Epoch 575 Training Loss (NLML): 1.0851 Train RMSE: 0.0848 Test RMSE: 0.1396, Test Mean Module RMSE: 0.1411\n",
      "Epoch 576 Training Loss (NLML): 1.0851 Train RMSE: 0.0837 Test RMSE: 0.1428, Test Mean Module RMSE: 0.1458\n",
      "Epoch 577 Training Loss (NLML): 1.0850 Train RMSE: 0.0826 Test RMSE: 0.1351, Test Mean Module RMSE: 0.1432\n",
      "Epoch 578 Training Loss (NLML): 1.0850 Train RMSE: 0.0823 Test RMSE: 0.1312, Test Mean Module RMSE: 0.1386\n",
      "Epoch 579 Training Loss (NLML): 1.0850 Train RMSE: 0.0811 Test RMSE: 0.1321, Test Mean Module RMSE: 0.1349\n",
      "Epoch 580 Training Loss (NLML): 1.0849 Train RMSE: 0.0801 Test RMSE: 0.1360, Test Mean Module RMSE: 0.1395\n",
      "Epoch 581 Training Loss (NLML): 1.0848 Train RMSE: 0.0793 Test RMSE: 0.1317, Test Mean Module RMSE: 0.1393\n",
      "Epoch 582 Training Loss (NLML): 1.0848 Train RMSE: 0.0793 Test RMSE: 0.1274, Test Mean Module RMSE: 0.1350\n",
      "Epoch 583 Training Loss (NLML): 1.0848 Train RMSE: 0.0774 Test RMSE: 0.1271, Test Mean Module RMSE: 0.1314\n",
      "Epoch 584 Training Loss (NLML): 1.0847 Train RMSE: 0.0770 Test RMSE: 0.1313, Test Mean Module RMSE: 0.1356\n",
      "Epoch 585 Training Loss (NLML): 1.0847 Train RMSE: 0.0763 Test RMSE: 0.1302, Test Mean Module RMSE: 0.1365\n",
      "Epoch 586 Training Loss (NLML): 1.0846 Train RMSE: 0.0756 Test RMSE: 0.1271, Test Mean Module RMSE: 0.1338\n",
      "Epoch 587 Training Loss (NLML): 1.0846 Train RMSE: 0.0747 Test RMSE: 0.1270, Test Mean Module RMSE: 0.1327\n",
      "Epoch 588 Training Loss (NLML): 1.0845 Train RMSE: 0.0742 Test RMSE: 0.1288, Test Mean Module RMSE: 0.1335\n",
      "Epoch 589 Training Loss (NLML): 1.0845 Train RMSE: 0.0732 Test RMSE: 0.1274, Test Mean Module RMSE: 0.1326\n",
      "Epoch 590 Training Loss (NLML): 1.0845 Train RMSE: 0.0726 Test RMSE: 0.1249, Test Mean Module RMSE: 0.1321\n",
      "Epoch 591 Training Loss (NLML): 1.0844 Train RMSE: 0.0721 Test RMSE: 0.1233, Test Mean Module RMSE: 0.1305\n",
      "Epoch 592 Training Loss (NLML): 1.0844 Train RMSE: 0.0712 Test RMSE: 0.1225, Test Mean Module RMSE: 0.1274\n",
      "Epoch 593 Training Loss (NLML): 1.0843 Train RMSE: 0.0705 Test RMSE: 0.1233, Test Mean Module RMSE: 0.1281\n",
      "Epoch 594 Training Loss (NLML): 1.0843 Train RMSE: 0.0703 Test RMSE: 0.1239, Test Mean Module RMSE: 0.1308\n",
      "Epoch 595 Training Loss (NLML): 1.0843 Train RMSE: 0.0692 Test RMSE: 0.1216, Test Mean Module RMSE: 0.1279\n",
      "Epoch 596 Training Loss (NLML): 1.0842 Train RMSE: 0.0692 Test RMSE: 0.1199, Test Mean Module RMSE: 0.1246\n",
      "Epoch 597 Training Loss (NLML): 1.0842 Train RMSE: 0.0680 Test RMSE: 0.1210, Test Mean Module RMSE: 0.1274\n",
      "Epoch 598 Training Loss (NLML): 1.0842 Train RMSE: 0.0681 Test RMSE: 0.1219, Test Mean Module RMSE: 0.1294\n",
      "Epoch 599 Training Loss (NLML): 1.0842 Train RMSE: 0.0670 Test RMSE: 0.1191, Test Mean Module RMSE: 0.1249\n",
      "Epoch 600 Training Loss (NLML): 1.0841 Train RMSE: 0.0670 Test RMSE: 0.1186, Test Mean Module RMSE: 0.1239\n",
      "Epoch 601 Training Loss (NLML): 1.0841 Train RMSE: 0.0661 Test RMSE: 0.1197, Test Mean Module RMSE: 0.1265\n",
      "Epoch 602 Training Loss (NLML): 1.0841 Train RMSE: 0.0659 Test RMSE: 0.1190, Test Mean Module RMSE: 0.1253\n",
      "Epoch 603 Training Loss (NLML): 1.0841 Train RMSE: 0.0652 Test RMSE: 0.1169, Test Mean Module RMSE: 0.1223\n",
      "Epoch 604 Training Loss (NLML): 1.0841 Train RMSE: 0.0650 Test RMSE: 0.1167, Test Mean Module RMSE: 0.1229\n",
      "Epoch 605 Training Loss (NLML): 1.0840 Train RMSE: 0.0642 Test RMSE: 0.1168, Test Mean Module RMSE: 0.1236\n",
      "Epoch 606 Training Loss (NLML): 1.0840 Train RMSE: 0.0642 Test RMSE: 0.1170, Test Mean Module RMSE: 0.1235\n",
      "Epoch 607 Training Loss (NLML): 1.0840 Train RMSE: 0.0634 Test RMSE: 0.1163, Test Mean Module RMSE: 0.1225\n",
      "Epoch 608 Training Loss (NLML): 1.0840 Train RMSE: 0.0631 Test RMSE: 0.1143, Test Mean Module RMSE: 0.1204\n",
      "Epoch 609 Training Loss (NLML): 1.0840 Train RMSE: 0.0627 Test RMSE: 0.1136, Test Mean Module RMSE: 0.1203\n",
      "Epoch 610 Training Loss (NLML): 1.0839 Train RMSE: 0.0623 Test RMSE: 0.1147, Test Mean Module RMSE: 0.1213\n",
      "Epoch 611 Training Loss (NLML): 1.0839 Train RMSE: 0.0618 Test RMSE: 0.1137, Test Mean Module RMSE: 0.1193\n",
      "Epoch 612 Training Loss (NLML): 1.0839 Train RMSE: 0.0615 Test RMSE: 0.1120, Test Mean Module RMSE: 0.1184\n",
      "Epoch 613 Training Loss (NLML): 1.0839 Train RMSE: 0.0610 Test RMSE: 0.1118, Test Mean Module RMSE: 0.1193\n",
      "Epoch 614 Training Loss (NLML): 1.0839 Train RMSE: 0.0606 Test RMSE: 0.1112, Test Mean Module RMSE: 0.1175\n",
      "Epoch 615 Training Loss (NLML): 1.0838 Train RMSE: 0.0602 Test RMSE: 0.1105, Test Mean Module RMSE: 0.1165\n",
      "Epoch 616 Training Loss (NLML): 1.0838 Train RMSE: 0.0598 Test RMSE: 0.1098, Test Mean Module RMSE: 0.1167\n",
      "Epoch 617 Training Loss (NLML): 1.0838 Train RMSE: 0.0594 Test RMSE: 0.1092, Test Mean Module RMSE: 0.1157\n",
      "Epoch 618 Training Loss (NLML): 1.0838 Train RMSE: 0.0590 Test RMSE: 0.1094, Test Mean Module RMSE: 0.1158\n",
      "Epoch 619 Training Loss (NLML): 1.0838 Train RMSE: 0.0586 Test RMSE: 0.1081, Test Mean Module RMSE: 0.1143\n",
      "Epoch 620 Training Loss (NLML): 1.0838 Train RMSE: 0.0583 Test RMSE: 0.1068, Test Mean Module RMSE: 0.1137\n",
      "Epoch 621 Training Loss (NLML): 1.0837 Train RMSE: 0.0578 Test RMSE: 0.1074, Test Mean Module RMSE: 0.1146\n",
      "Epoch 622 Training Loss (NLML): 1.0837 Train RMSE: 0.0575 Test RMSE: 0.1068, Test Mean Module RMSE: 0.1127\n",
      "Epoch 623 Training Loss (NLML): 1.0837 Train RMSE: 0.0570 Test RMSE: 0.1055, Test Mean Module RMSE: 0.1120\n",
      "Epoch 624 Training Loss (NLML): 1.0837 Train RMSE: 0.0567 Test RMSE: 0.1046, Test Mean Module RMSE: 0.1118\n",
      "Epoch 625 Training Loss (NLML): 1.0837 Train RMSE: 0.0563 Test RMSE: 0.1043, Test Mean Module RMSE: 0.1105\n",
      "Epoch 626 Training Loss (NLML): 1.0837 Train RMSE: 0.0559 Test RMSE: 0.1045, Test Mean Module RMSE: 0.1110\n",
      "Epoch 627 Training Loss (NLML): 1.0836 Train RMSE: 0.0556 Test RMSE: 0.1030, Test Mean Module RMSE: 0.1097\n",
      "Epoch 628 Training Loss (NLML): 1.0836 Train RMSE: 0.0552 Test RMSE: 0.1025, Test Mean Module RMSE: 0.1095\n",
      "Epoch 629 Training Loss (NLML): 1.0836 Train RMSE: 0.0548 Test RMSE: 0.1025, Test Mean Module RMSE: 0.1090\n",
      "Epoch 630 Training Loss (NLML): 1.0836 Train RMSE: 0.0544 Test RMSE: 0.1016, Test Mean Module RMSE: 0.1079\n",
      "Epoch 631 Training Loss (NLML): 1.0836 Train RMSE: 0.0541 Test RMSE: 0.1010, Test Mean Module RMSE: 0.1080\n",
      "Epoch 632 Training Loss (NLML): 1.0836 Train RMSE: 0.0537 Test RMSE: 0.1003, Test Mean Module RMSE: 0.1069\n",
      "Epoch 633 Training Loss (NLML): 1.0836 Train RMSE: 0.0534 Test RMSE: 0.1003, Test Mean Module RMSE: 0.1069\n",
      "Epoch 634 Training Loss (NLML): 1.0835 Train RMSE: 0.0530 Test RMSE: 0.0989, Test Mean Module RMSE: 0.1055\n",
      "Epoch 635 Training Loss (NLML): 1.0835 Train RMSE: 0.0527 Test RMSE: 0.0988, Test Mean Module RMSE: 0.1061\n",
      "Epoch 636 Training Loss (NLML): 1.0835 Train RMSE: 0.0523 Test RMSE: 0.0980, Test Mean Module RMSE: 0.1041\n",
      "Epoch 637 Training Loss (NLML): 1.0835 Train RMSE: 0.0520 Test RMSE: 0.0983, Test Mean Module RMSE: 0.1052\n",
      "Epoch 638 Training Loss (NLML): 1.0835 Train RMSE: 0.0517 Test RMSE: 0.0966, Test Mean Module RMSE: 0.1029\n",
      "Epoch 639 Training Loss (NLML): 1.0835 Train RMSE: 0.0515 Test RMSE: 0.0973, Test Mean Module RMSE: 0.1048\n",
      "Epoch 640 Training Loss (NLML): 1.0835 Train RMSE: 0.0516 Test RMSE: 0.0957, Test Mean Module RMSE: 0.1012\n",
      "Epoch 641 Training Loss (NLML): 1.0835 Train RMSE: 0.0523 Test RMSE: 0.0981, Test Mean Module RMSE: 0.1063\n",
      "Epoch 642 Training Loss (NLML): 1.0835 Train RMSE: 0.0545 Test RMSE: 0.0949, Test Mean Module RMSE: 0.0990\n",
      "Epoch 643 Training Loss (NLML): 1.0836 Train RMSE: 0.0612 Test RMSE: 0.1048, Test Mean Module RMSE: 0.1149\n",
      "Epoch 644 Training Loss (NLML): 1.0839 Train RMSE: 0.0771 Test RMSE: 0.1055, Test Mean Module RMSE: 0.1042\n",
      "Epoch 645 Training Loss (NLML): 1.0846 Train RMSE: 0.1130 Test RMSE: 0.1462, Test Mean Module RMSE: 0.1591\n",
      "Epoch 646 Training Loss (NLML): 1.0871 Train RMSE: 0.1576 Test RMSE: 0.1673, Test Mean Module RMSE: 0.1594\n",
      "Epoch 647 Training Loss (NLML): 1.0914 Train RMSE: 0.2047 Test RMSE: 0.2318, Test Mean Module RMSE: 0.2453\n",
      "Epoch 648 Training Loss (NLML): 1.0974 Train RMSE: 0.1292 Test RMSE: 0.1365, Test Mean Module RMSE: 0.1319\n",
      "Epoch 649 Training Loss (NLML): 1.0885 Train RMSE: 0.1013 Test RMSE: 0.1243, Test Mean Module RMSE: 0.1283\n",
      "Epoch 650 Training Loss (NLML): 1.0862 Train RMSE: 0.1716 Test RMSE: 0.1926, Test Mean Module RMSE: 0.2042\n",
      "Epoch 651 Training Loss (NLML): 1.0930 Train RMSE: 0.0943 Test RMSE: 0.1188, Test Mean Module RMSE: 0.1117\n",
      "Epoch 652 Training Loss (NLML): 1.0857 Train RMSE: 0.1007 Test RMSE: 0.1157, Test Mean Module RMSE: 0.1124\n",
      "Epoch 653 Training Loss (NLML): 1.0862 Train RMSE: 0.1272 Test RMSE: 0.1615, Test Mean Module RMSE: 0.1756\n",
      "Epoch 654 Training Loss (NLML): 1.0883 Train RMSE: 0.0667 Test RMSE: 0.1101, Test Mean Module RMSE: 0.1183\n",
      "Epoch 655 Training Loss (NLML): 1.0841 Train RMSE: 0.1269 Test RMSE: 0.1456, Test Mean Module RMSE: 0.1404\n",
      "Epoch 656 Training Loss (NLML): 1.0883 Train RMSE: 0.1129 Test RMSE: 0.1502, Test Mean Module RMSE: 0.1612\n",
      "Epoch 657 Training Loss (NLML): 1.0871 Train RMSE: 0.0920 Test RMSE: 0.1244, Test Mean Module RMSE: 0.1384\n",
      "Epoch 658 Training Loss (NLML): 1.0855 Train RMSE: 0.1109 Test RMSE: 0.1267, Test Mean Module RMSE: 0.1229\n",
      "Epoch 659 Training Loss (NLML): 1.0869 Train RMSE: 0.0729 Test RMSE: 0.1112, Test Mean Module RMSE: 0.1113\n",
      "Epoch 660 Training Loss (NLML): 1.0844 Train RMSE: 0.1041 Test RMSE: 0.1412, Test Mean Module RMSE: 0.1527\n",
      "Epoch 661 Training Loss (NLML): 1.0864 Train RMSE: 0.0712 Test RMSE: 0.1047, Test Mean Module RMSE: 0.1136\n",
      "Epoch 662 Training Loss (NLML): 1.0843 Train RMSE: 0.0948 Test RMSE: 0.1175, Test Mean Module RMSE: 0.1130\n",
      "Epoch 663 Training Loss (NLML): 1.0857 Train RMSE: 0.0705 Test RMSE: 0.1122, Test Mean Module RMSE: 0.1168\n",
      "Epoch 664 Training Loss (NLML): 1.0843 Train RMSE: 0.0867 Test RMSE: 0.1287, Test Mean Module RMSE: 0.1431\n",
      "Epoch 665 Training Loss (NLML): 1.0852 Train RMSE: 0.0754 Test RMSE: 0.1147, Test Mean Module RMSE: 0.1246\n",
      "Epoch 666 Training Loss (NLML): 1.0846 Train RMSE: 0.0738 Test RMSE: 0.1091, Test Mean Module RMSE: 0.1070\n",
      "Epoch 667 Training Loss (NLML): 1.0845 Train RMSE: 0.0774 Test RMSE: 0.1208, Test Mean Module RMSE: 0.1235\n",
      "Epoch 668 Training Loss (NLML): 1.0847 Train RMSE: 0.0695 Test RMSE: 0.1079, Test Mean Module RMSE: 0.1207\n",
      "Epoch 669 Training Loss (NLML): 1.0842 Train RMSE: 0.0729 Test RMSE: 0.1081, Test Mean Module RMSE: 0.1159\n",
      "Epoch 670 Training Loss (NLML): 1.0844 Train RMSE: 0.0696 Test RMSE: 0.1058, Test Mean Module RMSE: 0.1055\n",
      "Epoch 671 Training Loss (NLML): 1.0843 Train RMSE: 0.0686 Test RMSE: 0.1088, Test Mean Module RMSE: 0.1143\n",
      "Epoch 672 Training Loss (NLML): 1.0842 Train RMSE: 0.0664 Test RMSE: 0.1031, Test Mean Module RMSE: 0.1155\n",
      "Epoch 673 Training Loss (NLML): 1.0841 Train RMSE: 0.0672 Test RMSE: 0.1040, Test Mean Module RMSE: 0.1111\n",
      "Epoch 674 Training Loss (NLML): 1.0841 Train RMSE: 0.0637 Test RMSE: 0.1065, Test Mean Module RMSE: 0.1082\n",
      "Epoch 675 Training Loss (NLML): 1.0840 Train RMSE: 0.0640 Test RMSE: 0.1124, Test Mean Module RMSE: 0.1194\n",
      "Epoch 676 Training Loss (NLML): 1.0840 Train RMSE: 0.0635 Test RMSE: 0.1073, Test Mean Module RMSE: 0.1189\n",
      "Epoch 677 Training Loss (NLML): 1.0840 Train RMSE: 0.0600 Test RMSE: 0.0999, Test Mean Module RMSE: 0.1079\n",
      "Epoch 678 Training Loss (NLML): 1.0838 Train RMSE: 0.0633 Test RMSE: 0.1036, Test Mean Module RMSE: 0.1054\n",
      "Epoch 679 Training Loss (NLML): 1.0840 Train RMSE: 0.0569 Test RMSE: 0.0991, Test Mean Module RMSE: 0.1046\n",
      "Epoch 680 Training Loss (NLML): 1.0837 Train RMSE: 0.0614 Test RMSE: 0.0999, Test Mean Module RMSE: 0.1098\n",
      "Epoch 681 Training Loss (NLML): 1.0839 Train RMSE: 0.0561 Test RMSE: 0.0954, Test Mean Module RMSE: 0.1040\n",
      "Epoch 682 Training Loss (NLML): 1.0837 Train RMSE: 0.0588 Test RMSE: 0.0967, Test Mean Module RMSE: 0.1010\n",
      "Epoch 683 Training Loss (NLML): 1.0838 Train RMSE: 0.0548 Test RMSE: 0.0947, Test Mean Module RMSE: 0.1002\n",
      "Epoch 684 Training Loss (NLML): 1.0836 Train RMSE: 0.0567 Test RMSE: 0.0980, Test Mean Module RMSE: 0.1072\n",
      "Epoch 685 Training Loss (NLML): 1.0837 Train RMSE: 0.0543 Test RMSE: 0.1013, Test Mean Module RMSE: 0.1104\n",
      "Epoch 686 Training Loss (NLML): 1.0836 Train RMSE: 0.0547 Test RMSE: 0.0995, Test Mean Module RMSE: 0.1047\n",
      "Epoch 687 Training Loss (NLML): 1.0836 Train RMSE: 0.0529 Test RMSE: 0.0951, Test Mean Module RMSE: 0.1014\n",
      "Epoch 688 Training Loss (NLML): 1.0835 Train RMSE: 0.0531 Test RMSE: 0.0945, Test Mean Module RMSE: 0.1044\n",
      "Epoch 689 Training Loss (NLML): 1.0835 Train RMSE: 0.0517 Test RMSE: 0.0935, Test Mean Module RMSE: 0.1017\n",
      "Epoch 690 Training Loss (NLML): 1.0835 Train RMSE: 0.0523 Test RMSE: 0.0924, Test Mean Module RMSE: 0.0969\n",
      "Epoch 691 Training Loss (NLML): 1.0835 Train RMSE: 0.0503 Test RMSE: 0.0918, Test Mean Module RMSE: 0.0979\n",
      "Epoch 692 Training Loss (NLML): 1.0834 Train RMSE: 0.0511 Test RMSE: 0.0931, Test Mean Module RMSE: 0.1027\n",
      "Epoch 693 Training Loss (NLML): 1.0835 Train RMSE: 0.0492 Test RMSE: 0.0911, Test Mean Module RMSE: 0.0989\n",
      "Epoch 694 Training Loss (NLML): 1.0834 Train RMSE: 0.0499 Test RMSE: 0.0918, Test Mean Module RMSE: 0.0969\n",
      "Epoch 695 Training Loss (NLML): 1.0834 Train RMSE: 0.0485 Test RMSE: 0.0947, Test Mean Module RMSE: 0.1028\n",
      "Epoch 696 Training Loss (NLML): 1.0834 Train RMSE: 0.0489 Test RMSE: 0.0949, Test Mean Module RMSE: 0.1041\n",
      "Epoch 697 Training Loss (NLML): 1.0834 Train RMSE: 0.0478 Test RMSE: 0.0911, Test Mean Module RMSE: 0.0978\n",
      "Epoch 698 Training Loss (NLML): 1.0833 Train RMSE: 0.0479 Test RMSE: 0.0908, Test Mean Module RMSE: 0.0975\n",
      "Epoch 699 Training Loss (NLML): 1.0833 Train RMSE: 0.0469 Test RMSE: 0.0901, Test Mean Module RMSE: 0.0984\n",
      "Epoch 700 Training Loss (NLML): 1.0833 Train RMSE: 0.0474 Test RMSE: 0.0897, Test Mean Module RMSE: 0.0974\n",
      "Epoch 701 Training Loss (NLML): 1.0833 Train RMSE: 0.0462 Test RMSE: 0.0896, Test Mean Module RMSE: 0.0963\n",
      "Epoch 702 Training Loss (NLML): 1.0833 Train RMSE: 0.0464 Test RMSE: 0.0897, Test Mean Module RMSE: 0.0963\n",
      "Epoch 703 Training Loss (NLML): 1.0833 Train RMSE: 0.0459 Test RMSE: 0.0890, Test Mean Module RMSE: 0.0969\n",
      "Epoch 704 Training Loss (NLML): 1.0833 Train RMSE: 0.0456 Test RMSE: 0.0904, Test Mean Module RMSE: 0.0989\n",
      "Epoch 705 Training Loss (NLML): 1.0833 Train RMSE: 0.0454 Test RMSE: 0.0914, Test Mean Module RMSE: 0.0986\n",
      "Epoch 706 Training Loss (NLML): 1.0833 Train RMSE: 0.0452 Test RMSE: 0.0900, Test Mean Module RMSE: 0.0964\n",
      "Epoch 707 Training Loss (NLML): 1.0833 Train RMSE: 0.0446 Test RMSE: 0.0890, Test Mean Module RMSE: 0.0973\n",
      "Epoch 708 Training Loss (NLML): 1.0832 Train RMSE: 0.0447 Test RMSE: 0.0888, Test Mean Module RMSE: 0.0974\n",
      "Epoch 709 Training Loss (NLML): 1.0832 Train RMSE: 0.0443 Test RMSE: 0.0877, Test Mean Module RMSE: 0.0939\n",
      "Epoch 710 Training Loss (NLML): 1.0832 Train RMSE: 0.0441 Test RMSE: 0.0881, Test Mean Module RMSE: 0.0950\n",
      "Epoch 711 Training Loss (NLML): 1.0832 Train RMSE: 0.0438 Test RMSE: 0.0884, Test Mean Module RMSE: 0.0971\n",
      "Epoch 712 Training Loss (NLML): 1.0832 Train RMSE: 0.0437 Test RMSE: 0.0874, Test Mean Module RMSE: 0.0947\n",
      "Epoch 713 Training Loss (NLML): 1.0832 Train RMSE: 0.0433 Test RMSE: 0.0881, Test Mean Module RMSE: 0.0949\n",
      "Epoch 714 Training Loss (NLML): 1.0832 Train RMSE: 0.0431 Test RMSE: 0.0889, Test Mean Module RMSE: 0.0965\n",
      "Epoch 715 Training Loss (NLML): 1.0832 Train RMSE: 0.0431 Test RMSE: 0.0883, Test Mean Module RMSE: 0.0962\n",
      "Epoch 716 Training Loss (NLML): 1.0832 Train RMSE: 0.0427 Test RMSE: 0.0879, Test Mean Module RMSE: 0.0954\n",
      "Epoch 717 Training Loss (NLML): 1.0832 Train RMSE: 0.0425 Test RMSE: 0.0870, Test Mean Module RMSE: 0.0936\n",
      "Epoch 718 Training Loss (NLML): 1.0832 Train RMSE: 0.0423 Test RMSE: 0.0865, Test Mean Module RMSE: 0.0937\n",
      "Epoch 719 Training Loss (NLML): 1.0832 Train RMSE: 0.0422 Test RMSE: 0.0872, Test Mean Module RMSE: 0.0954\n",
      "Epoch 720 Training Loss (NLML): 1.0832 Train RMSE: 0.0418 Test RMSE: 0.0866, Test Mean Module RMSE: 0.0936\n",
      "Epoch 721 Training Loss (NLML): 1.0832 Train RMSE: 0.0417 Test RMSE: 0.0863, Test Mean Module RMSE: 0.0931\n",
      "Epoch 722 Training Loss (NLML): 1.0831 Train RMSE: 0.0415 Test RMSE: 0.0869, Test Mean Module RMSE: 0.0950\n",
      "Epoch 723 Training Loss (NLML): 1.0831 Train RMSE: 0.0413 Test RMSE: 0.0867, Test Mean Module RMSE: 0.0944\n",
      "Epoch 724 Training Loss (NLML): 1.0831 Train RMSE: 0.0411 Test RMSE: 0.0867, Test Mean Module RMSE: 0.0936\n",
      "Epoch 725 Training Loss (NLML): 1.0831 Train RMSE: 0.0409 Test RMSE: 0.0863, Test Mean Module RMSE: 0.0935\n",
      "Epoch 726 Training Loss (NLML): 1.0831 Train RMSE: 0.0407 Test RMSE: 0.0854, Test Mean Module RMSE: 0.0931\n",
      "Epoch 727 Training Loss (NLML): 1.0831 Train RMSE: 0.0405 Test RMSE: 0.0854, Test Mean Module RMSE: 0.0930\n",
      "Epoch 728 Training Loss (NLML): 1.0831 Train RMSE: 0.0403 Test RMSE: 0.0852, Test Mean Module RMSE: 0.0921\n",
      "Epoch 729 Training Loss (NLML): 1.0831 Train RMSE: 0.0401 Test RMSE: 0.0850, Test Mean Module RMSE: 0.0921\n",
      "Epoch 730 Training Loss (NLML): 1.0831 Train RMSE: 0.0399 Test RMSE: 0.0854, Test Mean Module RMSE: 0.0935\n",
      "Epoch 731 Training Loss (NLML): 1.0831 Train RMSE: 0.0397 Test RMSE: 0.0851, Test Mean Module RMSE: 0.0924\n",
      "Epoch 732 Training Loss (NLML): 1.0831 Train RMSE: 0.0395 Test RMSE: 0.0852, Test Mean Module RMSE: 0.0922\n",
      "Epoch 733 Training Loss (NLML): 1.0831 Train RMSE: 0.0393 Test RMSE: 0.0851, Test Mean Module RMSE: 0.0927\n",
      "Epoch 734 Training Loss (NLML): 1.0831 Train RMSE: 0.0392 Test RMSE: 0.0845, Test Mean Module RMSE: 0.0920\n",
      "Epoch 735 Training Loss (NLML): 1.0831 Train RMSE: 0.0390 Test RMSE: 0.0842, Test Mean Module RMSE: 0.0916\n",
      "Epoch 736 Training Loss (NLML): 1.0831 Train RMSE: 0.0388 Test RMSE: 0.0838, Test Mean Module RMSE: 0.0909\n",
      "Epoch 737 Training Loss (NLML): 1.0831 Train RMSE: 0.0386 Test RMSE: 0.0838, Test Mean Module RMSE: 0.0914\n",
      "Epoch 738 Training Loss (NLML): 1.0831 Train RMSE: 0.0384 Test RMSE: 0.0838, Test Mean Module RMSE: 0.0914\n",
      "Epoch 739 Training Loss (NLML): 1.0831 Train RMSE: 0.0383 Test RMSE: 0.0834, Test Mean Module RMSE: 0.0905\n",
      "Epoch 740 Training Loss (NLML): 1.0830 Train RMSE: 0.0381 Test RMSE: 0.0835, Test Mean Module RMSE: 0.0909\n",
      "Epoch 741 Training Loss (NLML): 1.0830 Train RMSE: 0.0379 Test RMSE: 0.0835, Test Mean Module RMSE: 0.0912\n",
      "Epoch 742 Training Loss (NLML): 1.0830 Train RMSE: 0.0377 Test RMSE: 0.0831, Test Mean Module RMSE: 0.0904\n",
      "Epoch 743 Training Loss (NLML): 1.0830 Train RMSE: 0.0376 Test RMSE: 0.0831, Test Mean Module RMSE: 0.0906\n",
      "Epoch 744 Training Loss (NLML): 1.0830 Train RMSE: 0.0375 Test RMSE: 0.0823, Test Mean Module RMSE: 0.0896\n",
      "Epoch 745 Training Loss (NLML): 1.0830 Train RMSE: 0.0374 Test RMSE: 0.0831, Test Mean Module RMSE: 0.0911\n",
      "Epoch 746 Training Loss (NLML): 1.0830 Train RMSE: 0.0376 Test RMSE: 0.0820, Test Mean Module RMSE: 0.0887\n",
      "Epoch 747 Training Loss (NLML): 1.0830 Train RMSE: 0.0386 Test RMSE: 0.0839, Test Mean Module RMSE: 0.0922\n",
      "Epoch 748 Training Loss (NLML): 1.0831 Train RMSE: 0.0419 Test RMSE: 0.0819, Test Mean Module RMSE: 0.0876\n",
      "Epoch 749 Training Loss (NLML): 1.0832 Train RMSE: 0.0504 Test RMSE: 0.0899, Test Mean Module RMSE: 0.0999\n",
      "Epoch 750 Training Loss (NLML): 1.0834 Train RMSE: 0.0720 Test RMSE: 0.0917, Test Mean Module RMSE: 0.0927\n",
      "Epoch 751 Training Loss (NLML): 1.0844 Train RMSE: 0.0878 Test RMSE: 0.1111, Test Mean Module RMSE: 0.1234\n",
      "Epoch 752 Training Loss (NLML): 1.0853 Train RMSE: 0.0947 Test RMSE: 0.1048, Test Mean Module RMSE: 0.1036\n",
      "Epoch 753 Training Loss (NLML): 1.0857 Train RMSE: 0.0575 Test RMSE: 0.0897, Test Mean Module RMSE: 0.0995\n",
      "Epoch 754 Training Loss (NLML): 1.0837 Train RMSE: 0.0783 Test RMSE: 0.1053, Test Mean Module RMSE: 0.1199\n",
      "Epoch 755 Training Loss (NLML): 1.0847 Train RMSE: 0.0684 Test RMSE: 0.0946, Test Mean Module RMSE: 0.0871\n",
      "Epoch 756 Training Loss (NLML): 1.0842 Train RMSE: 0.0541 Test RMSE: 0.0949, Test Mean Module RMSE: 0.1045\n",
      "Epoch 757 Training Loss (NLML): 1.0836 Train RMSE: 0.0615 Test RMSE: 0.1090, Test Mean Module RMSE: 0.1243\n",
      "Epoch 758 Training Loss (NLML): 1.0839 Train RMSE: 0.0531 Test RMSE: 0.0959, Test Mean Module RMSE: 0.0930\n",
      "Epoch 759 Training Loss (NLML): 1.0835 Train RMSE: 0.0537 Test RMSE: 0.0934, Test Mean Module RMSE: 0.0937\n",
      "Epoch 760 Training Loss (NLML): 1.0836 Train RMSE: 0.0610 Test RMSE: 0.1024, Test Mean Module RMSE: 0.1173\n",
      "Epoch 761 Training Loss (NLML): 1.0838 Train RMSE: 0.0458 Test RMSE: 0.0901, Test Mean Module RMSE: 0.1007\n",
      "Epoch 762 Training Loss (NLML): 1.0833 Train RMSE: 0.0600 Test RMSE: 0.0971, Test Mean Module RMSE: 0.0882\n",
      "Epoch 763 Training Loss (NLML): 1.0838 Train RMSE: 0.0411 Test RMSE: 0.0861, Test Mean Module RMSE: 0.0953\n",
      "Epoch 764 Training Loss (NLML): 1.0831 Train RMSE: 0.0524 Test RMSE: 0.0943, Test Mean Module RMSE: 0.1097\n",
      "Epoch 765 Training Loss (NLML): 1.0835 Train RMSE: 0.0423 Test RMSE: 0.0851, Test Mean Module RMSE: 0.0873\n",
      "Epoch 766 Training Loss (NLML): 1.0832 Train RMSE: 0.0454 Test RMSE: 0.0914, Test Mean Module RMSE: 0.0919\n",
      "Epoch 767 Training Loss (NLML): 1.0833 Train RMSE: 0.0467 Test RMSE: 0.0958, Test Mean Module RMSE: 0.1068\n",
      "Epoch 768 Training Loss (NLML): 1.0833 Train RMSE: 0.0416 Test RMSE: 0.0915, Test Mean Module RMSE: 0.1031\n",
      "Epoch 769 Training Loss (NLML): 1.0831 Train RMSE: 0.0486 Test RMSE: 0.0872, Test Mean Module RMSE: 0.0868\n",
      "Epoch 770 Training Loss (NLML): 1.0834 Train RMSE: 0.0410 Test RMSE: 0.0884, Test Mean Module RMSE: 0.0976\n",
      "Epoch 771 Training Loss (NLML): 1.0831 Train RMSE: 0.0470 Test RMSE: 0.0913, Test Mean Module RMSE: 0.1004\n",
      "Epoch 772 Training Loss (NLML): 1.0833 Train RMSE: 0.0436 Test RMSE: 0.0866, Test Mean Module RMSE: 0.0910\n",
      "Epoch 773 Training Loss (NLML): 1.0832 Train RMSE: 0.0446 Test RMSE: 0.0834, Test Mean Module RMSE: 0.0849\n",
      "Epoch 774 Training Loss (NLML): 1.0832 Train RMSE: 0.0469 Test RMSE: 0.0891, Test Mean Module RMSE: 0.1033\n",
      "Epoch 775 Training Loss (NLML): 1.0833 Train RMSE: 0.0448 Test RMSE: 0.0862, Test Mean Module RMSE: 0.0917\n",
      "Epoch 776 Training Loss (NLML): 1.0832 Train RMSE: 0.0509 Test RMSE: 0.0925, Test Mean Module RMSE: 0.0964\n",
      "Epoch 777 Training Loss (NLML): 1.0835 Train RMSE: 0.0510 Test RMSE: 0.0910, Test Mean Module RMSE: 0.0947\n",
      "Epoch 778 Training Loss (NLML): 1.0835 Train RMSE: 0.0597 Test RMSE: 0.0996, Test Mean Module RMSE: 0.1140\n",
      "Epoch 779 Training Loss (NLML): 1.0838 Train RMSE: 0.0657 Test RMSE: 0.0952, Test Mean Module RMSE: 0.0946\n",
      "Epoch 780 Training Loss (NLML): 1.0841 Train RMSE: 0.0759 Test RMSE: 0.1085, Test Mean Module RMSE: 0.1171\n",
      "Epoch 781 Training Loss (NLML): 1.0846 Train RMSE: 0.0804 Test RMSE: 0.1082, Test Mean Module RMSE: 0.1098\n",
      "Epoch 782 Training Loss (NLML): 1.0848 Train RMSE: 0.0826 Test RMSE: 0.1118, Test Mean Module RMSE: 0.1237\n",
      "Epoch 783 Training Loss (NLML): 1.0850 Train RMSE: 0.0732 Test RMSE: 0.0965, Test Mean Module RMSE: 0.0937\n",
      "Epoch 784 Training Loss (NLML): 1.0844 Train RMSE: 0.0568 Test RMSE: 0.0916, Test Mean Module RMSE: 0.1025\n",
      "Epoch 785 Training Loss (NLML): 1.0837 Train RMSE: 0.0406 Test RMSE: 0.0826, Test Mean Module RMSE: 0.0903\n",
      "Epoch 786 Training Loss (NLML): 1.0831 Train RMSE: 0.0370 Test RMSE: 0.0802, Test Mean Module RMSE: 0.0835\n",
      "Epoch 787 Training Loss (NLML): 1.0830 Train RMSE: 0.0482 Test RMSE: 0.0885, Test Mean Module RMSE: 0.0960\n",
      "Epoch 788 Training Loss (NLML): 1.0834 Train RMSE: 0.0579 Test RMSE: 0.0912, Test Mean Module RMSE: 0.0964\n",
      "Epoch 789 Training Loss (NLML): 1.0837 Train RMSE: 0.0604 Test RMSE: 0.0972, Test Mean Module RMSE: 0.1092\n",
      "Epoch 790 Training Loss (NLML): 1.0838 Train RMSE: 0.0516 Test RMSE: 0.0854, Test Mean Module RMSE: 0.0859\n",
      "Epoch 791 Training Loss (NLML): 1.0835 Train RMSE: 0.0386 Test RMSE: 0.0822, Test Mean Module RMSE: 0.0907\n",
      "Epoch 792 Training Loss (NLML): 1.0831 Train RMSE: 0.0365 Test RMSE: 0.0806, Test Mean Module RMSE: 0.0913\n",
      "Epoch 793 Training Loss (NLML): 1.0830 Train RMSE: 0.0440 Test RMSE: 0.0810, Test Mean Module RMSE: 0.0848\n",
      "Epoch 794 Training Loss (NLML): 1.0832 Train RMSE: 0.0505 Test RMSE: 0.0892, Test Mean Module RMSE: 0.0973\n",
      "Epoch 795 Training Loss (NLML): 1.0834 Train RMSE: 0.0479 Test RMSE: 0.0850, Test Mean Module RMSE: 0.0898\n",
      "Epoch 796 Training Loss (NLML): 1.0833 Train RMSE: 0.0400 Test RMSE: 0.0828, Test Mean Module RMSE: 0.0928\n",
      "Epoch 797 Training Loss (NLML): 1.0831 Train RMSE: 0.0349 Test RMSE: 0.0772, Test Mean Module RMSE: 0.0831\n",
      "Epoch 798 Training Loss (NLML): 1.0830 Train RMSE: 0.0376 Test RMSE: 0.0777, Test Mean Module RMSE: 0.0839\n",
      "Epoch 799 Training Loss (NLML): 1.0830 Train RMSE: 0.0435 Test RMSE: 0.0843, Test Mean Module RMSE: 0.0955\n",
      "Epoch 800 Training Loss (NLML): 1.0832 Train RMSE: 0.0440 Test RMSE: 0.0811, Test Mean Module RMSE: 0.0848\n",
      "Epoch 801 Training Loss (NLML): 1.0832 Train RMSE: 0.0403 Test RMSE: 0.0816, Test Mean Module RMSE: 0.0896\n",
      "Epoch 802 Training Loss (NLML): 1.0831 Train RMSE: 0.0350 Test RMSE: 0.0772, Test Mean Module RMSE: 0.0853\n",
      "Epoch 803 Training Loss (NLML): 1.0830 Train RMSE: 0.0342 Test RMSE: 0.0763, Test Mean Module RMSE: 0.0843\n",
      "Epoch 804 Training Loss (NLML): 1.0829 Train RMSE: 0.0376 Test RMSE: 0.0789, Test Mean Module RMSE: 0.0864\n",
      "Epoch 805 Training Loss (NLML): 1.0830 Train RMSE: 0.0398 Test RMSE: 0.0784, Test Mean Module RMSE: 0.0832\n",
      "Epoch 806 Training Loss (NLML): 1.0831 Train RMSE: 0.0396 Test RMSE: 0.0819, Test Mean Module RMSE: 0.0925\n",
      "Epoch 807 Training Loss (NLML): 1.0831 Train RMSE: 0.0364 Test RMSE: 0.0776, Test Mean Module RMSE: 0.0837\n",
      "Epoch 808 Training Loss (NLML): 1.0830 Train RMSE: 0.0337 Test RMSE: 0.0775, Test Mean Module RMSE: 0.0849\n",
      "Epoch 809 Training Loss (NLML): 1.0829 Train RMSE: 0.0333 Test RMSE: 0.0775, Test Mean Module RMSE: 0.0860\n",
      "Epoch 810 Training Loss (NLML): 1.0829 Train RMSE: 0.0348 Test RMSE: 0.0769, Test Mean Module RMSE: 0.0835\n",
      "Epoch 811 Training Loss (NLML): 1.0830 Train RMSE: 0.0367 Test RMSE: 0.0790, Test Mean Module RMSE: 0.0873\n",
      "Epoch 812 Training Loss (NLML): 1.0830 Train RMSE: 0.0370 Test RMSE: 0.0771, Test Mean Module RMSE: 0.0833\n",
      "Epoch 813 Training Loss (NLML): 1.0830 Train RMSE: 0.0361 Test RMSE: 0.0789, Test Mean Module RMSE: 0.0889\n",
      "Epoch 814 Training Loss (NLML): 1.0830 Train RMSE: 0.0342 Test RMSE: 0.0760, Test Mean Module RMSE: 0.0818\n",
      "Epoch 815 Training Loss (NLML): 1.0829 Train RMSE: 0.0328 Test RMSE: 0.0763, Test Mean Module RMSE: 0.0836\n",
      "Epoch 816 Training Loss (NLML): 1.0829 Train RMSE: 0.0322 Test RMSE: 0.0762, Test Mean Module RMSE: 0.0847\n",
      "Epoch 817 Training Loss (NLML): 1.0829 Train RMSE: 0.0326 Test RMSE: 0.0758, Test Mean Module RMSE: 0.0834\n",
      "Epoch 818 Training Loss (NLML): 1.0829 Train RMSE: 0.0336 Test RMSE: 0.0770, Test Mean Module RMSE: 0.0851\n",
      "Epoch 819 Training Loss (NLML): 1.0829 Train RMSE: 0.0343 Test RMSE: 0.0760, Test Mean Module RMSE: 0.0823\n",
      "Epoch 820 Training Loss (NLML): 1.0829 Train RMSE: 0.0348 Test RMSE: 0.0778, Test Mean Module RMSE: 0.0872\n",
      "Epoch 821 Training Loss (NLML): 1.0830 Train RMSE: 0.0347 Test RMSE: 0.0755, Test Mean Module RMSE: 0.0812\n",
      "Epoch 822 Training Loss (NLML): 1.0830 Train RMSE: 0.0345 Test RMSE: 0.0772, Test Mean Module RMSE: 0.0859\n",
      "Epoch 823 Training Loss (NLML): 1.0829 Train RMSE: 0.0338 Test RMSE: 0.0757, Test Mean Module RMSE: 0.0825\n",
      "Epoch 824 Training Loss (NLML): 1.0829 Train RMSE: 0.0332 Test RMSE: 0.0766, Test Mean Module RMSE: 0.0852\n",
      "Epoch 825 Training Loss (NLML): 1.0829 Train RMSE: 0.0325 Test RMSE: 0.0748, Test Mean Module RMSE: 0.0809\n",
      "Epoch 826 Training Loss (NLML): 1.0829 Train RMSE: 0.0320 Test RMSE: 0.0758, Test Mean Module RMSE: 0.0844\n",
      "Epoch 827 Training Loss (NLML): 1.0829 Train RMSE: 0.0316 Test RMSE: 0.0752, Test Mean Module RMSE: 0.0827\n",
      "Epoch 828 Training Loss (NLML): 1.0829 Train RMSE: 0.0313 Test RMSE: 0.0756, Test Mean Module RMSE: 0.0833\n",
      "Epoch 829 Training Loss (NLML): 1.0829 Train RMSE: 0.0311 Test RMSE: 0.0749, Test Mean Module RMSE: 0.0822\n",
      "Epoch 830 Training Loss (NLML): 1.0829 Train RMSE: 0.0310 Test RMSE: 0.0749, Test Mean Module RMSE: 0.0833\n",
      "Epoch 831 Training Loss (NLML): 1.0829 Train RMSE: 0.0308 Test RMSE: 0.0743, Test Mean Module RMSE: 0.0818\n",
      "Epoch 832 Training Loss (NLML): 1.0829 Train RMSE: 0.0307 Test RMSE: 0.0744, Test Mean Module RMSE: 0.0818\n",
      "Epoch 833 Training Loss (NLML): 1.0829 Train RMSE: 0.0306 Test RMSE: 0.0746, Test Mean Module RMSE: 0.0824\n",
      "Epoch 834 Training Loss (NLML): 1.0829 Train RMSE: 0.0305 Test RMSE: 0.0745, Test Mean Module RMSE: 0.0823\n",
      "Epoch 835 Training Loss (NLML): 1.0829 Train RMSE: 0.0305 Test RMSE: 0.0743, Test Mean Module RMSE: 0.0817\n",
      "Epoch 836 Training Loss (NLML): 1.0829 Train RMSE: 0.0304 Test RMSE: 0.0742, Test Mean Module RMSE: 0.0819\n",
      "Epoch 837 Training Loss (NLML): 1.0829 Train RMSE: 0.0303 Test RMSE: 0.0744, Test Mean Module RMSE: 0.0824\n",
      "Epoch 838 Training Loss (NLML): 1.0829 Train RMSE: 0.0302 Test RMSE: 0.0740, Test Mean Module RMSE: 0.0814\n",
      "Epoch 839 Training Loss (NLML): 1.0829 Train RMSE: 0.0302 Test RMSE: 0.0741, Test Mean Module RMSE: 0.0820\n",
      "Epoch 840 Training Loss (NLML): 1.0828 Train RMSE: 0.0302 Test RMSE: 0.0738, Test Mean Module RMSE: 0.0814\n",
      "Epoch 841 Training Loss (NLML): 1.0828 Train RMSE: 0.0304 Test RMSE: 0.0741, Test Mean Module RMSE: 0.0823\n",
      "Epoch 842 Training Loss (NLML): 1.0829 Train RMSE: 0.0308 Test RMSE: 0.0734, Test Mean Module RMSE: 0.0800\n",
      "Epoch 843 Training Loss (NLML): 1.0829 Train RMSE: 0.0318 Test RMSE: 0.0750, Test Mean Module RMSE: 0.0839\n",
      "Epoch 844 Training Loss (NLML): 1.0829 Train RMSE: 0.0343 Test RMSE: 0.0743, Test Mean Module RMSE: 0.0802\n",
      "Epoch 845 Training Loss (NLML): 1.0829 Train RMSE: 0.0400 Test RMSE: 0.0794, Test Mean Module RMSE: 0.0893\n",
      "Epoch 846 Training Loss (NLML): 1.0831 Train RMSE: 0.0511 Test RMSE: 0.0818, Test Mean Module RMSE: 0.0844\n",
      "Epoch 847 Training Loss (NLML): 1.0835 Train RMSE: 0.0731 Test RMSE: 0.1019, Test Mean Module RMSE: 0.1133\n",
      "Epoch 848 Training Loss (NLML): 1.0844 Train RMSE: 0.1042 Test RMSE: 0.1196, Test Mean Module RMSE: 0.1169\n",
      "Epoch 849 Training Loss (NLML): 1.0864 Train RMSE: 0.1535 Test RMSE: 0.1716, Test Mean Module RMSE: 0.1827\n",
      "Epoch 850 Training Loss (NLML): 1.0909 Train RMSE: 0.1692 Test RMSE: 0.1764, Test Mean Module RMSE: 0.1722\n",
      "Epoch 851 Training Loss (NLML): 1.0928 Train RMSE: 0.1514 Test RMSE: 0.1685, Test Mean Module RMSE: 0.1793\n",
      "Epoch 852 Training Loss (NLML): 1.0907 Train RMSE: 0.0553 Test RMSE: 0.0790, Test Mean Module RMSE: 0.0861\n",
      "Epoch 853 Training Loss (NLML): 1.0836 Train RMSE: 0.1160 Test RMSE: 0.1302, Test Mean Module RMSE: 0.1267\n",
      "Epoch 854 Training Loss (NLML): 1.0873 Train RMSE: 0.1615 Test RMSE: 0.1766, Test Mean Module RMSE: 0.1849\n",
      "Epoch 855 Training Loss (NLML): 1.0918 Train RMSE: 0.0724 Test RMSE: 0.0931, Test Mean Module RMSE: 0.0975\n",
      "Epoch 856 Training Loss (NLML): 1.0844 Train RMSE: 0.0943 Test RMSE: 0.1123, Test Mean Module RMSE: 0.1112\n",
      "Epoch 857 Training Loss (NLML): 1.0857 Train RMSE: 0.1365 Test RMSE: 0.1565, Test Mean Module RMSE: 0.1655\n",
      "Epoch 858 Training Loss (NLML): 1.0892 Train RMSE: 0.0460 Test RMSE: 0.0774, Test Mean Module RMSE: 0.0887\n",
      "Epoch 859 Training Loss (NLML): 1.0833 Train RMSE: 0.1126 Test RMSE: 0.1230, Test Mean Module RMSE: 0.1225\n",
      "Epoch 860 Training Loss (NLML): 1.0871 Train RMSE: 0.1048 Test RMSE: 0.1322, Test Mean Module RMSE: 0.1386\n",
      "Epoch 861 Training Loss (NLML): 1.0865 Train RMSE: 0.0620 Test RMSE: 0.0966, Test Mean Module RMSE: 0.1087\n",
      "Epoch 862 Training Loss (NLML): 1.0839 Train RMSE: 0.1169 Test RMSE: 0.1393, Test Mean Module RMSE: 0.1415\n",
      "Epoch 863 Training Loss (NLML): 1.0874 Train RMSE: 0.0544 Test RMSE: 0.0916, Test Mean Module RMSE: 0.0940\n",
      "Epoch 864 Training Loss (NLML): 1.0836 Train RMSE: 0.0922 Test RMSE: 0.1230, Test Mean Module RMSE: 0.1303\n",
      "Epoch 865 Training Loss (NLML): 1.0856 Train RMSE: 0.0675 Test RMSE: 0.0938, Test Mean Module RMSE: 0.1035\n",
      "Epoch 866 Training Loss (NLML): 1.0842 Train RMSE: 0.0689 Test RMSE: 0.0927, Test Mean Module RMSE: 0.0945\n",
      "Epoch 867 Training Loss (NLML): 1.0842 Train RMSE: 0.0785 Test RMSE: 0.1120, Test Mean Module RMSE: 0.1171\n",
      "Epoch 868 Training Loss (NLML): 1.0847 Train RMSE: 0.0557 Test RMSE: 0.0933, Test Mean Module RMSE: 0.1058\n",
      "Epoch 869 Training Loss (NLML): 1.0836 Train RMSE: 0.0791 Test RMSE: 0.1049, Test Mean Module RMSE: 0.1101\n",
      "Epoch 870 Training Loss (NLML): 1.0848 Train RMSE: 0.0492 Test RMSE: 0.0843, Test Mean Module RMSE: 0.0865\n",
      "Epoch 871 Training Loss (NLML): 1.0834 Train RMSE: 0.0700 Test RMSE: 0.1048, Test Mean Module RMSE: 0.1147\n",
      "Epoch 872 Training Loss (NLML): 1.0843 Train RMSE: 0.0509 Test RMSE: 0.0869, Test Mean Module RMSE: 0.0981\n",
      "Epoch 873 Training Loss (NLML): 1.0835 Train RMSE: 0.0643 Test RMSE: 0.0925, Test Mean Module RMSE: 0.0926\n",
      "Epoch 874 Training Loss (NLML): 1.0840 Train RMSE: 0.0524 Test RMSE: 0.0882, Test Mean Module RMSE: 0.0939\n",
      "Epoch 875 Training Loss (NLML): 1.0835 Train RMSE: 0.0598 Test RMSE: 0.0947, Test Mean Module RMSE: 0.1088\n",
      "Epoch 876 Training Loss (NLML): 1.0838 Train RMSE: 0.0524 Test RMSE: 0.0883, Test Mean Module RMSE: 0.0940\n",
      "Epoch 877 Training Loss (NLML): 1.0835 Train RMSE: 0.0508 Test RMSE: 0.0850, Test Mean Module RMSE: 0.0852\n",
      "Epoch 878 Training Loss (NLML): 1.0834 Train RMSE: 0.0546 Test RMSE: 0.0939, Test Mean Module RMSE: 0.1046\n",
      "Epoch 879 Training Loss (NLML): 1.0836 Train RMSE: 0.0437 Test RMSE: 0.0857, Test Mean Module RMSE: 0.0974\n",
      "Epoch 880 Training Loss (NLML): 1.0832 Train RMSE: 0.0555 Test RMSE: 0.0882, Test Mean Module RMSE: 0.0911\n",
      "Epoch 881 Training Loss (NLML): 1.0836 Train RMSE: 0.0398 Test RMSE: 0.0827, Test Mean Module RMSE: 0.0901\n",
      "Epoch 882 Training Loss (NLML): 1.0831 Train RMSE: 0.0523 Test RMSE: 0.0922, Test Mean Module RMSE: 0.1036\n",
      "Epoch 883 Training Loss (NLML): 1.0835 Train RMSE: 0.0404 Test RMSE: 0.0817, Test Mean Module RMSE: 0.0865\n",
      "Epoch 884 Training Loss (NLML): 1.0831 Train RMSE: 0.0470 Test RMSE: 0.0838, Test Mean Module RMSE: 0.0879\n",
      "Epoch 885 Training Loss (NLML): 1.0833 Train RMSE: 0.0426 Test RMSE: 0.0841, Test Mean Module RMSE: 0.0960\n",
      "Epoch 886 Training Loss (NLML): 1.0832 Train RMSE: 0.0418 Test RMSE: 0.0836, Test Mean Module RMSE: 0.0930\n",
      "Epoch 887 Training Loss (NLML): 1.0831 Train RMSE: 0.0436 Test RMSE: 0.0822, Test Mean Module RMSE: 0.0846\n",
      "Epoch 888 Training Loss (NLML): 1.0832 Train RMSE: 0.0377 Test RMSE: 0.0831, Test Mean Module RMSE: 0.0910\n",
      "Epoch 889 Training Loss (NLML): 1.0830 Train RMSE: 0.0430 Test RMSE: 0.0870, Test Mean Module RMSE: 0.0991\n",
      "Epoch 890 Training Loss (NLML): 1.0832 Train RMSE: 0.0364 Test RMSE: 0.0803, Test Mean Module RMSE: 0.0863\n",
      "Epoch 891 Training Loss (NLML): 1.0830 Train RMSE: 0.0407 Test RMSE: 0.0809, Test Mean Module RMSE: 0.0845\n",
      "Epoch 892 Training Loss (NLML): 1.0831 Train RMSE: 0.0372 Test RMSE: 0.0834, Test Mean Module RMSE: 0.0945\n",
      "Epoch 893 Training Loss (NLML): 1.0830 Train RMSE: 0.0373 Test RMSE: 0.0828, Test Mean Module RMSE: 0.0937\n",
      "Epoch 894 Training Loss (NLML): 1.0830 Train RMSE: 0.0381 Test RMSE: 0.0795, Test Mean Module RMSE: 0.0832\n",
      "Epoch 895 Training Loss (NLML): 1.0830 Train RMSE: 0.0345 Test RMSE: 0.0799, Test Mean Module RMSE: 0.0861\n",
      "Epoch 896 Training Loss (NLML): 1.0829 Train RMSE: 0.0380 Test RMSE: 0.0852, Test Mean Module RMSE: 0.0966\n",
      "Epoch 897 Training Loss (NLML): 1.0830 Train RMSE: 0.0338 Test RMSE: 0.0816, Test Mean Module RMSE: 0.0894\n",
      "Epoch 898 Training Loss (NLML): 1.0829 Train RMSE: 0.0361 Test RMSE: 0.0803, Test Mean Module RMSE: 0.0845\n",
      "Epoch 899 Training Loss (NLML): 1.0830 Train RMSE: 0.0346 Test RMSE: 0.0819, Test Mean Module RMSE: 0.0911\n",
      "Epoch 900 Training Loss (NLML): 1.0830 Train RMSE: 0.0341 Test RMSE: 0.0821, Test Mean Module RMSE: 0.0929\n",
      "Epoch 901 Training Loss (NLML): 1.0829 Train RMSE: 0.0349 Test RMSE: 0.0804, Test Mean Module RMSE: 0.0864\n",
      "Epoch 902 Training Loss (NLML): 1.0830 Train RMSE: 0.0332 Test RMSE: 0.0808, Test Mean Module RMSE: 0.0871\n",
      "Epoch 903 Training Loss (NLML): 1.0829 Train RMSE: 0.0339 Test RMSE: 0.0825, Test Mean Module RMSE: 0.0924\n",
      "Epoch 904 Training Loss (NLML): 1.0829 Train RMSE: 0.0334 Test RMSE: 0.0812, Test Mean Module RMSE: 0.0895\n",
      "Epoch 905 Training Loss (NLML): 1.0829 Train RMSE: 0.0330 Test RMSE: 0.0802, Test Mean Module RMSE: 0.0862\n",
      "Epoch 906 Training Loss (NLML): 1.0829 Train RMSE: 0.0331 Test RMSE: 0.0814, Test Mean Module RMSE: 0.0898\n",
      "Epoch 907 Training Loss (NLML): 1.0829 Train RMSE: 0.0328 Test RMSE: 0.0812, Test Mean Module RMSE: 0.0904\n",
      "Epoch 908 Training Loss (NLML): 1.0829 Train RMSE: 0.0323 Test RMSE: 0.0804, Test Mean Module RMSE: 0.0873\n",
      "Epoch 909 Training Loss (NLML): 1.0829 Train RMSE: 0.0328 Test RMSE: 0.0814, Test Mean Module RMSE: 0.0887\n",
      "Epoch 910 Training Loss (NLML): 1.0829 Train RMSE: 0.0321 Test RMSE: 0.0814, Test Mean Module RMSE: 0.0907\n",
      "Epoch 911 Training Loss (NLML): 1.0829 Train RMSE: 0.0321 Test RMSE: 0.0810, Test Mean Module RMSE: 0.0891\n",
      "Epoch 912 Training Loss (NLML): 1.0829 Train RMSE: 0.0324 Test RMSE: 0.0810, Test Mean Module RMSE: 0.0876\n",
      "Epoch 913 Training Loss (NLML): 1.0829 Train RMSE: 0.0315 Test RMSE: 0.0807, Test Mean Module RMSE: 0.0887\n",
      "Epoch 914 Training Loss (NLML): 1.0829 Train RMSE: 0.0321 Test RMSE: 0.0810, Test Mean Module RMSE: 0.0900\n",
      "Epoch 915 Training Loss (NLML): 1.0829 Train RMSE: 0.0316 Test RMSE: 0.0808, Test Mean Module RMSE: 0.0884\n",
      "Epoch 916 Training Loss (NLML): 1.0829 Train RMSE: 0.0315 Test RMSE: 0.0807, Test Mean Module RMSE: 0.0875\n",
      "Epoch 917 Training Loss (NLML): 1.0829 Train RMSE: 0.0316 Test RMSE: 0.0810, Test Mean Module RMSE: 0.0893\n",
      "Epoch 918 Training Loss (NLML): 1.0829 Train RMSE: 0.0312 Test RMSE: 0.0810, Test Mean Module RMSE: 0.0895\n",
      "Epoch 919 Training Loss (NLML): 1.0829 Train RMSE: 0.0313 Test RMSE: 0.0804, Test Mean Module RMSE: 0.0874\n",
      "Epoch 920 Training Loss (NLML): 1.0829 Train RMSE: 0.0312 Test RMSE: 0.0803, Test Mean Module RMSE: 0.0878\n",
      "Epoch 921 Training Loss (NLML): 1.0829 Train RMSE: 0.0311 Test RMSE: 0.0809, Test Mean Module RMSE: 0.0897\n",
      "Epoch 922 Training Loss (NLML): 1.0829 Train RMSE: 0.0309 Test RMSE: 0.0804, Test Mean Module RMSE: 0.0880\n",
      "Epoch 923 Training Loss (NLML): 1.0829 Train RMSE: 0.0310 Test RMSE: 0.0798, Test Mean Module RMSE: 0.0867\n",
      "Epoch 924 Training Loss (NLML): 1.0829 Train RMSE: 0.0307 Test RMSE: 0.0803, Test Mean Module RMSE: 0.0888\n",
      "Epoch 925 Training Loss (NLML): 1.0829 Train RMSE: 0.0307 Test RMSE: 0.0803, Test Mean Module RMSE: 0.0886\n",
      "Epoch 926 Training Loss (NLML): 1.0829 Train RMSE: 0.0307 Test RMSE: 0.0797, Test Mean Module RMSE: 0.0865\n",
      "Epoch 927 Training Loss (NLML): 1.0829 Train RMSE: 0.0304 Test RMSE: 0.0800, Test Mean Module RMSE: 0.0878\n",
      "Epoch 928 Training Loss (NLML): 1.0829 Train RMSE: 0.0305 Test RMSE: 0.0803, Test Mean Module RMSE: 0.0888\n",
      "Epoch 929 Training Loss (NLML): 1.0829 Train RMSE: 0.0304 Test RMSE: 0.0796, Test Mean Module RMSE: 0.0869\n",
      "Epoch 930 Training Loss (NLML): 1.0829 Train RMSE: 0.0302 Test RMSE: 0.0797, Test Mean Module RMSE: 0.0870\n",
      "Epoch 931 Training Loss (NLML): 1.0829 Train RMSE: 0.0302 Test RMSE: 0.0801, Test Mean Module RMSE: 0.0885\n",
      "Epoch 932 Training Loss (NLML): 1.0828 Train RMSE: 0.0301 Test RMSE: 0.0796, Test Mean Module RMSE: 0.0874\n",
      "Epoch 933 Training Loss (NLML): 1.0828 Train RMSE: 0.0300 Test RMSE: 0.0792, Test Mean Module RMSE: 0.0865\n",
      "Epoch 934 Training Loss (NLML): 1.0828 Train RMSE: 0.0300 Test RMSE: 0.0795, Test Mean Module RMSE: 0.0874\n",
      "Epoch 935 Training Loss (NLML): 1.0828 Train RMSE: 0.0299 Test RMSE: 0.0795, Test Mean Module RMSE: 0.0875\n",
      "Epoch 936 Training Loss (NLML): 1.0828 Train RMSE: 0.0298 Test RMSE: 0.0792, Test Mean Module RMSE: 0.0867\n",
      "Epoch 937 Training Loss (NLML): 1.0828 Train RMSE: 0.0298 Test RMSE: 0.0792, Test Mean Module RMSE: 0.0867\n",
      "Epoch 938 Training Loss (NLML): 1.0828 Train RMSE: 0.0297 Test RMSE: 0.0791, Test Mean Module RMSE: 0.0871\n",
      "Epoch 939 Training Loss (NLML): 1.0828 Train RMSE: 0.0296 Test RMSE: 0.0790, Test Mean Module RMSE: 0.0869\n",
      "Epoch 940 Training Loss (NLML): 1.0828 Train RMSE: 0.0296 Test RMSE: 0.0788, Test Mean Module RMSE: 0.0863\n",
      "Epoch 941 Training Loss (NLML): 1.0828 Train RMSE: 0.0295 Test RMSE: 0.0787, Test Mean Module RMSE: 0.0865\n",
      "Epoch 942 Training Loss (NLML): 1.0828 Train RMSE: 0.0294 Test RMSE: 0.0788, Test Mean Module RMSE: 0.0869\n",
      "Epoch 943 Training Loss (NLML): 1.0828 Train RMSE: 0.0294 Test RMSE: 0.0788, Test Mean Module RMSE: 0.0864\n",
      "Epoch 944 Training Loss (NLML): 1.0828 Train RMSE: 0.0293 Test RMSE: 0.0786, Test Mean Module RMSE: 0.0861\n",
      "Epoch 945 Training Loss (NLML): 1.0828 Train RMSE: 0.0293 Test RMSE: 0.0787, Test Mean Module RMSE: 0.0867\n",
      "Epoch 946 Training Loss (NLML): 1.0828 Train RMSE: 0.0292 Test RMSE: 0.0785, Test Mean Module RMSE: 0.0863\n",
      "Epoch 947 Training Loss (NLML): 1.0828 Train RMSE: 0.0291 Test RMSE: 0.0783, Test Mean Module RMSE: 0.0858\n",
      "Epoch 948 Training Loss (NLML): 1.0828 Train RMSE: 0.0291 Test RMSE: 0.0784, Test Mean Module RMSE: 0.0863\n",
      "Epoch 949 Training Loss (NLML): 1.0828 Train RMSE: 0.0290 Test RMSE: 0.0783, Test Mean Module RMSE: 0.0862\n",
      "Epoch 950 Training Loss (NLML): 1.0828 Train RMSE: 0.0290 Test RMSE: 0.0781, Test Mean Module RMSE: 0.0855\n",
      "Epoch 951 Training Loss (NLML): 1.0828 Train RMSE: 0.0289 Test RMSE: 0.0781, Test Mean Module RMSE: 0.0859\n",
      "Epoch 952 Training Loss (NLML): 1.0828 Train RMSE: 0.0289 Test RMSE: 0.0781, Test Mean Module RMSE: 0.0860\n",
      "Epoch 953 Training Loss (NLML): 1.0828 Train RMSE: 0.0288 Test RMSE: 0.0778, Test Mean Module RMSE: 0.0854\n",
      "Epoch 954 Training Loss (NLML): 1.0828 Train RMSE: 0.0287 Test RMSE: 0.0779, Test Mean Module RMSE: 0.0855\n",
      "Epoch 955 Training Loss (NLML): 1.0828 Train RMSE: 0.0287 Test RMSE: 0.0779, Test Mean Module RMSE: 0.0858\n",
      "Epoch 956 Training Loss (NLML): 1.0828 Train RMSE: 0.0286 Test RMSE: 0.0777, Test Mean Module RMSE: 0.0854\n",
      "Epoch 957 Training Loss (NLML): 1.0828 Train RMSE: 0.0286 Test RMSE: 0.0777, Test Mean Module RMSE: 0.0853\n",
      "Epoch 958 Training Loss (NLML): 1.0828 Train RMSE: 0.0285 Test RMSE: 0.0777, Test Mean Module RMSE: 0.0854\n",
      "Epoch 959 Training Loss (NLML): 1.0828 Train RMSE: 0.0285 Test RMSE: 0.0776, Test Mean Module RMSE: 0.0854\n",
      "Epoch 960 Training Loss (NLML): 1.0828 Train RMSE: 0.0284 Test RMSE: 0.0775, Test Mean Module RMSE: 0.0852\n",
      "Epoch 961 Training Loss (NLML): 1.0828 Train RMSE: 0.0284 Test RMSE: 0.0774, Test Mean Module RMSE: 0.0851\n",
      "Epoch 962 Training Loss (NLML): 1.0828 Train RMSE: 0.0283 Test RMSE: 0.0774, Test Mean Module RMSE: 0.0852\n",
      "Epoch 963 Training Loss (NLML): 1.0828 Train RMSE: 0.0283 Test RMSE: 0.0773, Test Mean Module RMSE: 0.0851\n",
      "Epoch 964 Training Loss (NLML): 1.0828 Train RMSE: 0.0282 Test RMSE: 0.0772, Test Mean Module RMSE: 0.0848\n",
      "Epoch 965 Training Loss (NLML): 1.0828 Train RMSE: 0.0282 Test RMSE: 0.0772, Test Mean Module RMSE: 0.0850\n",
      "Epoch 966 Training Loss (NLML): 1.0828 Train RMSE: 0.0281 Test RMSE: 0.0771, Test Mean Module RMSE: 0.0849\n",
      "Epoch 967 Training Loss (NLML): 1.0828 Train RMSE: 0.0281 Test RMSE: 0.0770, Test Mean Module RMSE: 0.0846\n",
      "Epoch 968 Training Loss (NLML): 1.0828 Train RMSE: 0.0280 Test RMSE: 0.0770, Test Mean Module RMSE: 0.0848\n",
      "Epoch 969 Training Loss (NLML): 1.0828 Train RMSE: 0.0280 Test RMSE: 0.0770, Test Mean Module RMSE: 0.0848\n",
      "Epoch 970 Training Loss (NLML): 1.0828 Train RMSE: 0.0279 Test RMSE: 0.0768, Test Mean Module RMSE: 0.0845\n",
      "Epoch 971 Training Loss (NLML): 1.0828 Train RMSE: 0.0279 Test RMSE: 0.0768, Test Mean Module RMSE: 0.0846\n",
      "Epoch 972 Training Loss (NLML): 1.0828 Train RMSE: 0.0278 Test RMSE: 0.0768, Test Mean Module RMSE: 0.0846\n",
      "Epoch 973 Training Loss (NLML): 1.0828 Train RMSE: 0.0278 Test RMSE: 0.0767, Test Mean Module RMSE: 0.0843\n",
      "Epoch 974 Training Loss (NLML): 1.0828 Train RMSE: 0.0277 Test RMSE: 0.0767, Test Mean Module RMSE: 0.0844\n",
      "Epoch 975 Training Loss (NLML): 1.0828 Train RMSE: 0.0277 Test RMSE: 0.0766, Test Mean Module RMSE: 0.0844\n",
      "Epoch 976 Training Loss (NLML): 1.0828 Train RMSE: 0.0276 Test RMSE: 0.0765, Test Mean Module RMSE: 0.0843\n",
      "Epoch 977 Training Loss (NLML): 1.0828 Train RMSE: 0.0276 Test RMSE: 0.0765, Test Mean Module RMSE: 0.0842\n",
      "Epoch 978 Training Loss (NLML): 1.0828 Train RMSE: 0.0276 Test RMSE: 0.0764, Test Mean Module RMSE: 0.0841\n",
      "Epoch 979 Training Loss (NLML): 1.0828 Train RMSE: 0.0275 Test RMSE: 0.0764, Test Mean Module RMSE: 0.0841\n",
      "Epoch 980 Training Loss (NLML): 1.0828 Train RMSE: 0.0275 Test RMSE: 0.0763, Test Mean Module RMSE: 0.0840\n",
      "Epoch 981 Training Loss (NLML): 1.0828 Train RMSE: 0.0274 Test RMSE: 0.0762, Test Mean Module RMSE: 0.0839\n",
      "Epoch 982 Training Loss (NLML): 1.0828 Train RMSE: 0.0274 Test RMSE: 0.0762, Test Mean Module RMSE: 0.0840\n",
      "Epoch 983 Training Loss (NLML): 1.0828 Train RMSE: 0.0273 Test RMSE: 0.0761, Test Mean Module RMSE: 0.0838\n",
      "Epoch 984 Training Loss (NLML): 1.0828 Train RMSE: 0.0273 Test RMSE: 0.0760, Test Mean Module RMSE: 0.0837\n",
      "Epoch 985 Training Loss (NLML): 1.0828 Train RMSE: 0.0273 Test RMSE: 0.0760, Test Mean Module RMSE: 0.0838\n",
      "Epoch 986 Training Loss (NLML): 1.0828 Train RMSE: 0.0272 Test RMSE: 0.0759, Test Mean Module RMSE: 0.0836\n",
      "Epoch 987 Training Loss (NLML): 1.0828 Train RMSE: 0.0272 Test RMSE: 0.0759, Test Mean Module RMSE: 0.0835\n",
      "Epoch 988 Training Loss (NLML): 1.0828 Train RMSE: 0.0271 Test RMSE: 0.0758, Test Mean Module RMSE: 0.0836\n",
      "Epoch 989 Training Loss (NLML): 1.0828 Train RMSE: 0.0271 Test RMSE: 0.0758, Test Mean Module RMSE: 0.0835\n",
      "Epoch 990 Training Loss (NLML): 1.0828 Train RMSE: 0.0270 Test RMSE: 0.0757, Test Mean Module RMSE: 0.0834\n",
      "Epoch 991 Training Loss (NLML): 1.0828 Train RMSE: 0.0270 Test RMSE: 0.0756, Test Mean Module RMSE: 0.0834\n",
      "Epoch 992 Training Loss (NLML): 1.0828 Train RMSE: 0.0270 Test RMSE: 0.0756, Test Mean Module RMSE: 0.0833\n",
      "Epoch 993 Training Loss (NLML): 1.0828 Train RMSE: 0.0269 Test RMSE: 0.0755, Test Mean Module RMSE: 0.0832\n",
      "Epoch 994 Training Loss (NLML): 1.0828 Train RMSE: 0.0269 Test RMSE: 0.0754, Test Mean Module RMSE: 0.0832\n",
      "Epoch 995 Training Loss (NLML): 1.0828 Train RMSE: 0.0268 Test RMSE: 0.0754, Test Mean Module RMSE: 0.0831\n",
      "Epoch 996 Training Loss (NLML): 1.0828 Train RMSE: 0.0268 Test RMSE: 0.0753, Test Mean Module RMSE: 0.0830\n",
      "Epoch 997 Training Loss (NLML): 1.0828 Train RMSE: 0.0267 Test RMSE: 0.0752, Test Mean Module RMSE: 0.0830\n",
      "Epoch 998 Training Loss (NLML): 1.0828 Train RMSE: 0.0267 Test RMSE: 0.0752, Test Mean Module RMSE: 0.0829\n",
      "Epoch 999 Training Loss (NLML): 1.0828 Train RMSE: 0.0267 Test RMSE: 0.0751, Test Mean Module RMSE: 0.0829\n",
      "Epoch 1000 Training Loss (NLML): 1.0828 Train RMSE: 0.0266 Test RMSE: 0.0751, Test Mean Module RMSE: 0.0828\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from simulate import (simulate_detailed_curve,)\n",
    "simulations = {\"curve\": simulate_detailed_curve}\n",
    "\n",
    "from utils import make_grid\n",
    "N_SIDE = 20\n",
    "_, x_test = make_grid(N_SIDE)\n",
    "x_train = torch.load(\"data/sim_data/x_train_lines_discretised_0to1.pt\", weights_only = False).float()\n",
    "\n",
    "y_train = simulate_detailed_curve(x_train.cpu()).to(device)\n",
    "y_test = simulate_detailed_curve(x_test.cpu()).to(device)\n",
    "\n",
    "x_test = x_test.to(device)\n",
    "x_train = x_train.to(device)\n",
    "\n",
    "# likelihood = gpytorch.likelihoods.GaussianLikelihood().to(device)\n",
    "likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks = 2).to(device)\n",
    "\n",
    "model = dfNGP(\n",
    "            x_train,\n",
    "            y_train, \n",
    "            likelihood,\n",
    "            ).to(device)\n",
    "\n",
    "for name, param in likelihood.named_parameters():\n",
    "    print(name, param.shape)\n",
    "\n",
    "# likelihood.raw_task_noises\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape)\n",
    "\n",
    "# We can actually set it as such\n",
    "# model.covar_module.outputscale = 0.001\n",
    "# print(model.covar_module.raw_outputscale)\n",
    "# print(model.covar_module.outputscale)\n",
    "\n",
    "# Set to not require grad\n",
    "# model.covar_module.raw_outputscale.requires_grad_(False)\n",
    "\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "            {\"params\": model.mean_module.parameters(), \"weight_decay\": 0.0001, \"lr\": 0.05},\n",
    "            # {\"params\": list(model.covar_module.parameters()) + list(model.likelihood.parameters()), \"weight_decay\": # WEIGHT_DECAY, \"lr\": MODEL_LEARNING_RATE},\n",
    "            ])\n",
    "\n",
    "for i in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    train_pred_dist = model(x_train.to(device))\n",
    "    # Train on noisy or true targets?\n",
    "    loss = - mll(train_pred_dist, y_train.to(device))  # negative marginal log likelihood\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "\n",
    "    train_pred_dist = model(x_train.to(device))\n",
    "    test_pred_dist = model(x_test.to(device))\n",
    "\n",
    "    train_RMSE = torch.sqrt(torch.mean(torch.square(train_pred_dist.mean - y_train)))\n",
    "    test_RMSE = torch.sqrt(torch.mean(torch.square(test_pred_dist.mean - y_test)))\n",
    "    test_mean_module_RMSE = torch.sqrt(torch.mean(torch.square(model.mean_module(x_test.to(device)).detach() - y_test.to(device))))\n",
    "\n",
    "    print(f\"Epoch {i + 1} Training Loss (NLML): {loss:.4f} Train RMSE: {train_RMSE:.4f} Test RMSE: {test_RMSE:.4f}, Test Mean Module RMSE: {test_mean_module_RMSE:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9bba45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6931, device='cuda:0', grad_fn=<SoftplusBackward0>)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likelihood.eval()\n",
    "likelihood.raw_task_noises\n",
    "likelihood.task_noises\n",
    "\n",
    "model.covar_module.outputscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "248bd0d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from visualise import visualise_v_quiver\n",
    "\n",
    "mean_model_out = model.mean_module(x_test.to(device)).detach()\n",
    "\n",
    "visualise_v_quiver(\n",
    "    test_pred_dist.mean,\n",
    "    x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "1acedb55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]], device='cuda:0', grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likelihood(test_pred_dist).mean - test_pred_dist.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "4a60cba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7644, 0.7659],\n",
       "        [0.7662, 0.7651],\n",
       "        [0.7627, 0.7591],\n",
       "        [0.7542, 0.7480],\n",
       "        [0.7406, 0.7320],\n",
       "        [0.7222, 0.7113],\n",
       "        [0.6994, 0.6864],\n",
       "        [0.6724, 0.6576],\n",
       "        [0.6419, 0.6254],\n",
       "        [0.6083, 0.5905],\n",
       "        [0.7789, 0.7809],\n",
       "        [0.7816, 0.7809],\n",
       "        [0.7789, 0.7755],\n",
       "        [0.7708, 0.7647],\n",
       "        [0.7574, 0.7488],\n",
       "        [0.7390, 0.7280],\n",
       "        [0.7158, 0.7026],\n",
       "        [0.6884, 0.6732],\n",
       "        [0.6571, 0.6402],\n",
       "        [0.6226, 0.6043],\n",
       "        [0.7907, 0.7932],\n",
       "        [0.7942, 0.7939],\n",
       "        [0.7922, 0.7890],\n",
       "        [0.7845, 0.7786],\n",
       "        [0.7714, 0.7628],\n",
       "        [0.7529, 0.7418],\n",
       "        [0.7295, 0.7161],\n",
       "        [0.7016, 0.6861],\n",
       "        [0.6697, 0.6524],\n",
       "        [0.6343, 0.6155],\n",
       "        [0.7998, 0.8026],\n",
       "        [0.8041, 0.8041],\n",
       "        [0.8026, 0.7997],\n",
       "        [0.7954, 0.7896],\n",
       "        [0.7824, 0.7739],\n",
       "        [0.7640, 0.7529],\n",
       "        [0.7404, 0.7268],\n",
       "        [0.7121, 0.6963],\n",
       "        [0.6796, 0.6619],\n",
       "        [0.6434, 0.6242],\n",
       "        [0.8060, 0.8092],\n",
       "        [0.8110, 0.8114],\n",
       "        [0.8102, 0.8075],\n",
       "        [0.8034, 0.7977],\n",
       "        [0.7907, 0.7822],\n",
       "        [0.7723, 0.7611],\n",
       "        [0.7485, 0.7348],\n",
       "        [0.7199, 0.7039],\n",
       "        [0.6869, 0.6689],\n",
       "        [0.6501, 0.6305],\n",
       "        [0.8093, 0.8130],\n",
       "        [0.8151, 0.8158],\n",
       "        [0.8148, 0.8124],\n",
       "        [0.8085, 0.8030],\n",
       "        [0.7960, 0.7876],\n",
       "        [0.7777, 0.7665],\n",
       "        [0.7539, 0.7400],\n",
       "        [0.7250, 0.7088],\n",
       "        [0.6915, 0.6733],\n",
       "        [0.6542, 0.6343],\n",
       "        [0.8099, 0.8139],\n",
       "        [0.8163, 0.8173],\n",
       "        [0.8166, 0.8144],\n",
       "        [0.8107, 0.8053],\n",
       "        [0.7985, 0.7901],\n",
       "        [0.7803, 0.7691],\n",
       "        [0.7565, 0.7426],\n",
       "        [0.7274, 0.7111],\n",
       "        [0.6937, 0.6753],\n",
       "        [0.6559, 0.6358],\n",
       "        [0.8076, 0.8119],\n",
       "        [0.8147, 0.8159],\n",
       "        [0.8155, 0.8136],\n",
       "        [0.8100, 0.8049],\n",
       "        [0.7982, 0.7899],\n",
       "        [0.7802, 0.7690],\n",
       "        [0.7564, 0.7425],\n",
       "        [0.7273, 0.7109],\n",
       "        [0.6934, 0.6748],\n",
       "        [0.6553, 0.6350],\n",
       "        [0.8025, 0.8071],\n",
       "        [0.8102, 0.8117],\n",
       "        [0.8116, 0.8099],\n",
       "        [0.8066, 0.8016],\n",
       "        [0.7951, 0.7870],\n",
       "        [0.7774, 0.7663],\n",
       "        [0.7537, 0.7398],\n",
       "        [0.7247, 0.7082],\n",
       "        [0.6907, 0.6721],\n",
       "        [0.6525, 0.6321],\n",
       "        [0.7946, 0.7996],\n",
       "        [0.8030, 0.8048],\n",
       "        [0.8049, 0.8035],\n",
       "        [0.8004, 0.7957],\n",
       "        [0.7893, 0.7814],\n",
       "        [0.7719, 0.7610],\n",
       "        [0.7486, 0.7348],\n",
       "        [0.7196, 0.7033],\n",
       "        [0.6858, 0.6672],\n",
       "        [0.6476, 0.6271],\n",
       "        [0.7841, 0.7893],\n",
       "        [0.7930, 0.7951],\n",
       "        [0.7956, 0.7944],\n",
       "        [0.7915, 0.7870],\n",
       "        [0.7809, 0.7732],\n",
       "        [0.7640, 0.7532],\n",
       "        [0.7409, 0.7273],\n",
       "        [0.7123, 0.6961],\n",
       "        [0.6787, 0.6602],\n",
       "        [0.6407, 0.6203],\n",
       "        [0.7709, 0.7764],\n",
       "        [0.7804, 0.7828],\n",
       "        [0.7835, 0.7826],\n",
       "        [0.7801, 0.7759],\n",
       "        [0.7700, 0.7626],\n",
       "        [0.7536, 0.7430],\n",
       "        [0.7310, 0.7176],\n",
       "        [0.7028, 0.6868],\n",
       "        [0.6695, 0.6512],\n",
       "        [0.6318, 0.6116],\n",
       "        [0.7551, 0.7609],\n",
       "        [0.7652, 0.7679],\n",
       "        [0.7690, 0.7684],\n",
       "        [0.7661, 0.7622],\n",
       "        [0.7567, 0.7496],\n",
       "        [0.7409, 0.7306],\n",
       "        [0.7189, 0.7057],\n",
       "        [0.6912, 0.6754],\n",
       "        [0.6585, 0.6404],\n",
       "        [0.6213, 0.6013],\n",
       "        [0.7368, 0.7430],\n",
       "        [0.7476, 0.7506],\n",
       "        [0.7520, 0.7517],\n",
       "        [0.7498, 0.7463],\n",
       "        [0.7411, 0.7343],\n",
       "        [0.7259, 0.7161],\n",
       "        [0.7047, 0.6919],\n",
       "        [0.6777, 0.6623],\n",
       "        [0.6456, 0.6279],\n",
       "        [0.6091, 0.5894],\n",
       "        [0.7162, 0.7227],\n",
       "        [0.7276, 0.7309],\n",
       "        [0.7326, 0.7327],\n",
       "        [0.7312, 0.7281],\n",
       "        [0.7233, 0.7169],\n",
       "        [0.7090, 0.6995],\n",
       "        [0.6885, 0.6761],\n",
       "        [0.6624, 0.6474],\n",
       "        [0.6312, 0.6138],\n",
       "        [0.5955, 0.5762],\n",
       "        [0.6933, 0.7001],\n",
       "        [0.7053, 0.7090],\n",
       "        [0.7111, 0.7116],\n",
       "        [0.7105, 0.7077],\n",
       "        [0.7034, 0.6975],\n",
       "        [0.6900, 0.6810],\n",
       "        [0.6705, 0.6587],\n",
       "        [0.6454, 0.6309],\n",
       "        [0.6152, 0.5983],\n",
       "        [0.5805, 0.5617],\n",
       "        [0.6683, 0.6754],\n",
       "        [0.6809, 0.6850],\n",
       "        [0.6875, 0.6884],\n",
       "        [0.6877, 0.6855],\n",
       "        [0.6816, 0.6762],\n",
       "        [0.6693, 0.6608],\n",
       "        [0.6509, 0.6396],\n",
       "        [0.6269, 0.6129],\n",
       "        [0.5978, 0.5815],\n",
       "        [0.5643, 0.5461],\n",
       "        [0.6413, 0.6486],\n",
       "        [0.6546, 0.6590],\n",
       "        [0.6619, 0.6633],\n",
       "        [0.6631, 0.6614],\n",
       "        [0.6581, 0.6532],\n",
       "        [0.6468, 0.6390],\n",
       "        [0.6297, 0.6190],\n",
       "        [0.6070, 0.5937],\n",
       "        [0.5792, 0.5636],\n",
       "        [0.5470, 0.5295],\n",
       "        [0.6124, 0.6201],\n",
       "        [0.6263, 0.6312],\n",
       "        [0.6346, 0.6364],\n",
       "        [0.6368, 0.6356],\n",
       "        [0.6328, 0.6286],\n",
       "        [0.6229, 0.6157],\n",
       "        [0.6071, 0.5971],\n",
       "        [0.5858, 0.5732],\n",
       "        [0.5595, 0.5446],\n",
       "        [0.5288, 0.5120],\n",
       "        [0.5817, 0.5898],\n",
       "        [0.5964, 0.6017],\n",
       "        [0.6055, 0.6079],\n",
       "        [0.6088, 0.6082],\n",
       "        [0.6061, 0.6026],\n",
       "        [0.5975, 0.5911],\n",
       "        [0.5832, 0.5740],\n",
       "        [0.5635, 0.5517],\n",
       "        [0.5388, 0.5248],\n",
       "        [0.5098, 0.4938],\n",
       "        [0.8126, 0.8084],\n",
       "        [0.8002, 0.7881],\n",
       "        [0.7722, 0.7525],\n",
       "        [0.7292, 0.7024],\n",
       "        [0.6724, 0.6394],\n",
       "        [0.6035, 0.5652],\n",
       "        [0.5245, 0.4819],\n",
       "        [0.4377, 0.3921],\n",
       "        [0.3454, 0.2981],\n",
       "        [0.2503, 0.2025],\n",
       "        [0.8102, 0.8073],\n",
       "        [0.8004, 0.7897],\n",
       "        [0.7750, 0.7566],\n",
       "        [0.7346, 0.7091],\n",
       "        [0.6804, 0.6486],\n",
       "        [0.6139, 0.5767],\n",
       "        [0.5371, 0.4955],\n",
       "        [0.4522, 0.4075],\n",
       "        [0.3617, 0.3151],\n",
       "        [0.2681, 0.2209],\n",
       "        [0.8073, 0.8058],\n",
       "        [0.8003, 0.7909],\n",
       "        [0.7776, 0.7606],\n",
       "        [0.7399, 0.7158],\n",
       "        [0.6883, 0.6578],\n",
       "        [0.6244, 0.5883],\n",
       "        [0.5499, 0.5094],\n",
       "        [0.4671, 0.4234],\n",
       "        [0.3784, 0.3326],\n",
       "        [0.2863, 0.2397],\n",
       "        [0.8040, 0.8038],\n",
       "        [0.7997, 0.7917],\n",
       "        [0.7799, 0.7643],\n",
       "        [0.7451, 0.7223],\n",
       "        [0.6962, 0.6670],\n",
       "        [0.6349, 0.6001],\n",
       "        [0.5628, 0.5235],\n",
       "        [0.4822, 0.4395],\n",
       "        [0.3955, 0.3505],\n",
       "        [0.3050, 0.2591],\n",
       "        [0.8002, 0.8015],\n",
       "        [0.7988, 0.7923],\n",
       "        [0.7820, 0.7678],\n",
       "        [0.7500, 0.7287],\n",
       "        [0.7041, 0.6762],\n",
       "        [0.6454, 0.6119],\n",
       "        [0.5759, 0.5377],\n",
       "        [0.4976, 0.4559],\n",
       "        [0.4128, 0.3688],\n",
       "        [0.3240, 0.2788],\n",
       "        [0.7960, 0.7987],\n",
       "        [0.7976, 0.7926],\n",
       "        [0.7837, 0.7711],\n",
       "        [0.7548, 0.7350],\n",
       "        [0.7118, 0.6854],\n",
       "        [0.6560, 0.6238],\n",
       "        [0.5891, 0.5521],\n",
       "        [0.5132, 0.4725],\n",
       "        [0.4305, 0.3873],\n",
       "        [0.3434, 0.2990],\n",
       "        [0.7914, 0.7957],\n",
       "        [0.7961, 0.7926],\n",
       "        [0.7853, 0.7742],\n",
       "        [0.7595, 0.7412],\n",
       "        [0.7195, 0.6945],\n",
       "        [0.6665, 0.6357],\n",
       "        [0.6023, 0.5666],\n",
       "        [0.5289, 0.4893],\n",
       "        [0.4483, 0.4061],\n",
       "        [0.3630, 0.3194],\n",
       "        [0.7865, 0.7923],\n",
       "        [0.7942, 0.7923],\n",
       "        [0.7866, 0.7771],\n",
       "        [0.7639, 0.7472],\n",
       "        [0.7270, 0.7036],\n",
       "        [0.6770, 0.6477],\n",
       "        [0.6156, 0.5812],\n",
       "        [0.5447, 0.5063],\n",
       "        [0.4663, 0.4251],\n",
       "        [0.3829, 0.3400],\n",
       "        [0.7812, 0.7885],\n",
       "        [0.7921, 0.7917],\n",
       "        [0.7876, 0.7797],\n",
       "        [0.7682, 0.7530],\n",
       "        [0.7344, 0.7125],\n",
       "        [0.6875, 0.6595],\n",
       "        [0.6289, 0.5958],\n",
       "        [0.5605, 0.5233],\n",
       "        [0.4844, 0.4442],\n",
       "        [0.4029, 0.3608],\n",
       "        [0.7757, 0.7845],\n",
       "        [0.7896, 0.7909],\n",
       "        [0.7884, 0.7822],\n",
       "        [0.7722, 0.7587],\n",
       "        [0.7416, 0.7213],\n",
       "        [0.6978, 0.6713],\n",
       "        [0.6420, 0.6103],\n",
       "        [0.5763, 0.5402],\n",
       "        [0.5025, 0.4633],\n",
       "        [0.4229, 0.3816],\n",
       "        [0.7698, 0.7802],\n",
       "        [0.7869, 0.7898],\n",
       "        [0.7889, 0.7843],\n",
       "        [0.7760, 0.7641],\n",
       "        [0.7487, 0.7299],\n",
       "        [0.7079, 0.6829],\n",
       "        [0.6551, 0.6247],\n",
       "        [0.5920, 0.5571],\n",
       "        [0.5205, 0.4823],\n",
       "        [0.4429, 0.4025],\n",
       "        [0.7636, 0.7756],\n",
       "        [0.7838, 0.7884],\n",
       "        [0.7891, 0.7862],\n",
       "        [0.7795, 0.7692],\n",
       "        [0.7554, 0.7383],\n",
       "        [0.7178, 0.6943],\n",
       "        [0.6679, 0.6389],\n",
       "        [0.6075, 0.5739],\n",
       "        [0.5384, 0.5013],\n",
       "        [0.4628, 0.4232],\n",
       "        [0.7571, 0.7706],\n",
       "        [0.7805, 0.7867],\n",
       "        [0.7891, 0.7878],\n",
       "        [0.7828, 0.7741],\n",
       "        [0.7619, 0.7463],\n",
       "        [0.7275, 0.7055],\n",
       "        [0.6805, 0.6529],\n",
       "        [0.6228, 0.5904],\n",
       "        [0.5561, 0.5200],\n",
       "        [0.4825, 0.4438],\n",
       "        [0.7503, 0.7654],\n",
       "        [0.7768, 0.7846],\n",
       "        [0.7887, 0.7890],\n",
       "        [0.7857, 0.7787],\n",
       "        [0.7681, 0.7541],\n",
       "        [0.7368, 0.7163],\n",
       "        [0.6928, 0.6666],\n",
       "        [0.6378, 0.6067],\n",
       "        [0.5735, 0.5385],\n",
       "        [0.5019, 0.4641],\n",
       "        [0.7433, 0.7599],\n",
       "        [0.7729, 0.7823],\n",
       "        [0.7879, 0.7899],\n",
       "        [0.7882, 0.7828],\n",
       "        [0.7739, 0.7615],\n",
       "        [0.7457, 0.7267],\n",
       "        [0.7047, 0.6799],\n",
       "        [0.6524, 0.6225],\n",
       "        [0.5905, 0.5565],\n",
       "        [0.5210, 0.4840],\n",
       "        [0.7359, 0.7540],\n",
       "        [0.7686, 0.7795],\n",
       "        [0.7868, 0.7904],\n",
       "        [0.7903, 0.7866],\n",
       "        [0.7792, 0.7684],\n",
       "        [0.7542, 0.7367],\n",
       "        [0.7162, 0.6927],\n",
       "        [0.6665, 0.6379],\n",
       "        [0.6070, 0.5742],\n",
       "        [0.5396, 0.5035],\n",
       "        [0.7283, 0.7479],\n",
       "        [0.7640, 0.7765],\n",
       "        [0.7853, 0.7905],\n",
       "        [0.7920, 0.7899],\n",
       "        [0.7841, 0.7749],\n",
       "        [0.7622, 0.7462],\n",
       "        [0.7271, 0.7050],\n",
       "        [0.6802, 0.6528],\n",
       "        [0.6230, 0.5912],\n",
       "        [0.5576, 0.5224],\n",
       "        [0.7204, 0.7414],\n",
       "        [0.7590, 0.7730],\n",
       "        [0.7834, 0.7902],\n",
       "        [0.7932, 0.7927],\n",
       "        [0.7885, 0.7808],\n",
       "        [0.7696, 0.7551],\n",
       "        [0.7374, 0.7167],\n",
       "        [0.6932, 0.6670],\n",
       "        [0.6384, 0.6077],\n",
       "        [0.5751, 0.5407],\n",
       "        [0.7122, 0.7346],\n",
       "        [0.7536, 0.7691],\n",
       "        [0.7810, 0.7893],\n",
       "        [0.7939, 0.7949],\n",
       "        [0.7923, 0.7861],\n",
       "        [0.7764, 0.7633],\n",
       "        [0.7471, 0.7277],\n",
       "        [0.7055, 0.6805],\n",
       "        [0.6531, 0.6235],\n",
       "        [0.5918, 0.5583],\n",
       "        [0.7037, 0.7274],\n",
       "        [0.7478, 0.7648],\n",
       "        [0.7782, 0.7879],\n",
       "        [0.7941, 0.7966],\n",
       "        [0.7954, 0.7907],\n",
       "        [0.7825, 0.7709],\n",
       "        [0.7560, 0.7380],\n",
       "        [0.7170, 0.6933],\n",
       "        [0.6670, 0.6384],\n",
       "        [0.6077, 0.5751]], device='cuda:0', grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_dist.mean - mean_model_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "143159ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     0.7644,      1.9055],\n",
       "        [     0.5802,      0.0026],\n",
       "        [     0.8037,      1.8130],\n",
       "        [     0.7528,      0.1417],\n",
       "        [     0.7647,      2.0711],\n",
       "        [     0.8023,      0.6639],\n",
       "        [     0.7512,      2.4637],\n",
       "        [     0.8062,      0.9762],\n",
       "        [     0.8091,      2.4176],\n",
       "        [     0.6901,      0.9267],\n",
       "        [     0.8289,      2.1596],\n",
       "        [     0.4711,      0.8379],\n",
       "        [     0.7856,      1.9160],\n",
       "        [     0.2825,      0.8057],\n",
       "        [     0.6993,      1.7157],\n",
       "        [     0.1883,      0.8185],\n",
       "        [     0.5814,      1.5591],\n",
       "        [     0.1468,      0.8689],\n",
       "        [     0.4246,      1.4624],\n",
       "        [     0.0880,      0.9589],\n",
       "        [     0.3032,      1.8953],\n",
       "        [     0.1624,      1.3340],\n",
       "        [     0.2863,      1.5895],\n",
       "        [     0.2196,      1.2484],\n",
       "        [     0.2588,      1.6623],\n",
       "        [     0.2735,      1.4566],\n",
       "        [     0.3149,      1.8483],\n",
       "        [     0.4094,      1.4189],\n",
       "        [     0.4488,      1.6914],\n",
       "        [     0.5428,      1.0560],\n",
       "        [     0.5674,      1.4018],\n",
       "        [     0.6184,      0.6615],\n",
       "        [     0.6531,      1.1736],\n",
       "        [     0.6304,      0.3444],\n",
       "        [     0.7031,      1.0336],\n",
       "        [     0.5669,      0.1206],\n",
       "        [     0.7037,      0.9790],\n",
       "        [     0.4109,     -0.0008],\n",
       "        [     0.6436,      1.0107],\n",
       "        [     0.1627,     -0.0215],\n",
       "        [     0.6600,      1.7464],\n",
       "        [     0.1101,     -0.1110],\n",
       "        [     0.5821,      1.6754],\n",
       "        [     0.1563,      0.0545],\n",
       "        [     0.5399,      1.9556],\n",
       "        [     0.2192,      0.5060],\n",
       "        [     0.6333,      2.2830],\n",
       "        [     0.3077,      0.7642],\n",
       "        [     0.8068,      2.2832],\n",
       "        [     0.3159,      0.7705],\n",
       "        [     0.9406,      2.1322],\n",
       "        [     0.2551,      0.7580],\n",
       "        [     0.9969,      1.9988],\n",
       "        [     0.2056,      0.8086],\n",
       "        [     0.9696,      1.9148],\n",
       "        [     0.1876,      0.9194],\n",
       "        [     0.8562,      1.8804],\n",
       "        [     0.1610,      1.0759],\n",
       "        [     0.6654,      1.8911],\n",
       "        [     0.0955,      1.2485],\n",
       "        [     0.3825,      2.2643],\n",
       "        [    -0.0209,      1.5285],\n",
       "        [     0.2134,      1.9463],\n",
       "        [    -0.0485,      1.4188],\n",
       "        [     0.1217,      1.9233],\n",
       "        [     0.0218,      1.4811],\n",
       "        [     0.1943,      1.9434],\n",
       "        [     0.2087,      1.3392],\n",
       "        [     0.3631,      1.7359],\n",
       "        [     0.3978,      0.9741],\n",
       "        [     0.5302,      1.4563],\n",
       "        [     0.5278,      0.6045],\n",
       "        [     0.6656,      1.2584],\n",
       "        [     0.5768,      0.3338],\n",
       "        [     0.7498,      1.1748],\n",
       "        [     0.5232,      0.1818],\n",
       "        [     0.7644,      1.1965],\n",
       "        [     0.3602,      0.1395],\n",
       "        [     0.7147,      1.2945],\n",
       "        [     0.1185,      0.1674],\n",
       "        [     0.7561,      1.6425],\n",
       "        [     0.0114,     -0.0552],\n",
       "        [     0.6593,      1.4601],\n",
       "        [     0.0276,      0.0047],\n",
       "        [     0.6478,      1.5845],\n",
       "        [     0.1120,      0.2712],\n",
       "        [     0.7905,      1.7757],\n",
       "        [     0.2337,      0.4399],\n",
       "        [     0.9895,      1.8034],\n",
       "        [     0.2866,      0.4757],\n",
       "        [     1.1295,      1.7672],\n",
       "        [     0.2781,      0.5330],\n",
       "        [     1.1791,      1.7869],\n",
       "        [     0.2673,      0.6835],\n",
       "        [     1.1363,      1.8865],\n",
       "        [     0.2658,      0.9209],\n",
       "        [     1.0063,      2.0427],\n",
       "        [     0.2505,      1.2010],\n",
       "        [     0.8157,      2.2047],\n",
       "        [     0.2167,      1.4528],\n",
       "        [     0.4610,      2.6557],\n",
       "        [     0.0089,      1.7568],\n",
       "        [     0.2492,      2.3392],\n",
       "        [    -0.0330,      1.5863],\n",
       "        [     0.1481,      2.1872],\n",
       "        [     0.0457,      1.4875],\n",
       "        [     0.2206,      2.0526],\n",
       "        [     0.2301,      1.2605],\n",
       "        [     0.3822,      1.8026],\n",
       "        [     0.4032,      0.9019],\n",
       "        [     0.5357,      1.5332],\n",
       "        [     0.5031,      0.5621],\n",
       "        [     0.6522,      1.3647],\n",
       "        [     0.5161,      0.3391],\n",
       "        [     0.7216,      1.3346],\n",
       "        [     0.4362,      0.2526],\n",
       "        [     0.7387,      1.4188],\n",
       "        [     0.2755,      0.2657],\n",
       "        [     0.7205,      1.5528],\n",
       "        [     0.0742,      0.3083],\n",
       "        [     0.7234,      1.7988],\n",
       "        [    -0.1225,      0.1664],\n",
       "        [     0.6559,      1.4055],\n",
       "        [    -0.0868,      0.0626],\n",
       "        [     0.6810,      1.2232],\n",
       "        [     0.0208,      0.0933],\n",
       "        [     0.8416,      1.1591],\n",
       "        [     0.1658,      0.1322],\n",
       "        [     1.0402,      1.1121],\n",
       "        [     0.2535,      0.1590],\n",
       "        [     1.1639,      1.1203],\n",
       "        [     0.2735,      0.2550],\n",
       "        [     1.1827,      1.2515],\n",
       "        [     0.2722,      0.4737],\n",
       "        [     1.1111,      1.5164],\n",
       "        [     0.2738,      0.8024],\n",
       "        [     0.9752,      1.8621],\n",
       "        [     0.2787,      1.1700],\n",
       "        [     0.8123,      2.1933],\n",
       "        [     0.2889,      1.4825],\n",
       "        [     0.3350,      2.8394],\n",
       "        [    -0.0623,      1.8903],\n",
       "        [     0.1317,      2.5863],\n",
       "        [    -0.0969,      1.6751],\n",
       "        [     0.0390,      2.3698],\n",
       "        [    -0.0250,      1.4592],\n",
       "        [     0.1062,      2.1457],\n",
       "        [     0.1424,      1.1824],\n",
       "        [     0.2690,      1.8896],\n",
       "        [     0.3042,      0.8582],\n",
       "        [     0.4271,      1.6577],\n",
       "        [     0.3830,      0.5703],\n",
       "        [     0.5342,      1.5277],\n",
       "        [     0.3619,      0.3937],\n",
       "        [     0.5877,      1.5337],\n",
       "        [     0.2567,      0.3444],\n",
       "        [     0.6036,      1.6425],\n",
       "        [     0.1014,      0.3711],\n",
       "        [     0.6077,      1.7750],\n",
       "        [    -0.0643,      0.3997],\n",
       "        [     0.5068,      2.2457],\n",
       "        [    -0.4240,      0.4625],\n",
       "        [     0.4566,      1.7514],\n",
       "        [    -0.3747,      0.2554],\n",
       "        [     0.4891,      1.3047],\n",
       "        [    -0.2643,      0.0906],\n",
       "        [     0.6347,      0.9311],\n",
       "        [    -0.1027,     -0.0224],\n",
       "        [     0.8318,      0.6696],\n",
       "        [     0.0454,     -0.0511],\n",
       "        [     0.9760,      0.5680],\n",
       "        [     0.1362,      0.0418],\n",
       "        [     1.0096,      0.6592],\n",
       "        [     0.1772,      0.2739],\n",
       "        [     0.9407,      0.9380],\n",
       "        [     0.1968,      0.6185],\n",
       "        [     0.8131,      1.3402],\n",
       "        [     0.2183,      0.9968],\n",
       "        [     0.6747,      1.7617],\n",
       "        [     0.2513,      1.3234],\n",
       "        [     0.1128,      2.6486],\n",
       "        [    -0.2268,      1.8688],\n",
       "        [    -0.1176,      2.5009],\n",
       "        [    -0.3018,      1.6281],\n",
       "        [    -0.2430,      2.3184],\n",
       "        [    -0.2891,      1.3462],\n",
       "        [    -0.2218,      2.0963],\n",
       "        [    -0.1781,      1.0415],\n",
       "        [    -0.0738,      1.8820],\n",
       "        [    -0.0240,      0.7666],\n",
       "        [     0.1213,      1.7343],\n",
       "        [     0.0848,      0.5672],\n",
       "        [     0.2788,      1.6890],\n",
       "        [     0.0903,      0.4667],\n",
       "        [     0.3625,      1.7486],\n",
       "        [    -0.0025,      0.4533],\n",
       "        [     0.3910,      1.8691],\n",
       "        [    -0.1461,      0.4731],\n",
       "        [     0.4016,      1.9821],\n",
       "        [    -0.2940,      0.4732],\n",
       "        [     0.4148,      2.7432],\n",
       "        [    -0.6604,      0.7280],\n",
       "        [     0.2850,      2.2821],\n",
       "        [    -0.6916,      0.4828],\n",
       "        [     0.2209,      1.7735],\n",
       "        [    -0.6689,      0.2168],\n",
       "        [     0.2560,      1.2534],\n",
       "        [    -0.5703,     -0.0105],\n",
       "        [     0.3815,      0.8189],\n",
       "        [    -0.4034,     -0.1102],\n",
       "        [     0.5404,      0.5665],\n",
       "        [    -0.2139,     -0.0332],\n",
       "        [     0.6488,      0.5329],\n",
       "        [    -0.0616,      0.2026],\n",
       "        [     0.6541,      0.6878],\n",
       "        [     0.0301,      0.5266],\n",
       "        [     0.5735,      0.9568],\n",
       "        [     0.0831,      0.8501],\n",
       "        [     0.4610,      1.2646],\n",
       "        [     0.1237,      1.1172],\n",
       "        [     0.2748,      2.2976],\n",
       "        [    -0.0330,      1.8259],\n",
       "        [    -0.0423,      2.1680],\n",
       "        [    -0.2610,      1.5336],\n",
       "        [    -0.3348,      2.0066],\n",
       "        [    -0.4589,      1.1846],\n",
       "        [    -0.5134,      1.8153],\n",
       "        [    -0.5622,      0.8286],\n",
       "        [    -0.5296,      1.6529],\n",
       "        [    -0.5448,      0.5550],\n",
       "        [    -0.3891,      1.5991],\n",
       "        [    -0.4447,      0.4305],\n",
       "        [    -0.1715,      1.6878],\n",
       "        [    -0.3579,      0.4469],\n",
       "        [     0.0124,      1.8729],\n",
       "        [    -0.3619,      0.5228],\n",
       "        [     0.1097,      2.0594],\n",
       "        [    -0.4523,      0.5681],\n",
       "        [     0.1429,      2.1800],\n",
       "        [    -0.5781,      0.5518],\n",
       "        [     0.8490,      3.1269],\n",
       "        [    -0.2896,      0.9770],\n",
       "        [     0.8002,      2.8018],\n",
       "        [    -0.3615,      0.7770],\n",
       "        [     0.5715,      2.3418],\n",
       "        [    -0.5644,      0.4715],\n",
       "        [     0.2987,      1.8063],\n",
       "        [    -0.7625,      0.1474],\n",
       "        [     0.0956,      1.3047],\n",
       "        [    -0.8363,     -0.0663],\n",
       "        [     0.0382,      0.9703],\n",
       "        [    -0.7302,     -0.0562],\n",
       "        [     0.1148,      0.8841],\n",
       "        [    -0.5014,      0.1903],\n",
       "        [     0.2211,      1.0026],\n",
       "        [    -0.2770,      0.5507],\n",
       "        [     0.2553,      1.1831],\n",
       "        [    -0.1344,      0.8649],\n",
       "        [     0.2071,      1.3240],\n",
       "        [    -0.0659,      1.0703],\n",
       "        [     0.5201,      2.1154],\n",
       "        [     0.4960,      1.7722],\n",
       "        [     0.6507,      2.0223],\n",
       "        [     0.5601,      1.5791],\n",
       "        [     0.4936,      1.8634],\n",
       "        [     0.2971,      1.2373],\n",
       "        [     0.1095,      1.6431],\n",
       "        [    -0.1622,      0.8103],\n",
       "        [    -0.3289,      1.4282],\n",
       "        [    -0.6204,      0.4177],\n",
       "        [    -0.6237,      1.3265],\n",
       "        [    -0.8953,      0.2025],\n",
       "        [    -0.6565,      1.4378],\n",
       "        [    -0.9393,      0.2367],\n",
       "        [    -0.4842,      1.7470],\n",
       "        [    -0.8722,      0.4259],\n",
       "        [    -0.2813,      2.0913],\n",
       "        [    -0.8465,      0.5804],\n",
       "        [    -0.1658,      2.3164],\n",
       "        [    -0.9046,      0.6113],\n",
       "        [     0.6151,      3.0254],\n",
       "        [    -0.4913,      0.7941],\n",
       "        [     1.0602,      2.9082],\n",
       "        [     0.0349,      0.8609],\n",
       "        [     1.3868,      2.7389],\n",
       "        [     0.2850,      0.8013],\n",
       "        [     1.3553,      2.4234],\n",
       "        [     0.1502,      0.5887],\n",
       "        [     0.9674,      1.9960],\n",
       "        [    -0.2518,      0.3068],\n",
       "        [     0.4120,      1.5784],\n",
       "        [    -0.6579,      0.1168],\n",
       "        [    -0.0281,      1.3481],\n",
       "        [    -0.8125,      0.1849],\n",
       "        [    -0.1826,      1.4077],\n",
       "        [    -0.6831,      0.5117],\n",
       "        [    -0.1378,      1.6442],\n",
       "        [    -0.4608,      0.8932],\n",
       "        [    -0.0898,      1.8378],\n",
       "        [    -0.3166,      1.1462],\n",
       "        [    -0.0555,      2.3032],\n",
       "        [    -0.1275,      1.4937],\n",
       "        [     0.1009,      2.0249],\n",
       "        [     0.1890,      1.3288],\n",
       "        [     0.4320,      1.8819],\n",
       "        [     0.5351,      1.1906],\n",
       "        [     0.7076,      1.7809],\n",
       "        [     0.6450,      0.9823],\n",
       "        [     0.6935,      1.6575],\n",
       "        [     0.3684,      0.6853],\n",
       "        [     0.3351,      1.5208],\n",
       "        [    -0.2281,      0.3672],\n",
       "        [    -0.1959,      1.4562],\n",
       "        [    -0.8575,      0.1750],\n",
       "        [    -0.5750,      1.5928],\n",
       "        [    -1.2131,      0.2229],\n",
       "        [    -0.6314,      1.9387],\n",
       "        [    -1.2815,      0.4219],\n",
       "        [    -0.5159,      2.3095],\n",
       "        [    -1.2795,      0.5672],\n",
       "        [     0.1682,      3.0151],\n",
       "        [    -1.2969,      0.5619],\n",
       "        [     0.3090,      2.7328],\n",
       "        [    -0.9938,      0.5263],\n",
       "        [     0.5666,      2.5439],\n",
       "        [    -0.5715,      0.5633],\n",
       "        [     0.9165,      2.4246],\n",
       "        [    -0.1101,      0.6214],\n",
       "        [     1.2068,      2.3186],\n",
       "        [     0.2081,      0.6465],\n",
       "        [     1.2393,      2.1869],\n",
       "        [     0.2166,      0.6216],\n",
       "        [     0.9089,      2.0344],\n",
       "        [    -0.0923,      0.5863],\n",
       "        [     0.3466,      1.9334],\n",
       "        [    -0.4682,      0.6477],\n",
       "        [    -0.1090,      1.9923],\n",
       "        [    -0.6196,      0.8758],\n",
       "        [    -0.2891,      2.1952],\n",
       "        [    -0.5762,      1.1586],\n",
       "        [    -0.3386,      2.7778],\n",
       "        [    -0.5384,      1.4955],\n",
       "        [    -0.3978,      2.4008],\n",
       "        [    -0.5211,      1.1893],\n",
       "        [    -0.3904,      2.1080],\n",
       "        [    -0.4782,      0.9439],\n",
       "        [    -0.2967,      1.9089],\n",
       "        [    -0.3921,      0.7539],\n",
       "        [    -0.1171,      1.8028],\n",
       "        [    -0.2831,      0.6146],\n",
       "        [     0.0838,      1.7884],\n",
       "        [    -0.2422,      0.5229],\n",
       "        [     0.1709,      1.8536],\n",
       "        [    -0.4018,      0.4600],\n",
       "        [     0.0180,      1.9634],\n",
       "        [    -0.8048,      0.4028],\n",
       "        [    -0.3197,      2.1111],\n",
       "        [    -1.2626,      0.3927],\n",
       "        [    -0.5946,      2.3511],\n",
       "        [    -1.5524,      0.4679],\n",
       "        [    -0.0682,      3.1234],\n",
       "        [    -1.7304,      0.5344],\n",
       "        [     0.0432,      2.7892],\n",
       "        [    -1.5088,      0.4511],\n",
       "        [     0.1538,      2.5309],\n",
       "        [    -1.2920,      0.4267],\n",
       "        [     0.2451,      2.3438],\n",
       "        [    -1.0796,      0.4414],\n",
       "        [     0.3207,      2.2195],\n",
       "        [    -0.8539,      0.4946],\n",
       "        [     0.3925,      2.1712],\n",
       "        [    -0.6123,      0.6067],\n",
       "        [     0.4454,      2.2149],\n",
       "        [    -0.4021,      0.7819],\n",
       "        [     0.4135,      2.3250],\n",
       "        [    -0.3202,      0.9702],\n",
       "        [     0.2181,      2.4358],\n",
       "        [    -0.4244,      1.1256],\n",
       "        [    -0.1150,      2.5432],\n",
       "        [    -0.6355,      1.2838],\n",
       "        [    -0.4506,      3.0067],\n",
       "        [    -0.7351,      1.5056],\n",
       "        [    -0.5140,      2.6493],\n",
       "        [    -0.7339,      1.1847],\n",
       "        [    -0.5435,      2.3830],\n",
       "        [    -0.7664,      0.9296],\n",
       "        [    -0.5636,      2.1940],\n",
       "        [    -0.8466,      0.7150],\n",
       "        [    -0.5825,      2.0614],\n",
       "        [    -0.9624,      0.5382],\n",
       "        [    -0.5910,      1.9980],\n",
       "        [    -1.0872,      0.4308],\n",
       "        [    -0.5787,      2.0370],\n",
       "        [    -1.2004,      0.4195],\n",
       "        [    -0.5524,      2.1799],\n",
       "        [    -1.3084,      0.4723],\n",
       "        [    -0.5467,      2.3731],\n",
       "        [    -1.4457,      0.5247],\n",
       "        [    -0.6082,      2.5868],\n",
       "        [    -1.6517,      0.5751]], device='cuda:0', grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_printoptions(sci_mode = False)\n",
    "test_pred_dist_likeli = likelihood(test_pred_dist)\n",
    "test_pred_dist_likeli.mean.reshape(2, -1).T - mean_model_out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ice_thickness_gpytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
