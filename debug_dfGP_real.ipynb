{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66bf128c",
   "metadata": {},
   "source": [
    "# Start with dfGP for real data\n",
    "\n",
    "- check if noise is enough\n",
    "- liklihood: observation specific noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21528f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "\n",
      "Training for REGION_LOWER_BYRD...\n",
      "=== REGION_LOWER_BYRD ===\n",
      "Training inputs shape: torch.Size([536, 2])\n",
      "Training observations shape: torch.Size([536, 2])\n",
      "Training inputs dtype: torch.float32\n",
      "\n",
      "=== REGION_LOWER_BYRD ===\n",
      "Test inputs shape: torch.Size([293, 2])\n",
      "Test observations shape: torch.Size([293, 2])\n",
      "Test inputs dtype: torch.float32\n",
      "\n",
      "\n",
      "--- Training Run 1/1 ---\n",
      "\n",
      "Start Training\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MultivariateNormal' object has no attribute '_interleaved'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 219\u001b[0m\n\u001b[1;32m    216\u001b[0m train_pred_dist \u001b[38;5;241m=\u001b[39m model(x_train\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m    217\u001b[0m \u001b[38;5;66;03m# Train on noisy or targets\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# NOTE: We only have observational y_train i.e. noisy data\u001b[39;00m\n\u001b[0;32m--> 219\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[43mmll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_pred_dist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# negative marginal log likelihood\u001b[39;00m\n\u001b[1;32m    220\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    221\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/envs/gpytorch-env/lib/python3.10/site-packages/gpytorch/module.py:82\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, Distribution, LinearOperator]:\n\u001b[0;32m---> 82\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "File \u001b[0;32m~/anaconda3/envs/gpytorch-env/lib/python3.10/site-packages/gpytorch/mlls/exact_marginal_log_likelihood.py:66\u001b[0m, in \u001b[0;36mExactMarginalLogLikelihood.forward\u001b[0;34m(self, function_dist, target, *params, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExactMarginalLogLikelihood can only operate on Gaussian random variables\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Determine output likelihood\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlikelihood\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction_dist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Remove NaN values if enabled\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m settings\u001b[38;5;241m.\u001b[39mobservation_nan_policy\u001b[38;5;241m.\u001b[39mvalue() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/gpytorch-env/lib/python3.10/site-packages/gpytorch/likelihoods/likelihood.py:76\u001b[0m, in \u001b[0;36m_Likelihood.__call__\u001b[0;34m(self, input, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Marginal\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, MultivariateNormal):\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmarginal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Error\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLikelihoods expects a MultivariateNormal input to make marginal predictions, or a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.Tensor for conditional predictions. Got a \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     82\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/gpytorch-env/lib/python3.10/site-packages/gpytorch/likelihoods/multitask_gaussian_likelihood.py:299\u001b[0m, in \u001b[0;36mMultitaskGaussianLikelihood.marginal\u001b[0;34m(self, function_dist, *args, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmarginal\u001b[39m(\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;28mself\u001b[39m, function_dist: MultitaskMultivariateNormal, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[1;32m    295\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m MultitaskMultivariateNormal:\n\u001b[1;32m    296\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;124;03m    :return: Analytic marginal :math:`p(\\mathbf y)`.\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmarginal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction_dist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/gpytorch-env/lib/python3.10/site-packages/gpytorch/likelihoods/multitask_gaussian_likelihood.py:110\u001b[0m, in \u001b[0;36m_MultitaskGaussianLikelihoodBase.marginal\u001b[0;34m(self, function_dist, *params, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(covar, LazyEvaluatedKernelTensor):\n\u001b[1;32m    107\u001b[0m     covar \u001b[38;5;241m=\u001b[39m covar\u001b[38;5;241m.\u001b[39mevaluate_kernel()\n\u001b[1;32m    109\u001b[0m covar_kron_lt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shaped_noise_covar(\n\u001b[0;32m--> 110\u001b[0m     mean\u001b[38;5;241m.\u001b[39mshape, add_noise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_global_noise, interleaved\u001b[38;5;241m=\u001b[39m\u001b[43mfunction_dist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interleaved\u001b[49m\n\u001b[1;32m    111\u001b[0m )\n\u001b[1;32m    112\u001b[0m covar \u001b[38;5;241m=\u001b[39m covar \u001b[38;5;241m+\u001b[39m covar_kron_lt\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function_dist\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m(mean, covar, interleaved\u001b[38;5;241m=\u001b[39mfunction_dist\u001b[38;5;241m.\u001b[39m_interleaved)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MultivariateNormal' object has no attribute '_interleaved'"
     ]
    }
   ],
   "source": [
    "model_name = \"dfGP\"\n",
    "from gpytorch_models import dfGP\n",
    "\n",
    "# import configs to we can access the hypers with getattr\n",
    "import configs\n",
    "from configs import PATIENCE, MAX_NUM_EPOCHS, NUM_RUNS, WEIGHT_DECAY\n",
    "from configs import TRACK_EMISSIONS_BOOL\n",
    "\n",
    "# Reiterating import for visibility\n",
    "MAX_NUM_EPOCHS = MAX_NUM_EPOCHS\n",
    "MAX_NUM_EPOCHS = 20\n",
    "NUM_RUNS = NUM_RUNS\n",
    "NUM_RUNS = 1\n",
    "WEIGHT_DECAY = WEIGHT_DECAY\n",
    "PATIENCE = PATIENCE\n",
    "\n",
    "# assign model-specific variable\n",
    "MODEL_LEARNING_RATE = getattr(configs, f\"{model_name}_REAL_LEARNING_RATE\")\n",
    "MODEL_REAL_RESULTS_DIR = getattr(configs, f\"{model_name}_REAL_RESULTS_DIR\")\n",
    "import os\n",
    "os.makedirs(MODEL_REAL_RESULTS_DIR, exist_ok = True)\n",
    "\n",
    "# basics\n",
    "import pandas as pd\n",
    "import torch\n",
    "import gpytorch\n",
    "\n",
    "# universals \n",
    "from metrics import compute_divergence_field, quantile_coverage_error_2d\n",
    "from utils import set_seed, make_grid\n",
    "import gc\n",
    "import warnings\n",
    "set_seed(42)\n",
    "\n",
    "# setting device to GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# overwrite if needed: # device = 'cpu'\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "### START TIMING ###\n",
    "import time\n",
    "start_time = time.time()  # Start timing after imports\n",
    "\n",
    "### START TRACKING EXPERIMENT EMISSIONS ###\n",
    "if TRACK_EMISSIONS_BOOL:\n",
    "    from codecarbon import EmissionsTracker\n",
    "    tracker = EmissionsTracker(project_name = \"dfGP_real_experiments\", output_dir = MODEL_REAL_RESULTS_DIR)\n",
    "    tracker.start()\n",
    "\n",
    "#############################\n",
    "### LOOP 1 - over REGIONS ###\n",
    "#############################\n",
    "\n",
    "# for region_name in [\"region_lower_byrd\", \"region_mid_byrd\", \"region_upper_byrd\"]:\n",
    "for region_name in [\"region_lower_byrd\"]:\n",
    "\n",
    "    print(f\"\\nTraining for {region_name.upper()}...\")\n",
    "\n",
    "    # Store metrics for the current region (used for *metrics_summary* report and *metrics_per_run*)\n",
    "    region_results = []\n",
    "\n",
    "    ##########################################\n",
    "    ### x_train & y_train, x_test & x_test ###\n",
    "    ##########################################\n",
    "\n",
    "    # define paths based on region_name\n",
    "    path_to_training_tensor = \"data/real_data/\" + region_name + \"_train_tensor.pt\"\n",
    "    path_to_test_tensor = \"data/real_data/\" + region_name + \"_test_tensor.pt\"\n",
    "\n",
    "    # load and tranpose to have rows as points\n",
    "    train = torch.load(path_to_training_tensor, weights_only = False).T \n",
    "    test = torch.load(path_to_test_tensor, weights_only = False).T\n",
    "\n",
    "    # The train and test tensors have the following columns:\n",
    "    # [:, 0] = x\n",
    "    # [:, 1] = y\n",
    "    # [:, 2] = surface elevation (s)\n",
    "    # [:, 3] = ice flux in x direction (u)\n",
    "    # [:, 4] = ice flux in y direction (v)\n",
    "    # [:, 5] = ice flux error in x direction (u_err)\n",
    "    # [:, 6] = ice flux error in y direction (v_err)\n",
    "    # [:, 7] = source age\n",
    "\n",
    "    # train\n",
    "    x_train = train[:, [0, 1]].to(device)\n",
    "    y_train = train[:, [3, 4]].to(device)\n",
    "\n",
    "    # test\n",
    "    x_test = test[:, [0, 1]].to(device)\n",
    "    y_test = test[:, [3, 4]].to(device)\n",
    "\n",
    "    x_test = x_test\n",
    "    y_test = y_test\n",
    "\n",
    "    x_train = x_train * 10\n",
    "    x_test = x_test * 10\n",
    "\n",
    "    # NOTE: Here we estimate the noise variance \n",
    "    \"\"\"\n",
    "    ### NOISE MODEL ###\n",
    "    # TRAIN\n",
    "    # noise variance (h * sigma_u)^2 and (h * sigma_v)^2 (tensor contains [h sig_u, h sig_v] stds)\n",
    "    noise_var_h_times_uv_train = torch.concat((train[:, 5], train[:, 6]), dim = 0)**2\n",
    "    # assume age dependent noise sigma_h on ice thickness measurements: ~10 - 20 m std (1000 scaling)\n",
    "    sigma_h = 0.01 * torch.log(train[:, 7] + 3)\n",
    "    # calculate noise variance (u * sigma_h)^2 and (v * sigma_h)^2\n",
    "    noise_var_uv_times_h_train = (torch.concat((train[:, 3], train[:, 4]), dim = 0) * torch.cat([sigma_h, sigma_h]))**2\n",
    "    # combine both noise variances into the std for each dimension\n",
    "    train_noise_diag = torch.sqrt(noise_var_h_times_uv_train + noise_var_uv_times_h_train).to(device)\n",
    "\n",
    "    # Compute midpoint\n",
    "    midpoint = train_noise_diag.shape[0] // 2\n",
    "\n",
    "    # Print noise levels for train, formatted to 4 decimal places\n",
    "    print(f\"Mean noise std per x dimension: {train_noise_diag[:midpoint].mean(dim = 0).item():.4f}\")\n",
    "    print(f\"Mean noise std per y dimension: {train_noise_diag[midpoint:].mean(dim = 0).item():.4f}\")\n",
    "\n",
    "    # TEST\n",
    "    # noise variance (h * sigma_u)^2 and (h * sigma_v)^2 (tensor contains [h sig_u, h sig_v] stds)\n",
    "    noise_var_h_times_uv_test = torch.concat((test[:, 5], test[:, 6]), dim = 0)**2\n",
    "    # assume age dependent noise sigma_h on ice thickness measurements: ~10 - 20 m std (1000 scaling)\n",
    "    sigma_h = 0.01 * torch.log(test[:, 7] + 3)\n",
    "    # calculate noise variance (u * sigma_h)^2 and (v * sigma_h)^2\n",
    "    noise_var_uv_times_h_test = (torch.concat((test[:, 3], test[:, 4]), dim = 0) * torch.cat([sigma_h, sigma_h]))**2\n",
    "    # combine both noise variances into the std for each dimension\n",
    "    test_noise_diag = torch.sqrt(noise_var_h_times_uv_test + noise_var_uv_times_h_test).to(device)\n",
    "    \"\"\"\n",
    "\n",
    "    # Print train details\n",
    "    print(f\"=== {region_name.upper()} ===\")\n",
    "    print(f\"Training inputs shape: {x_train.shape}\")\n",
    "    print(f\"Training observations shape: {y_train.shape}\")\n",
    "    print(f\"Training inputs dtype: {x_train.dtype}\")\n",
    "    print()\n",
    "\n",
    "    # Print test details\n",
    "    print(f\"=== {region_name.upper()} ===\")\n",
    "    print(f\"Test inputs shape: {x_test.shape}\")\n",
    "    print(f\"Test observations shape: {y_test.shape}\")\n",
    "    print(f\"Test inputs dtype: {x_test.dtype}\")\n",
    "    print()\n",
    "\n",
    "    ##################################\n",
    "    ### LOOP 2 - over training run ###\n",
    "    ##################################\n",
    "\n",
    "    # NOTE: GPs don't train on batches, use full data\n",
    "\n",
    "    for run in range(NUM_RUNS):\n",
    "\n",
    "        print(f\"\\n--- Training Run {run + 1}/{NUM_RUNS} ---\")\n",
    "\n",
    "        # Initialise the likelihood for the GP model (estimates noise)\n",
    "        # NOTE: we use a multitask likelihood for the dfGP model but with a global noise term\n",
    "        likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(\n",
    "            num_tasks = 2,\n",
    "            rank = 2,\n",
    "            has_global_noise = True, \n",
    "            has_task_noise = False, # HACK: This still needs to be manually turned off\n",
    "            ).to(device)\n",
    "\n",
    "        model = dfGP(\n",
    "            x_train,\n",
    "            y_train, \n",
    "            likelihood\n",
    "            ).to(device)\n",
    "        \n",
    "        model.likelihood.noise = torch.tensor([0.02]).to(device)\n",
    "        model.covar_module.outputscale = torch.tensor([1.8]).to(device)\n",
    "        model.covar_module.base_kernel.lengthscale = torch.tensor([0.5, 0.3]).to(device)\n",
    "        \n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr = MODEL_LEARNING_RATE, weight_decay = WEIGHT_DECAY)\n",
    "        \n",
    "        # Use ExactMarginalLogLikelihood\n",
    "        mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "        model.train()\n",
    "        likelihood.train()\n",
    "        # _________________\n",
    "        # BEFORE EPOCH LOOP\n",
    "        \n",
    "        # Export the convergence just for first run only\n",
    "        if run == 0:\n",
    "            # initialise tensors to store losses over epochs (for convergence plot)\n",
    "            train_losses_NLML_over_epochs = torch.zeros(MAX_NUM_EPOCHS) # objective\n",
    "            train_losses_RMSE_over_epochs = torch.zeros(MAX_NUM_EPOCHS) # by-product\n",
    "            # monitor performance transfer to test (only RMSE easy to calc without covar)\n",
    "            test_losses_RMSE_over_epochs = torch.zeros(MAX_NUM_EPOCHS)\n",
    "\n",
    "            # NOTE: Here, we estimate the noise\n",
    "            l1_over_epochs = torch.zeros(MAX_NUM_EPOCHS)\n",
    "            l2_over_epochs = torch.zeros(MAX_NUM_EPOCHS)\n",
    "            outputscale_var_over_epochs = torch.zeros(MAX_NUM_EPOCHS)\n",
    "            noise_var_over_epochs = torch.zeros(MAX_NUM_EPOCHS)\n",
    "\n",
    "        # Early stopping variables\n",
    "        best_loss = float('inf')\n",
    "        # counter starts at 0\n",
    "        epochs_no_improve = 0\n",
    "\n",
    "        ############################\n",
    "        ### LOOP 3 - over EPOCHS ###\n",
    "        ############################\n",
    "        print(\"\\nStart Training\")\n",
    "\n",
    "        for epoch in range(MAX_NUM_EPOCHS):\n",
    "\n",
    "            # Set to train\n",
    "            model.train()\n",
    "            likelihood.train()\n",
    "\n",
    "            # Do a step\n",
    "            optimizer.zero_grad()\n",
    "            # model outputs a multivariate normal distribution\n",
    "            train_pred_dist = model(x_train.to(device))\n",
    "            # Train on noisy or targets\n",
    "            # NOTE: We only have observational y_train i.e. noisy data\n",
    "            loss = - mll(train_pred_dist, y_train.reshape(-1).to(device))  # negative marginal log likelihood\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if run == 0:\n",
    "\n",
    "                model.eval()\n",
    "                likelihood.eval()\n",
    "                \n",
    "                with gpytorch.settings.debug(False):\n",
    "                    with torch.no_grad():\n",
    "                            # Only in eval it computes it the right way\n",
    "                            train_pred_dist_eval = model(x_train.to(device))\n",
    "                    test_pred_dist_eval = model(x_test.to(device))\n",
    "\n",
    "                # Compute RMSE for training and test predictions (given true data, not noisy)\n",
    "                train_RMSE = torch.sqrt(gpytorch.metrics.mean_squared_error(train_pred_dist_eval, y_train.to(device)).mean())\n",
    "                test_RMSE = torch.sqrt(gpytorch.metrics.mean_squared_error(test_pred_dist_eval, y_test.to(device)).mean())\n",
    "\n",
    "                # Save losses for convergence plot\n",
    "                train_losses_NLML_over_epochs[epoch] = loss.item()\n",
    "                train_losses_RMSE_over_epochs[epoch] = train_RMSE.item()\n",
    "                test_losses_RMSE_over_epochs[epoch] = test_RMSE.item()\n",
    "\n",
    "                # Save evolution of hypers for convergence plot\n",
    "                l1_over_epochs[epoch] = model.base_kernel.lengthscale[0].item()\n",
    "                l2_over_epochs[epoch] = model.base_kernel.lengthscale[1].item()\n",
    "                outputscale_var_over_epochs[epoch] = model.covar_module.outputscale.item()\n",
    "                noise_var_over_epochs[epoch] = model.likelihood.noise.item()\n",
    "\n",
    "                # Print a bit more information for the first run\n",
    "                if epoch % 20 == 0:\n",
    "                    print(f\"{region_name} {model_name} Run {run + 1}/{NUM_RUNS}, Epoch {epoch + 1}/{MAX_NUM_EPOCHS}, Training Loss (NLML): {loss:.4f}, Training RMSE: {train_RMSE:.4f}\")\n",
    "\n",
    "                # delete after printing and saving\n",
    "                # NOTE: keep loss for early stopping check\n",
    "                # del train_pred_dist, test_pred_dist, train_RMSE, test_RMSE\n",
    "                \n",
    "                # Free up memory every 20 epochs\n",
    "                if epoch % 20 == 0:\n",
    "                    gc.collect() and torch.cuda.empty_cache()\n",
    "\n",
    "        ##############################\n",
    "        ### END LOOP 3 over EPOCHS ###\n",
    "        ##############################\n",
    "\n",
    "        # for every run...\n",
    "        #######################################################\n",
    "        ### EVALUATE after all training for RUN is finished ###\n",
    "        #######################################################\n",
    "\n",
    "        model.eval()\n",
    "        likelihood.eval()\n",
    "\n",
    "        # Underlying (latent) distribution and predictive distribution\n",
    "        # with gpytorch.settings.debug(False):\n",
    "        #    with torch.no_grad():\n",
    "                # dist_train = model(x_train)\n",
    "                # pred_dist_train = likelihood(dist_train)\n",
    "                # Make it interleaved structure?\n",
    "\n",
    "        import sys\n",
    "\n",
    "        def trace_calls(frame, event, arg):\n",
    "            if event != 'call':\n",
    "                return\n",
    "            code = frame.f_code\n",
    "            func_name = code.co_name\n",
    "            filename = code.co_filename\n",
    "            lineno = frame.f_lineno\n",
    "            print(f\"Call to {func_name} in {filename}:{lineno}\")\n",
    "            return trace_calls\n",
    "\n",
    "        sys.settrace(trace_calls)\n",
    "\n",
    "        # Your GP prediction\n",
    "        dist_test = model(x_test)\n",
    "\n",
    "\n",
    "        dist_test = model(x_test)\n",
    "        \n",
    "        sys.settrace(None)  # Turn off tracing afterward\n",
    "        pred_dist_test = likelihood(dist_test)\n",
    "\n",
    "        # with warnings.catch_warnings():\n",
    "        #    warnings.simplefilter(\"ignore\", gpytorch.utils.warnings.GPInputWarning)\n",
    "            # dist_train = model(x_train_grad)\n",
    "        \n",
    "        # pred_dist_train = likelihood(dist_train)\n",
    "        \n",
    "        # Compute divergence field (from latent distribution)\n",
    "        # test_div_field = compute_divergence_field(dist_test.mean, x_test_grad)\n",
    "        # train_div_field = compute_divergence_field(dist_train.mean, x_train_grad)\n",
    "\n",
    "        # Compute TEST metrics (convert tensors to float) for every run's tuned model\n",
    "        test_RMSE = torch.sqrt(gpytorch.metrics.mean_squared_error(\n",
    "            pred_dist_test, y_test.to(device)).mean()).item()\n",
    "        test_MAE = gpytorch.metrics.mean_absolute_error(\n",
    "            pred_dist_test, y_test.to(device)).mean().item()\n",
    "        print(\"Here\")\n",
    "        test_NLL = gpytorch.metrics.negative_log_predictive_density(\n",
    "            pred_dist_test, y_test.to(device)).item()\n",
    "        test_QCE = quantile_coverage_error_2d(\n",
    "            pred_dist_test, y_test.to(device), quantile = 95.0).item()\n",
    "        ## NOTE: It is important to use the absolute value of the divergence field, since both positive and negative deviations are violations and shouldn't cancel each other out \n",
    "        # test_MAD = test_div_field.abs().mean().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2a058e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dist_test.batch_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e4a82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dist_test._batch_shape\n",
    "pred_dist_test._event_shape\n",
    "pred_dist_test._extended_shape\n",
    "pred_dist_test._interleaved\n",
    "pred_dist_test._validate_args # Why?\n",
    "pred_dist_test.covariance_matrix - pred_dist_test.covariance_matrix.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10d2942",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dist_test.covariance_matrix - pred_dist_test.covariance_matrix.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fda8a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision = 3, sci_mode = False)\n",
    "pred_dist_test.covariance_matrix[0:4, 0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57eb6101",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch_models import dfRBFKernel\n",
    "my_kernel = dfRBFKernel()\n",
    "\n",
    "noise = torch.tensor([0.02]).to(device)\n",
    "outputscale = torch.tensor([1.8]).to(device)\n",
    "my_kernel.lengthscale = torch.tensor([0.5, 0.3]).to(device)\n",
    "\n",
    "K_train_train = my_kernel(x_train, x_train).evaluate().detach()\n",
    "K_test_train = my_kernel(x_test, x_train).evaluate().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdb0268",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision = 4, sci_mode = False)\n",
    "K_test_train[0:4, 0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246d6db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "(K_train_train - K_train_train.T).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ec9360",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(train_pred_dist_eval.covariance_matrix.detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014884a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = dist_test.from_batch_mvn(dist_test, interleaved = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40026a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(testy._interleaved)\n",
    "testy.covariance_matrix - testy.covariance_matrix.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd58b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpytorch\n",
    "\n",
    "gpytorch.settings.debug(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b14ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "isinstance(train_pred_dist, gpytorch.distributions.MultitaskMultivariateNormal) \n",
    "\n",
    "# train pred is block diagonal & symmetric\n",
    "print(train_pred_dist._interleaved)\n",
    "print(pred_dist_test._interleaved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8285120e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_NLL = gpytorch.metrics.negative_log_predictive_density(\n",
    "            likelihood(train_pred_dist), y_train.to(device)).item()\n",
    "print(f\"Train NLL: {train_NLL:.4f}\")\n",
    "# For train it helps to wrap a large likelihood around the train_pred_dist\n",
    "\n",
    "(train_pred_dist.covariance_matrix - train_pred_dist.covariance_matrix.T).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b6bb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.likelihood.noise.item())\n",
    "print(model.covar_module.outputscale.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de20a897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_NLL = gpytorch.metrics.negative_log_predictive_density(\n",
    "#            likelihood(pred_dist_test), y_train.to(device)).item()\n",
    "# print(f\"Train NLL: {train_NLL:.4f}\")\n",
    "# For train it helps to wrap a large likelihood around the train_pred_dist\n",
    "\n",
    "(pred_dist_test.covariance_matrix - pred_dist_test.covariance_matrix.T).max()\n",
    "\n",
    "# Look "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b50e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dist_test.covariance_matrix - pred_dist_test.covariance_matrix.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a15dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dist_test.covariance_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Confusing block and interleaved structure?\n",
    "plt.imshow(pred_dist_test.covariance_matrix.cpu().numpy(), cmap = 'viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c630c6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.transpose(-1, -2).reshape(*y_train.shape[:-2], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f973b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1110c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.reshape(*y_train.shape[:-2], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8138e955",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_pred_dist.covariance_matrix.detach().cpu().numpy(), cmap = 'viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf67e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def block_to_interleaved(K_block):\n",
    "    \"\"\"\n",
    "    Convert a (2N, 2N) block covariance matrix to an interleaved (2N, 2N) format.\n",
    "    Assumes that K_block is ordered as:\n",
    "        [ K_uu  K_uv ]\n",
    "        [ K_vu  K_vv ]\n",
    "    \"\"\"\n",
    "    N = K_block.shape[-1] // 2\n",
    "\n",
    "    # Extract blocks\n",
    "    K_uu = K_block[:N, :N]\n",
    "    K_uv = K_block[:N, N:]\n",
    "    K_vu = K_block[N:, :N]\n",
    "    K_vv = K_block[N:, N:]\n",
    "\n",
    "    # Allocate interleaved matrix\n",
    "    K_interleaved = torch.zeros_like(K_block)\n",
    "\n",
    "    # Fill interleaved format\n",
    "    K_interleaved[0::2, 0::2] = K_uu\n",
    "    K_interleaved[0::2, 1::2] = K_uv\n",
    "    K_interleaved[1::2, 0::2] = K_vu\n",
    "    K_interleaved[1::2, 1::2] = K_vv\n",
    "\n",
    "    return K_interleaved\n",
    "\n",
    "plt.imshow(block_to_interleaved(train_pred_dist.covariance_matrix.detach().cpu())[0:100, 0:100], cmap = 'viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953fa61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_uv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230c5943",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_to_interleaved(train_pred_dist.covariance_matrix.detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8944d676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Assume you have the original model and its init args\n",
    "original_model = dfGP(\n",
    "    train_x, \n",
    "    train_y, \n",
    "    likelihood)\n",
    "\n",
    "original_model.train()  # (or eval, if you want to copy at that stage)\n",
    "\n",
    "# 2. Save state dict\n",
    "state = original_model.state_dict()\n",
    "\n",
    "# 3. Re-instantiate a clean model and load weights\n",
    "copied_model = MyGPModel(train_x, train_y, likelihood)\n",
    "copied_model.load_state_dict(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600601aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_pred_dist.covariance_matrix - train_pred_dist.covariance_matrix.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d062288",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, value in model.named_parameters():\n",
    "    print(f\"{name}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef630c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dist_test.covariance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b79d1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REAL DATA EXPERIMENTS\n",
    "# RUN WITH python run_real_experiments_dfGP.py\n",
    "#               _                 _   _      \n",
    "#              | |               | | (_)     \n",
    "#    __ _ _ __ | |_ __ _ _ __ ___| |_ _  ___ \n",
    "#   / _` | '_ \\| __/ _` | '__/ __| __| |/ __|\n",
    "#  | (_| | | | | || (_| | | | (__| |_| | (__ \n",
    "#   \\__,_|_| |_|\\__\\__,_|_|  \\___|\\__|_|\\___|\n",
    "# \n",
    "model_name = \"dfGP\"\n",
    "from gpytorch_models import dfGP, dfRBFKernel\n",
    "\n",
    "# import configs to we can access the hypers with getattr\n",
    "import configs\n",
    "from configs import PATIENCE, MAX_NUM_EPOCHS, NUM_RUNS, WEIGHT_DECAY, N_SIDE\n",
    "\n",
    "# Reiterating import for visibility\n",
    "MAX_NUM_EPOCHS = MAX_NUM_EPOCHS\n",
    "MAX_NUM_EPOCHS = 1\n",
    "NUM_RUNS = NUM_RUNS\n",
    "NUM_RUNS = 1\n",
    "WEIGHT_DECAY = WEIGHT_DECAY\n",
    "PATIENCE = PATIENCE\n",
    "\n",
    "# assign model-specific variable\n",
    "MODEL_LEARNING_RATE = getattr(configs, f\"{model_name}_REAL_LEARNING_RATE\")\n",
    "MODEL_REAL_RESULTS_DIR = getattr(configs, f\"{model_name}_REAL_RESULTS_DIR\")\n",
    "import os\n",
    "os.makedirs(MODEL_REAL_RESULTS_DIR, exist_ok = True)\n",
    "\n",
    "# basics\n",
    "import pandas as pd\n",
    "import torch\n",
    "import gpytorch\n",
    "\n",
    "# universals \n",
    "from metrics import compute_divergence_field, quantile_coverage_error_2d\n",
    "from utils import set_seed, make_grid\n",
    "import gc\n",
    "import warnings\n",
    "set_seed(42)\n",
    "\n",
    "# setting device to GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# overwrite if needed: # device = 'cpu'\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#############################\n",
    "### LOOP 1 - over REGIONS ###\n",
    "#############################\n",
    "\n",
    "# for region_name in [\"region_lower_byrd\"]:\n",
    "# for region_name in [\"region_lower_byrd\", \"region_mid_byrd\", \"region_upper_byrd\"]:\n",
    "for region_name in [\"region_mid_byrd\"]:\n",
    "\n",
    "    print(f\"\\nTraining for {region_name.upper()}...\")\n",
    "\n",
    "    # Store metrics for the current region (used for *metrics_summary* report and *metrics_per_run*)\n",
    "    region_results = []\n",
    "\n",
    "    ##########################################\n",
    "    ### x_train & y_train, x_test & x_test ###\n",
    "    ##########################################\n",
    "\n",
    "    # define paths based on region_name\n",
    "    path_to_training_tensor = \"data/real_data/\" + region_name + \"_train_tensor.pt\"\n",
    "    path_to_test_tensor = \"data/real_data/\" + region_name + \"_test_tensor.pt\"\n",
    "\n",
    "    # load and tranpose to have rows as points\n",
    "    train = torch.load(path_to_training_tensor, weights_only = False).T \n",
    "    test = torch.load(path_to_test_tensor, weights_only = False).T\n",
    "\n",
    "    # The train and test tensors have the following columns:\n",
    "    # [:, 0] = x\n",
    "    # [:, 1] = y\n",
    "    # [:, 2] = surface elevation (s)\n",
    "    # [:, 3] = ice flux in x direction (u)\n",
    "    # [:, 4] = ice flux in y direction (v)\n",
    "    # [:, 5] = ice flux error in x direction (u_err)\n",
    "    # [:, 6] = ice flux error in y direction (v_err)\n",
    "    # [:, 7] = source age\n",
    "\n",
    "    # train\n",
    "    x_train = train[:, [0, 1]].to(device)\n",
    "    y_train = train[:, [3, 4]].to(device)\n",
    "\n",
    "    # test\n",
    "    x_test = test[:, [0, 1]].to(device)\n",
    "    y_test = test[:, [3, 4]].to(device)\n",
    "\n",
    "    _, x_test_grid = make_grid(N_SIDE)\n",
    "\n",
    "    # Print train details\n",
    "    print(f\"=== {region_name.upper()} ===\")\n",
    "    print(f\"Training inputs shape: {x_train.shape}\")\n",
    "    print(f\"Training observations shape: {y_train.shape}\")\n",
    "    print(f\"Training inputs dtype: {x_train.dtype}\")\n",
    "    print()\n",
    "\n",
    "    # Print test details\n",
    "    print(f\"=== {region_name.upper()} ===\")\n",
    "    print(f\"Test inputs shape: {x_test.shape}\")\n",
    "    print(f\"Test observations shape: {y_test.shape}\")\n",
    "    print(f\"Test inputs dtype: {x_test.dtype}\")\n",
    "    print()\n",
    "\n",
    "    ##################################\n",
    "    ### LOOP 2 - over training run ###\n",
    "    ##################################\n",
    "\n",
    "    # NOTE: GPs don't train on batches, use full data\n",
    "\n",
    "    for run in range(NUM_RUNS):\n",
    "\n",
    "        print(f\"\\n--- Training Run {run + 1}/{NUM_RUNS} ---\")\n",
    "\n",
    "        # Initialise the likelihood for the GP model (estimates noise)\n",
    "        # NOTE: we use a multitask likelihood for the dfGP model but with a global noise term\n",
    "        likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(\n",
    "            num_tasks = 2,\n",
    "            has_global_noise = True, \n",
    "            has_task_noise = False, # HACK: This still needs to be manually turned off\n",
    "            ).to(device)\n",
    "        \n",
    "        # likelihood.noise = torch.tensor([0.02], device = device)  # initial noise variance (global noise)\n",
    "\n",
    "        # Intialise fresh GP model with flat x_train and y_train_noisy (block-flat)\n",
    "        model = dfGP(\n",
    "            x_train,\n",
    "            y_train, \n",
    "            likelihood\n",
    "            ).to(device)\n",
    "        \n",
    "        # model.covar_module.outputscale = torch.tensor(10.0, device = device)\n",
    "        # model.base_kernel.register_constraint(\"raw_lengthscale\", gpytorch.constraints.GreaterThan(0.3))\n",
    "        \n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr = MODEL_LEARNING_RATE, weight_decay = WEIGHT_DECAY)\n",
    "        \n",
    "        # Use ExactMarginalLogLikelihood\n",
    "        mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "        ############################\n",
    "        ### LOOP 3 - over EPOCHS ###\n",
    "        ############################\n",
    "        print(\"\\nStart Training\")\n",
    "\n",
    "        for epoch in range(MAX_NUM_EPOCHS):\n",
    "\n",
    "            # Set to train\n",
    "            model.train()\n",
    "            likelihood.train()\n",
    "\n",
    "            # Do a step\n",
    "            optimizer.zero_grad()\n",
    "            # model outputs a multivariate normal distribution\n",
    "            train_pred_dist = model(x_train.to(device))\n",
    "    \n",
    "            # loss = - mll(train_pred_dist, y_train.to(device))  # negative marginal log likelihood\n",
    "            # loss.backward()\n",
    "            # optimizer.step()\n",
    "\n",
    "        print(\"Training was no problem \")\n",
    "        \n",
    "        model.eval()\n",
    "        likelihood.eval()\n",
    "\n",
    "        with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "            # train_pred_dist = model(x_train.to(device))\n",
    "\n",
    "            dist_test = model(x_test)\n",
    "            pred_dist_test = likelihood(dist_test)\n",
    "\n",
    "            print(\"No problem yet\")\n",
    "            test_NLL = gpytorch.metrics.negative_log_predictive_density(\n",
    "                    pred_dist_test, y_test.to(device)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f4dc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_dist.covariance_matrix[0:5, 0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adfdaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dist_test.covariance_matrix[0:5, 0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedfe5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision = 3, sci_mode=False)\n",
    "dist_test.covariance_matrix[0:5, 0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae72dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_dist.covariance_matrix[0:5, 0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb6856d",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = dfRBFKernel()\n",
    "kernel = model.covar_module\n",
    "\n",
    "K_test_test = kernel(x_test).evaluate()\n",
    "# same as K_test_test = kernel(x_test, x_test).evaluate()\n",
    "\n",
    "K_train_train = kernel(x_train).evaluate()\n",
    "\n",
    "K_test_train = kernel(x_test, x_train).evaluate()\n",
    "\n",
    "(K_train_train - K_train_train.T).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e81528d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=3, sci_mode=False)\n",
    "K_train_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8682ee24",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_test_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1252d2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_test_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f857ea28",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_dist.covariance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57f6381",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_test.covariance_matrix[0:5, 0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc72962a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIMULATED DATA EXPERIMENTS\n",
    "# RUN WITH python run_sim_experiments_dfGP.py\n",
    "# \n",
    "#       ooooooooooooooooooooooooooooooooooooo\n",
    "#      8                                .d88\n",
    "#      8  oooooooooooooooooooooooooooood8888\n",
    "#      8  8888888888888888888888888P\"   8888    oooooooooooooooo\n",
    "#      8  8888888888888888888888P\"      8888    8              8\n",
    "#      8  8888888888888888888P\"         8888    8             d8\n",
    "#      8  8888888888888888P\"            8888    8            d88\n",
    "#      8  8888888888888P\"               8888    8           d888\n",
    "#      8  8888888888P\"                  8888    8          d8888\n",
    "#      8  8888888P\"                     8888    8         d88888\n",
    "#      8  8888P\"                        8888    8        d888888\n",
    "#      8  8888oooooooooooooooooooooocgmm8888    8       d8888888\n",
    "#      8 .od88888888888888888888888888888888    8      d88888888\n",
    "#      8888888888888888888888888888888888888    8     d888888888\n",
    "#                                               8    d8888888888\n",
    "#         ooooooooooooooooooooooooooooooo       8   d88888888888\n",
    "#        d                       ...oood8b      8  d888888888888\n",
    "#       d              ...oood888888888888b     8 d8888888888888\n",
    "#      d     ...oood88888888888888888888888b    8d88888888888888\n",
    "#     dood8888888888888888888888888888888888b\n",
    "#\n",
    "#\n",
    "# This artwork is a visual reminder that this script is for the sim experiments.\n",
    "\n",
    "model_name = \"dfGP\"\n",
    "from gpytorch_models import dfGP\n",
    "\n",
    "# import configs to we can access the hypers with getattr\n",
    "import configs\n",
    "from configs import PATIENCE, MAX_NUM_EPOCHS, NUM_RUNS, WEIGHT_DECAY\n",
    "# also import x_test grid size and std noise for training data\n",
    "from configs import N_SIDE, STD_GAUSSIAN_NOISE\n",
    "from configs import TRACK_EMISSIONS_BOOL\n",
    "\n",
    "# Reiterating import for visibility\n",
    "MAX_NUM_EPOCHS = 100\n",
    "NUM_RUNS = 1\n",
    "WEIGHT_DECAY = WEIGHT_DECAY\n",
    "PATIENCE = PATIENCE\n",
    "\n",
    "# assign model-specific variable\n",
    "MODEL_LEARNING_RATE = getattr(configs, f\"{model_name}_SIM_LEARNING_RATE\")\n",
    "MODEL_SIM_RESULTS_DIR = getattr(configs, f\"{model_name}_SIM_RESULTS_DIR\")\n",
    "import os\n",
    "os.makedirs(MODEL_SIM_RESULTS_DIR, exist_ok = True)\n",
    "\n",
    "# basics\n",
    "import pandas as pd\n",
    "import torch\n",
    "import gpytorch\n",
    "\n",
    "# universals \n",
    "from metrics import compute_divergence_field, quantile_coverage_error_2d\n",
    "from utils import set_seed, make_grid\n",
    "import gc\n",
    "import warnings\n",
    "set_seed(42)\n",
    "\n",
    "# setting device to GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# overwrite if needed: # device = 'cpu'\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "### START TIMING ###\n",
    "import time\n",
    "start_time = time.time()  # Start timing after imports\n",
    "\n",
    "### START TRACKING EXPERIMENT EMISSIONS ###\n",
    "if TRACK_EMISSIONS_BOOL:\n",
    "    from codecarbon import EmissionsTracker\n",
    "    tracker = EmissionsTracker(project_name = \"dfGP_simulation_experiments\", output_dir = MODEL_SIM_RESULTS_DIR)\n",
    "    tracker.start()\n",
    "\n",
    "### SIMULATION ###\n",
    "# Import all simulation functions\n",
    "from simulate import (\n",
    "    simulate_detailed_branching,\n",
    "    simulate_detailed_curve,\n",
    "    simulate_detailed_deflection,\n",
    "    simulate_detailed_edge,\n",
    "    simulate_detailed_ridges,\n",
    ")\n",
    "\n",
    "# Define simulations as a dictionary with names as keys to function objects\n",
    "# alphabectic order here\n",
    "simulations = {\n",
    "    \"curve\": simulate_detailed_curve,\n",
    "}\n",
    "\n",
    "########################\n",
    "### x_train & x_test ###\n",
    "########################\n",
    "\n",
    "# Load training inputs (once for all simulations)\n",
    "x_train = torch.load(\"data/sim_data/x_train_lines_discretised_0to1.pt\", weights_only = False).float()\n",
    "\n",
    "# Generate x_test (long) once for all simulations\n",
    "_, x_test = make_grid(N_SIDE)\n",
    "# x_test is long format (N_SIDE ** 2, 2)\n",
    "\n",
    "#################################\n",
    "### LOOP 1 - over SIMULATIONS ###\n",
    "#################################\n",
    "\n",
    "# Make y_train_dict: Iterate over all simulation functions\n",
    "for sim_name, sim_func in simulations.items():\n",
    "\n",
    "    ########################\n",
    "    ### y_train & y_test ###\n",
    "    ########################\n",
    "\n",
    "    # Generate training observations\n",
    "    # NOTE: sim_func() needs to be on CPU, so we move x_train to CPU\n",
    "    y_train = sim_func(x_train.cpu()).to(device)\n",
    "    y_test = sim_func(x_test.cpu()).to(device)\n",
    "    \n",
    "    x_test = x_test.to(device)\n",
    "    x_train = x_train.to(device)\n",
    "\n",
    "    # Print details\n",
    "    print(f\"=== {sim_name.upper()} ===\")\n",
    "    print(f\"Training inputs shape: {x_train.shape}\")\n",
    "    print(f\"Training observations shape: {y_train.shape}\")\n",
    "    print(f\"Training inputs dtype: {x_train.dtype}\")\n",
    "    print(f\"Training inputs device: {y_train.device}\")\n",
    "    print(f\"Training observations device: {y_train.device}\")\n",
    "    print()\n",
    "\n",
    "    # Print details\n",
    "    print(f\"=== {sim_name.upper()} ===\")\n",
    "    print(f\"Test inputs shape: {x_test.shape}\")\n",
    "    print(f\"Test observations shape: {y_test.shape}\")\n",
    "    print(f\"Test inputs dtype: {x_test.dtype}\")\n",
    "    print(f\"Test inputs device: {x_test.device}\")\n",
    "    print(f\"Test observations device: {y_test.device}\")\n",
    "    print()\n",
    "\n",
    "    # NOTE: This is different to the real data experiments\n",
    "    # calculate the mean magnitude of the test data as we use this to scale the noise\n",
    "    sim_mean_magnitude_for_noise = torch.norm(y_test, dim = -1).mean().to(device)\n",
    "    sim_noise = STD_GAUSSIAN_NOISE * sim_mean_magnitude_for_noise\n",
    "\n",
    "    # Store metrics for the simulation (used for *metrics_summary* report and *metrics_per_run*)\n",
    "    simulation_results = [] \n",
    "\n",
    "    ##################################\n",
    "    ### LOOP 2 - over training run ###\n",
    "    ##################################\n",
    "\n",
    "    # NOTE: GPs don't train on batches, use full data\n",
    "\n",
    "    for run in range(NUM_RUNS):\n",
    "\n",
    "        print(f\"\\n--- Training Run {run + 1}/{NUM_RUNS} ---\")\n",
    "\n",
    "        # Additive noise model: independent Gaussian noise\n",
    "        # For every run we have a FIXED NOISY TARGET. Draw from standard normal with appropriate std\n",
    "        y_train_noisy = y_train + (torch.randn(y_train.shape, device = device) * sim_noise)\n",
    "\n",
    "        # Initialise the likelihood for the GP model (estimates noise)\n",
    "        # NOTE: we use a multitask likelihood for the dfGP model but with a global noise term\n",
    "        likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(\n",
    "            num_tasks = 2,\n",
    "            has_global_noise = True, \n",
    "            has_task_noise = False, # HACK: This still needs to be manually turned off\n",
    "            ).to(device)\n",
    "\n",
    "        # Intialise fresh GP model with flat x_train and y_train_noisy (block-flat)\n",
    "        model = dfGP(\n",
    "            x_train,\n",
    "            y_train_noisy, \n",
    "            likelihood\n",
    "            ).to(device)\n",
    "        \n",
    "        # NOTE: model parameters contains likelihood parameters as well\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr = MODEL_LEARNING_RATE, weight_decay = WEIGHT_DECAY)\n",
    "        \n",
    "        # Use ExactMarginalLogLikelihood as the reward i.e. inverse loss function\n",
    "        mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "        model.train()\n",
    "        likelihood.train()\n",
    "        \n",
    "        # _________________\n",
    "        # BEFORE EPOCH LOOP\n",
    "\n",
    "        # Export the convergence just for first run only\n",
    "        if run == 0:\n",
    "            # initialise tensors to store losses over epochs (for convergence plot)\n",
    "            train_losses_NLML_over_epochs = torch.zeros(MAX_NUM_EPOCHS) # objective\n",
    "            train_losses_RMSE_over_epochs = torch.zeros(MAX_NUM_EPOCHS) # by-product\n",
    "            # monitor performance transfer to test (only RMSE easy to calc without covar)\n",
    "            test_losses_RMSE_over_epochs = torch.zeros(MAX_NUM_EPOCHS)\n",
    "\n",
    "            l1_over_epochs = torch.zeros(MAX_NUM_EPOCHS)\n",
    "            l2_over_epochs = torch.zeros(MAX_NUM_EPOCHS)\n",
    "            outputscale_var_over_epochs = torch.zeros(MAX_NUM_EPOCHS)\n",
    "            noise_var_over_epochs = torch.zeros(MAX_NUM_EPOCHS)\n",
    "\n",
    "        # Early stopping variables\n",
    "        best_loss = float('inf')\n",
    "        # counter starts at 0\n",
    "        epochs_no_improve = 0\n",
    "\n",
    "        ############################\n",
    "        ### LOOP 3 - over EPOCHS ###\n",
    "        ############################\n",
    "\n",
    "        print(\"\\nStart Training\")\n",
    "\n",
    "        for epoch in range(MAX_NUM_EPOCHS):\n",
    "\n",
    "            # Set to train\n",
    "            model.train()\n",
    "            likelihood.train()\n",
    "\n",
    "            # Do a step\n",
    "            optimizer.zero_grad()\n",
    "            # model outputs a multivariate normal distribution\n",
    "            train_pred_dist = model(x_train.to(device))\n",
    "            # Train on noisy or targets\n",
    "    \n",
    "            loss = - mll(train_pred_dist, y_train_noisy.to(device))  # negative marginal log likelihood\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # For Run 1 we save a bunch of metrics and update, while for the rest we only update\n",
    "            if run == 0:\n",
    "\n",
    "                model.eval()\n",
    "                likelihood.eval()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                # with warnings.catch_warnings():\n",
    "                #    warnings.simplefilter(\"ignore\", gpytorch.utils.warnings.GPInputWarning)\n",
    "                #    train_pred_dist = model(x_train.to(device))\n",
    "                    test_pred_dist = model(x_test.to(device))\n",
    "\n",
    "                # Compute RMSE for training and test predictions (given true data, not noisy)\n",
    "                train_RMSE = torch.sqrt(gpytorch.metrics.mean_squared_error(train_pred_dist, y_train.to(device)).mean())\n",
    "                test_RMSE = torch.sqrt(gpytorch.metrics.mean_squared_error(test_pred_dist, y_test.to(device)).mean())\n",
    "\n",
    "                # Save losses for convergence plot\n",
    "                train_losses_NLML_over_epochs[epoch] = loss.item()\n",
    "                train_losses_RMSE_over_epochs[epoch] = train_RMSE.item()\n",
    "                test_losses_RMSE_over_epochs[epoch] = test_RMSE.item()\n",
    "\n",
    "                # Save evolution of hypers for convergence plot\n",
    "                l1_over_epochs[epoch] = model.base_kernel.lengthscale[0].item()\n",
    "                l2_over_epochs[epoch] = model.base_kernel.lengthscale[1].item()\n",
    "                outputscale_var_over_epochs[epoch] = model.covar_module.outputscale.item()\n",
    "                noise_var_over_epochs[epoch] = model.likelihood.noise.item()\n",
    "\n",
    "                # Print a bit more information for the first run\n",
    "                if epoch % 20 == 0:\n",
    "                    print(f\"{sim_name} {model_name} Run {run + 1}/{NUM_RUNS}, Epoch {epoch + 1}/{MAX_NUM_EPOCHS}, Training Loss (NLML): {loss:.4f}, Training RMSE: {train_RMSE:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e8e495",
   "metadata": {},
   "outputs": [],
   "source": [
    "from linear_operator.operators import to_linear_operator \n",
    "from linear_operator.operators import CatLinearOperator\n",
    "\n",
    "l1, l2 = model.covar_module.base_kernel.lengthscale[0], model.covar_module.base_kernel.lengthscale[1]\n",
    "diff = x_test[:, None, :] - x_train[None, :, :]\n",
    "r1, r2 = diff[..., 0], diff[..., 1]\n",
    "exp_term = to_linear_operator(torch.exp(-0.5 * ((r1 / l1)**2 + (r2 / l2)**2)))\n",
    "\n",
    "K_uu = to_linear_operator((1 - (r2**2 / l2**2)) / l2**2)\n",
    "K_uv = to_linear_operator((r1 * r2) / (l1**2 * l2**2))\n",
    "K_vv = to_linear_operator((1 - (r1**2 / l1**2)) / l1**2)\n",
    "\n",
    "\n",
    "# STEP 4: Combine and stack\n",
    "# Final scaled components (each shape N  M)\n",
    "K_uu = K_uu * exp_term\n",
    "K_uv = K_uv * exp_term\n",
    "K_vu = K_uv # NOTE: K_vu is equal to K_uv\n",
    "K_vv = K_vv * exp_term\n",
    "\n",
    "# Row-wise stacking: (N, 2M)\n",
    "top = CatLinearOperator(K_uu.expand(0), K_uv.expand(0), dim = 1)\n",
    "bottom = CatLinearOperator(K_vu, K_vv, dim = 1)\n",
    "K_block = CatLinearOperator(top, bottom, dim = 0)\n",
    "\n",
    "# Final 2N  2M block matrix\n",
    "# K_block = CatLinearOperator(top, bottom, dim = 0)\n",
    "K_block.to_dense().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403fc8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_uv.to_dense()[0:4, 0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a96795",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_uv.to_dense().T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c369315",
   "metadata": {},
   "outputs": [],
   "source": [
    "from linear_operator.operators import to_linear_operator \n",
    "from linear_operator.operators import CatLinearOperator\n",
    "\n",
    "l1, l2 = model.covar_module.base_kernel.lengthscale[0], model.covar_module.base_kernel.lengthscale[1]\n",
    "diff = x_test[:, None, :] - x_train[None, :, :]\n",
    "r1, r2 = diff[..., 0], diff[..., 1]\n",
    "exp_term = to_linear_operator(torch.exp(-0.5 * ((r1 / l1)**2 + (r2 / l2)**2)))\n",
    "\n",
    "K_uu = to_linear_operator((1 - (r2**2 / l2**2)) / l2**2)\n",
    "K_uv = to_linear_operator((r1 * r2) / (l1**2 * l2**2))\n",
    "K_vv = to_linear_operator((1 - (r1**2 / l1**2)) / l1**2)\n",
    "\n",
    "\n",
    "# STEP 4: Combine and stack\n",
    "# Final scaled components (each shape N  M)\n",
    "K_uu = K_uu * exp_term\n",
    "K_uv = K_uv * exp_term\n",
    "K_vu = K_uv # NOTE: K_vu is equal to K_uv\n",
    "K_vv = K_vv * exp_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf4e4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import linear_operator\n",
    "\n",
    "Z_uu = linear_operator.operators.ZeroLinearOperator(2, 2)\n",
    "\n",
    "I = linear_operator.operators.IdentityLinearOperator((2))\n",
    "\n",
    "ones = torch.ones(2, 2)\n",
    "\n",
    "M = linear_operator.operators.MaskedLinearOperator(\n",
    "    base = ones,\n",
    "    row_mask = torch.tensor([True, False]),\n",
    "    col_mask = torch.tensor([False, False]))\n",
    "\n",
    "# It slices, it does not mask\n",
    "M.to_dense()\n",
    "\n",
    "indicator = torch.zeros(2, 2)\n",
    "indicator[0, 0] = 1.0\n",
    "indicator  \n",
    "\n",
    "I = linear_operator.operators.IdentityLinearOperator((2))\n",
    "inticator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311dad68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import linear_operator.operators.kronecker_product_linear_operator\n",
    "\n",
    "### Step 1: Create inticators for \"interleaving\" Kronecker product\n",
    "K_uu_indicator = torch.zeros(2, 2).to(device)\n",
    "K_uu_indicator[0, 0] = 1.0\n",
    "K_uu_indicator_lo = to_linear_operator(indicator)\n",
    "\n",
    "K_uv_indicator = torch.zeros(2, 2).to(device)\n",
    "K_uv_indicator[0, 1] = 1.0\n",
    "K_uv_indicator_lo = to_linear_operator(K_uv_indicator)\n",
    "\n",
    "K_vu_indicator = torch.zeros(2, 2).to(device)\n",
    "K_vu_indicator[1, 0] = 1.0\n",
    "K_vu_indicator_lo = to_linear_operator(K_vu_indicator)\n",
    "\n",
    "K_vv_indicator = torch.zeros(2, 2).to(device)\n",
    "K_vv_indicator[1, 1] = 1.0\n",
    "K_vv_indicator_lo = to_linear_operator(K_vv_indicator)\n",
    "\n",
    "# Step 2: Create Kronecker product linear operators\n",
    "K_uu_expand = linear_operator.operators.KroneckerProductLinearOperator(\n",
    "    K_uu,\n",
    "    K_uu_indicator_lo, # NOTE: The order is important here, it is not commutative\n",
    ")\n",
    "\n",
    "K_uv_expand = linear_operator.operators.KroneckerProductLinearOperator(\n",
    "    K_uv,\n",
    "    K_uv_indicator_lo, # NOTE: The order is important here, it is not commutative\n",
    ")\n",
    "\n",
    "K_vu_expand = linear_operator.operators.KroneckerProductLinearOperator(\n",
    "    K_vu,\n",
    "    K_vu_indicator_lo, # NOTE: The order is important here, it is not\n",
    ")\n",
    "\n",
    "K_vv_expand = linear_operator.operators.KroneckerProductLinearOperator(\n",
    "    K_vv,\n",
    "    K_vv_indicator_lo, # NOTE: The order is important here, it is not commutative\n",
    ")\n",
    "\n",
    "K_interleave = K_uu_expand + K_uv_expand + K_vu_expand + K_vv_expand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfa7b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_interleave = K_uu_expand + K_uv_expand + K_vu_expand + K_vv_expand\n",
    "\n",
    "print(K_interleave.to_dense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba8d0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_uu.shape\n",
    "\n",
    "torch.sum(K_uu_expand, K_uv_expand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05801bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_uu_expand.sum(K_uv_expand).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e0f6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(K_uv_expand.to_dense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8989c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import linear_operator\n",
    "\n",
    "nothong = linear_operator.operators.ZeroLinearOperator((10, 10))\n",
    "linear_operator.operators.BlockDiagLinearOperator(base_linear_op = nothong)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4669f6df",
   "metadata": {},
   "source": [
    "classlinear_operator.operators.KernelLinearOperator(x1, x2, covar_func, num_outputs_per_input=(1, 1), num_nonbatch_dimensions=None, **params)[source]\n",
    "# https://linear-operator.readthedocs.io/en/latest/data_sparse_operators.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfe233b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.stack([K_uu.to_dense(), K_uv.to_dense()], dim = -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef09ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nothong.to_dense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f78ea21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interleave_permutation(n):\n",
    "    # Returns indices to interleave [0, n), [n, 2n)  [0, n, 1, n+1, ..., n-1, 2n-1]\n",
    "    idx = torch.arange(n)\n",
    "    return (torch.stack([idx, idx + n], dim=1)\n",
    "                .reshape(-1))\n",
    "\n",
    "N, M = K_uu.shape[0], K_uu.shape[1]  # assuming all blocks have shape (N, M)\n",
    "\n",
    "perm_rows = interleave_permutation(N)\n",
    "perm_cols = interleave_permutation(M)\n",
    "\n",
    "# Use these to permute via gather or torch.index_select if dense\n",
    "# OR to build permutation matrices for matmul\n",
    "P_row = torch.eye(2 * N)[perm_rows].to(device)\n",
    "P_col = torch.eye(2 * M)[perm_cols].to(device)\n",
    "\n",
    "# If your K_block is a dense tensor\n",
    "K_interleaved = P_row @ K_block @ P_col.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34446420",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(K_interleaved.to_dense().detach().cpu().numpy(), cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33be4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build interleaved rows: each is [u_row, v_row]\n",
    "rows = []\n",
    "for i in range(K_uu.shape[0] * 2):  # iterate over N\n",
    "    row_u = CatLinearOperator(K_uu[i, :], K_uv[i, :], dim = 0)\n",
    "    row_v = CatLinearOperator(K_vu[i, :], K_vv[i, :], dim = 0)\n",
    "    rows.append(row_u)\n",
    "    rows.append(row_v)\n",
    "\n",
    "# Stack rows interleaved  shape (2N, 2M)\n",
    "K_interleaved = CatLinearOperator(*rows, dim=-2)\n",
    "\n",
    "K_interleaved.to_dense\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(K_interleaved.to_dense().detach().cpu().numpy(), cmap = 'viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3865d41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_u = CatLinearOperator(K_uu[0, :].expand(1, -1), K_uv[0, :].expand(1, -1), dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99edc88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_uu[0].expand(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8334f979",
   "metadata": {},
   "outputs": [],
   "source": [
    "top = CatLinearOperator(K_uu.expand(1, -1, -1), K_uv.expand(1, -1, -1), dim = 0)\n",
    "bottom = CatLinearOperator(K_vu.expand(1, -1, -1), K_vv.expand(1, -1, -1), dim = 0)\n",
    "block = CatLinearOperator(top.expand(1, 2, -1, -1), bottom.expand(1, 2, -1, -1), dim = 0)\n",
    "block.shape\n",
    "out = block.permute(1, 0, 2, 3)\n",
    "# torch.Size([2, 2, 293, 536])\n",
    "out.view(2, 2*293, 536)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdce4fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_block.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214596f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "top.expand(1, 2, -1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22e10de",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_uu.expand(1, -1, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7151dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_block.permute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dde4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_dist.covariance_matrix - train_pred_dist.covariance_matrix.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56309095",
   "metadata": {},
   "outputs": [],
   "source": [
    "(test_pred_dist.covariance_matrix - test_pred_dist.covariance_matrix.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359bbf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_pred_dist._interleaved)\n",
    "print(test_pred_dist._interleaved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a7826f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plt.imshow(train_pred_dist.covariance_matrix.detach().cpu().numpy()[0:20, 0:20], cmap='viridis')\n",
    "plt.imshow(train_pred_dist.covariance_matrix.detach().cpu().numpy(), cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb9b977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(test_pred_dist.covariance_matrix.detach().cpu().numpy()[0:20, 0:20], cmap='viridis')\n",
    "plt.imshow(test_pred_dist.covariance_matrix.detach().cpu().numpy(), cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf768a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision = 7, sci_mode=False)\n",
    "train_pred_dist.covariance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ca50bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_dist.covariance_matrix - test_pred_dist.covariance_matrix.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d5834a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_dist.covariance_matrix - test_pred_dist.covariance_matrix .T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcd70f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.diag(test_pred_dist_eval.covariance_matrix).min()\n",
    "torch.diag(train_pred_dist_eval.covariance_matrix).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc3065b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.base_kernel.lengthscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0be81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.diag(dist_test.covariance_matrix).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db8acde",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.diag(dist_test.covariance_matrix).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1247ec19",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.diag(pred_dist_test.covariance_matrix).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b5bcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the diagonal (variances at test points)\n",
    "diagonal_variances = torch.diag(pred_dist_test.covariance_matrix)\n",
    "\n",
    "# Get the index of the minimum variance\n",
    "min_index = torch.argmin(diagonal_variances)\n",
    "\n",
    "x_test[min_index]\n",
    "y_train[min_index]\n",
    "\n",
    "y_train[min_index -1]\n",
    "x_test[min_index ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2ddeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.diag(test_pred_dist.covariance_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740db997",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dist_early = train_pred_dist\n",
    "test_dist_early = test_pred_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bc7fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_tensor = x_train\n",
    "column_tensor = x_train\n",
    "\n",
    "l1, l2 = model.base_kernel.lengthscale[0].to(device), model.base_kernel.lengthscale[1].to(device)\n",
    "\n",
    "# STEP 1: Pairwise differences of shape [N, M, 2]\n",
    "# Expand row_tensor [N, 2] -> [N, 1, 2] and column_tensor [M, 2] -> [1, M, 2]\n",
    "diff = (row_tensor[:, None, :] - column_tensor[None, :, :]).to(device)\n",
    "# diffs are negative too\n",
    "\n",
    "# Extract the relative components (columns of diff) for convenience, matching paper notation\n",
    "r1 = diff[:, :, 0]\n",
    "r2 = diff[:, :, 1]\n",
    "# diagonal of r1 and r2 are 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a5c028",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_uu # diagonal is 1/l2**2\n",
    "K_vv # diagonal is 1/l1**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c335c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1, l2 = model.base_kernel.lengthscale[0].to(device), model.base_kernel.lengthscale[1].to(device)\n",
    "\n",
    "# STEP 1: Pairwise differences of shape [N, M, 2]\n",
    "# Expand row_tensor [N, 2] -> [N, 1, 2] and column_tensor [M, 2] -> [1, M, 2]\n",
    "diff = (row_tensor[:, None, :] - column_tensor[None, :, :]).to(device)\n",
    "\n",
    "# Extract the relative components (columns of diff) for convenience, matching paper notation\n",
    "r1 = diff[:, :, 0]\n",
    "r2 = diff[:, :, 1]\n",
    "        \n",
    "# STEP 2: Block matrix\n",
    "\n",
    "# Block components (shape N  M each)\n",
    "K_uu = (1 - (r2**2 / l2**2)) / l2**2\n",
    "K_uv = (r1 * r2) / (l1**2 * l2**2)\n",
    "K_vu = K_uv  \n",
    "K_vv = (1 - (r1**2 / l1**2)) / l1**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2400a9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dist_early.covariance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6dcb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.diag(train_dist_early.covariance_matrix).min()\n",
    "torch.diag(test_dist_early.covariance_matrix).min()\n",
    "\n",
    "exp_term = torch.exp(-0.5 * ((r1 / l1) ** 2 + (r2 / l2) ** 2))\n",
    "\n",
    " # Now interleave rows and columns\n",
    "# Stack into shape (N, M, 2, 2)\n",
    "K_blocks = torch.stack([\n",
    "            torch.stack([K_uu, K_uv], dim = -1),\n",
    "            torch.stack([K_vu, K_vv], dim = -1)\n",
    "        ], dim = -2)  # shape (N, M, 2, 2)\n",
    "\n",
    "# HACK: GPytorch needs the interleaved matrix for the Multitask distribution\n",
    "# Reshape into (2N, 2M) interleaved matrix\n",
    "K_interleaved = K_blocks.permute(0, 2, 1, 3).reshape(2 * row_tensor.shape[0], 2 * column_tensor.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4a9167",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Now interleave rows and columns\n",
    "# Stack into shape (N, M, 2, 2)\n",
    "K_blocks = torch.stack([\n",
    "            torch.stack([K_uu, K_uv], dim = -1),\n",
    "            torch.stack([K_vu, K_vv], dim = -1)\n",
    "        ], dim = -2)  # shape (N, M, 2, 2)\n",
    "\n",
    "        # HACK: GPytorch needs the interleaved matrix for the Multitask distribution\n",
    "        # Reshape into (2N, 2M) interleaved matrix\n",
    "K_interleaved = K_blocks.permute(0, 2, 1, 3).reshape(2 * row_tensor.shape[0], 2 * column_tensor.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3993d43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_uu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f93bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_interleaved[0:6, 0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a18d4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.covar_module.outputscale.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334858b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_interleaved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa753f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the covar\n",
    "torch.diag(train_dist_early.covariance_matrix).min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109cb5f2",
   "metadata": {},
   "source": [
    "# GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efddad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REAL DATA EXPERIMENTS\n",
    "# RUN WITH python run_real_experiments_GP.py\n",
    "#               _                 _   _      \n",
    "#              | |               | | (_)     \n",
    "#    __ _ _ __ | |_ __ _ _ __ ___| |_ _  ___ \n",
    "#   / _` | '_ \\| __/ _` | '__/ __| __| |/ __|\n",
    "#  | (_| | | | | || (_| | | | (__| |_| | (__ \n",
    "#   \\__,_|_| |_|\\__\\__,_|_|  \\___|\\__|_|\\___|\n",
    "# \n",
    "model_name = \"GP\"\n",
    "from gpytorch_models import GP\n",
    "\n",
    "# import configs to we can access the hypers with getattr\n",
    "import configs\n",
    "from configs import PATIENCE, MAX_NUM_EPOCHS, NUM_RUNS, WEIGHT_DECAY\n",
    "from configs import TRACK_EMISSIONS_BOOL\n",
    "\n",
    "# Reiterating import for visibility\n",
    "MAX_NUM_EPOCHS = MAX_NUM_EPOCHS\n",
    "MAX_NUM_EPOCHS = 100\n",
    "NUM_RUNS = NUM_RUNS\n",
    "NUM_RUNS = 1\n",
    "WEIGHT_DECAY = WEIGHT_DECAY\n",
    "PATIENCE = PATIENCE\n",
    "\n",
    "# assign model-specific variable\n",
    "MODEL_LEARNING_RATE = getattr(configs, f\"{model_name}_REAL_LEARNING_RATE\")\n",
    "MODEL_REAL_RESULTS_DIR = getattr(configs, f\"{model_name}_REAL_RESULTS_DIR\")\n",
    "import os\n",
    "os.makedirs(MODEL_REAL_RESULTS_DIR, exist_ok = True)\n",
    "\n",
    "# basics\n",
    "import pandas as pd\n",
    "import torch\n",
    "import gpytorch\n",
    "\n",
    "# universals \n",
    "from metrics import compute_divergence_field, quantile_coverage_error_2d\n",
    "from utils import set_seed, make_grid\n",
    "import gc\n",
    "import warnings\n",
    "set_seed(42)\n",
    "\n",
    "# setting device to GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# overwrite if needed: # device = 'cpu'\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "### START TIMING ###\n",
    "import time\n",
    "start_time = time.time()  # Start timing after imports\n",
    "\n",
    "### START TRACKING EXPERIMENT EMISSIONS ###\n",
    "if TRACK_EMISSIONS_BOOL:\n",
    "    from codecarbon import EmissionsTracker\n",
    "    tracker = EmissionsTracker(project_name = \"GP_real_experiments\", output_dir = MODEL_REAL_RESULTS_DIR)\n",
    "    tracker.start()\n",
    "\n",
    "#############################\n",
    "### LOOP 1 - over REGIONS ###\n",
    "#############################\n",
    "\n",
    "for region_name in [\"region_lower_byrd\"]:\n",
    "\n",
    "    print(f\"\\nTraining for {region_name.upper()}...\")\n",
    "\n",
    "    # Store metrics for the current region (used for *metrics_summary* report and *metrics_per_run*)\n",
    "    region_results = []\n",
    "\n",
    "    ##########################################\n",
    "    ### x_train & y_train, x_test & x_test ###\n",
    "    ##########################################\n",
    "\n",
    "    # define paths based on region_name\n",
    "    path_to_training_tensor = \"data/real_data/\" + region_name + \"_train_tensor.pt\"\n",
    "    path_to_test_tensor = \"data/real_data/\" + region_name + \"_test_tensor.pt\"\n",
    "\n",
    "    # load and tranpose to have rows as points\n",
    "    train = torch.load(path_to_training_tensor, weights_only = False).T \n",
    "    test = torch.load(path_to_test_tensor, weights_only = False).T\n",
    "\n",
    "    # The train and test tensors have the following columns:\n",
    "    # [:, 0] = x\n",
    "    # [:, 1] = y\n",
    "    # [:, 2] = surface elevation (s)\n",
    "    # [:, 3] = ice flux in x direction (u)\n",
    "    # [:, 4] = ice flux in y direction (v)\n",
    "    # [:, 5] = ice flux error in x direction (u_err)\n",
    "    # [:, 6] = ice flux error in y direction (v_err)\n",
    "    # [:, 7] = source age\n",
    "\n",
    "    # train\n",
    "    x_train = train[:, [0, 1]].to(device)\n",
    "    y_train = train[:, [3, 4]].to(device)\n",
    "\n",
    "    # test\n",
    "    x_test = test[:, [0, 1]].to(device)\n",
    "    y_test = test[:, [3, 4]].to(device)\n",
    "\n",
    "    # NOTE: Here we estimate the noise variance \n",
    "    \"\"\"\n",
    "    ### NOISE MODEL ###\n",
    "    # TRAIN\n",
    "    # noise variance (h * sigma_u)^2 and (h * sigma_v)^2 (tensor contains [h sig_u, h sig_v] stds)\n",
    "    noise_var_h_times_uv_train = torch.concat((train[:, 5], train[:, 6]), dim = 0)**2\n",
    "    # assume age dependent noise sigma_h on ice thickness measurements: ~10 - 20 m std (1000 scaling)\n",
    "    sigma_h = 0.01 * torch.log(train[:, 7] + 3)\n",
    "    # calculate noise variance (u * sigma_h)^2 and (v * sigma_h)^2\n",
    "    noise_var_uv_times_h_train = (torch.concat((train[:, 3], train[:, 4]), dim = 0) * torch.cat([sigma_h, sigma_h]))**2\n",
    "    # combine both noise variances into the std for each dimension\n",
    "    train_noise_diag = torch.sqrt(noise_var_h_times_uv_train + noise_var_uv_times_h_train).to(device)\n",
    "\n",
    "    # Compute midpoint\n",
    "    midpoint = train_noise_diag.shape[0] // 2\n",
    "\n",
    "    # Print noise levels for train, formatted to 4 decimal places\n",
    "    print(f\"Mean noise std per x dimension: {train_noise_diag[:midpoint].mean(dim = 0).item():.4f}\")\n",
    "    print(f\"Mean noise std per y dimension: {train_noise_diag[midpoint:].mean(dim = 0).item():.4f}\")\n",
    "\n",
    "    # TEST\n",
    "    # noise variance (h * sigma_u)^2 and (h * sigma_v)^2 (tensor contains [h sig_u, h sig_v] stds)\n",
    "    noise_var_h_times_uv_test = torch.concat((test[:, 5], test[:, 6]), dim = 0)**2\n",
    "    # assume age dependent noise sigma_h on ice thickness measurements: ~10 - 20 m std (1000 scaling)\n",
    "    sigma_h = 0.01 * torch.log(test[:, 7] + 3)\n",
    "    # calculate noise variance (u * sigma_h)^2 and (v * sigma_h)^2\n",
    "    noise_var_uv_times_h_test = (torch.concat((test[:, 3], test[:, 4]), dim = 0) * torch.cat([sigma_h, sigma_h]))**2\n",
    "    # combine both noise variances into the std for each dimension\n",
    "    test_noise_diag = torch.sqrt(noise_var_h_times_uv_test + noise_var_uv_times_h_test).to(device)\n",
    "    \"\"\"\n",
    "\n",
    "    # Print train details\n",
    "    print(f\"=== {region_name.upper()} ===\")\n",
    "    print(f\"Training inputs shape: {x_train.shape}\")\n",
    "    print(f\"Training observations shape: {y_train.shape}\")\n",
    "    print(f\"Training inputs dtype: {x_train.dtype}\")\n",
    "    print()\n",
    "\n",
    "    # Print test details\n",
    "    print(f\"=== {region_name.upper()} ===\")\n",
    "    print(f\"Test inputs shape: {x_test.shape}\")\n",
    "    print(f\"Test observations shape: {y_test.shape}\")\n",
    "    print(f\"Test inputs dtype: {x_test.dtype}\")\n",
    "    print()\n",
    "\n",
    "    ##################################\n",
    "    ### LOOP 2 - over training run ###\n",
    "    ##################################\n",
    "\n",
    "    # NOTE: GPs don't train on batches, use full data\n",
    "\n",
    "    for run in range(NUM_RUNS):\n",
    "\n",
    "        print(f\"\\n--- Training Run {run + 1}/{NUM_RUNS} ---\")\n",
    "\n",
    "        # Initialise the likelihood for the GP model (estimates noise)\n",
    "        # NOTE: we use a multitask likelihood for the GP model but with a global noise term\n",
    "        likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(\n",
    "            num_tasks = 2,\n",
    "            has_global_noise = True, \n",
    "            has_task_noise = False, # HACK: This still needs to be manually turned off\n",
    "            ).to(device)\n",
    "\n",
    "        model = GP(\n",
    "            x_train,\n",
    "            y_train, \n",
    "            likelihood\n",
    "            ).to(device)\n",
    "        \n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr = MODEL_LEARNING_RATE, weight_decay = WEIGHT_DECAY)\n",
    "        \n",
    "        # Use ExactMarginalLogLikelihood\n",
    "        mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "        model.train()\n",
    "        likelihood.train()\n",
    "        # _________________\n",
    "        # BEFORE EPOCH LOOP\n",
    "        \n",
    "        # Export the convergence just for first run only\n",
    "        if run == 0:\n",
    "            # initialise tensors to store losses over epochs (for convergence plot)\n",
    "            train_losses_NLML_over_epochs = torch.zeros(MAX_NUM_EPOCHS) # objective\n",
    "            train_losses_RMSE_over_epochs = torch.zeros(MAX_NUM_EPOCHS) # by-product\n",
    "            # monitor performance transfer to test (only RMSE easy to calc without covar)\n",
    "            test_losses_RMSE_over_epochs = torch.zeros(MAX_NUM_EPOCHS)\n",
    "\n",
    "            # NOTE: Here, we estimate the noise\n",
    "            l1_over_epochs = torch.zeros(MAX_NUM_EPOCHS)\n",
    "            l2_over_epochs = torch.zeros(MAX_NUM_EPOCHS)\n",
    "            Buu_over_epochs = torch.zeros(MAX_NUM_EPOCHS)\n",
    "            Buv_over_epochs = torch.zeros(MAX_NUM_EPOCHS)\n",
    "            Bvu_over_epochs = torch.zeros(MAX_NUM_EPOCHS)\n",
    "            Bvv_over_epochs = torch.zeros(MAX_NUM_EPOCHS)\n",
    "            noise_var_over_epochs = torch.zeros(MAX_NUM_EPOCHS)\n",
    "\n",
    "        # Early stopping variables\n",
    "        best_loss = float('inf')\n",
    "        # counter starts at 0\n",
    "        epochs_no_improve = 0\n",
    "\n",
    "        ############################\n",
    "        ### LOOP 3 - over EPOCHS ###\n",
    "        ############################\n",
    "        print(\"\\nStart Training\")\n",
    "\n",
    "        for epoch in range(MAX_NUM_EPOCHS):\n",
    "\n",
    "            # Set to train\n",
    "            model.train()\n",
    "            likelihood.train()\n",
    "\n",
    "            # Do a step\n",
    "            optimizer.zero_grad()\n",
    "            # model outputs a multivariate normal distribution\n",
    "            train_pred_dist = model(x_train.to(device))\n",
    "            # Train on noisy or targets\n",
    "            # NOTE: We only have observational y_train i.e. noisy data\n",
    "            loss = - mll(train_pred_dist, y_train.to(device))  # negative marginal log likelihood\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # For Run 1 we save a bunch of metrics and update, while for the rest we only update\n",
    "            if run == 0:\n",
    "\n",
    "                model.eval()\n",
    "                likelihood.eval()\n",
    "\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\", gpytorch.utils.warnings.GPInputWarning)\n",
    "                    train_pred_dist_eval = model(x_train.to(device))\n",
    "                test_pred_dist = model(x_test.to(device))\n",
    "\n",
    "                # Compute RMSE for training and test predictions (given true data, not noisy)\n",
    "                train_RMSE = torch.sqrt(gpytorch.metrics.mean_squared_error(train_pred_dist_eval, y_train.to(device)).mean())\n",
    "                test_RMSE = torch.sqrt(gpytorch.metrics.mean_squared_error(test_pred_dist, y_test.to(device)).mean())\n",
    "\n",
    "                # Save losses for convergence plot\n",
    "                train_losses_NLML_over_epochs[epoch] = loss.item()\n",
    "                train_losses_RMSE_over_epochs[epoch] = train_RMSE.item()\n",
    "                test_losses_RMSE_over_epochs[epoch] = test_RMSE.item()\n",
    "\n",
    "                # Save evolution of hypers for convergence plot\n",
    "                # NOTE: This is different to dfGPs\n",
    "                l1_over_epochs[epoch] = model.covar_module.data_covar_module.lengthscale[0, 0].item()\n",
    "                l2_over_epochs[epoch] = model.covar_module.data_covar_module.lengthscale[0, 1].item()\n",
    "\n",
    "                # Reconstruct B first via FF.T + D where F is the covar_factor and D is the diagonal matrix of task variances var\n",
    "                B = model.covar_module.task_covar_module.covar_factor @ model.covar_module.task_covar_module.covar_factor.T + torch.diag(model.covar_module.task_covar_module.var)\n",
    "                # Extract items\n",
    "                Buu_over_epochs[epoch] = B[0, 0].item()\n",
    "                Buv_over_epochs[epoch] = B[0, 1].item()\n",
    "                Bvu_over_epochs[epoch] = B[1, 0].item()\n",
    "                Bvv_over_epochs[epoch] = B[1, 1].item()\n",
    "\n",
    "                noise_var_over_epochs[epoch] = model.likelihood.noise.item()\n",
    "\n",
    "                # Print a bit more information for the first run\n",
    "                if epoch % 20 == 0:\n",
    "                    print(f\"{region_name} {model_name} Run {run + 1}/{NUM_RUNS}, Epoch {epoch + 1}/{MAX_NUM_EPOCHS}, Training Loss (NLML): {loss:.4f}, Training RMSE: {train_RMSE:.4f}\")\n",
    "\n",
    "                # delete after printing and saving\n",
    "                # NOTE: keep loss for early stopping check\n",
    "                # del train_pred_dist, test_pred_dist, train_RMSE, test_RMSE\n",
    "                \n",
    "                # Free up memory every 20 epochs\n",
    "                if epoch % 20 == 0:\n",
    "                    gc.collect() and torch.cuda.empty_cache()\n",
    "            \n",
    "            # For all runs after the first we run a minimal version using only lml_train\n",
    "            else:\n",
    "\n",
    "                if epoch % 20 == 0:\n",
    "                    # After run 1 we only print lml, nothing else\n",
    "                    print(f\"{region_name} {model_name} Run {run + 1}/{NUM_RUNS}, Epoch {epoch + 1}/{MAX_NUM_EPOCHS}, Training Loss (NLML): {loss:.4f}\")\n",
    "                \n",
    "            # EVERY EPOCH: Early stopping check\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                # reset counter if loss improves\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "\n",
    "            if epochs_no_improve >= PATIENCE:\n",
    "                print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
    "                # exit epoch loop\n",
    "                break\n",
    "\n",
    "        ##############################\n",
    "        ### END LOOP 3 over EPOCHS ###\n",
    "        ##############################\n",
    "\n",
    "        # for every run...\n",
    "        #######################################################\n",
    "        ### EVALUATE after all training for RUN is finished ###\n",
    "        #######################################################\n",
    "\n",
    "        model.eval()\n",
    "        likelihood.eval()\n",
    "\n",
    "        # Need gradients for autograd divergence: We clone and detach\n",
    "        x_test_grad = x_test.to(device).clone().requires_grad_(True)\n",
    "        x_train_grad = x_train.to(device).clone().requires_grad_(True)\n",
    "\n",
    "        # Underlying (latent) distribution and predictive distribution\n",
    "        dist_test = model(x_test_grad)\n",
    "        pred_dist_test = likelihood(dist_test)\n",
    "\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", gpytorch.utils.warnings.GPInputWarning)\n",
    "            dist_train = model(x_train_grad)\n",
    "            pred_dist_train = likelihood(dist_train)\n",
    "        \n",
    "        # Compute divergence field (from latent distribution)\n",
    "        test_div_field = compute_divergence_field(dist_test.mean, x_test_grad)\n",
    "        train_div_field = compute_divergence_field(dist_train.mean, x_train_grad)\n",
    "\n",
    "        # Only save mean_pred, covar_pred and divergence fields for the first run\n",
    "        if run == 0:\n",
    "\n",
    "            # (1) Save predictions from first run so we can visualise them later\n",
    "            torch.save(pred_dist_test.mean, f\"{MODEL_REAL_RESULTS_DIR}/{region_name}_{model_name}_test_mean_predictions.pt\")\n",
    "            torch.save(pred_dist_test.covariance_matrix, f\"{MODEL_REAL_RESULTS_DIR}/{region_name}_{model_name}_test_covar_predictions.pt\")\n",
    "\n",
    "            # (2) Save divergence field\n",
    "            torch.save(test_div_field, f\"{MODEL_REAL_RESULTS_DIR}/{region_name}_{model_name}_test_prediction_divergence_field.pt\")\n",
    "\n",
    "            # (3) Since all epoch training is finished, we can save the losses over epochs\n",
    "            df_losses = pd.DataFrame({\n",
    "                'Epoch': list(range(train_losses_NLML_over_epochs.shape[0])), # pythonic indexing\n",
    "                'Train NLML': train_losses_NLML_over_epochs.tolist(),\n",
    "                'Train RMSE': train_losses_RMSE_over_epochs.tolist(),\n",
    "                'Test RMSE': test_losses_RMSE_over_epochs.tolist(),\n",
    "                # hyperparameters\n",
    "                'l1': l1_over_epochs.tolist(),\n",
    "                'l2': l2_over_epochs.tolist(),\n",
    "                'Buu': Buu_over_epochs.tolist(),\n",
    "                'Buv': Buv_over_epochs.tolist(),\n",
    "                'Bvu': Bvu_over_epochs.tolist(),\n",
    "                'Bvv': Bvv_over_epochs.tolist(),\n",
    "                'noise_var': noise_var_over_epochs.tolist(),\n",
    "                })\n",
    "            \n",
    "            df_losses.to_csv(f\"{MODEL_REAL_RESULTS_DIR}/{region_name}_{model_name}_losses_over_epochs.csv\", index = False, float_format = \"%.5f\") # reduce to 5 decimals for readability\n",
    "\n",
    "        # Compute TRAIN metrics (convert tensors to float) for every run's tuned model\n",
    "        # NOTE: gpytorch outputs metrics per task\n",
    "        train_RMSE = torch.sqrt(gpytorch.metrics.mean_squared_error(\n",
    "            pred_dist_train, y_train.to(device)).mean()).item()\n",
    "        train_MAE = gpytorch.metrics.mean_absolute_error(\n",
    "            pred_dist_train, y_train.to(device)).mean().item()\n",
    "        train_NLL = gpytorch.metrics.negative_log_predictive_density(\n",
    "            pred_dist_train, y_train.to(device)).item()\n",
    "        train_QCE = quantile_coverage_error_2d(\n",
    "            pred_dist_train, y_train.to(device), quantile = 95.0).item()\n",
    "        ## NOTE: It is important to use the absolute value of the divergence field, since both positive and negative deviations are violations and shouldn't cancel each other out \n",
    "        train_MAD = train_div_field.abs().mean().item()\n",
    "\n",
    "        # Compute TEST metrics (convert tensors to float) for every run's tuned model\n",
    "        test_RMSE = torch.sqrt(gpytorch.metrics.mean_squared_error(\n",
    "            pred_dist_test, y_test.to(device)).mean()).item()\n",
    "        test_MAE = gpytorch.metrics.mean_absolute_error(\n",
    "            pred_dist_test, y_test.to(device)).mean().item()\n",
    "        test_NLL = gpytorch.metrics.negative_log_predictive_density(\n",
    "            pred_dist_test, y_test.to(device)).item()\n",
    "        test_QCE = quantile_coverage_error_2d(\n",
    "            pred_dist_test, y_test.to(device), quantile = 95.0).item()\n",
    "        ## NOTE: It is important to use the absolute value of the divergence field, since both positive and negative deviations are violations and shouldn't cancel each other out \n",
    "        test_MAD = test_div_field.abs().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45ab494",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision = 7, sci_mode=False)\n",
    "(pred_dist_test.covariance_matrix - pred_dist_test.covariance_matrix.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0751e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_pred_dist._interleaved)\n",
    "print(train_pred_dist_eval._interleaved)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# plt.imshow(train_pred_dist.covariance_matrix.detach().cpu().numpy()[0:20, 0:20], cmap = 'viridis')\n",
    "plt.imshow(train_pred_dist_eval.covariance_matrix.detach().cpu().numpy()[0:20, 0:20], cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11261390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interleave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ebe841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import gpytorch\n",
    "\n",
    "mean_u = torch.linspace(0, 7, 8).unsqueeze(-1)\n",
    "mean_v = torch.linspace(10, 17, 8).unsqueeze(-1)\n",
    "\n",
    "mean_2d = torch.cat((mean_u, mean_v), dim = 1)\n",
    "\n",
    "diff = (mean_2d[:, None, :] - mean_2d[None, :, :]).square()\n",
    "r1 = torch.exp(- diff[:, :, 0])\n",
    "r2 = torch.exp(- diff[:, :, 1])\n",
    "\n",
    "K_uu = 0.5 * r1\n",
    "K_vv = 0.8 * r2\n",
    "K_uv = 0.1 * r1 * r2\n",
    "K_vu = K_uv\n",
    "\n",
    "K_block = [[K_uu, K_uv], [K_vu, K_vv]]\n",
    "\n",
    "K_upper = torch.cat((K_uu, K_uv), dim = 0)\n",
    "K_lower = torch.cat((K_vu, K_vv), dim = 0)\n",
    "K_block = torch.cat((K_upper, K_lower), dim = 1)\n",
    "\n",
    "plt.imshow(K_block, cmap = 'viridis')\n",
    "\n",
    "dist_2d = gpytorch.distributions.MultitaskMultivariateNormal(\n",
    "    mean = mean_2d,\n",
    "    covariance_matrix = K_block,\n",
    "    interleaved = False\n",
    ")\n",
    "\n",
    "print(dist_2d._interleaved)\n",
    "print(dist_2d.mean)\n",
    "\n",
    "plt.imshow(dist_2d.covariance_matrix.detach().cpu().numpy(), cmap = 'viridis')\n",
    "\n",
    "mean_mvn = mean_2d.transpose(-1, -2).reshape(*mean_2d.shape[:-2], -1)\n",
    "mean_mvn\n",
    "\n",
    "print(dist_2d.mean.shape) \n",
    "print(dist_2d._output_shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
