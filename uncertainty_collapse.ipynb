{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c74d7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "=== BRANCHING ===\n",
      "Training inputs shape: torch.Size([196, 2])\n",
      "Training observations shape: torch.Size([196, 2])\n",
      "Training inputs dtype: torch.float32\n",
      "Training inputs device: cuda:0\n",
      "Training observations device: cuda:0\n",
      "\n",
      "=== BRANCHING ===\n",
      "Test inputs shape: torch.Size([400, 2])\n",
      "Test observations shape: torch.Size([400, 2])\n",
      "Test inputs dtype: torch.float32\n",
      "Test inputs device: cuda:0\n",
      "Test observations device: cuda:0\n",
      "\n",
      "\n",
      "--- Training Run 1/1 ---\n",
      "\n",
      "Start Training\n",
      "branching dfNGP Run 1/1, Epoch 1/2000, Training Loss (NLML): -634.0804, (RMSE): 0.0556\n",
      "branching dfNGP Run 1/1, Epoch 2/2000, Training Loss (NLML): -646.2526, (RMSE): 0.0561\n",
      "branching dfNGP Run 1/1, Epoch 3/2000, Training Loss (NLML): -644.4355, (RMSE): 0.0560\n",
      "branching dfNGP Run 1/1, Epoch 4/2000, Training Loss (NLML): -645.0752, (RMSE): 0.0555\n",
      "branching dfNGP Run 1/1, Epoch 5/2000, Training Loss (NLML): -649.2019, (RMSE): 0.0548\n",
      "branching dfNGP Run 1/1, Epoch 6/2000, Training Loss (NLML): -654.8210, (RMSE): 0.0539\n",
      "branching dfNGP Run 1/1, Epoch 7/2000, Training Loss (NLML): -659.6844, (RMSE): 0.0529\n",
      "branching dfNGP Run 1/1, Epoch 8/2000, Training Loss (NLML): -662.0396, (RMSE): 0.0519\n",
      "branching dfNGP Run 1/1, Epoch 9/2000, Training Loss (NLML): -662.5042, (RMSE): 0.0510\n",
      "branching dfNGP Run 1/1, Epoch 10/2000, Training Loss (NLML): -664.4548, (RMSE): 0.0502\n",
      "branching dfNGP Run 1/1, Epoch 11/2000, Training Loss (NLML): -668.5370, (RMSE): 0.0496\n",
      "branching dfNGP Run 1/1, Epoch 12/2000, Training Loss (NLML): -672.3062, (RMSE): 0.0491\n",
      "branching dfNGP Run 1/1, Epoch 13/2000, Training Loss (NLML): -674.4846, (RMSE): 0.0485\n",
      "branching dfNGP Run 1/1, Epoch 14/2000, Training Loss (NLML): -675.8182, (RMSE): 0.0479\n",
      "branching dfNGP Run 1/1, Epoch 15/2000, Training Loss (NLML): -677.6115, (RMSE): 0.0471\n",
      "branching dfNGP Run 1/1, Epoch 16/2000, Training Loss (NLML): -680.4827, (RMSE): 0.0462\n",
      "branching dfNGP Run 1/1, Epoch 17/2000, Training Loss (NLML): -684.3964, (RMSE): 0.0451\n",
      "branching dfNGP Run 1/1, Epoch 18/2000, Training Loss (NLML): -688.7509, (RMSE): 0.0438\n",
      "branching dfNGP Run 1/1, Epoch 19/2000, Training Loss (NLML): -693.1211, (RMSE): 0.0422\n",
      "branching dfNGP Run 1/1, Epoch 20/2000, Training Loss (NLML): -697.5869, (RMSE): 0.0406\n",
      "branching dfNGP Run 1/1, Epoch 21/2000, Training Loss (NLML): -703.3048, (RMSE): 0.0388\n",
      "branching dfNGP Run 1/1, Epoch 22/2000, Training Loss (NLML): -711.0183, (RMSE): 0.0370\n",
      "branching dfNGP Run 1/1, Epoch 23/2000, Training Loss (NLML): -720.3052, (RMSE): 0.0350\n",
      "branching dfNGP Run 1/1, Epoch 24/2000, Training Loss (NLML): -730.8657, (RMSE): 0.0329\n",
      "branching dfNGP Run 1/1, Epoch 25/2000, Training Loss (NLML): -742.9960, (RMSE): 0.0307\n",
      "branching dfNGP Run 1/1, Epoch 26/2000, Training Loss (NLML): -756.4014, (RMSE): 0.0283\n",
      "branching dfNGP Run 1/1, Epoch 27/2000, Training Loss (NLML): -768.6443, (RMSE): 0.0262\n",
      "branching dfNGP Run 1/1, Epoch 28/2000, Training Loss (NLML): -776.5238, (RMSE): 0.0244\n",
      "branching dfNGP Run 1/1, Epoch 29/2000, Training Loss (NLML): -781.6693, (RMSE): 0.0231\n",
      "branching dfNGP Run 1/1, Epoch 30/2000, Training Loss (NLML): -785.3464, (RMSE): 0.0224\n",
      "branching dfNGP Run 1/1, Epoch 31/2000, Training Loss (NLML): -787.0161, (RMSE): 0.0221\n",
      "branching dfNGP Run 1/1, Epoch 32/2000, Training Loss (NLML): -789.4623, (RMSE): 0.0219\n",
      "branching dfNGP Run 1/1, Epoch 33/2000, Training Loss (NLML): -792.8197, (RMSE): 0.0218\n",
      "branching dfNGP Run 1/1, Epoch 34/2000, Training Loss (NLML): -795.6082, (RMSE): 0.0217\n",
      "branching dfNGP Run 1/1, Epoch 35/2000, Training Loss (NLML): -798.1847, (RMSE): 0.0217\n",
      "branching dfNGP Run 1/1, Epoch 36/2000, Training Loss (NLML): -800.4728, (RMSE): 0.0218\n",
      "branching dfNGP Run 1/1, Epoch 37/2000, Training Loss (NLML): -802.4535, (RMSE): 0.0220\n",
      "branching dfNGP Run 1/1, Epoch 38/2000, Training Loss (NLML): -803.9221, (RMSE): 0.0221\n",
      "branching dfNGP Run 1/1, Epoch 39/2000, Training Loss (NLML): -805.5400, (RMSE): 0.0220\n",
      "branching dfNGP Run 1/1, Epoch 40/2000, Training Loss (NLML): -807.6676, (RMSE): 0.0218\n",
      "branching dfNGP Run 1/1, Epoch 41/2000, Training Loss (NLML): -809.9386, (RMSE): 0.0214\n",
      "branching dfNGP Run 1/1, Epoch 42/2000, Training Loss (NLML): -814.3408, (RMSE): 0.0209\n",
      "branching dfNGP Run 1/1, Epoch 43/2000, Training Loss (NLML): -820.9258, (RMSE): 0.0201\n",
      "branching dfNGP Run 1/1, Epoch 44/2000, Training Loss (NLML): -833.2018, (RMSE): 0.0186\n",
      "branching dfNGP Run 1/1, Epoch 45/2000, Training Loss (NLML): -851.8849, (RMSE): 0.0166\n",
      "branching dfNGP Run 1/1, Epoch 46/2000, Training Loss (NLML): -870.2467, (RMSE): 0.0150\n",
      "branching dfNGP Run 1/1, Epoch 47/2000, Training Loss (NLML): -885.8909, (RMSE): 0.0143\n",
      "branching dfNGP Run 1/1, Epoch 48/2000, Training Loss (NLML): -893.7302, (RMSE): 0.0139\n",
      "branching dfNGP Run 1/1, Epoch 49/2000, Training Loss (NLML): -895.7555, (RMSE): 0.0133\n",
      "branching dfNGP Run 1/1, Epoch 50/2000, Training Loss (NLML): -896.4536, (RMSE): 0.0126\n",
      "branching dfNGP Run 1/1, Epoch 51/2000, Training Loss (NLML): -896.7341, (RMSE): 0.0118\n",
      "branching dfNGP Run 1/1, Epoch 52/2000, Training Loss (NLML): -899.3705, (RMSE): 0.0112\n",
      "branching dfNGP Run 1/1, Epoch 53/2000, Training Loss (NLML): -901.6858, (RMSE): 0.0106\n",
      "branching dfNGP Run 1/1, Epoch 54/2000, Training Loss (NLML): -900.9835, (RMSE): 0.0101\n",
      "branching dfNGP Run 1/1, Epoch 55/2000, Training Loss (NLML): -899.7811, (RMSE): 0.0101\n",
      "branching dfNGP Run 1/1, Epoch 56/2000, Training Loss (NLML): -898.9590, (RMSE): 0.0101\n",
      "branching dfNGP Run 1/1, Epoch 57/2000, Training Loss (NLML): -900.4543, (RMSE): 0.0100\n",
      "branching dfNGP Run 1/1, Epoch 58/2000, Training Loss (NLML): -900.7496, (RMSE): 0.0101\n",
      "branching dfNGP Run 1/1, Epoch 59/2000, Training Loss (NLML): -902.8479, (RMSE): 0.0104\n",
      "branching dfNGP Run 1/1, Epoch 60/2000, Training Loss (NLML): -904.1267, (RMSE): 0.0108\n",
      "branching dfNGP Run 1/1, Epoch 61/2000, Training Loss (NLML): -906.6915, (RMSE): 0.0110\n",
      "branching dfNGP Run 1/1, Epoch 62/2000, Training Loss (NLML): -905.2034, (RMSE): 0.0113\n",
      "branching dfNGP Run 1/1, Epoch 63/2000, Training Loss (NLML): -907.2599, (RMSE): 0.0119\n",
      "branching dfNGP Run 1/1, Epoch 64/2000, Training Loss (NLML): -906.9424, (RMSE): 0.0123\n",
      "branching dfNGP Run 1/1, Epoch 65/2000, Training Loss (NLML): -907.4456, (RMSE): 0.0125\n",
      "branching dfNGP Run 1/1, Epoch 66/2000, Training Loss (NLML): -906.7815, (RMSE): 0.0125\n",
      "branching dfNGP Run 1/1, Epoch 67/2000, Training Loss (NLML): -908.2175, (RMSE): 0.0126\n",
      "branching dfNGP Run 1/1, Epoch 68/2000, Training Loss (NLML): -908.9453, (RMSE): 0.0127\n",
      "branching dfNGP Run 1/1, Epoch 69/2000, Training Loss (NLML): -910.5865, (RMSE): 0.0125\n",
      "branching dfNGP Run 1/1, Epoch 70/2000, Training Loss (NLML): -912.3867, (RMSE): 0.0121\n",
      "branching dfNGP Run 1/1, Epoch 71/2000, Training Loss (NLML): -914.3455, (RMSE): 0.0118\n",
      "branching dfNGP Run 1/1, Epoch 72/2000, Training Loss (NLML): -915.7142, (RMSE): 0.0116\n",
      "branching dfNGP Run 1/1, Epoch 73/2000, Training Loss (NLML): -916.6115, (RMSE): 0.0113\n",
      "branching dfNGP Run 1/1, Epoch 74/2000, Training Loss (NLML): -917.6893, (RMSE): 0.0110\n",
      "branching dfNGP Run 1/1, Epoch 75/2000, Training Loss (NLML): -917.8517, (RMSE): 0.0108\n",
      "branching dfNGP Run 1/1, Epoch 76/2000, Training Loss (NLML): -918.1212, (RMSE): 0.0107\n",
      "branching dfNGP Run 1/1, Epoch 77/2000, Training Loss (NLML): -918.3311, (RMSE): 0.0106\n",
      "branching dfNGP Run 1/1, Epoch 78/2000, Training Loss (NLML): -919.1079, (RMSE): 0.0105\n",
      "branching dfNGP Run 1/1, Epoch 79/2000, Training Loss (NLML): -919.7009, (RMSE): 0.0105\n",
      "branching dfNGP Run 1/1, Epoch 80/2000, Training Loss (NLML): -921.0850, (RMSE): 0.0107\n",
      "branching dfNGP Run 1/1, Epoch 81/2000, Training Loss (NLML): -922.2007, (RMSE): 0.0108\n",
      "branching dfNGP Run 1/1, Epoch 82/2000, Training Loss (NLML): -923.4027, (RMSE): 0.0109\n",
      "branching dfNGP Run 1/1, Epoch 83/2000, Training Loss (NLML): -924.5787, (RMSE): 0.0110\n",
      "branching dfNGP Run 1/1, Epoch 84/2000, Training Loss (NLML): -925.5176, (RMSE): 0.0112\n",
      "branching dfNGP Run 1/1, Epoch 85/2000, Training Loss (NLML): -926.5833, (RMSE): 0.0113\n",
      "branching dfNGP Run 1/1, Epoch 86/2000, Training Loss (NLML): -927.5688, (RMSE): 0.0113\n",
      "branching dfNGP Run 1/1, Epoch 87/2000, Training Loss (NLML): -928.6172, (RMSE): 0.0113\n",
      "branching dfNGP Run 1/1, Epoch 88/2000, Training Loss (NLML): -930.1139, (RMSE): 0.0114\n",
      "branching dfNGP Run 1/1, Epoch 89/2000, Training Loss (NLML): -931.5116, (RMSE): 0.0113\n",
      "branching dfNGP Run 1/1, Epoch 90/2000, Training Loss (NLML): -932.7124, (RMSE): 0.0112\n",
      "branching dfNGP Run 1/1, Epoch 91/2000, Training Loss (NLML): -934.2372, (RMSE): 0.0111\n",
      "branching dfNGP Run 1/1, Epoch 92/2000, Training Loss (NLML): -935.4742, (RMSE): 0.0111\n",
      "branching dfNGP Run 1/1, Epoch 93/2000, Training Loss (NLML): -936.8375, (RMSE): 0.0110\n",
      "branching dfNGP Run 1/1, Epoch 94/2000, Training Loss (NLML): -938.3392, (RMSE): 0.0109\n",
      "branching dfNGP Run 1/1, Epoch 95/2000, Training Loss (NLML): -940.0077, (RMSE): 0.0109\n",
      "branching dfNGP Run 1/1, Epoch 96/2000, Training Loss (NLML): -941.7324, (RMSE): 0.0108\n",
      "branching dfNGP Run 1/1, Epoch 97/2000, Training Loss (NLML): -943.6167, (RMSE): 0.0108\n",
      "branching dfNGP Run 1/1, Epoch 98/2000, Training Loss (NLML): -945.7872, (RMSE): 0.0108\n",
      "branching dfNGP Run 1/1, Epoch 99/2000, Training Loss (NLML): -947.9081, (RMSE): 0.0108\n",
      "branching dfNGP Run 1/1, Epoch 100/2000, Training Loss (NLML): -950.1555, (RMSE): 0.0109\n",
      "branching dfNGP Run 1/1, Epoch 101/2000, Training Loss (NLML): -952.5709, (RMSE): 0.0109\n",
      "branching dfNGP Run 1/1, Epoch 102/2000, Training Loss (NLML): -954.9742, (RMSE): 0.0109\n",
      "branching dfNGP Run 1/1, Epoch 103/2000, Training Loss (NLML): -957.5842, (RMSE): 0.0110\n",
      "branching dfNGP Run 1/1, Epoch 104/2000, Training Loss (NLML): -960.2404, (RMSE): 0.0110\n",
      "branching dfNGP Run 1/1, Epoch 105/2000, Training Loss (NLML): -963.0509, (RMSE): 0.0109\n",
      "branching dfNGP Run 1/1, Epoch 106/2000, Training Loss (NLML): -965.8667, (RMSE): 0.0109\n",
      "branching dfNGP Run 1/1, Epoch 107/2000, Training Loss (NLML): -968.5406, (RMSE): 0.0109\n",
      "branching dfNGP Run 1/1, Epoch 108/2000, Training Loss (NLML): -970.7960, (RMSE): 0.0108\n",
      "branching dfNGP Run 1/1, Epoch 109/2000, Training Loss (NLML): -972.3638, (RMSE): 0.0107\n",
      "branching dfNGP Run 1/1, Epoch 110/2000, Training Loss (NLML): -973.6344, (RMSE): 0.0104\n",
      "branching dfNGP Run 1/1, Epoch 111/2000, Training Loss (NLML): -975.1005, (RMSE): 0.0100\n",
      "branching dfNGP Run 1/1, Epoch 112/2000, Training Loss (NLML): -976.1078, (RMSE): 0.0095\n",
      "branching dfNGP Run 1/1, Epoch 113/2000, Training Loss (NLML): -975.9810, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 114/2000, Training Loss (NLML): -975.7396, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 115/2000, Training Loss (NLML): -976.2358, (RMSE): 0.0089\n",
      "branching dfNGP Run 1/1, Epoch 116/2000, Training Loss (NLML): -977.0027, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 117/2000, Training Loss (NLML): -977.5197, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 118/2000, Training Loss (NLML): -979.1471, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 119/2000, Training Loss (NLML): -980.9203, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 120/2000, Training Loss (NLML): -982.3848, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 121/2000, Training Loss (NLML): -984.0785, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 122/2000, Training Loss (NLML): -985.6764, (RMSE): 0.0092\n",
      "branching dfNGP Run 1/1, Epoch 123/2000, Training Loss (NLML): -986.7458, (RMSE): 0.0094\n",
      "branching dfNGP Run 1/1, Epoch 124/2000, Training Loss (NLML): -987.7355, (RMSE): 0.0095\n",
      "branching dfNGP Run 1/1, Epoch 125/2000, Training Loss (NLML): -988.8774, (RMSE): 0.0095\n",
      "branching dfNGP Run 1/1, Epoch 126/2000, Training Loss (NLML): -989.8219, (RMSE): 0.0095\n",
      "branching dfNGP Run 1/1, Epoch 127/2000, Training Loss (NLML): -990.7330, (RMSE): 0.0095\n",
      "branching dfNGP Run 1/1, Epoch 128/2000, Training Loss (NLML): -991.8893, (RMSE): 0.0095\n",
      "branching dfNGP Run 1/1, Epoch 129/2000, Training Loss (NLML): -993.2450, (RMSE): 0.0095\n",
      "branching dfNGP Run 1/1, Epoch 130/2000, Training Loss (NLML): -994.6654, (RMSE): 0.0096\n",
      "branching dfNGP Run 1/1, Epoch 131/2000, Training Loss (NLML): -996.2235, (RMSE): 0.0096\n",
      "branching dfNGP Run 1/1, Epoch 132/2000, Training Loss (NLML): -998.0952, (RMSE): 0.0094\n",
      "branching dfNGP Run 1/1, Epoch 133/2000, Training Loss (NLML): -1000.0338, (RMSE): 0.0093\n",
      "branching dfNGP Run 1/1, Epoch 134/2000, Training Loss (NLML): -1002.0103, (RMSE): 0.0092\n",
      "branching dfNGP Run 1/1, Epoch 135/2000, Training Loss (NLML): -1004.1400, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 136/2000, Training Loss (NLML): -1006.0205, (RMSE): 0.0092\n",
      "branching dfNGP Run 1/1, Epoch 137/2000, Training Loss (NLML): -1007.9403, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 138/2000, Training Loss (NLML): -1009.5878, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 139/2000, Training Loss (NLML): -1011.4113, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 140/2000, Training Loss (NLML): -1013.1497, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 141/2000, Training Loss (NLML): -1014.9556, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 142/2000, Training Loss (NLML): -1016.6823, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 143/2000, Training Loss (NLML): -1017.9561, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 144/2000, Training Loss (NLML): -1018.4833, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 145/2000, Training Loss (NLML): -1016.5164, (RMSE): 0.0089\n",
      "branching dfNGP Run 1/1, Epoch 146/2000, Training Loss (NLML): -1019.6635, (RMSE): 0.0089\n",
      "branching dfNGP Run 1/1, Epoch 147/2000, Training Loss (NLML): -1019.0568, (RMSE): 0.0092\n",
      "branching dfNGP Run 1/1, Epoch 148/2000, Training Loss (NLML): -1018.2725, (RMSE): 0.0089\n",
      "branching dfNGP Run 1/1, Epoch 149/2000, Training Loss (NLML): -1017.2101, (RMSE): 0.0088\n",
      "branching dfNGP Run 1/1, Epoch 150/2000, Training Loss (NLML): -1018.4181, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 151/2000, Training Loss (NLML): -1017.8093, (RMSE): 0.0092\n",
      "branching dfNGP Run 1/1, Epoch 152/2000, Training Loss (NLML): -1018.2982, (RMSE): 0.0089\n",
      "branching dfNGP Run 1/1, Epoch 153/2000, Training Loss (NLML): -1018.7347, (RMSE): 0.0089\n",
      "branching dfNGP Run 1/1, Epoch 154/2000, Training Loss (NLML): -1019.1711, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 155/2000, Training Loss (NLML): -1018.9169, (RMSE): 0.0092\n",
      "branching dfNGP Run 1/1, Epoch 156/2000, Training Loss (NLML): -1019.8646, (RMSE): 0.0089\n",
      "branching dfNGP Run 1/1, Epoch 157/2000, Training Loss (NLML): -1019.6371, (RMSE): 0.0089\n",
      "branching dfNGP Run 1/1, Epoch 158/2000, Training Loss (NLML): -1021.2352, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 159/2000, Training Loss (NLML): -1020.0385, (RMSE): 0.0092\n",
      "branching dfNGP Run 1/1, Epoch 160/2000, Training Loss (NLML): -1021.6780, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 161/2000, Training Loss (NLML): -1021.1056, (RMSE): 0.0089\n",
      "branching dfNGP Run 1/1, Epoch 162/2000, Training Loss (NLML): -1022.1199, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 163/2000, Training Loss (NLML): -1022.0137, (RMSE): 0.0092\n",
      "branching dfNGP Run 1/1, Epoch 164/2000, Training Loss (NLML): -1022.3763, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 165/2000, Training Loss (NLML): -1022.6024, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 166/2000, Training Loss (NLML): -1022.5055, (RMSE): 0.0089\n",
      "branching dfNGP Run 1/1, Epoch 167/2000, Training Loss (NLML): -1022.9579, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 168/2000, Training Loss (NLML): -1022.5237, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 169/2000, Training Loss (NLML): -1023.0492, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 170/2000, Training Loss (NLML): -1022.7891, (RMSE): 0.0089\n",
      "branching dfNGP Run 1/1, Epoch 171/2000, Training Loss (NLML): -1023.4076, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 172/2000, Training Loss (NLML): -1023.1049, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 173/2000, Training Loss (NLML): -1023.6849, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 174/2000, Training Loss (NLML): -1023.4723, (RMSE): 0.0089\n",
      "branching dfNGP Run 1/1, Epoch 175/2000, Training Loss (NLML): -1024.0706, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 176/2000, Training Loss (NLML): -1023.8938, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 177/2000, Training Loss (NLML): -1024.4332, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 178/2000, Training Loss (NLML): -1024.3308, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 179/2000, Training Loss (NLML): -1024.7272, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 180/2000, Training Loss (NLML): -1024.6565, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 181/2000, Training Loss (NLML): -1025.0110, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 182/2000, Training Loss (NLML): -1024.9799, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 183/2000, Training Loss (NLML): -1025.2466, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 184/2000, Training Loss (NLML): -1025.2637, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 185/2000, Training Loss (NLML): -1025.5138, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 186/2000, Training Loss (NLML): -1025.5746, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 187/2000, Training Loss (NLML): -1025.8108, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 188/2000, Training Loss (NLML): -1025.8833, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 189/2000, Training Loss (NLML): -1026.1083, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 190/2000, Training Loss (NLML): -1026.2002, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 191/2000, Training Loss (NLML): -1026.4232, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 192/2000, Training Loss (NLML): -1026.5366, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 193/2000, Training Loss (NLML): -1026.7644, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 194/2000, Training Loss (NLML): -1026.8734, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 195/2000, Training Loss (NLML): -1027.0833, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 196/2000, Training Loss (NLML): -1027.1930, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 197/2000, Training Loss (NLML): -1027.3850, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 198/2000, Training Loss (NLML): -1027.4907, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 199/2000, Training Loss (NLML): -1027.6631, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 200/2000, Training Loss (NLML): -1027.7714, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 201/2000, Training Loss (NLML): -1027.9313, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 202/2000, Training Loss (NLML): -1028.1310, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 203/2000, Training Loss (NLML): -1028.3423, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 204/2000, Training Loss (NLML): -1028.4773, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 205/2000, Training Loss (NLML): -1028.5817, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 206/2000, Training Loss (NLML): -1028.7772, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 207/2000, Training Loss (NLML): -1028.9758, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 208/2000, Training Loss (NLML): -1029.1517, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 209/2000, Training Loss (NLML): -1029.2664, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 210/2000, Training Loss (NLML): -1029.4312, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 211/2000, Training Loss (NLML): -1029.6091, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 212/2000, Training Loss (NLML): -1029.8048, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 213/2000, Training Loss (NLML): -1029.9717, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 214/2000, Training Loss (NLML): -1030.1150, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 215/2000, Training Loss (NLML): -1030.2670, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 216/2000, Training Loss (NLML): -1030.4235, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 217/2000, Training Loss (NLML): -1030.6193, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 218/2000, Training Loss (NLML): -1030.8077, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 219/2000, Training Loss (NLML): -1030.9888, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 220/2000, Training Loss (NLML): -1031.1583, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 221/2000, Training Loss (NLML): -1031.3164, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 222/2000, Training Loss (NLML): -1031.4771, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 223/2000, Training Loss (NLML): -1031.6318, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 224/2000, Training Loss (NLML): -1031.7999, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 225/2000, Training Loss (NLML): -1031.9769, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 226/2000, Training Loss (NLML): -1032.1617, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 227/2000, Training Loss (NLML): -1032.3650, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 228/2000, Training Loss (NLML): -1032.5603, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 229/2000, Training Loss (NLML): -1032.7474, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 230/2000, Training Loss (NLML): -1032.9244, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 231/2000, Training Loss (NLML): -1033.0939, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 232/2000, Training Loss (NLML): -1033.2545, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 233/2000, Training Loss (NLML): -1033.3917, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 234/2000, Training Loss (NLML): -1033.4927, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 235/2000, Training Loss (NLML): -1033.5370, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 236/2000, Training Loss (NLML): -1033.6591, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 237/2000, Training Loss (NLML): -1033.9471, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 238/2000, Training Loss (NLML): -1034.2618, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 239/2000, Training Loss (NLML): -1034.2947, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 240/2000, Training Loss (NLML): -1034.2758, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 241/2000, Training Loss (NLML): -1034.1440, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 242/2000, Training Loss (NLML): -1034.1893, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 243/2000, Training Loss (NLML): -1033.1135, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 244/2000, Training Loss (NLML): -1034.0238, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 245/2000, Training Loss (NLML): -1034.2368, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 246/2000, Training Loss (NLML): -1035.3000, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 247/2000, Training Loss (NLML): -1035.5729, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 248/2000, Training Loss (NLML): -1035.3757, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 249/2000, Training Loss (NLML): -1035.2889, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 250/2000, Training Loss (NLML): -1034.8400, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 251/2000, Training Loss (NLML): -1035.8207, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 252/2000, Training Loss (NLML): -1036.2733, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 253/2000, Training Loss (NLML): -1036.4830, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 254/2000, Training Loss (NLML): -1036.2455, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 255/2000, Training Loss (NLML): -1035.8118, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 256/2000, Training Loss (NLML): -1036.2386, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 257/2000, Training Loss (NLML): -1036.1212, (RMSE): 0.0089\n",
      "branching dfNGP Run 1/1, Epoch 258/2000, Training Loss (NLML): -1037.2064, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 259/2000, Training Loss (NLML): -1037.0089, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 260/2000, Training Loss (NLML): -1036.5380, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 261/2000, Training Loss (NLML): -1036.6169, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 262/2000, Training Loss (NLML): -1036.5902, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 263/2000, Training Loss (NLML): -1037.8488, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 264/2000, Training Loss (NLML): -1036.7792, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 265/2000, Training Loss (NLML): -1036.7631, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 266/2000, Training Loss (NLML): -1036.2247, (RMSE): 0.0092\n",
      "branching dfNGP Run 1/1, Epoch 267/2000, Training Loss (NLML): -1034.4537, (RMSE): 0.0089\n",
      "branching dfNGP Run 1/1, Epoch 268/2000, Training Loss (NLML): -1037.2421, (RMSE): 0.0089\n",
      "branching dfNGP Run 1/1, Epoch 269/2000, Training Loss (NLML): -1032.7966, (RMSE): 0.0094\n",
      "branching dfNGP Run 1/1, Epoch 270/2000, Training Loss (NLML): -1031.7728, (RMSE): 0.0089\n",
      "branching dfNGP Run 1/1, Epoch 271/2000, Training Loss (NLML): -1031.6173, (RMSE): 0.0088\n",
      "branching dfNGP Run 1/1, Epoch 272/2000, Training Loss (NLML): -1038.3212, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 273/2000, Training Loss (NLML): -1031.2826, (RMSE): 0.0096\n",
      "branching dfNGP Run 1/1, Epoch 274/2000, Training Loss (NLML): -1037.0072, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 275/2000, Training Loss (NLML): -1031.3536, (RMSE): 0.0088\n",
      "branching dfNGP Run 1/1, Epoch 276/2000, Training Loss (NLML): -1035.7279, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 277/2000, Training Loss (NLML): -1036.0037, (RMSE): 0.0094\n",
      "branching dfNGP Run 1/1, Epoch 278/2000, Training Loss (NLML): -1036.7136, (RMSE): 0.0092\n",
      "branching dfNGP Run 1/1, Epoch 279/2000, Training Loss (NLML): -1036.3784, (RMSE): 0.0089\n",
      "branching dfNGP Run 1/1, Epoch 280/2000, Training Loss (NLML): -1035.3147, (RMSE): 0.0089\n",
      "branching dfNGP Run 1/1, Epoch 281/2000, Training Loss (NLML): -1039.2278, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 282/2000, Training Loss (NLML): -1034.8560, (RMSE): 0.0094\n",
      "branching dfNGP Run 1/1, Epoch 283/2000, Training Loss (NLML): -1035.7986, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 284/2000, Training Loss (NLML): -1035.2174, (RMSE): 0.0089\n",
      "branching dfNGP Run 1/1, Epoch 285/2000, Training Loss (NLML): -1038.9114, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 286/2000, Training Loss (NLML): -1035.6063, (RMSE): 0.0094\n",
      "branching dfNGP Run 1/1, Epoch 287/2000, Training Loss (NLML): -1037.6927, (RMSE): 0.0092\n",
      "branching dfNGP Run 1/1, Epoch 288/2000, Training Loss (NLML): -1037.4230, (RMSE): 0.0089\n",
      "branching dfNGP Run 1/1, Epoch 289/2000, Training Loss (NLML): -1038.4418, (RMSE): 0.0089\n",
      "branching dfNGP Run 1/1, Epoch 290/2000, Training Loss (NLML): -1039.1654, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 291/2000, Training Loss (NLML): -1037.8796, (RMSE): 0.0093\n",
      "branching dfNGP Run 1/1, Epoch 292/2000, Training Loss (NLML): -1038.4070, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 293/2000, Training Loss (NLML): -1037.0709, (RMSE): 0.0089\n",
      "branching dfNGP Run 1/1, Epoch 294/2000, Training Loss (NLML): -1039.8118, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 295/2000, Training Loss (NLML): -1035.9442, (RMSE): 0.0093\n",
      "branching dfNGP Run 1/1, Epoch 296/2000, Training Loss (NLML): -1039.1488, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 297/2000, Training Loss (NLML): -1038.2571, (RMSE): 0.0089\n",
      "branching dfNGP Run 1/1, Epoch 298/2000, Training Loss (NLML): -1039.5020, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 299/2000, Training Loss (NLML): -1039.0538, (RMSE): 0.0092\n",
      "branching dfNGP Run 1/1, Epoch 300/2000, Training Loss (NLML): -1039.1439, (RMSE): 0.0092\n",
      "branching dfNGP Run 1/1, Epoch 301/2000, Training Loss (NLML): -1040.1190, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 302/2000, Training Loss (NLML): -1039.3464, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 303/2000, Training Loss (NLML): -1040.2760, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 304/2000, Training Loss (NLML): -1039.9556, (RMSE): 0.0092\n",
      "branching dfNGP Run 1/1, Epoch 305/2000, Training Loss (NLML): -1040.7952, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 306/2000, Training Loss (NLML): -1040.2335, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 307/2000, Training Loss (NLML): -1040.6714, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 308/2000, Training Loss (NLML): -1040.5875, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 309/2000, Training Loss (NLML): -1041.1179, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 310/2000, Training Loss (NLML): -1040.8016, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 311/2000, Training Loss (NLML): -1041.0553, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 312/2000, Training Loss (NLML): -1041.0813, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 313/2000, Training Loss (NLML): -1041.1740, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 314/2000, Training Loss (NLML): -1041.2831, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 315/2000, Training Loss (NLML): -1041.1830, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 316/2000, Training Loss (NLML): -1041.4149, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 317/2000, Training Loss (NLML): -1041.3357, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 318/2000, Training Loss (NLML): -1041.5140, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 319/2000, Training Loss (NLML): -1041.4430, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 320/2000, Training Loss (NLML): -1041.5483, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 321/2000, Training Loss (NLML): -1041.5508, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 322/2000, Training Loss (NLML): -1041.6398, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 323/2000, Training Loss (NLML): -1041.6288, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 324/2000, Training Loss (NLML): -1041.6886, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 325/2000, Training Loss (NLML): -1041.7291, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 326/2000, Training Loss (NLML): -1041.7533, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 327/2000, Training Loss (NLML): -1041.8033, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 328/2000, Training Loss (NLML): -1041.7919, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 329/2000, Training Loss (NLML): -1041.8647, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 330/2000, Training Loss (NLML): -1041.8572, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 331/2000, Training Loss (NLML): -1041.9139, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 332/2000, Training Loss (NLML): -1041.9208, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 333/2000, Training Loss (NLML): -1041.9583, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 334/2000, Training Loss (NLML): -1041.9734, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 335/2000, Training Loss (NLML): -1042.0118, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 336/2000, Training Loss (NLML): -1042.0156, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 337/2000, Training Loss (NLML): -1042.0605, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 338/2000, Training Loss (NLML): -1042.0592, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 339/2000, Training Loss (NLML): -1042.1062, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 340/2000, Training Loss (NLML): -1042.1053, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 341/2000, Training Loss (NLML): -1042.1449, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 342/2000, Training Loss (NLML): -1042.1472, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 343/2000, Training Loss (NLML): -1042.1836, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 344/2000, Training Loss (NLML): -1042.1881, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 345/2000, Training Loss (NLML): -1042.2220, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 346/2000, Training Loss (NLML): -1042.2289, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 347/2000, Training Loss (NLML): -1042.2561, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 348/2000, Training Loss (NLML): -1042.2687, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 349/2000, Training Loss (NLML): -1042.2882, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 350/2000, Training Loss (NLML): -1042.3047, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 351/2000, Training Loss (NLML): -1042.3228, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 352/2000, Training Loss (NLML): -1042.3386, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 353/2000, Training Loss (NLML): -1042.3579, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 354/2000, Training Loss (NLML): -1042.3704, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 355/2000, Training Loss (NLML): -1042.3905, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 356/2000, Training Loss (NLML): -1042.4020, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 357/2000, Training Loss (NLML): -1042.4225, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 358/2000, Training Loss (NLML): -1042.4326, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 359/2000, Training Loss (NLML): -1042.4529, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 360/2000, Training Loss (NLML): -1042.4631, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 361/2000, Training Loss (NLML): -1042.4814, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 362/2000, Training Loss (NLML): -1042.4930, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 363/2000, Training Loss (NLML): -1042.5096, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 364/2000, Training Loss (NLML): -1042.5221, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 365/2000, Training Loss (NLML): -1042.5370, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 366/2000, Training Loss (NLML): -1042.5500, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 367/2000, Training Loss (NLML): -1042.5652, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 368/2000, Training Loss (NLML): -1042.5762, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 369/2000, Training Loss (NLML): -1042.5916, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 370/2000, Training Loss (NLML): -1042.6025, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 371/2000, Training Loss (NLML): -1042.6168, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 372/2000, Training Loss (NLML): -1042.6283, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 373/2000, Training Loss (NLML): -1042.6421, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 374/2000, Training Loss (NLML): -1042.6533, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 375/2000, Training Loss (NLML): -1042.6661, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 376/2000, Training Loss (NLML): -1042.6779, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 377/2000, Training Loss (NLML): -1042.6899, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 378/2000, Training Loss (NLML): -1042.7015, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 379/2000, Training Loss (NLML): -1042.7134, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 380/2000, Training Loss (NLML): -1042.7245, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 381/2000, Training Loss (NLML): -1042.7365, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 382/2000, Training Loss (NLML): -1042.7474, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 383/2000, Training Loss (NLML): -1042.7587, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 384/2000, Training Loss (NLML): -1042.7694, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 385/2000, Training Loss (NLML): -1042.7800, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 386/2000, Training Loss (NLML): -1042.7909, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 387/2000, Training Loss (NLML): -1042.8015, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 388/2000, Training Loss (NLML): -1042.8121, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 389/2000, Training Loss (NLML): -1042.8223, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 390/2000, Training Loss (NLML): -1042.8322, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 391/2000, Training Loss (NLML): -1042.8424, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 392/2000, Training Loss (NLML): -1042.8530, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 393/2000, Training Loss (NLML): -1042.8622, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 394/2000, Training Loss (NLML): -1042.8724, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 395/2000, Training Loss (NLML): -1042.8820, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 396/2000, Training Loss (NLML): -1042.8920, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 397/2000, Training Loss (NLML): -1042.9012, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 398/2000, Training Loss (NLML): -1042.9105, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 399/2000, Training Loss (NLML): -1042.9198, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 400/2000, Training Loss (NLML): -1042.9299, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 401/2000, Training Loss (NLML): -1042.9386, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 402/2000, Training Loss (NLML): -1042.9475, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 403/2000, Training Loss (NLML): -1042.9562, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 404/2000, Training Loss (NLML): -1042.9651, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 405/2000, Training Loss (NLML): -1042.9736, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 406/2000, Training Loss (NLML): -1042.9828, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 407/2000, Training Loss (NLML): -1042.9913, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 408/2000, Training Loss (NLML): -1042.9999, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 409/2000, Training Loss (NLML): -1043.0082, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 410/2000, Training Loss (NLML): -1043.0167, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 411/2000, Training Loss (NLML): -1043.0254, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 412/2000, Training Loss (NLML): -1043.0330, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 413/2000, Training Loss (NLML): -1043.0414, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 414/2000, Training Loss (NLML): -1043.0500, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 415/2000, Training Loss (NLML): -1043.0581, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 416/2000, Training Loss (NLML): -1043.0657, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 417/2000, Training Loss (NLML): -1043.0739, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 418/2000, Training Loss (NLML): -1043.0819, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 419/2000, Training Loss (NLML): -1043.0896, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 420/2000, Training Loss (NLML): -1043.0973, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 421/2000, Training Loss (NLML): -1043.1046, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 422/2000, Training Loss (NLML): -1043.1124, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 423/2000, Training Loss (NLML): -1043.1201, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 424/2000, Training Loss (NLML): -1043.1277, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 425/2000, Training Loss (NLML): -1043.1349, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 426/2000, Training Loss (NLML): -1043.1418, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 427/2000, Training Loss (NLML): -1043.1495, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 428/2000, Training Loss (NLML): -1043.1567, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 429/2000, Training Loss (NLML): -1043.1643, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 430/2000, Training Loss (NLML): -1043.1713, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 431/2000, Training Loss (NLML): -1043.1786, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 432/2000, Training Loss (NLML): -1043.1852, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 433/2000, Training Loss (NLML): -1043.1930, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 434/2000, Training Loss (NLML): -1043.1997, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 435/2000, Training Loss (NLML): -1043.2065, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 436/2000, Training Loss (NLML): -1043.2136, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 437/2000, Training Loss (NLML): -1043.2200, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 438/2000, Training Loss (NLML): -1043.2272, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 439/2000, Training Loss (NLML): -1043.2340, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 440/2000, Training Loss (NLML): -1043.2410, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 441/2000, Training Loss (NLML): -1043.2476, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 442/2000, Training Loss (NLML): -1043.2539, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 443/2000, Training Loss (NLML): -1043.2605, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 444/2000, Training Loss (NLML): -1043.2675, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 445/2000, Training Loss (NLML): -1043.2736, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 446/2000, Training Loss (NLML): -1043.2799, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 447/2000, Training Loss (NLML): -1043.2865, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 448/2000, Training Loss (NLML): -1043.2933, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 449/2000, Training Loss (NLML): -1043.2992, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 450/2000, Training Loss (NLML): -1043.3055, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 451/2000, Training Loss (NLML): -1043.3124, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 452/2000, Training Loss (NLML): -1043.3186, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 453/2000, Training Loss (NLML): -1043.3250, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 454/2000, Training Loss (NLML): -1043.3312, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 455/2000, Training Loss (NLML): -1043.3372, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 456/2000, Training Loss (NLML): -1043.3438, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 457/2000, Training Loss (NLML): -1043.3495, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 458/2000, Training Loss (NLML): -1043.3558, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 459/2000, Training Loss (NLML): -1043.3618, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 460/2000, Training Loss (NLML): -1043.3678, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 461/2000, Training Loss (NLML): -1043.3739, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 462/2000, Training Loss (NLML): -1043.3800, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 463/2000, Training Loss (NLML): -1043.3865, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 464/2000, Training Loss (NLML): -1043.3922, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 465/2000, Training Loss (NLML): -1043.3978, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 466/2000, Training Loss (NLML): -1043.4042, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 467/2000, Training Loss (NLML): -1043.4098, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 468/2000, Training Loss (NLML): -1043.4155, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 469/2000, Training Loss (NLML): -1043.4218, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 470/2000, Training Loss (NLML): -1043.4277, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 471/2000, Training Loss (NLML): -1043.4330, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 472/2000, Training Loss (NLML): -1043.4387, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 473/2000, Training Loss (NLML): -1043.4447, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 474/2000, Training Loss (NLML): -1043.4507, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 475/2000, Training Loss (NLML): -1043.4564, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 476/2000, Training Loss (NLML): -1043.4619, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 477/2000, Training Loss (NLML): -1043.4677, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 478/2000, Training Loss (NLML): -1043.4734, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 479/2000, Training Loss (NLML): -1043.4794, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 480/2000, Training Loss (NLML): -1043.4844, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 481/2000, Training Loss (NLML): -1043.4906, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 482/2000, Training Loss (NLML): -1043.4961, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 483/2000, Training Loss (NLML): -1043.5020, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 484/2000, Training Loss (NLML): -1043.5076, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 485/2000, Training Loss (NLML): -1043.5129, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 486/2000, Training Loss (NLML): -1043.5184, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 487/2000, Training Loss (NLML): -1043.5238, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 488/2000, Training Loss (NLML): -1043.5297, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 489/2000, Training Loss (NLML): -1043.5348, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 490/2000, Training Loss (NLML): -1043.5406, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 491/2000, Training Loss (NLML): -1043.5460, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 492/2000, Training Loss (NLML): -1043.5514, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 493/2000, Training Loss (NLML): -1043.5570, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 494/2000, Training Loss (NLML): -1043.5624, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 495/2000, Training Loss (NLML): -1043.5680, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 496/2000, Training Loss (NLML): -1043.5735, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 497/2000, Training Loss (NLML): -1043.5789, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 498/2000, Training Loss (NLML): -1043.5846, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 499/2000, Training Loss (NLML): -1043.5897, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 500/2000, Training Loss (NLML): -1043.5951, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 501/2000, Training Loss (NLML): -1043.6006, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 502/2000, Training Loss (NLML): -1043.6057, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 503/2000, Training Loss (NLML): -1043.6112, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 504/2000, Training Loss (NLML): -1043.6166, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 505/2000, Training Loss (NLML): -1043.6222, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 506/2000, Training Loss (NLML): -1043.6274, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 507/2000, Training Loss (NLML): -1043.6331, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 508/2000, Training Loss (NLML): -1043.6382, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 509/2000, Training Loss (NLML): -1043.6432, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 510/2000, Training Loss (NLML): -1043.6490, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 511/2000, Training Loss (NLML): -1043.6544, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 512/2000, Training Loss (NLML): -1043.6598, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 513/2000, Training Loss (NLML): -1043.6652, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 514/2000, Training Loss (NLML): -1043.6702, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 515/2000, Training Loss (NLML): -1043.6761, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 516/2000, Training Loss (NLML): -1043.6804, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 517/2000, Training Loss (NLML): -1043.6864, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 518/2000, Training Loss (NLML): -1043.6913, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 519/2000, Training Loss (NLML): -1043.6970, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 520/2000, Training Loss (NLML): -1043.7024, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 521/2000, Training Loss (NLML): -1043.7075, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 522/2000, Training Loss (NLML): -1043.7128, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 523/2000, Training Loss (NLML): -1043.7184, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 524/2000, Training Loss (NLML): -1043.7238, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 525/2000, Training Loss (NLML): -1043.7289, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 526/2000, Training Loss (NLML): -1043.7343, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 527/2000, Training Loss (NLML): -1043.7396, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 528/2000, Training Loss (NLML): -1043.7448, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 529/2000, Training Loss (NLML): -1043.7496, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 530/2000, Training Loss (NLML): -1043.7550, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 531/2000, Training Loss (NLML): -1043.7607, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 532/2000, Training Loss (NLML): -1043.7656, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 533/2000, Training Loss (NLML): -1043.7711, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 534/2000, Training Loss (NLML): -1043.7762, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 535/2000, Training Loss (NLML): -1043.7820, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 536/2000, Training Loss (NLML): -1043.7874, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 537/2000, Training Loss (NLML): -1043.7925, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 538/2000, Training Loss (NLML): -1043.7979, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 539/2000, Training Loss (NLML): -1043.8033, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 540/2000, Training Loss (NLML): -1043.8083, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 541/2000, Training Loss (NLML): -1043.8137, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 542/2000, Training Loss (NLML): -1043.8196, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 543/2000, Training Loss (NLML): -1043.8246, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 544/2000, Training Loss (NLML): -1043.8302, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 545/2000, Training Loss (NLML): -1043.8352, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 546/2000, Training Loss (NLML): -1043.8401, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 547/2000, Training Loss (NLML): -1043.8462, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 548/2000, Training Loss (NLML): -1043.8516, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 549/2000, Training Loss (NLML): -1043.8563, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 550/2000, Training Loss (NLML): -1043.8619, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 551/2000, Training Loss (NLML): -1043.8672, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 552/2000, Training Loss (NLML): -1043.8730, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 553/2000, Training Loss (NLML): -1043.8781, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 554/2000, Training Loss (NLML): -1043.8833, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 555/2000, Training Loss (NLML): -1043.8888, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 556/2000, Training Loss (NLML): -1043.8945, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 557/2000, Training Loss (NLML): -1043.8999, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 558/2000, Training Loss (NLML): -1043.9054, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 559/2000, Training Loss (NLML): -1043.9105, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 560/2000, Training Loss (NLML): -1043.9155, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 561/2000, Training Loss (NLML): -1043.9211, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 562/2000, Training Loss (NLML): -1043.9264, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 563/2000, Training Loss (NLML): -1043.9324, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 564/2000, Training Loss (NLML): -1043.9376, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 565/2000, Training Loss (NLML): -1043.9426, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 566/2000, Training Loss (NLML): -1043.9487, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 567/2000, Training Loss (NLML): -1043.9537, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 568/2000, Training Loss (NLML): -1043.9594, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 569/2000, Training Loss (NLML): -1043.9652, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 570/2000, Training Loss (NLML): -1043.9708, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 571/2000, Training Loss (NLML): -1043.9761, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 572/2000, Training Loss (NLML): -1043.9817, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 573/2000, Training Loss (NLML): -1043.9871, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 574/2000, Training Loss (NLML): -1043.9927, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 575/2000, Training Loss (NLML): -1043.9980, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 576/2000, Training Loss (NLML): -1044.0038, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 577/2000, Training Loss (NLML): -1044.0092, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 578/2000, Training Loss (NLML): -1044.0150, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 579/2000, Training Loss (NLML): -1044.0201, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 580/2000, Training Loss (NLML): -1044.0260, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 581/2000, Training Loss (NLML): -1044.0314, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 582/2000, Training Loss (NLML): -1044.0366, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 583/2000, Training Loss (NLML): -1044.0428, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 584/2000, Training Loss (NLML): -1044.0482, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 585/2000, Training Loss (NLML): -1044.0541, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 586/2000, Training Loss (NLML): -1044.0596, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 587/2000, Training Loss (NLML): -1044.0658, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 588/2000, Training Loss (NLML): -1044.0710, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 589/2000, Training Loss (NLML): -1044.0768, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 590/2000, Training Loss (NLML): -1044.0825, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 591/2000, Training Loss (NLML): -1044.0886, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 592/2000, Training Loss (NLML): -1044.0940, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 593/2000, Training Loss (NLML): -1044.1000, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 594/2000, Training Loss (NLML): -1044.1049, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 595/2000, Training Loss (NLML): -1044.1110, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 596/2000, Training Loss (NLML): -1044.1173, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 597/2000, Training Loss (NLML): -1044.1234, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 598/2000, Training Loss (NLML): -1044.1285, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 599/2000, Training Loss (NLML): -1044.1346, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 600/2000, Training Loss (NLML): -1044.1410, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 601/2000, Training Loss (NLML): -1044.1465, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 602/2000, Training Loss (NLML): -1044.1527, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 603/2000, Training Loss (NLML): -1044.1583, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 604/2000, Training Loss (NLML): -1044.1643, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 605/2000, Training Loss (NLML): -1044.1704, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 606/2000, Training Loss (NLML): -1044.1764, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 607/2000, Training Loss (NLML): -1044.1815, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 608/2000, Training Loss (NLML): -1044.1881, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 609/2000, Training Loss (NLML): -1044.1932, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 610/2000, Training Loss (NLML): -1044.1991, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 611/2000, Training Loss (NLML): -1044.2037, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 612/2000, Training Loss (NLML): -1044.2078, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 613/2000, Training Loss (NLML): -1044.2102, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 614/2000, Training Loss (NLML): -1044.2106, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 615/2000, Training Loss (NLML): -1044.2039, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 616/2000, Training Loss (NLML): -1044.1913, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 617/2000, Training Loss (NLML): -1044.1533, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 618/2000, Training Loss (NLML): -1044.0989, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 619/2000, Training Loss (NLML): -1043.9440, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 620/2000, Training Loss (NLML): -1043.7905, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 621/2000, Training Loss (NLML): -1043.2482, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 622/2000, Training Loss (NLML): -1043.1069, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 623/2000, Training Loss (NLML): -1041.9442, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 624/2000, Training Loss (NLML): -1042.8031, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 625/2000, Training Loss (NLML): -1042.6003, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 626/2000, Training Loss (NLML): -1043.7159, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 627/2000, Training Loss (NLML): -1044.1785, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 628/2000, Training Loss (NLML): -1044.2780, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 629/2000, Training Loss (NLML): -1044.0426, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 630/2000, Training Loss (NLML): -1043.5117, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 631/2000, Training Loss (NLML): -1043.4149, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 632/2000, Training Loss (NLML): -1042.8352, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 633/2000, Training Loss (NLML): -1043.4742, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 634/2000, Training Loss (NLML): -1043.6228, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 635/2000, Training Loss (NLML): -1044.1151, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 636/2000, Training Loss (NLML): -1044.3285, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 637/2000, Training Loss (NLML): -1044.3470, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 638/2000, Training Loss (NLML): -1044.2179, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 639/2000, Training Loss (NLML): -1043.9832, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 640/2000, Training Loss (NLML): -1043.9464, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 641/2000, Training Loss (NLML): -1043.8179, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 642/2000, Training Loss (NLML): -1044.0475, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 643/2000, Training Loss (NLML): -1044.1702, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 644/2000, Training Loss (NLML): -1044.3416, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 645/2000, Training Loss (NLML): -1044.4177, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 646/2000, Training Loss (NLML): -1044.4130, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 647/2000, Training Loss (NLML): -1044.3563, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 648/2000, Training Loss (NLML): -1044.2665, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 649/2000, Training Loss (NLML): -1044.2397, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 650/2000, Training Loss (NLML): -1044.1846, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 651/2000, Training Loss (NLML): -1044.2526, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 652/2000, Training Loss (NLML): -1044.2817, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 653/2000, Training Loss (NLML): -1044.3707, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 654/2000, Training Loss (NLML): -1044.4288, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 655/2000, Training Loss (NLML): -1044.4771, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 656/2000, Training Loss (NLML): -1044.4985, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 657/2000, Training Loss (NLML): -1044.4989, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 658/2000, Training Loss (NLML): -1044.4865, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 659/2000, Training Loss (NLML): -1044.4658, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 660/2000, Training Loss (NLML): -1044.4556, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 661/2000, Training Loss (NLML): -1044.4401, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 662/2000, Training Loss (NLML): -1044.4502, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 663/2000, Training Loss (NLML): -1044.4500, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 664/2000, Training Loss (NLML): -1044.4751, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 665/2000, Training Loss (NLML): -1044.4873, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 666/2000, Training Loss (NLML): -1044.5150, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 667/2000, Training Loss (NLML): -1044.5333, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 668/2000, Training Loss (NLML): -1044.5560, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 669/2000, Training Loss (NLML): -1044.5734, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 670/2000, Training Loss (NLML): -1044.5905, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 671/2000, Training Loss (NLML): -1044.6033, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 672/2000, Training Loss (NLML): -1044.6149, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 673/2000, Training Loss (NLML): -1044.6240, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 674/2000, Training Loss (NLML): -1044.6316, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 675/2000, Training Loss (NLML): -1044.6381, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 676/2000, Training Loss (NLML): -1044.6442, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 677/2000, Training Loss (NLML): -1044.6493, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 678/2000, Training Loss (NLML): -1044.6533, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 679/2000, Training Loss (NLML): -1044.6569, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 680/2000, Training Loss (NLML): -1044.6583, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 681/2000, Training Loss (NLML): -1044.6594, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 682/2000, Training Loss (NLML): -1044.6552, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 683/2000, Training Loss (NLML): -1044.6505, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 684/2000, Training Loss (NLML): -1044.6328, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 685/2000, Training Loss (NLML): -1044.6194, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 686/2000, Training Loss (NLML): -1044.5736, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 687/2000, Training Loss (NLML): -1044.5461, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 688/2000, Training Loss (NLML): -1044.4421, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 689/2000, Training Loss (NLML): -1044.4073, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 690/2000, Training Loss (NLML): -1044.2083, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 691/2000, Training Loss (NLML): -1044.2208, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 692/2000, Training Loss (NLML): -1043.9507, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 693/2000, Training Loss (NLML): -1044.1202, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 694/2000, Training Loss (NLML): -1043.9645, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 695/2000, Training Loss (NLML): -1044.2770, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 696/2000, Training Loss (NLML): -1044.3528, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 697/2000, Training Loss (NLML): -1044.5933, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 698/2000, Training Loss (NLML): -1044.7144, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 699/2000, Training Loss (NLML): -1044.8033, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 700/2000, Training Loss (NLML): -1044.8373, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 701/2000, Training Loss (NLML): -1044.8304, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 702/2000, Training Loss (NLML): -1044.7968, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 703/2000, Training Loss (NLML): -1044.7235, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 704/2000, Training Loss (NLML): -1044.6503, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 705/2000, Training Loss (NLML): -1044.4606, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 706/2000, Training Loss (NLML): -1044.3885, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 707/2000, Training Loss (NLML): -1044.0403, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 708/2000, Training Loss (NLML): -1044.1661, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 709/2000, Training Loss (NLML): -1043.9331, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 710/2000, Training Loss (NLML): -1044.3475, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 711/2000, Training Loss (NLML): -1044.5022, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 712/2000, Training Loss (NLML): -1044.7825, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 713/2000, Training Loss (NLML): -1044.9032, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 714/2000, Training Loss (NLML): -1044.9095, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 715/2000, Training Loss (NLML): -1044.8333, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 716/2000, Training Loss (NLML): -1044.6669, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 717/2000, Training Loss (NLML): -1044.5748, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 718/2000, Training Loss (NLML): -1044.2809, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 719/2000, Training Loss (NLML): -1044.3407, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 720/2000, Training Loss (NLML): -1044.0583, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 721/2000, Training Loss (NLML): -1044.3700, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 722/2000, Training Loss (NLML): -1044.3922, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 723/2000, Training Loss (NLML): -1044.7332, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 724/2000, Training Loss (NLML): -1044.9098, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 725/2000, Training Loss (NLML): -1045.0127, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 726/2000, Training Loss (NLML): -1044.9989, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 727/2000, Training Loss (NLML): -1044.8944, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 728/2000, Training Loss (NLML): -1044.8090, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 729/2000, Training Loss (NLML): -1044.6373, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 730/2000, Training Loss (NLML): -1044.6771, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 731/2000, Training Loss (NLML): -1044.5641, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 732/2000, Training Loss (NLML): -1044.7209, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 733/2000, Training Loss (NLML): -1044.7391, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 734/2000, Training Loss (NLML): -1044.9130, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 735/2000, Training Loss (NLML): -1045.0117, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 736/2000, Training Loss (NLML): -1045.1095, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 737/2000, Training Loss (NLML): -1045.1553, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 738/2000, Training Loss (NLML): -1045.1576, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 739/2000, Training Loss (NLML): -1045.1304, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 740/2000, Training Loss (NLML): -1045.0780, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 741/2000, Training Loss (NLML): -1045.0452, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 742/2000, Training Loss (NLML): -1044.9751, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 743/2000, Training Loss (NLML): -1044.9840, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 744/2000, Training Loss (NLML): -1044.9351, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 745/2000, Training Loss (NLML): -1044.9996, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 746/2000, Training Loss (NLML): -1045.0060, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 747/2000, Training Loss (NLML): -1045.0901, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 748/2000, Training Loss (NLML): -1045.1263, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 749/2000, Training Loss (NLML): -1045.1853, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 750/2000, Training Loss (NLML): -1045.2122, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 751/2000, Training Loss (NLML): -1045.2418, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 752/2000, Training Loss (NLML): -1045.2552, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 753/2000, Training Loss (NLML): -1045.2687, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 754/2000, Training Loss (NLML): -1045.2745, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 755/2000, Training Loss (NLML): -1045.2838, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 756/2000, Training Loss (NLML): -1045.2931, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 757/2000, Training Loss (NLML): -1045.3065, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 758/2000, Training Loss (NLML): -1045.3241, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 759/2000, Training Loss (NLML): -1045.3423, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 760/2000, Training Loss (NLML): -1045.3610, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 761/2000, Training Loss (NLML): -1045.3778, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 762/2000, Training Loss (NLML): -1045.3929, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 763/2000, Training Loss (NLML): -1045.4044, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 764/2000, Training Loss (NLML): -1045.4131, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 765/2000, Training Loss (NLML): -1045.4189, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 766/2000, Training Loss (NLML): -1045.4211, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 767/2000, Training Loss (NLML): -1045.4166, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 768/2000, Training Loss (NLML): -1045.4095, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 769/2000, Training Loss (NLML): -1045.3853, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 770/2000, Training Loss (NLML): -1045.3594, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 771/2000, Training Loss (NLML): -1045.2784, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 772/2000, Training Loss (NLML): -1045.2136, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 773/2000, Training Loss (NLML): -1044.9781, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 774/2000, Training Loss (NLML): -1044.8907, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 775/2000, Training Loss (NLML): -1044.3696, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 776/2000, Training Loss (NLML): -1044.5247, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 777/2000, Training Loss (NLML): -1044.0262, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 778/2000, Training Loss (NLML): -1044.6898, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 779/2000, Training Loss (NLML): -1044.8572, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 780/2000, Training Loss (NLML): -1045.3317, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 781/2000, Training Loss (NLML): -1045.5424, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 782/2000, Training Loss (NLML): -1045.5818, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 783/2000, Training Loss (NLML): -1045.4777, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 784/2000, Training Loss (NLML): -1045.2244, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 785/2000, Training Loss (NLML): -1045.0331, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 786/2000, Training Loss (NLML): -1044.4088, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 787/2000, Training Loss (NLML): -1044.5320, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 788/2000, Training Loss (NLML): -1043.8766, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 789/2000, Training Loss (NLML): -1044.7294, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 790/2000, Training Loss (NLML): -1045.0179, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 791/2000, Training Loss (NLML): -1045.5280, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 792/2000, Training Loss (NLML): -1045.6648, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 793/2000, Training Loss (NLML): -1045.5048, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 794/2000, Training Loss (NLML): -1045.2822, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 795/2000, Training Loss (NLML): -1044.8934, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 796/2000, Training Loss (NLML): -1045.0726, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 797/2000, Training Loss (NLML): -1044.9839, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 798/2000, Training Loss (NLML): -1045.3423, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 799/2000, Training Loss (NLML): -1045.4677, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 800/2000, Training Loss (NLML): -1045.6180, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 801/2000, Training Loss (NLML): -1045.6531, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 802/2000, Training Loss (NLML): -1045.6227, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 803/2000, Training Loss (NLML): -1045.5956, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 804/2000, Training Loss (NLML): -1045.4968, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 805/2000, Training Loss (NLML): -1045.5249, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 806/2000, Training Loss (NLML): -1045.4597, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 807/2000, Training Loss (NLML): -1045.5710, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 808/2000, Training Loss (NLML): -1045.6113, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 809/2000, Training Loss (NLML): -1045.7490, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 810/2000, Training Loss (NLML): -1045.8384, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 811/2000, Training Loss (NLML): -1045.8881, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 812/2000, Training Loss (NLML): -1045.8822, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 813/2000, Training Loss (NLML): -1045.8376, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 814/2000, Training Loss (NLML): -1045.8007, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 815/2000, Training Loss (NLML): -1045.7443, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 816/2000, Training Loss (NLML): -1045.7568, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 817/2000, Training Loss (NLML): -1045.7195, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 818/2000, Training Loss (NLML): -1045.7529, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 819/2000, Training Loss (NLML): -1045.7085, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 820/2000, Training Loss (NLML): -1045.7600, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 821/2000, Training Loss (NLML): -1045.7594, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 822/2000, Training Loss (NLML): -1045.8566, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 823/2000, Training Loss (NLML): -1045.9174, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 824/2000, Training Loss (NLML): -1045.9861, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 825/2000, Training Loss (NLML): -1046.0205, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 826/2000, Training Loss (NLML): -1046.0389, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 827/2000, Training Loss (NLML): -1046.0461, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 828/2000, Training Loss (NLML): -1046.0483, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 829/2000, Training Loss (NLML): -1046.0483, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 830/2000, Training Loss (NLML): -1046.0333, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 831/2000, Training Loss (NLML): -1046.0167, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 832/2000, Training Loss (NLML): -1045.9662, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 833/2000, Training Loss (NLML): -1045.9418, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 834/2000, Training Loss (NLML): -1045.8514, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 835/2000, Training Loss (NLML): -1045.8578, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 836/2000, Training Loss (NLML): -1045.7611, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 837/2000, Training Loss (NLML): -1045.8323, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 838/2000, Training Loss (NLML): -1045.7839, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 839/2000, Training Loss (NLML): -1045.9032, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 840/2000, Training Loss (NLML): -1045.9259, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 841/2000, Training Loss (NLML): -1046.0394, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 842/2000, Training Loss (NLML): -1046.0905, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 843/2000, Training Loss (NLML): -1046.1588, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 844/2000, Training Loss (NLML): -1046.1927, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 845/2000, Training Loss (NLML): -1046.2194, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 846/2000, Training Loss (NLML): -1046.2275, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 847/2000, Training Loss (NLML): -1046.2286, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 848/2000, Training Loss (NLML): -1046.2135, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 849/2000, Training Loss (NLML): -1046.1901, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 850/2000, Training Loss (NLML): -1046.1512, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 851/2000, Training Loss (NLML): -1046.1108, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 852/2000, Training Loss (NLML): -1046.0951, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 853/2000, Training Loss (NLML): -1046.1273, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 854/2000, Training Loss (NLML): -1046.2206, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 855/2000, Training Loss (NLML): -1046.3202, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 856/2000, Training Loss (NLML): -1046.3727, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 857/2000, Training Loss (NLML): -1046.3622, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 858/2000, Training Loss (NLML): -1046.3192, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 859/2000, Training Loss (NLML): -1046.2834, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 860/2000, Training Loss (NLML): -1046.2861, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 861/2000, Training Loss (NLML): -1046.3177, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 862/2000, Training Loss (NLML): -1046.3755, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 863/2000, Training Loss (NLML): -1046.4154, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 864/2000, Training Loss (NLML): -1046.4270, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 865/2000, Training Loss (NLML): -1046.3921, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 866/2000, Training Loss (NLML): -1046.3385, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 867/2000, Training Loss (NLML): -1046.2087, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 868/2000, Training Loss (NLML): -1046.1262, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 869/2000, Training Loss (NLML): -1045.7747, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 870/2000, Training Loss (NLML): -1045.6726, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 871/2000, Training Loss (NLML): -1044.8583, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 872/2000, Training Loss (NLML): -1045.2540, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 873/2000, Training Loss (NLML): -1044.7626, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 874/2000, Training Loss (NLML): -1045.7576, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 875/2000, Training Loss (NLML): -1046.1595, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 876/2000, Training Loss (NLML): -1046.5172, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 877/2000, Training Loss (NLML): -1046.5691, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 878/2000, Training Loss (NLML): -1046.3911, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 879/2000, Training Loss (NLML): -1046.1302, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 880/2000, Training Loss (NLML): -1045.4435, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 881/2000, Training Loss (NLML): -1045.3865, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 882/2000, Training Loss (NLML): -1044.2037, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 883/2000, Training Loss (NLML): -1045.3303, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 884/2000, Training Loss (NLML): -1045.4940, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 885/2000, Training Loss (NLML): -1046.3859, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 886/2000, Training Loss (NLML): -1046.6652, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 887/2000, Training Loss (NLML): -1046.4716, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 888/2000, Training Loss (NLML): -1046.1393, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 889/2000, Training Loss (NLML): -1045.5521, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 890/2000, Training Loss (NLML): -1045.9530, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 891/2000, Training Loss (NLML): -1046.0225, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 892/2000, Training Loss (NLML): -1046.4855, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 893/2000, Training Loss (NLML): -1046.6826, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 894/2000, Training Loss (NLML): -1046.7533, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 895/2000, Training Loss (NLML): -1046.6847, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 896/2000, Training Loss (NLML): -1046.5011, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 897/2000, Training Loss (NLML): -1046.4246, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 898/2000, Training Loss (NLML): -1046.2141, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 899/2000, Training Loss (NLML): -1046.3755, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 900/2000, Training Loss (NLML): -1046.3976, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 901/2000, Training Loss (NLML): -1046.6510, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 902/2000, Training Loss (NLML): -1046.7848, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 903/2000, Training Loss (NLML): -1046.8616, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 904/2000, Training Loss (NLML): -1046.8589, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 905/2000, Training Loss (NLML): -1046.8105, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 906/2000, Training Loss (NLML): -1046.7793, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 907/2000, Training Loss (NLML): -1046.7234, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 908/2000, Training Loss (NLML): -1046.7404, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 909/2000, Training Loss (NLML): -1046.6920, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 910/2000, Training Loss (NLML): -1046.7404, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 911/2000, Training Loss (NLML): -1046.7118, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 912/2000, Training Loss (NLML): -1046.7802, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 913/2000, Training Loss (NLML): -1046.7982, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 914/2000, Training Loss (NLML): -1046.8915, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 915/2000, Training Loss (NLML): -1046.9551, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 916/2000, Training Loss (NLML): -1047.0189, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 917/2000, Training Loss (NLML): -1047.0509, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 918/2000, Training Loss (NLML): -1047.0587, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 919/2000, Training Loss (NLML): -1047.0504, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 920/2000, Training Loss (NLML): -1047.0369, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 921/2000, Training Loss (NLML): -1047.0317, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 922/2000, Training Loss (NLML): -1047.0154, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 923/2000, Training Loss (NLML): -1047.0156, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 924/2000, Training Loss (NLML): -1046.9845, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 925/2000, Training Loss (NLML): -1046.9818, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 926/2000, Training Loss (NLML): -1046.9263, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 927/2000, Training Loss (NLML): -1046.9390, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 928/2000, Training Loss (NLML): -1046.8793, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 929/2000, Training Loss (NLML): -1046.9351, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 930/2000, Training Loss (NLML): -1046.9182, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 931/2000, Training Loss (NLML): -1047.0079, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 932/2000, Training Loss (NLML): -1047.0402, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 933/2000, Training Loss (NLML): -1047.1217, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 934/2000, Training Loss (NLML): -1047.1664, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 935/2000, Training Loss (NLML): -1047.2162, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 936/2000, Training Loss (NLML): -1047.2465, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 937/2000, Training Loss (NLML): -1047.2709, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 938/2000, Training Loss (NLML): -1047.2882, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 939/2000, Training Loss (NLML): -1047.3014, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 940/2000, Training Loss (NLML): -1047.3126, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 941/2000, Training Loss (NLML): -1047.3212, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 942/2000, Training Loss (NLML): -1047.3270, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 943/2000, Training Loss (NLML): -1047.3278, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 944/2000, Training Loss (NLML): -1047.3220, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 945/2000, Training Loss (NLML): -1047.3027, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 946/2000, Training Loss (NLML): -1047.2721, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 947/2000, Training Loss (NLML): -1047.2173, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 948/2000, Training Loss (NLML): -1047.1654, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 949/2000, Training Loss (NLML): -1047.1074, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 950/2000, Training Loss (NLML): -1047.1167, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 951/2000, Training Loss (NLML): -1047.1630, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 952/2000, Training Loss (NLML): -1047.2755, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 953/2000, Training Loss (NLML): -1047.3596, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 954/2000, Training Loss (NLML): -1047.4097, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 955/2000, Training Loss (NLML): -1047.3927, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 956/2000, Training Loss (NLML): -1047.3615, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 957/2000, Training Loss (NLML): -1047.2747, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 958/2000, Training Loss (NLML): -1047.2439, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 959/2000, Training Loss (NLML): -1047.0840, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 960/2000, Training Loss (NLML): -1047.0618, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 961/2000, Training Loss (NLML): -1046.6776, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 962/2000, Training Loss (NLML): -1046.6532, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 963/2000, Training Loss (NLML): -1045.8722, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 964/2000, Training Loss (NLML): -1046.3973, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 965/2000, Training Loss (NLML): -1046.1763, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 966/2000, Training Loss (NLML): -1047.0287, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 967/2000, Training Loss (NLML): -1047.4005, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 968/2000, Training Loss (NLML): -1047.6024, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 969/2000, Training Loss (NLML): -1047.5624, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 970/2000, Training Loss (NLML): -1047.3381, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 971/2000, Training Loss (NLML): -1047.1122, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 972/2000, Training Loss (NLML): -1046.3887, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 973/2000, Training Loss (NLML): -1046.3907, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 974/2000, Training Loss (NLML): -1045.1835, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 975/2000, Training Loss (NLML): -1046.4031, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 976/2000, Training Loss (NLML): -1046.6555, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 977/2000, Training Loss (NLML): -1047.4974, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 978/2000, Training Loss (NLML): -1047.7230, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 979/2000, Training Loss (NLML): -1047.4711, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 980/2000, Training Loss (NLML): -1047.1445, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 981/2000, Training Loss (NLML): -1046.5431, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 982/2000, Training Loss (NLML): -1046.9922, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 983/2000, Training Loss (NLML): -1047.0544, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 984/2000, Training Loss (NLML): -1047.5308, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 985/2000, Training Loss (NLML): -1047.7401, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 986/2000, Training Loss (NLML): -1047.8444, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 987/2000, Training Loss (NLML): -1047.8234, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 988/2000, Training Loss (NLML): -1047.7032, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 989/2000, Training Loss (NLML): -1047.5973, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 990/2000, Training Loss (NLML): -1047.3524, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 991/2000, Training Loss (NLML): -1047.3864, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 992/2000, Training Loss (NLML): -1047.2100, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 993/2000, Training Loss (NLML): -1047.4929, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 994/2000, Training Loss (NLML): -1047.6078, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 995/2000, Training Loss (NLML): -1047.8423, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 996/2000, Training Loss (NLML): -1047.9546, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 997/2000, Training Loss (NLML): -1047.9662, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 998/2000, Training Loss (NLML): -1047.9033, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 999/2000, Training Loss (NLML): -1047.7830, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1000/2000, Training Loss (NLML): -1047.7336, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1001/2000, Training Loss (NLML): -1047.5695, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1002/2000, Training Loss (NLML): -1047.6245, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1003/2000, Training Loss (NLML): -1047.4800, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1004/2000, Training Loss (NLML): -1047.6293, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1005/2000, Training Loss (NLML): -1047.5917, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1006/2000, Training Loss (NLML): -1047.8029, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1007/2000, Training Loss (NLML): -1047.9056, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1008/2000, Training Loss (NLML): -1048.0405, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1009/2000, Training Loss (NLML): -1048.0992, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1010/2000, Training Loss (NLML): -1048.0859, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1011/2000, Training Loss (NLML): -1048.0377, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1012/2000, Training Loss (NLML): -1047.9398, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1013/2000, Training Loss (NLML): -1047.9191, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1014/2000, Training Loss (NLML): -1047.8082, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1015/2000, Training Loss (NLML): -1047.8708, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1016/2000, Training Loss (NLML): -1047.8065, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1017/2000, Training Loss (NLML): -1047.9315, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1018/2000, Training Loss (NLML): -1047.9454, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1019/2000, Training Loss (NLML): -1048.0704, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1020/2000, Training Loss (NLML): -1048.1174, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1021/2000, Training Loss (NLML): -1048.1926, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1022/2000, Training Loss (NLML): -1048.2312, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1023/2000, Training Loss (NLML): -1048.2684, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1024/2000, Training Loss (NLML): -1048.2894, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1025/2000, Training Loss (NLML): -1048.2935, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1026/2000, Training Loss (NLML): -1048.2889, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1027/2000, Training Loss (NLML): -1048.2561, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1028/2000, Training Loss (NLML): -1048.2361, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1029/2000, Training Loss (NLML): -1048.1649, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1030/2000, Training Loss (NLML): -1048.1516, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1031/2000, Training Loss (NLML): -1048.0397, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1032/2000, Training Loss (NLML): -1048.0673, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1033/2000, Training Loss (NLML): -1047.9473, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1034/2000, Training Loss (NLML): -1048.0577, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1035/2000, Training Loss (NLML): -1048.0228, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1036/2000, Training Loss (NLML): -1048.1902, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1037/2000, Training Loss (NLML): -1048.2585, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1038/2000, Training Loss (NLML): -1048.3882, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1039/2000, Training Loss (NLML): -1048.4603, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1040/2000, Training Loss (NLML): -1048.5109, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1041/2000, Training Loss (NLML): -1048.5291, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1042/2000, Training Loss (NLML): -1048.5222, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1043/2000, Training Loss (NLML): -1048.4941, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1044/2000, Training Loss (NLML): -1048.4229, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1045/2000, Training Loss (NLML): -1048.3351, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1046/2000, Training Loss (NLML): -1048.1217, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1047/2000, Training Loss (NLML): -1048.0459, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1048/2000, Training Loss (NLML): -1047.8162, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1049/2000, Training Loss (NLML): -1048.1005, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1050/2000, Training Loss (NLML): -1048.1906, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1051/2000, Training Loss (NLML): -1048.3478, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1052/2000, Training Loss (NLML): -1048.2500, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1053/2000, Training Loss (NLML): -1048.3553, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1054/2000, Training Loss (NLML): -1048.4641, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1055/2000, Training Loss (NLML): -1048.6547, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1056/2000, Training Loss (NLML): -1048.7317, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1057/2000, Training Loss (NLML): -1048.6967, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1058/2000, Training Loss (NLML): -1048.6134, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1059/2000, Training Loss (NLML): -1048.5189, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1060/2000, Training Loss (NLML): -1048.5166, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1061/2000, Training Loss (NLML): -1048.4346, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1062/2000, Training Loss (NLML): -1048.4071, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1063/2000, Training Loss (NLML): -1048.0167, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1064/2000, Training Loss (NLML): -1047.9086, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1065/2000, Training Loss (NLML): -1047.1595, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1066/2000, Training Loss (NLML): -1047.8154, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1067/2000, Training Loss (NLML): -1047.8977, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1068/2000, Training Loss (NLML): -1048.5139, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1069/2000, Training Loss (NLML): -1048.7704, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1070/2000, Training Loss (NLML): -1048.8402, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1071/2000, Training Loss (NLML): -1048.7255, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1072/2000, Training Loss (NLML): -1048.3673, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1073/2000, Training Loss (NLML): -1048.1157, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1074/2000, Training Loss (NLML): -1047.0790, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1075/2000, Training Loss (NLML): -1047.4139, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 1076/2000, Training Loss (NLML): -1046.3726, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1077/2000, Training Loss (NLML): -1047.9216, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1078/2000, Training Loss (NLML): -1048.5081, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1079/2000, Training Loss (NLML): -1048.9222, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1080/2000, Training Loss (NLML): -1048.7748, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1081/2000, Training Loss (NLML): -1048.2489, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1082/2000, Training Loss (NLML): -1048.1986, (RMSE): 0.0091\n",
      "branching dfNGP Run 1/1, Epoch 1083/2000, Training Loss (NLML): -1047.8473, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1084/2000, Training Loss (NLML): -1048.4794, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1085/2000, Training Loss (NLML): -1048.7354, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1086/2000, Training Loss (NLML): -1049.0043, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1087/2000, Training Loss (NLML): -1049.0826, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1088/2000, Training Loss (NLML): -1049.0374, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1089/2000, Training Loss (NLML): -1048.9286, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1090/2000, Training Loss (NLML): -1048.6886, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1091/2000, Training Loss (NLML): -1048.6339, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1092/2000, Training Loss (NLML): -1048.3188, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1093/2000, Training Loss (NLML): -1048.5382, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1094/2000, Training Loss (NLML): -1048.5148, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1095/2000, Training Loss (NLML): -1048.8898, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1096/2000, Training Loss (NLML): -1049.0884, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1097/2000, Training Loss (NLML): -1049.1917, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1098/2000, Training Loss (NLML): -1049.1556, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1099/2000, Training Loss (NLML): -1049.0212, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1100/2000, Training Loss (NLML): -1048.9406, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1101/2000, Training Loss (NLML): -1048.7535, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1102/2000, Training Loss (NLML): -1048.8123, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1103/2000, Training Loss (NLML): -1048.6465, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1104/2000, Training Loss (NLML): -1048.8286, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1105/2000, Training Loss (NLML): -1048.7902, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1106/2000, Training Loss (NLML): -1049.0142, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1107/2000, Training Loss (NLML): -1049.1237, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1108/2000, Training Loss (NLML): -1049.2760, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1109/2000, Training Loss (NLML): -1049.3511, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1110/2000, Training Loss (NLML): -1049.3630, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1111/2000, Training Loss (NLML): -1049.3279, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1112/2000, Training Loss (NLML): -1049.2578, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1113/2000, Training Loss (NLML): -1049.2159, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1114/2000, Training Loss (NLML): -1049.1134, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1115/2000, Training Loss (NLML): -1049.1190, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1116/2000, Training Loss (NLML): -1049.0006, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1117/2000, Training Loss (NLML): -1049.0802, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1118/2000, Training Loss (NLML): -1049.0144, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1119/2000, Training Loss (NLML): -1049.1609, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1120/2000, Training Loss (NLML): -1049.1979, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1121/2000, Training Loss (NLML): -1049.3312, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1122/2000, Training Loss (NLML): -1049.3861, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1123/2000, Training Loss (NLML): -1049.4220, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1124/2000, Training Loss (NLML): -1049.3904, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1125/2000, Training Loss (NLML): -1049.2915, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1126/2000, Training Loss (NLML): -1049.1836, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1127/2000, Training Loss (NLML): -1048.9648, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1128/2000, Training Loss (NLML): -1049.0281, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1129/2000, Training Loss (NLML): -1049.0148, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1130/2000, Training Loss (NLML): -1049.2916, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1131/2000, Training Loss (NLML): -1049.3248, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1132/2000, Training Loss (NLML): -1049.3373, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1133/2000, Training Loss (NLML): -1049.2665, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1134/2000, Training Loss (NLML): -1049.4254, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1135/2000, Training Loss (NLML): -1049.5652, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1136/2000, Training Loss (NLML): -1049.6414, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1137/2000, Training Loss (NLML): -1049.6078, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1138/2000, Training Loss (NLML): -1049.5508, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1139/2000, Training Loss (NLML): -1049.5559, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1140/2000, Training Loss (NLML): -1049.6058, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1141/2000, Training Loss (NLML): -1049.6600, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1142/2000, Training Loss (NLML): -1049.6387, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1143/2000, Training Loss (NLML): -1049.5767, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1144/2000, Training Loss (NLML): -1049.4570, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1145/2000, Training Loss (NLML): -1049.4470, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1146/2000, Training Loss (NLML): -1049.2980, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1147/2000, Training Loss (NLML): -1049.2938, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1148/2000, Training Loss (NLML): -1049.0077, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1149/2000, Training Loss (NLML): -1049.1605, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1150/2000, Training Loss (NLML): -1049.0490, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1151/2000, Training Loss (NLML): -1049.3738, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1152/2000, Training Loss (NLML): -1049.4773, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1153/2000, Training Loss (NLML): -1049.6639, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1154/2000, Training Loss (NLML): -1049.7576, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1155/2000, Training Loss (NLML): -1049.8301, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1156/2000, Training Loss (NLML): -1049.8563, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1157/2000, Training Loss (NLML): -1049.8405, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1158/2000, Training Loss (NLML): -1049.7897, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1159/2000, Training Loss (NLML): -1049.6696, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1160/2000, Training Loss (NLML): -1049.5455, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1161/2000, Training Loss (NLML): -1049.1438, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1162/2000, Training Loss (NLML): -1049.0337, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1163/2000, Training Loss (NLML): -1048.2362, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1164/2000, Training Loss (NLML): -1048.8226, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1165/2000, Training Loss (NLML): -1048.7896, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1166/2000, Training Loss (NLML): -1049.5475, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1167/2000, Training Loss (NLML): -1049.8766, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1168/2000, Training Loss (NLML): -1049.8922, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1169/2000, Training Loss (NLML): -1049.6722, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1170/2000, Training Loss (NLML): -1049.1971, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1171/2000, Training Loss (NLML): -1049.1239, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1172/2000, Training Loss (NLML): -1048.4218, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1173/2000, Training Loss (NLML): -1048.9161, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1174/2000, Training Loss (NLML): -1048.5997, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1175/2000, Training Loss (NLML): -1049.3595, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1176/2000, Training Loss (NLML): -1049.6627, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1177/2000, Training Loss (NLML): -1049.9781, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1178/2000, Training Loss (NLML): -1050.0369, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1179/2000, Training Loss (NLML): -1049.8947, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1180/2000, Training Loss (NLML): -1049.7159, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1181/2000, Training Loss (NLML): -1049.3871, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1182/2000, Training Loss (NLML): -1049.4735, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1183/2000, Training Loss (NLML): -1049.2714, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1184/2000, Training Loss (NLML): -1049.5946, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1185/2000, Training Loss (NLML): -1049.6664, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1186/2000, Training Loss (NLML): -1049.9226, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1187/2000, Training Loss (NLML): -1050.0486, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1188/2000, Training Loss (NLML): -1050.1168, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1189/2000, Training Loss (NLML): -1050.1012, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1190/2000, Training Loss (NLML): -1050.0118, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1191/2000, Training Loss (NLML): -1049.9088, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1192/2000, Training Loss (NLML): -1049.6649, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1193/2000, Training Loss (NLML): -1049.5918, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1194/2000, Training Loss (NLML): -1049.2125, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1195/2000, Training Loss (NLML): -1049.4431, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1196/2000, Training Loss (NLML): -1049.4921, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1197/2000, Training Loss (NLML): -1049.9554, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1198/2000, Training Loss (NLML): -1050.1890, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1199/2000, Training Loss (NLML): -1050.1647, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1200/2000, Training Loss (NLML): -1049.9865, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1201/2000, Training Loss (NLML): -1049.7489, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1202/2000, Training Loss (NLML): -1049.7722, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1203/2000, Training Loss (NLML): -1049.6729, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1204/2000, Training Loss (NLML): -1049.8578, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1205/2000, Training Loss (NLML): -1049.7825, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1206/2000, Training Loss (NLML): -1049.8744, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1207/2000, Training Loss (NLML): -1049.7645, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1208/2000, Training Loss (NLML): -1049.9568, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1209/2000, Training Loss (NLML): -1050.0453, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1210/2000, Training Loss (NLML): -1050.2089, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1211/2000, Training Loss (NLML): -1050.2699, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1212/2000, Training Loss (NLML): -1050.2828, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1213/2000, Training Loss (NLML): -1050.2740, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1214/2000, Training Loss (NLML): -1050.2645, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1215/2000, Training Loss (NLML): -1050.2723, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1216/2000, Training Loss (NLML): -1050.2361, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1217/2000, Training Loss (NLML): -1050.1970, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1218/2000, Training Loss (NLML): -1050.0558, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1219/2000, Training Loss (NLML): -1050.0084, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1220/2000, Training Loss (NLML): -1049.7726, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1221/2000, Training Loss (NLML): -1049.8584, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1222/2000, Training Loss (NLML): -1049.6659, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1223/2000, Training Loss (NLML): -1049.9016, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1224/2000, Training Loss (NLML): -1049.8990, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1225/2000, Training Loss (NLML): -1050.1562, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1226/2000, Training Loss (NLML): -1050.2908, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1227/2000, Training Loss (NLML): -1050.4169, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1228/2000, Training Loss (NLML): -1050.4604, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1229/2000, Training Loss (NLML): -1050.4327, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1230/2000, Training Loss (NLML): -1050.3567, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1231/2000, Training Loss (NLML): -1050.1917, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1232/2000, Training Loss (NLML): -1050.0624, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1233/2000, Training Loss (NLML): -1049.6069, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1234/2000, Training Loss (NLML): -1049.5680, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1235/2000, Training Loss (NLML): -1048.8003, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1236/2000, Training Loss (NLML): -1049.4717, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1237/2000, Training Loss (NLML): -1049.5490, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1238/2000, Training Loss (NLML): -1050.2273, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1239/2000, Training Loss (NLML): -1050.4865, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1240/2000, Training Loss (NLML): -1050.4036, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1241/2000, Training Loss (NLML): -1050.1520, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1242/2000, Training Loss (NLML): -1049.6921, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1243/2000, Training Loss (NLML): -1049.8043, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1244/2000, Training Loss (NLML): -1049.5107, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1245/2000, Training Loss (NLML): -1049.9264, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1246/2000, Training Loss (NLML): -1049.9352, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1247/2000, Training Loss (NLML): -1050.2527, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1248/2000, Training Loss (NLML): -1050.3705, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1249/2000, Training Loss (NLML): -1050.5048, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1250/2000, Training Loss (NLML): -1050.5525, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1251/2000, Training Loss (NLML): -1050.5507, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1252/2000, Training Loss (NLML): -1050.5250, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1253/2000, Training Loss (NLML): -1050.4623, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1254/2000, Training Loss (NLML): -1050.4447, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1255/2000, Training Loss (NLML): -1050.3641, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1256/2000, Training Loss (NLML): -1050.3884, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1257/2000, Training Loss (NLML): -1050.3164, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1258/2000, Training Loss (NLML): -1050.3843, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1259/2000, Training Loss (NLML): -1050.3510, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1260/2000, Training Loss (NLML): -1050.4427, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1261/2000, Training Loss (NLML): -1050.4526, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1262/2000, Training Loss (NLML): -1050.5353, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1263/2000, Training Loss (NLML): -1050.5701, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1264/2000, Training Loss (NLML): -1050.6316, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1265/2000, Training Loss (NLML): -1050.6713, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1266/2000, Training Loss (NLML): -1050.7064, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1267/2000, Training Loss (NLML): -1050.7285, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1268/2000, Training Loss (NLML): -1050.7385, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1269/2000, Training Loss (NLML): -1050.7401, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1270/2000, Training Loss (NLML): -1050.7330, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1271/2000, Training Loss (NLML): -1050.7219, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1272/2000, Training Loss (NLML): -1050.6954, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1273/2000, Training Loss (NLML): -1050.6680, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1274/2000, Training Loss (NLML): -1050.5973, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1275/2000, Training Loss (NLML): -1050.5524, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1276/2000, Training Loss (NLML): -1050.4077, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1277/2000, Training Loss (NLML): -1050.4041, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1278/2000, Training Loss (NLML): -1050.2427, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1279/2000, Training Loss (NLML): -1050.3696, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1280/2000, Training Loss (NLML): -1050.3185, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1281/2000, Training Loss (NLML): -1050.4923, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1282/2000, Training Loss (NLML): -1050.5286, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1283/2000, Training Loss (NLML): -1050.6553, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1284/2000, Training Loss (NLML): -1050.7109, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1285/2000, Training Loss (NLML): -1050.7781, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1286/2000, Training Loss (NLML): -1050.8126, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1287/2000, Training Loss (NLML): -1050.8334, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1288/2000, Training Loss (NLML): -1050.8368, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1289/2000, Training Loss (NLML): -1050.8215, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1290/2000, Training Loss (NLML): -1050.7910, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1291/2000, Training Loss (NLML): -1050.7119, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1292/2000, Training Loss (NLML): -1050.6345, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1293/2000, Training Loss (NLML): -1050.4128, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1294/2000, Training Loss (NLML): -1050.3607, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1295/2000, Training Loss (NLML): -1050.0316, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1296/2000, Training Loss (NLML): -1050.2467, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1297/2000, Training Loss (NLML): -1050.1538, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1298/2000, Training Loss (NLML): -1050.4840, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1299/2000, Training Loss (NLML): -1050.5730, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1300/2000, Training Loss (NLML): -1050.7334, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1301/2000, Training Loss (NLML): -1050.7844, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1302/2000, Training Loss (NLML): -1050.8262, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1303/2000, Training Loss (NLML): -1050.8368, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1304/2000, Training Loss (NLML): -1050.7941, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1305/2000, Training Loss (NLML): -1050.7394, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1306/2000, Training Loss (NLML): -1050.5079, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1307/2000, Training Loss (NLML): -1050.3378, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1308/2000, Training Loss (NLML): -1049.5591, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1309/2000, Training Loss (NLML): -1049.6793, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1310/2000, Training Loss (NLML): -1048.7731, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1311/2000, Training Loss (NLML): -1050.0012, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1312/2000, Training Loss (NLML): -1050.5032, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1313/2000, Training Loss (NLML): -1050.9230, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1314/2000, Training Loss (NLML): -1050.8311, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1315/2000, Training Loss (NLML): -1050.3724, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1316/2000, Training Loss (NLML): -1050.1993, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1317/2000, Training Loss (NLML): -1049.5631, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1318/2000, Training Loss (NLML): -1050.1235, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1319/2000, Training Loss (NLML): -1050.0332, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1320/2000, Training Loss (NLML): -1050.5314, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1321/2000, Training Loss (NLML): -1050.6749, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1322/2000, Training Loss (NLML): -1050.9125, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1323/2000, Training Loss (NLML): -1051.0127, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1324/2000, Training Loss (NLML): -1051.0443, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1325/2000, Training Loss (NLML): -1051.0229, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1326/2000, Training Loss (NLML): -1050.9500, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1327/2000, Training Loss (NLML): -1050.8947, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1328/2000, Training Loss (NLML): -1050.7698, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1329/2000, Training Loss (NLML): -1050.7661, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1330/2000, Training Loss (NLML): -1050.6182, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1331/2000, Training Loss (NLML): -1050.7130, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1332/2000, Training Loss (NLML): -1050.6492, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1333/2000, Training Loss (NLML): -1050.8097, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1334/2000, Training Loss (NLML): -1050.8459, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1335/2000, Training Loss (NLML): -1050.9771, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1336/2000, Training Loss (NLML): -1051.0321, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1337/2000, Training Loss (NLML): -1051.0873, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1338/2000, Training Loss (NLML): -1051.1140, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1339/2000, Training Loss (NLML): -1051.1259, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1340/2000, Training Loss (NLML): -1051.1272, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1341/2000, Training Loss (NLML): -1051.1116, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1342/2000, Training Loss (NLML): -1051.0970, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1343/2000, Training Loss (NLML): -1051.0502, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1344/2000, Training Loss (NLML): -1051.0237, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1345/2000, Training Loss (NLML): -1050.9224, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1346/2000, Training Loss (NLML): -1050.8853, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1347/2000, Training Loss (NLML): -1050.6912, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1348/2000, Training Loss (NLML): -1050.7103, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1349/2000, Training Loss (NLML): -1050.5055, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1350/2000, Training Loss (NLML): -1050.7102, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1351/2000, Training Loss (NLML): -1050.7511, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1352/2000, Training Loss (NLML): -1051.0145, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1353/2000, Training Loss (NLML): -1051.1495, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1354/2000, Training Loss (NLML): -1051.2183, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1355/2000, Training Loss (NLML): -1051.1993, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1356/2000, Training Loss (NLML): -1051.1273, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1357/2000, Training Loss (NLML): -1051.0399, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1358/2000, Training Loss (NLML): -1050.8707, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1359/2000, Training Loss (NLML): -1050.7900, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1360/2000, Training Loss (NLML): -1050.4945, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1361/2000, Training Loss (NLML): -1050.6045, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1362/2000, Training Loss (NLML): -1050.3698, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1363/2000, Training Loss (NLML): -1050.6688, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1364/2000, Training Loss (NLML): -1050.5483, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1365/2000, Training Loss (NLML): -1050.8326, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1366/2000, Training Loss (NLML): -1050.9493, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1367/2000, Training Loss (NLML): -1051.2114, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1368/2000, Training Loss (NLML): -1051.3215, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1369/2000, Training Loss (NLML): -1051.2617, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1370/2000, Training Loss (NLML): -1051.0970, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1371/2000, Training Loss (NLML): -1050.7451, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1372/2000, Training Loss (NLML): -1050.6199, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1373/2000, Training Loss (NLML): -1049.8917, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1374/2000, Training Loss (NLML): -1050.1621, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1375/2000, Training Loss (NLML): -1049.4349, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1376/2000, Training Loss (NLML): -1050.4044, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1377/2000, Training Loss (NLML): -1050.7111, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1378/2000, Training Loss (NLML): -1051.2233, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1379/2000, Training Loss (NLML): -1051.3093, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1380/2000, Training Loss (NLML): -1051.0385, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1381/2000, Training Loss (NLML): -1050.7948, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1382/2000, Training Loss (NLML): -1050.2957, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1383/2000, Training Loss (NLML): -1050.6498, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1384/2000, Training Loss (NLML): -1050.5760, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1385/2000, Training Loss (NLML): -1050.9839, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1386/2000, Training Loss (NLML): -1051.0972, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1387/2000, Training Loss (NLML): -1051.2827, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1388/2000, Training Loss (NLML): -1051.3580, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1389/2000, Training Loss (NLML): -1051.4130, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1390/2000, Training Loss (NLML): -1051.4318, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1391/2000, Training Loss (NLML): -1051.4290, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1392/2000, Training Loss (NLML): -1051.4154, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1393/2000, Training Loss (NLML): -1051.3759, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1394/2000, Training Loss (NLML): -1051.3494, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1395/2000, Training Loss (NLML): -1051.2755, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1396/2000, Training Loss (NLML): -1051.2753, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1397/2000, Training Loss (NLML): -1051.2098, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1398/2000, Training Loss (NLML): -1051.2638, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1399/2000, Training Loss (NLML): -1051.2335, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1400/2000, Training Loss (NLML): -1051.3033, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1401/2000, Training Loss (NLML): -1051.2823, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1402/2000, Training Loss (NLML): -1051.3342, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1403/2000, Training Loss (NLML): -1051.3057, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1404/2000, Training Loss (NLML): -1051.3436, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1405/2000, Training Loss (NLML): -1051.3115, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1406/2000, Training Loss (NLML): -1051.3542, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1407/2000, Training Loss (NLML): -1051.3455, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1408/2000, Training Loss (NLML): -1051.4071, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1409/2000, Training Loss (NLML): -1051.4364, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1410/2000, Training Loss (NLML): -1051.4987, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1411/2000, Training Loss (NLML): -1051.5306, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1412/2000, Training Loss (NLML): -1051.5601, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1413/2000, Training Loss (NLML): -1051.5658, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1414/2000, Training Loss (NLML): -1051.5673, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1415/2000, Training Loss (NLML): -1051.5486, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1416/2000, Training Loss (NLML): -1051.5349, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1417/2000, Training Loss (NLML): -1051.4896, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1418/2000, Training Loss (NLML): -1051.4698, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1419/2000, Training Loss (NLML): -1051.3859, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1420/2000, Training Loss (NLML): -1051.3729, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1421/2000, Training Loss (NLML): -1051.2346, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1422/2000, Training Loss (NLML): -1051.2544, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1423/2000, Training Loss (NLML): -1051.0695, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1424/2000, Training Loss (NLML): -1051.1689, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1425/2000, Training Loss (NLML): -1051.0189, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1426/2000, Training Loss (NLML): -1051.2209, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1427/2000, Training Loss (NLML): -1051.2129, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1428/2000, Training Loss (NLML): -1051.4200, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1429/2000, Training Loss (NLML): -1051.5031, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1430/2000, Training Loss (NLML): -1051.6200, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1431/2000, Training Loss (NLML): -1051.6798, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1432/2000, Training Loss (NLML): -1051.7197, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1433/2000, Training Loss (NLML): -1051.7390, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1434/2000, Training Loss (NLML): -1051.7461, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1435/2000, Training Loss (NLML): -1051.7458, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1436/2000, Training Loss (NLML): -1051.7371, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1437/2000, Training Loss (NLML): -1051.7178, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1438/2000, Training Loss (NLML): -1051.6664, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1439/2000, Training Loss (NLML): -1051.5996, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1440/2000, Training Loss (NLML): -1051.4159, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1441/2000, Training Loss (NLML): -1051.3070, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1442/2000, Training Loss (NLML): -1050.8848, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1443/2000, Training Loss (NLML): -1050.9789, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1444/2000, Training Loss (NLML): -1050.6077, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1445/2000, Training Loss (NLML): -1051.1089, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1446/2000, Training Loss (NLML): -1051.2484, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1447/2000, Training Loss (NLML): -1051.6107, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1448/2000, Training Loss (NLML): -1051.7721, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1449/2000, Training Loss (NLML): -1051.8142, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1450/2000, Training Loss (NLML): -1051.7498, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1451/2000, Training Loss (NLML): -1051.5577, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1452/2000, Training Loss (NLML): -1051.3458, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1453/2000, Training Loss (NLML): -1050.6130, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1454/2000, Training Loss (NLML): -1050.4775, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1455/2000, Training Loss (NLML): -1048.8743, (RMSE): 0.0089\n",
      "branching dfNGP Run 1/1, Epoch 1456/2000, Training Loss (NLML): -1050.4165, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1457/2000, Training Loss (NLML): -1050.8080, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1458/2000, Training Loss (NLML): -1051.6713, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1459/2000, Training Loss (NLML): -1051.7083, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1460/2000, Training Loss (NLML): -1051.2059, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1461/2000, Training Loss (NLML): -1051.0337, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1462/2000, Training Loss (NLML): -1050.5428, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1463/2000, Training Loss (NLML): -1051.2510, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1464/2000, Training Loss (NLML): -1051.4768, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1465/2000, Training Loss (NLML): -1051.7727, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1466/2000, Training Loss (NLML): -1051.8132, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1467/2000, Training Loss (NLML): -1051.7695, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1468/2000, Training Loss (NLML): -1051.6277, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1469/2000, Training Loss (NLML): -1051.1896, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1470/2000, Training Loss (NLML): -1051.0775, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1471/2000, Training Loss (NLML): -1050.2971, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1472/2000, Training Loss (NLML): -1050.8384, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1473/2000, Training Loss (NLML): -1050.9376, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1474/2000, Training Loss (NLML): -1051.6412, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1475/2000, Training Loss (NLML): -1051.7928, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1476/2000, Training Loss (NLML): -1051.5765, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1477/2000, Training Loss (NLML): -1051.4540, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1478/2000, Training Loss (NLML): -1051.2146, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1479/2000, Training Loss (NLML): -1051.5385, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1480/2000, Training Loss (NLML): -1051.6566, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1481/2000, Training Loss (NLML): -1051.8409, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1482/2000, Training Loss (NLML): -1051.8097, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1483/2000, Training Loss (NLML): -1051.7247, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1484/2000, Training Loss (NLML): -1051.5043, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1485/2000, Training Loss (NLML): -1051.4424, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1486/2000, Training Loss (NLML): -1051.5760, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1487/2000, Training Loss (NLML): -1051.9379, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1488/2000, Training Loss (NLML): -1052.0779, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1489/2000, Training Loss (NLML): -1051.9302, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1490/2000, Training Loss (NLML): -1051.8177, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1491/2000, Training Loss (NLML): -1051.9525, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1492/2000, Training Loss (NLML): -1052.0883, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1493/2000, Training Loss (NLML): -1052.0513, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1494/2000, Training Loss (NLML): -1051.9718, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1495/2000, Training Loss (NLML): -1051.9752, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1496/2000, Training Loss (NLML): -1052.0503, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1497/2000, Training Loss (NLML): -1052.0671, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1498/2000, Training Loss (NLML): -1052.0417, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1499/2000, Training Loss (NLML): -1052.0001, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1500/2000, Training Loss (NLML): -1052.0272, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1501/2000, Training Loss (NLML): -1051.9928, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1502/2000, Training Loss (NLML): -1051.9323, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1503/2000, Training Loss (NLML): -1051.6970, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1504/2000, Training Loss (NLML): -1051.6915, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1505/2000, Training Loss (NLML): -1051.4807, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1506/2000, Training Loss (NLML): -1051.6569, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1507/2000, Training Loss (NLML): -1051.5551, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1508/2000, Training Loss (NLML): -1051.8425, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1509/2000, Training Loss (NLML): -1051.9807, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1510/2000, Training Loss (NLML): -1052.1471, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1511/2000, Training Loss (NLML): -1052.2054, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1512/2000, Training Loss (NLML): -1052.2283, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1513/2000, Training Loss (NLML): -1052.2366, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1514/2000, Training Loss (NLML): -1052.2213, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1515/2000, Training Loss (NLML): -1052.1733, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1516/2000, Training Loss (NLML): -1052.0305, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1517/2000, Training Loss (NLML): -1051.8713, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1518/2000, Training Loss (NLML): -1051.3458, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1519/2000, Training Loss (NLML): -1051.2382, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1520/2000, Training Loss (NLML): -1050.2526, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1521/2000, Training Loss (NLML): -1051.1492, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1522/2000, Training Loss (NLML): -1051.3499, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1523/2000, Training Loss (NLML): -1052.0709, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1524/2000, Training Loss (NLML): -1052.2771, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1525/2000, Training Loss (NLML): -1052.0891, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1526/2000, Training Loss (NLML): -1051.7855, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1527/2000, Training Loss (NLML): -1051.1119, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1528/2000, Training Loss (NLML): -1051.3361, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1529/2000, Training Loss (NLML): -1050.8157, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1530/2000, Training Loss (NLML): -1051.4462, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1531/2000, Training Loss (NLML): -1051.3947, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1532/2000, Training Loss (NLML): -1051.9496, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1533/2000, Training Loss (NLML): -1052.1810, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1534/2000, Training Loss (NLML): -1052.3657, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1535/2000, Training Loss (NLML): -1052.4015, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1536/2000, Training Loss (NLML): -1052.3202, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1537/2000, Training Loss (NLML): -1052.2009, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1538/2000, Training Loss (NLML): -1051.9773, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1539/2000, Training Loss (NLML): -1051.9481, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1540/2000, Training Loss (NLML): -1051.6636, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1541/2000, Training Loss (NLML): -1051.8353, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1542/2000, Training Loss (NLML): -1051.6978, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1543/2000, Training Loss (NLML): -1052.0148, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1544/2000, Training Loss (NLML): -1052.1246, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1545/2000, Training Loss (NLML): -1052.3483, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1546/2000, Training Loss (NLML): -1052.4584, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1547/2000, Training Loss (NLML): -1052.4974, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1548/2000, Training Loss (NLML): -1052.4744, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1549/2000, Training Loss (NLML): -1052.4030, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1550/2000, Training Loss (NLML): -1052.3275, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1551/2000, Training Loss (NLML): -1052.1567, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1552/2000, Training Loss (NLML): -1052.1078, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1553/2000, Training Loss (NLML): -1051.8265, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1554/2000, Training Loss (NLML): -1051.9293, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1555/2000, Training Loss (NLML): -1051.7235, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1556/2000, Training Loss (NLML): -1052.0454, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1557/2000, Training Loss (NLML): -1052.1365, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1558/2000, Training Loss (NLML): -1052.4044, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1559/2000, Training Loss (NLML): -1052.5392, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1560/2000, Training Loss (NLML): -1052.5889, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1561/2000, Training Loss (NLML): -1052.5559, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1562/2000, Training Loss (NLML): -1052.4581, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1563/2000, Training Loss (NLML): -1052.3669, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1564/2000, Training Loss (NLML): -1052.1459, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1565/2000, Training Loss (NLML): -1052.1057, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1566/2000, Training Loss (NLML): -1051.7411, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1567/2000, Training Loss (NLML): -1051.8947, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1568/2000, Training Loss (NLML): -1051.6071, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1569/2000, Training Loss (NLML): -1052.0522, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1570/2000, Training Loss (NLML): -1052.1970, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1571/2000, Training Loss (NLML): -1052.5201, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1572/2000, Training Loss (NLML): -1052.6571, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1573/2000, Training Loss (NLML): -1052.6475, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1574/2000, Training Loss (NLML): -1052.5413, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1575/2000, Training Loss (NLML): -1052.3424, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1576/2000, Training Loss (NLML): -1052.2810, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1577/2000, Training Loss (NLML): -1052.0035, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1578/2000, Training Loss (NLML): -1052.1112, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1579/2000, Training Loss (NLML): -1051.8506, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1580/2000, Training Loss (NLML): -1052.1261, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1581/2000, Training Loss (NLML): -1052.0745, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1582/2000, Training Loss (NLML): -1052.4044, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1583/2000, Training Loss (NLML): -1052.5522, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1584/2000, Training Loss (NLML): -1052.7031, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1585/2000, Training Loss (NLML): -1052.7368, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1586/2000, Training Loss (NLML): -1052.6752, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1587/2000, Training Loss (NLML): -1052.5704, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1588/2000, Training Loss (NLML): -1052.3341, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1589/2000, Training Loss (NLML): -1052.2222, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1590/2000, Training Loss (NLML): -1051.6992, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1591/2000, Training Loss (NLML): -1051.6915, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1592/2000, Training Loss (NLML): -1051.1654, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1593/2000, Training Loss (NLML): -1051.8658, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1594/2000, Training Loss (NLML): -1052.5120, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1595/2000, Training Loss (NLML): -1052.7769, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1596/2000, Training Loss (NLML): -1052.4325, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1597/2000, Training Loss (NLML): -1051.9688, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1598/2000, Training Loss (NLML): -1052.3528, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1599/2000, Training Loss (NLML): -1052.6755, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1600/2000, Training Loss (NLML): -1052.7804, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1601/2000, Training Loss (NLML): -1052.5848, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1602/2000, Training Loss (NLML): -1052.4424, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1603/2000, Training Loss (NLML): -1052.4653, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1604/2000, Training Loss (NLML): -1052.6342, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1605/2000, Training Loss (NLML): -1052.8141, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1606/2000, Training Loss (NLML): -1052.8109, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1607/2000, Training Loss (NLML): -1052.7184, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1608/2000, Training Loss (NLML): -1052.5989, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1609/2000, Training Loss (NLML): -1052.6997, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1610/2000, Training Loss (NLML): -1052.7271, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1611/2000, Training Loss (NLML): -1052.7438, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1612/2000, Training Loss (NLML): -1052.6189, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1613/2000, Training Loss (NLML): -1052.7054, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1614/2000, Training Loss (NLML): -1052.7672, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1615/2000, Training Loss (NLML): -1052.8760, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1616/2000, Training Loss (NLML): -1052.8577, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1617/2000, Training Loss (NLML): -1052.8599, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1618/2000, Training Loss (NLML): -1052.8615, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1619/2000, Training Loss (NLML): -1052.9364, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1620/2000, Training Loss (NLML): -1052.9597, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1621/2000, Training Loss (NLML): -1052.9415, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1622/2000, Training Loss (NLML): -1052.8279, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1623/2000, Training Loss (NLML): -1052.7609, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1624/2000, Training Loss (NLML): -1052.5380, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1625/2000, Training Loss (NLML): -1052.5348, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1626/2000, Training Loss (NLML): -1052.1707, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1627/2000, Training Loss (NLML): -1052.3478, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1628/2000, Training Loss (NLML): -1052.1398, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1629/2000, Training Loss (NLML): -1052.5928, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1630/2000, Training Loss (NLML): -1052.7793, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1631/2000, Training Loss (NLML): -1053.0172, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1632/2000, Training Loss (NLML): -1053.1122, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1633/2000, Training Loss (NLML): -1053.1272, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1634/2000, Training Loss (NLML): -1053.0779, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1635/2000, Training Loss (NLML): -1052.9272, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1636/2000, Training Loss (NLML): -1052.7435, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1637/2000, Training Loss (NLML): -1052.1262, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1638/2000, Training Loss (NLML): -1051.9380, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1639/2000, Training Loss (NLML): -1050.4269, (RMSE): 0.0089\n",
      "branching dfNGP Run 1/1, Epoch 1640/2000, Training Loss (NLML): -1051.6927, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1641/2000, Training Loss (NLML): -1051.8065, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1642/2000, Training Loss (NLML): -1052.8705, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1643/2000, Training Loss (NLML): -1053.1265, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1644/2000, Training Loss (NLML): -1052.7689, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1645/2000, Training Loss (NLML): -1052.4266, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1646/2000, Training Loss (NLML): -1051.7794, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1647/2000, Training Loss (NLML): -1052.4780, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1648/2000, Training Loss (NLML): -1052.7078, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1649/2000, Training Loss (NLML): -1053.0977, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1650/2000, Training Loss (NLML): -1053.2307, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1651/2000, Training Loss (NLML): -1053.2808, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1652/2000, Training Loss (NLML): -1053.2496, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1653/2000, Training Loss (NLML): -1053.1288, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1654/2000, Training Loss (NLML): -1053.0106, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1655/2000, Training Loss (NLML): -1052.6429, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1656/2000, Training Loss (NLML): -1052.5437, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1657/2000, Training Loss (NLML): -1051.9380, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1658/2000, Training Loss (NLML): -1052.4930, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1659/2000, Training Loss (NLML): -1052.6298, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1660/2000, Training Loss (NLML): -1053.1473, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1661/2000, Training Loss (NLML): -1053.3302, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1662/2000, Training Loss (NLML): -1053.2235, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1663/2000, Training Loss (NLML): -1053.0125, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1664/2000, Training Loss (NLML): -1052.6410, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1665/2000, Training Loss (NLML): -1052.7186, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1666/2000, Training Loss (NLML): -1052.3674, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1667/2000, Training Loss (NLML): -1052.7101, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1668/2000, Training Loss (NLML): -1052.5452, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1669/2000, Training Loss (NLML): -1052.8749, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1670/2000, Training Loss (NLML): -1052.8804, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1671/2000, Training Loss (NLML): -1053.1478, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1672/2000, Training Loss (NLML): -1053.2581, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1673/2000, Training Loss (NLML): -1053.3503, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1674/2000, Training Loss (NLML): -1053.3838, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1675/2000, Training Loss (NLML): -1053.3394, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1676/2000, Training Loss (NLML): -1053.2988, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1677/2000, Training Loss (NLML): -1053.1774, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1678/2000, Training Loss (NLML): -1053.1676, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1679/2000, Training Loss (NLML): -1052.9811, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1680/2000, Training Loss (NLML): -1053.0260, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1681/2000, Training Loss (NLML): -1052.8225, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1682/2000, Training Loss (NLML): -1052.9906, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1683/2000, Training Loss (NLML): -1052.9773, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1684/2000, Training Loss (NLML): -1053.2803, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1685/2000, Training Loss (NLML): -1053.4493, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1686/2000, Training Loss (NLML): -1053.5459, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1687/2000, Training Loss (NLML): -1053.5277, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1688/2000, Training Loss (NLML): -1053.4611, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1689/2000, Training Loss (NLML): -1053.4196, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1690/2000, Training Loss (NLML): -1053.3663, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1691/2000, Training Loss (NLML): -1053.3867, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1692/2000, Training Loss (NLML): -1053.3088, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1693/2000, Training Loss (NLML): -1053.3170, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1694/2000, Training Loss (NLML): -1053.1516, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1695/2000, Training Loss (NLML): -1053.1760, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1696/2000, Training Loss (NLML): -1052.9359, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1697/2000, Training Loss (NLML): -1053.0995, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1698/2000, Training Loss (NLML): -1052.9828, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1699/2000, Training Loss (NLML): -1053.2731, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1700/2000, Training Loss (NLML): -1053.3885, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1701/2000, Training Loss (NLML): -1053.5885, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1702/2000, Training Loss (NLML): -1053.6857, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1703/2000, Training Loss (NLML): -1053.7059, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1704/2000, Training Loss (NLML): -1053.6604, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1705/2000, Training Loss (NLML): -1053.5408, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1706/2000, Training Loss (NLML): -1053.4280, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1707/2000, Training Loss (NLML): -1053.0898, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1708/2000, Training Loss (NLML): -1052.9917, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1709/2000, Training Loss (NLML): -1052.2472, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1710/2000, Training Loss (NLML): -1052.6495, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1711/2000, Training Loss (NLML): -1052.3049, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1712/2000, Training Loss (NLML): -1053.1913, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1713/2000, Training Loss (NLML): -1053.5930, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1714/2000, Training Loss (NLML): -1053.7489, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1715/2000, Training Loss (NLML): -1053.5930, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1716/2000, Training Loss (NLML): -1053.2361, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1717/2000, Training Loss (NLML): -1053.2057, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1718/2000, Training Loss (NLML): -1052.9069, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1719/2000, Training Loss (NLML): -1053.2328, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1720/2000, Training Loss (NLML): -1053.1803, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1721/2000, Training Loss (NLML): -1053.4589, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1722/2000, Training Loss (NLML): -1053.5023, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1723/2000, Training Loss (NLML): -1053.6698, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1724/2000, Training Loss (NLML): -1053.7263, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1725/2000, Training Loss (NLML): -1053.8009, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1726/2000, Training Loss (NLML): -1053.8201, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1727/2000, Training Loss (NLML): -1053.8267, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1728/2000, Training Loss (NLML): -1053.8070, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1729/2000, Training Loss (NLML): -1053.7726, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1730/2000, Training Loss (NLML): -1053.7487, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1731/2000, Training Loss (NLML): -1053.6954, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1732/2000, Training Loss (NLML): -1053.7069, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1733/2000, Training Loss (NLML): -1053.6775, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1734/2000, Training Loss (NLML): -1053.7374, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1735/2000, Training Loss (NLML): -1053.7490, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1736/2000, Training Loss (NLML): -1053.8257, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1737/2000, Training Loss (NLML): -1053.8507, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1738/2000, Training Loss (NLML): -1053.9008, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1739/2000, Training Loss (NLML): -1053.9069, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1740/2000, Training Loss (NLML): -1053.9242, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1741/2000, Training Loss (NLML): -1053.8973, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1742/2000, Training Loss (NLML): -1053.8917, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1743/2000, Training Loss (NLML): -1053.8197, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1744/2000, Training Loss (NLML): -1053.8042, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1745/2000, Training Loss (NLML): -1053.6661, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1746/2000, Training Loss (NLML): -1053.6724, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1747/2000, Training Loss (NLML): -1053.4677, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1748/2000, Training Loss (NLML): -1053.5681, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1749/2000, Training Loss (NLML): -1053.4000, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1750/2000, Training Loss (NLML): -1053.6245, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1751/2000, Training Loss (NLML): -1053.6227, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1752/2000, Training Loss (NLML): -1053.8478, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1753/2000, Training Loss (NLML): -1053.9388, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1754/2000, Training Loss (NLML): -1054.0552, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1755/2000, Training Loss (NLML): -1054.1075, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1756/2000, Training Loss (NLML): -1054.1342, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1757/2000, Training Loss (NLML): -1054.1279, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1758/2000, Training Loss (NLML): -1054.0892, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1759/2000, Training Loss (NLML): -1054.0222, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1760/2000, Training Loss (NLML): -1053.8427, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1761/2000, Training Loss (NLML): -1053.6660, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1762/2000, Training Loss (NLML): -1053.1152, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1763/2000, Training Loss (NLML): -1053.1438, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1764/2000, Training Loss (NLML): -1052.6936, (RMSE): 0.0089\n",
      "branching dfNGP Run 1/1, Epoch 1765/2000, Training Loss (NLML): -1053.5215, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1766/2000, Training Loss (NLML): -1053.8097, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1767/2000, Training Loss (NLML): -1053.9480, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1768/2000, Training Loss (NLML): -1053.8529, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1769/2000, Training Loss (NLML): -1053.9071, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1770/2000, Training Loss (NLML): -1054.0303, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1771/2000, Training Loss (NLML): -1053.8995, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1772/2000, Training Loss (NLML): -1053.7075, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1773/2000, Training Loss (NLML): -1052.9149, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1774/2000, Training Loss (NLML): -1052.9427, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1775/2000, Training Loss (NLML): -1051.6232, (RMSE): 0.0089\n",
      "branching dfNGP Run 1/1, Epoch 1776/2000, Training Loss (NLML): -1052.9648, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1777/2000, Training Loss (NLML): -1053.0551, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1778/2000, Training Loss (NLML): -1053.8641, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1779/2000, Training Loss (NLML): -1054.0909, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1780/2000, Training Loss (NLML): -1054.0808, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1781/2000, Training Loss (NLML): -1053.8976, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1782/2000, Training Loss (NLML): -1053.3687, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1783/2000, Training Loss (NLML): -1053.5469, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1784/2000, Training Loss (NLML): -1053.3507, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1785/2000, Training Loss (NLML): -1053.8573, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1786/2000, Training Loss (NLML): -1053.9855, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1787/2000, Training Loss (NLML): -1054.2212, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1788/2000, Training Loss (NLML): -1054.2753, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1789/2000, Training Loss (NLML): -1054.3070, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1790/2000, Training Loss (NLML): -1054.2892, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1791/2000, Training Loss (NLML): -1054.3033, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1792/2000, Training Loss (NLML): -1054.3043, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1793/2000, Training Loss (NLML): -1054.3516, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1794/2000, Training Loss (NLML): -1054.3790, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1795/2000, Training Loss (NLML): -1054.4043, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1796/2000, Training Loss (NLML): -1054.3940, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1797/2000, Training Loss (NLML): -1054.3912, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1798/2000, Training Loss (NLML): -1054.3767, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1799/2000, Training Loss (NLML): -1054.4050, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1800/2000, Training Loss (NLML): -1054.4183, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1801/2000, Training Loss (NLML): -1054.4430, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1802/2000, Training Loss (NLML): -1054.4116, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1803/2000, Training Loss (NLML): -1054.3773, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1804/2000, Training Loss (NLML): -1054.2219, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1805/2000, Training Loss (NLML): -1054.1217, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1806/2000, Training Loss (NLML): -1053.6882, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1807/2000, Training Loss (NLML): -1053.7047, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1808/2000, Training Loss (NLML): -1053.0736, (RMSE): 0.0089\n",
      "branching dfNGP Run 1/1, Epoch 1809/2000, Training Loss (NLML): -1053.6677, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1810/2000, Training Loss (NLML): -1053.7202, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1811/2000, Training Loss (NLML): -1054.2897, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1812/2000, Training Loss (NLML): -1054.5438, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1813/2000, Training Loss (NLML): -1054.5961, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1814/2000, Training Loss (NLML): -1054.4635, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1815/2000, Training Loss (NLML): -1054.1199, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1816/2000, Training Loss (NLML): -1053.9297, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1817/2000, Training Loss (NLML): -1053.0837, (RMSE): 0.0089\n",
      "branching dfNGP Run 1/1, Epoch 1818/2000, Training Loss (NLML): -1053.2993, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1819/2000, Training Loss (NLML): -1052.1890, (RMSE): 0.0089\n",
      "branching dfNGP Run 1/1, Epoch 1820/2000, Training Loss (NLML): -1053.5094, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1821/2000, Training Loss (NLML): -1053.8632, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1822/2000, Training Loss (NLML): -1054.5182, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1823/2000, Training Loss (NLML): -1054.6268, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1824/2000, Training Loss (NLML): -1054.3201, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1825/2000, Training Loss (NLML): -1054.0730, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1826/2000, Training Loss (NLML): -1053.5642, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1827/2000, Training Loss (NLML): -1054.0101, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1828/2000, Training Loss (NLML): -1054.0302, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1829/2000, Training Loss (NLML): -1054.3971, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1830/2000, Training Loss (NLML): -1054.4941, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1831/2000, Training Loss (NLML): -1054.6230, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1832/2000, Training Loss (NLML): -1054.6566, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1833/2000, Training Loss (NLML): -1054.6633, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1834/2000, Training Loss (NLML): -1054.6577, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1835/2000, Training Loss (NLML): -1054.6134, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1836/2000, Training Loss (NLML): -1054.6073, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1837/2000, Training Loss (NLML): -1054.5548, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1838/2000, Training Loss (NLML): -1054.6033, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1839/2000, Training Loss (NLML): -1054.5803, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1840/2000, Training Loss (NLML): -1054.6527, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1841/2000, Training Loss (NLML): -1054.6400, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1842/2000, Training Loss (NLML): -1054.6833, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1843/2000, Training Loss (NLML): -1054.6505, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1844/2000, Training Loss (NLML): -1054.6907, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1845/2000, Training Loss (NLML): -1054.6603, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1846/2000, Training Loss (NLML): -1054.7015, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1847/2000, Training Loss (NLML): -1054.6748, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1848/2000, Training Loss (NLML): -1054.7133, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1849/2000, Training Loss (NLML): -1054.6649, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1850/2000, Training Loss (NLML): -1054.6935, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1851/2000, Training Loss (NLML): -1054.5985, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1852/2000, Training Loss (NLML): -1054.6154, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1853/2000, Training Loss (NLML): -1054.4591, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1854/2000, Training Loss (NLML): -1054.5300, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1855/2000, Training Loss (NLML): -1054.3859, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1856/2000, Training Loss (NLML): -1054.5549, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1857/2000, Training Loss (NLML): -1054.5311, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1858/2000, Training Loss (NLML): -1054.7180, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1859/2000, Training Loss (NLML): -1054.7738, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1860/2000, Training Loss (NLML): -1054.8896, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1861/2000, Training Loss (NLML): -1054.9359, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1862/2000, Training Loss (NLML): -1054.9823, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1863/2000, Training Loss (NLML): -1054.9960, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1864/2000, Training Loss (NLML): -1055.0056, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1865/2000, Training Loss (NLML): -1054.9910, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1866/2000, Training Loss (NLML): -1054.9725, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1867/2000, Training Loss (NLML): -1054.9059, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1868/2000, Training Loss (NLML): -1054.8378, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1869/2000, Training Loss (NLML): -1054.6343, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1870/2000, Training Loss (NLML): -1054.5522, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1871/2000, Training Loss (NLML): -1054.1986, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1872/2000, Training Loss (NLML): -1054.4004, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1873/2000, Training Loss (NLML): -1054.2985, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1874/2000, Training Loss (NLML): -1054.6754, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1875/2000, Training Loss (NLML): -1054.7559, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1876/2000, Training Loss (NLML): -1054.9125, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1877/2000, Training Loss (NLML): -1054.9591, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1878/2000, Training Loss (NLML): -1055.0153, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1879/2000, Training Loss (NLML): -1055.0137, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1880/2000, Training Loss (NLML): -1054.8966, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1881/2000, Training Loss (NLML): -1054.7457, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1882/2000, Training Loss (NLML): -1054.2173, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1883/2000, Training Loss (NLML): -1054.0734, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1884/2000, Training Loss (NLML): -1052.9683, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1885/2000, Training Loss (NLML): -1053.9218, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1886/2000, Training Loss (NLML): -1053.8650, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1887/2000, Training Loss (NLML): -1054.6101, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1888/2000, Training Loss (NLML): -1054.7207, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1889/2000, Training Loss (NLML): -1054.9344, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1890/2000, Training Loss (NLML): -1054.9703, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1891/2000, Training Loss (NLML): -1054.6049, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1892/2000, Training Loss (NLML): -1054.4614, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1893/2000, Training Loss (NLML): -1053.8361, (RMSE): 0.0089\n",
      "branching dfNGP Run 1/1, Epoch 1894/2000, Training Loss (NLML): -1054.3118, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1895/2000, Training Loss (NLML): -1053.9486, (RMSE): 0.0089\n",
      "branching dfNGP Run 1/1, Epoch 1896/2000, Training Loss (NLML): -1054.4805, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1897/2000, Training Loss (NLML): -1054.3043, (RMSE): 0.0089\n",
      "branching dfNGP Run 1/1, Epoch 1898/2000, Training Loss (NLML): -1054.7333, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1899/2000, Training Loss (NLML): -1054.8285, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1900/2000, Training Loss (NLML): -1055.1295, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1901/2000, Training Loss (NLML): -1055.2426, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1902/2000, Training Loss (NLML): -1055.2845, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1903/2000, Training Loss (NLML): -1055.2780, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1904/2000, Training Loss (NLML): -1055.2418, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1905/2000, Training Loss (NLML): -1055.2084, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1906/2000, Training Loss (NLML): -1055.0420, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1907/2000, Training Loss (NLML): -1054.9113, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1908/2000, Training Loss (NLML): -1054.3434, (RMSE): 0.0089\n",
      "branching dfNGP Run 1/1, Epoch 1909/2000, Training Loss (NLML): -1054.3479, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1910/2000, Training Loss (NLML): -1053.4105, (RMSE): 0.0089\n",
      "branching dfNGP Run 1/1, Epoch 1911/2000, Training Loss (NLML): -1054.2742, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1912/2000, Training Loss (NLML): -1054.3265, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1913/2000, Training Loss (NLML): -1055.0875, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1914/2000, Training Loss (NLML): -1055.3644, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1915/2000, Training Loss (NLML): -1055.3093, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1916/2000, Training Loss (NLML): -1055.0782, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1917/2000, Training Loss (NLML): -1054.6237, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1918/2000, Training Loss (NLML): -1054.7294, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1919/2000, Training Loss (NLML): -1054.4091, (RMSE): 0.0089\n",
      "branching dfNGP Run 1/1, Epoch 1920/2000, Training Loss (NLML): -1054.8137, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1921/2000, Training Loss (NLML): -1054.7281, (RMSE): 0.0089\n",
      "branching dfNGP Run 1/1, Epoch 1922/2000, Training Loss (NLML): -1055.0493, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1923/2000, Training Loss (NLML): -1055.0663, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1924/2000, Training Loss (NLML): -1055.2629, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1925/2000, Training Loss (NLML): -1055.3215, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1926/2000, Training Loss (NLML): -1055.4406, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1927/2000, Training Loss (NLML): -1055.5072, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1928/2000, Training Loss (NLML): -1055.5619, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1929/2000, Training Loss (NLML): -1055.5875, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1930/2000, Training Loss (NLML): -1055.5903, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1931/2000, Training Loss (NLML): -1055.5754, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1932/2000, Training Loss (NLML): -1055.5426, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1933/2000, Training Loss (NLML): -1055.5063, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1934/2000, Training Loss (NLML): -1055.4133, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1935/2000, Training Loss (NLML): -1055.3436, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1936/2000, Training Loss (NLML): -1055.0779, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1937/2000, Training Loss (NLML): -1055.0060, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1938/2000, Training Loss (NLML): -1054.4657, (RMSE): 0.0089\n",
      "branching dfNGP Run 1/1, Epoch 1939/2000, Training Loss (NLML): -1054.7607, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1940/2000, Training Loss (NLML): -1054.5446, (RMSE): 0.0089\n",
      "branching dfNGP Run 1/1, Epoch 1941/2000, Training Loss (NLML): -1055.1700, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1942/2000, Training Loss (NLML): -1055.4722, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1943/2000, Training Loss (NLML): -1055.6600, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1944/2000, Training Loss (NLML): -1055.6191, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1945/2000, Training Loss (NLML): -1055.4030, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1946/2000, Training Loss (NLML): -1055.2329, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1947/2000, Training Loss (NLML): -1054.7448, (RMSE): 0.0089\n",
      "branching dfNGP Run 1/1, Epoch 1948/2000, Training Loss (NLML): -1054.8188, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1949/2000, Training Loss (NLML): -1054.1219, (RMSE): 0.0089\n",
      "branching dfNGP Run 1/1, Epoch 1950/2000, Training Loss (NLML): -1054.6632, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1951/2000, Training Loss (NLML): -1054.3844, (RMSE): 0.0089\n",
      "branching dfNGP Run 1/1, Epoch 1952/2000, Training Loss (NLML): -1055.1685, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1953/2000, Training Loss (NLML): -1055.5090, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1954/2000, Training Loss (NLML): -1055.7424, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1955/2000, Training Loss (NLML): -1055.7000, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1956/2000, Training Loss (NLML): -1055.4612, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1957/2000, Training Loss (NLML): -1055.3265, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1958/2000, Training Loss (NLML): -1054.9994, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1959/2000, Training Loss (NLML): -1055.1965, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1960/2000, Training Loss (NLML): -1055.0247, (RMSE): 0.0089\n",
      "branching dfNGP Run 1/1, Epoch 1961/2000, Training Loss (NLML): -1055.3079, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1962/2000, Training Loss (NLML): -1055.2596, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1963/2000, Training Loss (NLML): -1055.4769, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1964/2000, Training Loss (NLML): -1055.4873, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1965/2000, Training Loss (NLML): -1055.5964, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1966/2000, Training Loss (NLML): -1055.5791, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1967/2000, Training Loss (NLML): -1055.5464, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1968/2000, Training Loss (NLML): -1055.4675, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1969/2000, Training Loss (NLML): -1055.3109, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1970/2000, Training Loss (NLML): -1055.3231, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1971/2000, Training Loss (NLML): -1055.2570, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1972/2000, Training Loss (NLML): -1055.5225, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1973/2000, Training Loss (NLML): -1055.6879, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1974/2000, Training Loss (NLML): -1055.8582, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1975/2000, Training Loss (NLML): -1055.8824, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1976/2000, Training Loss (NLML): -1055.8241, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1977/2000, Training Loss (NLML): -1055.6898, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1978/2000, Training Loss (NLML): -1055.5636, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1979/2000, Training Loss (NLML): -1055.4258, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1980/2000, Training Loss (NLML): -1055.3417, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1981/2000, Training Loss (NLML): -1055.4382, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1982/2000, Training Loss (NLML): -1055.6224, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1983/2000, Training Loss (NLML): -1055.8875, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1984/2000, Training Loss (NLML): -1055.9449, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1985/2000, Training Loss (NLML): -1055.8440, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1986/2000, Training Loss (NLML): -1055.6951, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1987/2000, Training Loss (NLML): -1055.7683, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1988/2000, Training Loss (NLML): -1055.8715, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1989/2000, Training Loss (NLML): -1055.9934, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1990/2000, Training Loss (NLML): -1055.9980, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1991/2000, Training Loss (NLML): -1055.9398, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1992/2000, Training Loss (NLML): -1055.8199, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1993/2000, Training Loss (NLML): -1055.7955, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1994/2000, Training Loss (NLML): -1055.7024, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1995/2000, Training Loss (NLML): -1055.7546, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1996/2000, Training Loss (NLML): -1055.5428, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1997/2000, Training Loss (NLML): -1055.4915, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 1998/2000, Training Loss (NLML): -1054.9440, (RMSE): 0.0089\n",
      "branching dfNGP Run 1/1, Epoch 1999/2000, Training Loss (NLML): -1055.2444, (RMSE): 0.0090\n",
      "branching dfNGP Run 1/1, Epoch 2000/2000, Training Loss (NLML): -1055.0675, (RMSE): 0.0089\n",
      "Test NLL (full): nan, Jitter: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# SIMULATED DATA EXPERIMENTS\n",
    "# RUN WITH python run_sim_experiments_dfNGP.py\n",
    "# \n",
    "#       ooooooooooooooooooooooooooooooooooooo\n",
    "#      8                                .d88\n",
    "#      8  oooooooooooooooooooooooooooood8888\n",
    "#      8  8888888888888888888888888P\"   8888    oooooooooooooooo\n",
    "#      8  8888888888888888888888P\"      8888    8              8\n",
    "#      8  8888888888888888888P\"         8888    8             d8\n",
    "#      8  8888888888888888P\"            8888    8            d88\n",
    "#      8  8888888888888P\"               8888    8           d888\n",
    "#      8  8888888888P\"                  8888    8          d8888\n",
    "#      8  8888888P\"                     8888    8         d88888\n",
    "#      8  8888P\"                        8888    8        d888888\n",
    "#      8  8888oooooooooooooooooooooocgmm8888    8       d8888888\n",
    "#      8 .od88888888888888888888888888888888    8      d88888888\n",
    "#      8888888888888888888888888888888888888    8     d888888888\n",
    "#                                               8    d8888888888\n",
    "#         ooooooooooooooooooooooooooooooo       8   d88888888888\n",
    "#        d                       ...oood8b      8  d888888888888\n",
    "#       d              ...oood888888888888b     8 d8888888888888\n",
    "#      d     ...oood88888888888888888888888b    8d88888888888888\n",
    "#     dood8888888888888888888888888888888888b\n",
    "#\n",
    "#\n",
    "# This artwork is a visual reminder that this script is for the sim experiments.\n",
    "\n",
    "model_name = \"dfNGP\"\n",
    "\n",
    "# import configs to we can access the hypers with getattr\n",
    "import configs\n",
    "from configs import PATIENCE, MAX_NUM_EPOCHS, NUM_RUNS, WEIGHT_DECAY\n",
    "# also import x_test grid size and std noise for training data\n",
    "from configs import N_SIDE, STD_GAUSSIAN_NOISE\n",
    "\n",
    "# Reiterating import for visibility\n",
    "MAX_NUM_EPOCHS = MAX_NUM_EPOCHS\n",
    "NUM_RUNS = 1\n",
    "WEIGHT_DECAY = WEIGHT_DECAY\n",
    "PATIENCE = PATIENCE\n",
    "\n",
    "# assign model-specific variable\n",
    "MODEL_LEARNING_RATE = getattr(configs, f\"{model_name}_SIM_LEARNING_RATE\")\n",
    "MODEL_SIM_RESULTS_DIR = getattr(configs, f\"{model_name}_SIM_RESULTS_DIR\")\n",
    "import os\n",
    "os.makedirs(MODEL_SIM_RESULTS_DIR, exist_ok = True)\n",
    "\n",
    "# imports for probabilistic models\n",
    "if model_name in [\"GP\", \"dfGP\", \"dfNGP\"]:\n",
    "    from GP_models import GP_predict\n",
    "    from metrics import compute_NLL_sparse, compute_NLL_full\n",
    "    from configs import L_RANGE, SIGMA_N_RANGE, GP_PATIENCE\n",
    "    # overwrite with GP_PATIENCE\n",
    "    PATIENCE = GP_PATIENCE\n",
    "    if model_name == \"dfGP\":\n",
    "        from configs import SIGMA_F_RANGE\n",
    "    if model_name == \"dfNGP\":\n",
    "        # NOTE: This reflects that we have a mean model and sigma f for the residuals is smaller\n",
    "        from configs import SIGMA_F_RESIDUAL_MODEL_RANGE\n",
    "\n",
    "# for all models with NN components train on batches\n",
    "if model_name in [\"dfNGP\", \"dfNN\", \"PINN\"]:\n",
    "    from configs import BATCH_SIZE\n",
    "\n",
    "if model_name in [\"dfNGP\", \"dfNN\"]:\n",
    "    from NN_models import dfNN\n",
    "\n",
    "# universals \n",
    "from metrics import compute_RMSE, compute_MAE, compute_divergence_field\n",
    "\n",
    "# basics\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# from codecarbon import EmissionsTracker\n",
    "\n",
    "# utilitarian\n",
    "from utils import set_seed, make_grid\n",
    "# reproducibility\n",
    "set_seed(42)\n",
    "import gc\n",
    "\n",
    "# setting device to GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# overwrite if needed: # device = 'cpu'\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "### START TIMING ###\n",
    "import time\n",
    "start_time = time.time()  # Start timing after imports\n",
    "\n",
    "### START TRACKING EXPERIMENT EMISSIONS ###\n",
    "# tracker = EmissionsTracker(project_name = \"dfNGP_simulation_experiments\", output_dir = MODEL_SIM_RESULTS_DIR)\n",
    "# tracker.start()\n",
    "\n",
    "### SIMULATION ###\n",
    "# Import all simulation functions\n",
    "from simulate import (\n",
    "    simulate_detailed_branching,\n",
    "    # simulate_detailed_convergence,\n",
    "    simulate_detailed_curve,\n",
    "    simulate_detailed_deflection,\n",
    "    simulate_detailed_edge,\n",
    "    simulate_detailed_ridges,\n",
    ")\n",
    "\n",
    "# Define simulations as a dictionary with names as keys to function objects\n",
    "# alphabectic order here\n",
    "simulations = {\n",
    "    \"branching\": simulate_detailed_branching,\n",
    "}\n",
    "\n",
    "########################\n",
    "### x_train & x_test ###\n",
    "########################\n",
    "\n",
    "# Load training inputs (once for all simulations)\n",
    "x_train = torch.load(\"data/sim_data/x_train_lines_discretised_0to1.pt\", weights_only = False).float()\n",
    "\n",
    "# Generate x_test (long) once for all simulations\n",
    "_, x_test = make_grid(N_SIDE)\n",
    "# x_test is long format (N_SIDE ** 2, 2)\n",
    "\n",
    "#################################\n",
    "### LOOP 1 - over SIMULATIONS ###\n",
    "#################################\n",
    "\n",
    "# Make y_train_dict: Iterate over all simulation functions\n",
    "for sim_name, sim_func in simulations.items():\n",
    "\n",
    "    ########################\n",
    "    ### y_train & y_test ###\n",
    "    ########################\n",
    "\n",
    "    # Generate training observations\n",
    "    # NOTE: sim_func() needs to be on CPU, so we move x_train to CPU\n",
    "    y_train = sim_func(x_train.cpu()).to(device)\n",
    "    y_test = sim_func(x_test.cpu()).to(device)\n",
    "    \n",
    "    # x_test = x_test.to(device).requires_grad_(True)\n",
    "    x_test = x_test.to(device)\n",
    "    # x_train = x_train.to(device).requires_grad_(True)\n",
    "    x_train = x_train.to(device)\n",
    "\n",
    "    # Print details\n",
    "    print(f\"=== {sim_name.upper()} ===\")\n",
    "    print(f\"Training inputs shape: {x_train.shape}\")\n",
    "    print(f\"Training observations shape: {y_train.shape}\")\n",
    "    print(f\"Training inputs dtype: {x_train.dtype}\")\n",
    "    print(f\"Training inputs device: {y_train.device}\")\n",
    "    print(f\"Training observations device: {y_train.device}\")\n",
    "    print()\n",
    "\n",
    "    # Print details\n",
    "    print(f\"=== {sim_name.upper()} ===\")\n",
    "    print(f\"Test inputs shape: {x_test.shape}\")\n",
    "    print(f\"Test observations shape: {y_test.shape}\")\n",
    "    print(f\"Test inputs dtype: {x_test.dtype}\")\n",
    "    print(f\"Test inputs device: {x_test.device}\")\n",
    "    print(f\"Test observations device: {y_test.device}\")\n",
    "    print()\n",
    "\n",
    "    # NOTE: This is different to the real data experiments\n",
    "    # calculate the mean magnitude of the test data as we use this to scale the noise\n",
    "    sim_mean_magnitude_for_noise = torch.norm(y_test, dim = -1).mean().to(device)\n",
    "    sim_noise = STD_GAUSSIAN_NOISE * sim_mean_magnitude_for_noise\n",
    "\n",
    "    # Store metrics for the simulation (used for *metrics_summary* report and *metrics_per_run*)\n",
    "    simulation_results = [] \n",
    "\n",
    "    ##################################\n",
    "    ### LOOP 2 - over training run ###\n",
    "    ##################################\n",
    "    \n",
    "    # NOTE: GPs and hense dfNGPs don't train on batches, use full data\n",
    "\n",
    "    for run in range(NUM_RUNS):\n",
    "\n",
    "        print(f\"\\n--- Training Run {run + 1}/{NUM_RUNS} ---\")\n",
    "\n",
    "        # initialise trainable dfGP params\n",
    "        sigma_n = nn.Parameter(torch.empty(1, device = device).uniform_( * SIGMA_N_RANGE))\n",
    "        sigma_f = nn.Parameter(torch.empty(1, device = device).uniform_( * SIGMA_F_RESIDUAL_MODEL_RANGE))\n",
    "        l = nn.Parameter(torch.empty(2, device = device).uniform_( * L_RANGE))\n",
    "\n",
    "        # For every run initialise a (new) mean model\n",
    "        dfNN_mean_model = dfNN().to(device)\n",
    "\n",
    "        # NOTE: We don't need a criterion either\n",
    "\n",
    "        # AdamW as optimizer for some regularisation/weight decay\n",
    "        # HACK: create two param groups: one for the dfNN and one for the hypers\n",
    "        optimizer = optim.AdamW([\n",
    "            {\"params\": dfNN_mean_model.parameters(), \"weight_decay\": WEIGHT_DECAY, \"lr\": (0.1 * MODEL_LEARNING_RATE)},\n",
    "            {\"params\": [sigma_n, sigma_f, l], \"weight_decay\": WEIGHT_DECAY, \"lr\": MODEL_LEARNING_RATE},\n",
    "            ])\n",
    "\n",
    "        # _________________\n",
    "        # BEFORE EPOCH LOOP\n",
    "        \n",
    "        # Export the convergence just for first run only\n",
    "        if run == 0:\n",
    "            # initialise tensors to store losses over epochs (for convergence plot)\n",
    "            train_losses_NLML_over_epochs = torch.zeros(MAX_NUM_EPOCHS) # objective\n",
    "            train_losses_RMSE_over_epochs = torch.zeros(MAX_NUM_EPOCHS) # by-product\n",
    "            # monitor performance transfer to test (only RMSE easy to calc without covar)\n",
    "            test_losses_RMSE_over_epochs = torch.zeros(MAX_NUM_EPOCHS)\n",
    "\n",
    "            sigma_n_over_epochs = torch.zeros(MAX_NUM_EPOCHS)\n",
    "            sigma_f_over_epochs = torch.zeros(MAX_NUM_EPOCHS)\n",
    "            l1_over_epochs = torch.zeros(MAX_NUM_EPOCHS)\n",
    "            l2_over_epochs = torch.zeros(MAX_NUM_EPOCHS)\n",
    "\n",
    "        # Early stopping variables\n",
    "        best_loss = float('inf')\n",
    "        # counter starts at 0\n",
    "        epochs_no_improve = 0\n",
    "\n",
    "        # NOTE: This is a difference to the real data experiments\n",
    "        # Additive noise model: independent Gaussian noise\n",
    "        # For every run we have a FIXED NOISY TARGET. Draw from standard normal with appropriate std\n",
    "        y_train_noisy = y_train + (torch.randn(y_train.shape, device = device) * sim_noise)\n",
    "\n",
    "        ############################\n",
    "        ### LOOP 3 - over EPOCHS ###\n",
    "        ############################\n",
    "        \n",
    "        print(\"\\nStart Training\")\n",
    "\n",
    "        for epoch in range(MAX_NUM_EPOCHS):\n",
    "\n",
    "            # Assure model is in training mode\n",
    "            dfNN_mean_model.train()\n",
    "\n",
    "            # For Run 1 we save a bunch of metrics and update, while for the rest we only update\n",
    "            if run == 0:\n",
    "                mean_pred_train, _, lml_train = GP_predict(\n",
    "                        x_train,\n",
    "                        y_train_noisy,\n",
    "                        x_train, # predict training data\n",
    "                        [sigma_n, sigma_f, l], # list of (initial) hypers\n",
    "                        mean_func = dfNN_mean_model, # dfNN as mean function\n",
    "                        divergence_free_bool = True) # ensures we use a df kernel\n",
    "\n",
    "                # Compute test loss for loss convergence plot\n",
    "                mean_pred_test, _, _ = GP_predict(\n",
    "                        x_train,\n",
    "                        y_train_noisy,\n",
    "                        x_test.to(device), # have predictions for training data again\n",
    "                        # HACK: This is rather an eval, so we use detached hypers to avoid the computational tree\n",
    "                        [sigma_n.detach().clone(), sigma_f.detach().clone(), l.detach().clone()], # list of (initial) hypers\n",
    "                        mean_func = dfNN_mean_model, # dfNN as mean function\n",
    "                        divergence_free_bool = True) # ensures we use a df kernel\n",
    "                \n",
    "                # UPDATE HYPERS (after test loss is computed to use same model)\n",
    "                optimizer.zero_grad() # don't accumulate gradients\n",
    "                # negative for NLML. loss is always on train\n",
    "                loss = - lml_train\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # NOTE: it is important to detach here \n",
    "                train_RMSE = compute_RMSE(y_train.detach(), mean_pred_train.detach())\n",
    "                test_RMSE = compute_RMSE(y_test.detach(), mean_pred_test.detach())\n",
    "\n",
    "                # Save losses for convergence plot\n",
    "                train_losses_NLML_over_epochs[epoch] = - lml_train\n",
    "                train_losses_RMSE_over_epochs[epoch] = train_RMSE\n",
    "                # NOTE: lml is always just given training data. There is no TEST NLML\n",
    "                test_losses_RMSE_over_epochs[epoch] = test_RMSE\n",
    "\n",
    "                # Save evolution of hyprs for convergence plot\n",
    "                sigma_n_over_epochs[epoch] = sigma_n[0]\n",
    "                sigma_f_over_epochs[epoch] = sigma_f[0]\n",
    "                l1_over_epochs[epoch] = torch.exp(l[0]) # export effective not raw lengthscale\n",
    "                l2_over_epochs[epoch] = torch.exp(l[1])\n",
    "\n",
    "                print(f\"{sim_name} {model_name} Run {run + 1}/{NUM_RUNS}, Epoch {epoch + 1}/{MAX_NUM_EPOCHS}, Training Loss (NLML): {loss:.4f}, (RMSE): {train_RMSE:.4f}\")\n",
    "\n",
    "                # delete after printing and saving\n",
    "                # NOTE: keep loss for early stopping check\n",
    "                del mean_pred_train, mean_pred_test, lml_train, train_RMSE, test_RMSE\n",
    "                \n",
    "                # Free up memory every 20 epochs\n",
    "                if epoch % 20 == 0:\n",
    "                    gc.collect() and torch.cuda.empty_cache()\n",
    "            \n",
    "             # For all runs after the first we run a minimal version using only lml_train\n",
    "            else:\n",
    "\n",
    "                # NOTE: We can use x_train[0:2] since the predictions doesn;t matter and we only care about lml_train\n",
    "                _, _, lml_train = GP_predict(\n",
    "                        x_train,\n",
    "                        y_train_noisy,\n",
    "                        x_train[0:2], # predictions don't matter and we output lml_train already\n",
    "                        [sigma_n, sigma_f, l], # list of (initial) hypers\n",
    "                        mean_func = dfNN_mean_model, # dfNN as mean function\n",
    "                        divergence_free_bool = True) # ensures we use a df kernel\n",
    "                \n",
    "                # UPDATE HYPERS (after test loss is computed to use same model)\n",
    "                optimizer.zero_grad() # don't accumulate gradients\n",
    "                # negative for NLML\n",
    "                loss = - lml_train\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # After run 1 we only print lml, nothing else\n",
    "                print(f\"{sim_name} {model_name} Run {run + 1}/{NUM_RUNS}, Epoch {epoch + 1}/{MAX_NUM_EPOCHS}, Training Loss (NLML): {loss:.4f}\")\n",
    "\n",
    "                # NOTE: keep loss for early stopping check, del lml_train\n",
    "                del lml_train\n",
    "                \n",
    "                # Free up memory every 20 epochs\n",
    "                if epoch % 20 == 0:\n",
    "                    gc.collect() and torch.cuda.empty_cache()\n",
    "\n",
    "            # EVERY EPOCH: Early stopping check\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                # reset counter if loss improves\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "\n",
    "            if epochs_no_improve >= PATIENCE:\n",
    "                print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
    "                # exit epoch loop\n",
    "                break\n",
    "\n",
    "        ##############################\n",
    "        ### END LOOP 3 over EPOCHS ###\n",
    "        ##############################\n",
    "\n",
    "        # for every run...\n",
    "        #######################################################\n",
    "        ### EVALUATE after all training for RUN is finished ###\n",
    "        #######################################################\n",
    "\n",
    "        # Evaluate the trained model after all epochs are finished or early stopping was triggered\n",
    "        # NOTE: Detach tuned hyperparameters from the computational graph\n",
    "        best_sigma_n = sigma_n.detach().clone()\n",
    "        best_sigma_f = sigma_f.detach().clone()\n",
    "        best_l = l.detach().clone()\n",
    "\n",
    "        # Need gradients for autograd divergence: We clone and detach\n",
    "        x_test_grad = x_test.to(device).clone().requires_grad_(True)\n",
    "\n",
    "        mean_pred_test, covar_pred_test, _ = GP_predict(\n",
    "            x_train,\n",
    "            y_train, # NOTE: use original y_train, not noisy\n",
    "            x_test_grad,\n",
    "            [best_sigma_n, best_sigma_f, best_l], # list of (initial) hypers\n",
    "            mean_func = dfNN_mean_model, # dfNN as mean function\n",
    "            divergence_free_bool = True) # ensures we use a df kernel\n",
    "        \n",
    "        dfNGP_test_full_NLL, dfNGP_test_jitter = compute_NLL_full(y_test, mean_pred_test, covar_pred_test)\n",
    "        print(f\"Test NLL (full): {dfNGP_test_full_NLL:.4f}, Jitter: {dfNGP_test_jitter:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d814127f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpytorch # 3.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc56da3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test NLPD: 4.7001\n"
     ]
    }
   ],
   "source": [
    "# NLL == negative log predictive density NLPD\n",
    "# GPytorch uses block format, flat mean if we don't have batches\n",
    "# Flatten the mean for the MultivariateNormal distribution\n",
    "mean_pred_test_blockflat = mean_pred_test.T.reshape(-1)\n",
    "pred_dist_test = gpytorch.distributions.MultivariateNormal(\n",
    "    mean_pred_test_blockflat, \n",
    "    covar_pred_test + torch.eye(covar_pred_test.shape[0], device = device) * 1e-6)\n",
    "\n",
    "nlpd = gpytorch.metrics.negative_log_predictive_density(pred_dist_test, y_test.T.reshape(-1))\n",
    "print(f\"Test NLPD: {nlpd:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdf49cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Quantile Coverage Error: 0.0412\n"
     ]
    }
   ],
   "source": [
    "pred_dist_test = gpytorch.distributions.MultivariateNormal(\n",
    "    mean_pred_test_blockflat, \n",
    "    covar_pred_test + (torch.eye(covar_pred_test.shape[0], device = device) * 1e-6))\n",
    "\n",
    "quantile_coverage_error = gpytorch.metrics.quantile_coverage_error(\n",
    "    pred_dist_test, \n",
    "    y_test.T.reshape(-1), \n",
    "    quantile = 95) # NOTE: not 0.95 but 95 importantly\n",
    "\n",
    "# QuantileCoverageError = ObservedCoverage  ExpectedCoverage\n",
    "# e.g. for quantile = 95, 0.95 is the worst that can be achived\n",
    "print(f\"Test Quantile Coverage Error: {quantile_coverage_error:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4105811d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0.0018,     0.0013,     0.0005,     0.0001,     0.0001,     0.0001,\n",
       "            0.0001,     0.0001,     0.0001,     0.0001,     0.0001,     0.0001,\n",
       "            0.0001,     0.0001,     0.0004,     0.0007,     0.0010,     0.0013,\n",
       "            0.0015,     0.0017,     0.0015,     0.0009,     0.0002,     0.0001,\n",
       "            0.0001,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
       "            0.0001,     0.0001,     0.0000,     0.0000,     0.0001,     0.0001,\n",
       "            0.0003,     0.0005,     0.0007,     0.0009,     0.0013,     0.0005,\n",
       "            0.0001,     0.0001,     0.0001,     0.0000,     0.0000,     0.0000,\n",
       "            0.0000,     0.0001,     0.0001,     0.0001,     0.0000,     0.0000,\n",
       "            0.0000,     0.0001,     0.0001,     0.0001,     0.0001,     0.0002,\n",
       "            0.0009,     0.0002,     0.0001,     0.0001,     0.0000,     0.0000,\n",
       "            0.0000,     0.0000,     0.0001,     0.0005,     0.0006,     0.0004,\n",
       "            0.0001,     0.0000,     0.0000,     0.0001,     0.0001,     0.0001,\n",
       "            0.0001,     0.0001,     0.0006,     0.0001,     0.0001,     0.0000,\n",
       "            0.0000,     0.0000,     0.0001,     0.0000,     0.0004,     0.0011,\n",
       "            0.0013,     0.0010,     0.0003,     0.0000,     0.0001,     0.0001,\n",
       "            0.0001,     0.0001,     0.0001,     0.0001,     0.0002,     0.0001,\n",
       "            0.0001,     0.0000,     0.0000,     0.0000,     0.0000,     0.0002,\n",
       "            0.0008,     0.0015,     0.0017,     0.0014,     0.0005,     0.0001,\n",
       "            0.0001,     0.0003,     0.0006,     0.0006,     0.0004,     0.0003,\n",
       "            0.0001,     0.0001,     0.0000,     0.0000,     0.0000,     0.0001,\n",
       "            0.0001,     0.0004,     0.0012,     0.0016,     0.0017,     0.0014,\n",
       "            0.0007,     0.0001,     0.0001,     0.0004,     0.0011,     0.0014,\n",
       "            0.0012,     0.0011,     0.0001,     0.0001,     0.0000,     0.0000,\n",
       "            0.0001,     0.0000,     0.0002,     0.0007,     0.0013,     0.0017,\n",
       "            0.0019,     0.0016,     0.0009,     0.0001,     0.0001,     0.0003,\n",
       "            0.0011,     0.0017,     0.0017,     0.0017,     0.0001,     0.0000,\n",
       "            0.0000,     0.0000,     0.0001,     0.0001,     0.0003,     0.0007,\n",
       "            0.0012,     0.0019,     0.0021,     0.0019,     0.0010,     0.0002,\n",
       "            0.0001,     0.0003,     0.0010,     0.0016,     0.0017,     0.0018,\n",
       "            0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0001,\n",
       "            0.0002,     0.0004,     0.0010,     0.0018,     0.0022,     0.0021,\n",
       "            0.0012,     0.0003,     0.0001,     0.0003,     0.0011,     0.0017,\n",
       "            0.0018,     0.0018,     0.0000,     0.0000,     0.0000,     0.0000,\n",
       "            0.0000,     0.0001,     0.0001,     0.0005,     0.0011,     0.0016,\n",
       "            0.0019,     0.0019,     0.0013,     0.0003,     0.0001,     0.0003,\n",
       "            0.0013,     0.0021,     0.0022,     0.0021,     0.0001,     0.0000,\n",
       "            0.0000,     0.0000,     0.0001,     0.0001,     0.0003,     0.0008,\n",
       "            0.0014,     0.0017,     0.0017,     0.0017,     0.0012,     0.0004,\n",
       "            0.0001,     0.0002,     0.0013,     0.0023,     0.0025,     0.0025,\n",
       "            0.0001,     0.0000,     0.0000,     0.0000,     0.0000,     0.0001,\n",
       "            0.0002,     0.0007,     0.0013,     0.0017,     0.0017,     0.0016,\n",
       "            0.0011,     0.0003,     0.0001,     0.0002,     0.0011,     0.0022,\n",
       "            0.0027,     0.0027,     0.0001,     0.0000,     0.0000,     0.0001,\n",
       "            0.0001,     0.0001,     0.0001,     0.0002,     0.0006,     0.0011,\n",
       "            0.0015,     0.0016,     0.0011,     0.0003,     0.0001,     0.0001,\n",
       "            0.0009,     0.0021,     0.0027,     0.0028,     0.0001,     0.0000,\n",
       "            0.0000,     0.0001,     0.0001,     0.0001,     0.0001,     0.0001,\n",
       "            0.0001,     0.0004,     0.0008,     0.0013,     0.0012,     0.0004,\n",
       "            0.0001,     0.0001,     0.0008,     0.0019,     0.0026,     0.0028,\n",
       "            0.0001,     0.0001,     0.0001,     0.0001,     0.0001,     0.0001,\n",
       "            0.0002,     0.0002,     0.0001,     0.0001,     0.0002,     0.0005,\n",
       "            0.0009,     0.0004,     0.0001,     0.0001,     0.0006,     0.0016,\n",
       "            0.0024,     0.0028,     0.0001,     0.0001,     0.0003,     0.0003,\n",
       "            0.0001,     0.0001,     0.0001,     0.0002,     0.0002,     0.0002,\n",
       "            0.0001,     0.0001,     0.0003,     0.0003,     0.0001,     0.0001,\n",
       "            0.0004,     0.0014,     0.0023,     0.0027,     0.0001,     0.0002,\n",
       "            0.0008,     0.0010,     0.0006,     0.0002,     0.0001,     0.0001,\n",
       "            0.0002,     0.0002,     0.0003,     0.0003,     0.0001,     0.0001,\n",
       "            0.0001,     0.0001,     0.0003,     0.0013,     0.0023,     0.0027,\n",
       "            0.0002,     0.0006,     0.0013,     0.0015,     0.0014,     0.0009,\n",
       "            0.0004,     0.0001,     0.0001,     0.0002,     0.0004,     0.0008,\n",
       "            0.0007,     0.0003,     0.0001,     0.0000,     0.0002,     0.0012,\n",
       "            0.0023,     0.0028,     0.0008,     0.0012,     0.0016,     0.0017,\n",
       "            0.0017,     0.0016,     0.0012,     0.0006,     0.0002,     0.0002,\n",
       "            0.0006,     0.0012,     0.0014,     0.0009,     0.0003,     0.0001,\n",
       "            0.0002,     0.0012,     0.0024,     0.0028,     0.0064,     0.0046,\n",
       "            0.0019,     0.0001,     0.0002,     0.0001,     0.0001,     0.0001,\n",
       "            0.0001,     0.0001,     0.0001,     0.0001,     0.0001,     0.0001,\n",
       "            0.0003,     0.0005,     0.0008,     0.0012,     0.0017,     0.0029,\n",
       "            0.0056,     0.0034,     0.0007,     0.0001,     0.0002,     0.0001,\n",
       "            0.0000,     0.0000,     0.0000,     0.0001,     0.0001,     0.0001,\n",
       "            0.0001,     0.0000,     0.0001,     0.0001,     0.0002,     0.0003,\n",
       "            0.0004,     0.0011,     0.0047,     0.0019,     0.0001,     0.0002,\n",
       "            0.0001,     0.0001,     0.0000,     0.0001,     0.0001,     0.0001,\n",
       "            0.0001,     0.0001,     0.0001,     0.0000,     0.0001,     0.0001,\n",
       "            0.0001,     0.0001,     0.0001,     0.0003,     0.0035,     0.0007,\n",
       "            0.0001,     0.0002,     0.0001,     0.0001,     0.0001,     0.0001,\n",
       "            0.0003,     0.0003,     0.0004,     0.0003,     0.0002,     0.0000,\n",
       "            0.0001,     0.0001,     0.0001,     0.0001,     0.0001,     0.0001,\n",
       "            0.0019,     0.0001,     0.0002,     0.0001,     0.0001,     0.0001,\n",
       "            0.0001,     0.0001,     0.0010,     0.0010,     0.0013,     0.0010,\n",
       "            0.0007,     0.0001,     0.0002,     0.0001,     0.0001,     0.0001,\n",
       "            0.0001,     0.0001,     0.0007,     0.0001,     0.0002,     0.0001,\n",
       "            0.0001,     0.0001,     0.0001,     0.0005,     0.0023,     0.0024,\n",
       "            0.0031,     0.0025,     0.0019,     0.0003,     0.0004,     0.0004,\n",
       "            0.0004,     0.0004,     0.0003,     0.0003,     0.0002,     0.0002,\n",
       "            0.0001,     0.0001,     0.0001,     0.0002,     0.0001,     0.0016,\n",
       "            0.0039,     0.0045,     0.0054,     0.0043,     0.0037,     0.0008,\n",
       "            0.0006,     0.0012,     0.0012,     0.0013,     0.0011,     0.0014,\n",
       "            0.0002,     0.0001,     0.0001,     0.0001,     0.0001,     0.0001,\n",
       "            0.0005,     0.0030,     0.0051,     0.0065,     0.0075,     0.0058,\n",
       "            0.0052,     0.0018,     0.0007,     0.0027,     0.0026,     0.0032,\n",
       "            0.0031,     0.0038,     0.0002,     0.0001,     0.0001,     0.0001,\n",
       "            0.0001,     0.0001,     0.0013,     0.0041,     0.0060,     0.0078,\n",
       "            0.0087,     0.0068,     0.0060,     0.0030,     0.0005,     0.0043,\n",
       "            0.0041,     0.0054,     0.0058,     0.0066,     0.0001,     0.0000,\n",
       "            0.0001,     0.0001,     0.0001,     0.0003,     0.0020,     0.0044,\n",
       "            0.0062,     0.0082,     0.0091,     0.0074,     0.0062,     0.0042,\n",
       "            0.0002,     0.0051,     0.0053,     0.0070,     0.0081,     0.0087,\n",
       "            0.0001,     0.0000,     0.0001,     0.0001,     0.0001,     0.0004,\n",
       "            0.0018,     0.0037,     0.0055,     0.0073,     0.0084,     0.0073,\n",
       "            0.0059,     0.0051,     0.0001,     0.0051,     0.0060,     0.0077,\n",
       "            0.0094,     0.0098,     0.0001,     0.0001,     0.0001,     0.0001,\n",
       "            0.0001,     0.0003,     0.0010,     0.0023,     0.0037,     0.0053,\n",
       "            0.0067,     0.0065,     0.0052,     0.0053,     0.0002,     0.0044,\n",
       "            0.0063,     0.0077,     0.0098,     0.0102,     0.0001,     0.0001,\n",
       "            0.0001,     0.0000,     0.0001,     0.0001,     0.0003,     0.0009,\n",
       "            0.0017,     0.0030,     0.0044,     0.0050,     0.0040,     0.0047,\n",
       "            0.0006,     0.0034,     0.0064,     0.0073,     0.0097,     0.0103,\n",
       "            0.0002,     0.0001,     0.0001,     0.0001,     0.0001,     0.0001,\n",
       "            0.0001,     0.0002,     0.0005,     0.0012,     0.0022,     0.0031,\n",
       "            0.0025,     0.0033,     0.0009,     0.0022,     0.0063,     0.0069,\n",
       "            0.0094,     0.0103,     0.0005,     0.0001,     0.0000,     0.0001,\n",
       "            0.0001,     0.0002,     0.0001,     0.0001,     0.0001,     0.0003,\n",
       "            0.0007,     0.0014,     0.0013,     0.0017,     0.0010,     0.0012,\n",
       "            0.0058,     0.0065,     0.0089,     0.0102,     0.0009,     0.0001,\n",
       "            0.0001,     0.0001,     0.0001,     0.0002,     0.0002,     0.0002,\n",
       "            0.0001,     0.0001,     0.0002,     0.0004,     0.0006,     0.0006,\n",
       "            0.0007,     0.0005,     0.0048,     0.0061,     0.0083,     0.0099,\n",
       "            0.0008,     0.0001,     0.0003,     0.0002,     0.0002,     0.0001,\n",
       "            0.0002,     0.0002,     0.0002,     0.0002,     0.0001,     0.0001,\n",
       "            0.0002,     0.0002,     0.0003,     0.0002,     0.0035,     0.0057,\n",
       "            0.0076,     0.0095,     0.0003,     0.0005,     0.0008,     0.0007,\n",
       "            0.0006,     0.0002,     0.0001,     0.0001,     0.0002,     0.0003,\n",
       "            0.0003,     0.0002,     0.0001,     0.0001,     0.0002,     0.0001,\n",
       "            0.0020,     0.0052,     0.0070,     0.0089,     0.0002,     0.0018,\n",
       "            0.0020,     0.0020,     0.0019,     0.0010,     0.0004,     0.0001,\n",
       "            0.0001,     0.0003,     0.0008,     0.0005,     0.0005,     0.0002,\n",
       "            0.0001,     0.0000,     0.0008,     0.0046,     0.0065,     0.0083,\n",
       "            0.0015,     0.0036,     0.0038,     0.0042,     0.0039,     0.0026,\n",
       "            0.0014,     0.0006,     0.0002,     0.0002,     0.0018,     0.0013,\n",
       "            0.0016,     0.0010,     0.0004,     0.0001,     0.0003,     0.0044,\n",
       "            0.0064,     0.0081], device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_printoptions(precision = 4, sci_mode = False)\n",
    "torch.diag(covar_pred_test) + 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc9e18d",
   "metadata": {},
   "source": [
    "25 k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff30e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfNGP_train_full_NLL, dfNGP_train_jitter = compute_NLL_full(y_train, mean_pred_train, covar_pred_train)\n",
    "dfNGP_test_full_NLL, dfNGP_test_jitter = compute_NLL_full(y_test, mean_pred_test, covar_pred_test)\n",
    "print(f\"Test NLL (full): {dfNGP_test_full_NLL:.4f}, Jitter: {dfNGP_test_jitter:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f505b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNGP_test_full_NLL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54db1646",
   "metadata": {},
   "source": [
    "# Diagnostics\n",
    "\n",
    "branching dfNGP Run 1/1, Epoch 2000/2000, Training Loss (NLML): -1059.0770, (RMSE): 0.0090\n",
    "\n",
    "## (1) Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d484f7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean = dfNN_mean_model(x_test)\n",
    "# test_residual = y_test - mean \n",
    "\n",
    "avg_flow = y_train.mean(dim = 0)\n",
    "new_mean = torch.ones_like(x_test) * avg_flow\n",
    "new_residual = y_test - new_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a6f941",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualise import visualise_v_quiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1270296",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_v_quiver(test_residual, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce27e881",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_v_quiver(new_residual, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b87dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_v_quiver(mean_pred_test, x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600dad90",
   "metadata": {},
   "source": [
    "# Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec19fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_v_quiver(mean_pred_test, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d721bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_v_quiver(y_test, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a593ff16",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_flat = torch.concat([y_test[:, 0], y_test[:, 1]], dim = 0).unsqueeze(-1)  # Shape: (2 * N, 1)\n",
    "y_mean_pred_flat = torch.concat([mean_pred_test[:, 0], mean_pred_test[:, 1]], dim = 0).unsqueeze(-1)  \n",
    "diff = y_true_flat - y_mean_pred_flat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce07372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vis error\n",
    "visualise_v_quiver((y_test - mean_pred_test), x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f15c04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D error for vis\n",
    "error = y_test - mean_pred_test\n",
    "visualise_v_quiver(error, x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85eca86",
   "metadata": {},
   "source": [
    "# Calculate NLL test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2aacfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get diff\n",
    "y_true_flat = torch.concat([y_test[:, 0], y_test[:, 1]], dim = 0).unsqueeze(-1)  # Shape: (2 * N, 1)\n",
    "y_mean_pred_flat = torch.concat([mean_pred_test[:, 0], mean_pred_test[:, 1]], dim = 0).unsqueeze(-1)  \n",
    "diff = y_true_flat - y_mean_pred_flat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30f3036",
   "metadata": {},
   "outputs": [],
   "source": [
    "jitter = 1e-6  # small value to ensure numerical stability\n",
    "I = torch.eye(covar_pred_test.size(0), device = covar_pred_test.device)\n",
    "L = torch.linalg.cholesky(covar_pred_test + jitter * I) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c641b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get diff\n",
    "y_true_flat = torch.concat([y_test[:, 0], y_test[:, 1]], dim = 0).unsqueeze(-1)  # Shape: (2 * N, 1)\n",
    "y_mean_pred_flat = torch.concat([mean_pred_test[:, 0], mean_pred_test[:, 1]], dim = 0).unsqueeze(-1)  \n",
    "diff = y_true_flat - y_mean_pred_flat \n",
    "\n",
    "jitter = 1e-4  # small value to ensure numerical stability\n",
    "I = torch.eye(covar_pred_test.size(0), device = covar_pred_test.device)\n",
    "L = torch.linalg.cholesky(covar_pred_test + jitter * I) \n",
    "\n",
    "# This term is negative when model is very confident and variances are < 1\n",
    "# Contributes positively to log likelihood if it is negative\n",
    "log_det_Sigma = 2 * torch.sum(torch.log(torch.diagonal(L)))\n",
    "print(f\"Log det:\", log_det_Sigma.item())\n",
    "    \n",
    "# STEP 4: Compute normalisation term\n",
    "d = x_test.shape[0] * 2  # Dimensionality (since we have two outputs per point)\n",
    "normalisation_term = d * torch.log(torch.tensor(2 * torch.pi, device = y_test.device))\n",
    "print(f\"Norm:\", normalisation_term.item())\n",
    "\n",
    "# Large values reduce the likelihood\n",
    "maha = (torch.cholesky_solve(diff, L).T @ diff).squeeze()\n",
    "print(f\"Mahalanobis distance:\", maha.item())\n",
    "\n",
    "print(f\"sum of terms\", (maha + log_det_Sigma + normalisation_term).item())\n",
    "log_likelihood = -0.5 * (maha + log_det_Sigma + normalisation_term)\n",
    "print(f\"Log likelihood:\", log_likelihood.item())\n",
    "print(f\"NLL:\", -log_likelihood.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed331cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.diag(covar_pred_test).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a5e008",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow((diff[:400]+ diff[400:]).reshape(20, 20).cpu().detach().numpy(), origin = 'lower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96aa5c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "issues = torch.mul((1/torch.diag(covar_pred_test).unsqueeze(-1)), diff)\n",
    "plt.imshow((issues[:400] + issues[400:]).reshape(20, 20).cpu().detach().numpy(), cmap = 'grey', origin = 'lower')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
