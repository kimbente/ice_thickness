{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dfGP\n",
    "\n",
    "- write the functions that matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "=== CONVERGENCE ===\n",
      "Training inputs shape: torch.Size([196, 2])\n",
      "Training observations shape: torch.Size([196, 2])\n",
      "Training inputs dtype: torch.float32\n",
      "\n",
      "=== BRANCHING ===\n",
      "Training inputs shape: torch.Size([196, 2])\n",
      "Training observations shape: torch.Size([196, 2])\n",
      "Training inputs dtype: torch.float32\n",
      "\n",
      "=== MERGE ===\n",
      "Training inputs shape: torch.Size([196, 2])\n",
      "Training observations shape: torch.Size([196, 2])\n",
      "Training inputs dtype: torch.float32\n",
      "\n",
      "=== DEFLECTION ===\n",
      "Training inputs shape: torch.Size([196, 2])\n",
      "Training observations shape: torch.Size([196, 2])\n",
      "Training inputs dtype: torch.float32\n",
      "\n",
      "=== RIDGE ===\n",
      "Training inputs shape: torch.Size([196, 2])\n",
      "Training observations shape: torch.Size([196, 2])\n",
      "Training inputs dtype: torch.float32\n",
      "\n",
      "=== Generating test data ===\n",
      "=== CONVERGENCE ===\n",
      "Test inputs shape: torch.Size([400, 2])\n",
      "Test observations shape: torch.Size([400, 2])\n",
      "Test inputs dtype: torch.float32\n",
      "\n",
      "=== BRANCHING ===\n",
      "Test inputs shape: torch.Size([400, 2])\n",
      "Test observations shape: torch.Size([400, 2])\n",
      "Test inputs dtype: torch.float32\n",
      "\n",
      "=== MERGE ===\n",
      "Test inputs shape: torch.Size([400, 2])\n",
      "Test observations shape: torch.Size([400, 2])\n",
      "Test inputs dtype: torch.float32\n",
      "\n",
      "=== DEFLECTION ===\n",
      "Test inputs shape: torch.Size([400, 2])\n",
      "Test observations shape: torch.Size([400, 2])\n",
      "Test inputs dtype: torch.float32\n",
      "\n",
      "=== RIDGE ===\n",
      "Test inputs shape: torch.Size([400, 2])\n",
      "Test observations shape: torch.Size([400, 2])\n",
      "Test inputs dtype: torch.float32\n",
      "\n",
      "\n",
      "Training for CONVERGENCE...\n",
      "\n",
      "--- Training Run 1/2 ---\n",
      "\n",
      "Start Training\n",
      "convergence dfGP Run 1/2, Epoch 1/100, Training Loss (NLML): -892.0037, (RMSE): 0.4223\n",
      "convergence dfGP Run 1/2, Epoch 2/100, Training Loss (NLML): -894.0430, (RMSE): 0.4167\n",
      "convergence dfGP Run 1/2, Epoch 3/100, Training Loss (NLML): -896.0093, (RMSE): 0.4062\n",
      "convergence dfGP Run 1/2, Epoch 4/100, Training Loss (NLML): -897.9175, (RMSE): 0.4217\n",
      "convergence dfGP Run 1/2, Epoch 5/100, Training Loss (NLML): -899.7241, (RMSE): 0.4245\n",
      "convergence dfGP Run 1/2, Epoch 6/100, Training Loss (NLML): -901.4480, (RMSE): 0.4213\n",
      "convergence dfGP Run 1/2, Epoch 7/100, Training Loss (NLML): -903.1208, (RMSE): 0.4333\n",
      "convergence dfGP Run 1/2, Epoch 8/100, Training Loss (NLML): -904.7013, (RMSE): 0.4106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1258364/2015821307.py:146: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  l = torch.tensor(torch.empty(2).uniform_(* L_RANGE), requires_grad = True) # Trainable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence dfGP Run 1/2, Epoch 9/100, Training Loss (NLML): -906.2213, (RMSE): 0.4311\n",
      "convergence dfGP Run 1/2, Epoch 10/100, Training Loss (NLML): -907.6769, (RMSE): 0.4253\n",
      "convergence dfGP Run 1/2, Epoch 11/100, Training Loss (NLML): -909.1094, (RMSE): 0.4254\n",
      "convergence dfGP Run 1/2, Epoch 12/100, Training Loss (NLML): -910.4360, (RMSE): 0.4319\n",
      "convergence dfGP Run 1/2, Epoch 13/100, Training Loss (NLML): -911.7875, (RMSE): 0.4069\n",
      "convergence dfGP Run 1/2, Epoch 14/100, Training Loss (NLML): -913.0176, (RMSE): 0.4256\n",
      "convergence dfGP Run 1/2, Epoch 15/100, Training Loss (NLML): -914.2368, (RMSE): 0.4242\n",
      "convergence dfGP Run 1/2, Epoch 16/100, Training Loss (NLML): -915.4384, (RMSE): 0.4166\n",
      "convergence dfGP Run 1/2, Epoch 17/100, Training Loss (NLML): -916.5403, (RMSE): 0.4142\n",
      "convergence dfGP Run 1/2, Epoch 18/100, Training Loss (NLML): -917.6466, (RMSE): 0.4270\n",
      "convergence dfGP Run 1/2, Epoch 19/100, Training Loss (NLML): -918.6494, (RMSE): 0.4228\n",
      "convergence dfGP Run 1/2, Epoch 20/100, Training Loss (NLML): -919.6465, (RMSE): 0.4210\n",
      "convergence dfGP Run 1/2, Epoch 21/100, Training Loss (NLML): -920.6138, (RMSE): 0.4479\n",
      "convergence dfGP Run 1/2, Epoch 22/100, Training Loss (NLML): -921.5573, (RMSE): 0.4161\n",
      "convergence dfGP Run 1/2, Epoch 23/100, Training Loss (NLML): -922.4139, (RMSE): 0.4220\n",
      "convergence dfGP Run 1/2, Epoch 24/100, Training Loss (NLML): -923.2913, (RMSE): 0.4196\n",
      "convergence dfGP Run 1/2, Epoch 25/100, Training Loss (NLML): -924.1514, (RMSE): 0.4182\n",
      "convergence dfGP Run 1/2, Epoch 26/100, Training Loss (NLML): -924.9100, (RMSE): 0.4211\n",
      "convergence dfGP Run 1/2, Epoch 27/100, Training Loss (NLML): -925.6964, (RMSE): 0.4163\n",
      "convergence dfGP Run 1/2, Epoch 28/100, Training Loss (NLML): -926.4370, (RMSE): 0.4150\n",
      "convergence dfGP Run 1/2, Epoch 29/100, Training Loss (NLML): -927.1656, (RMSE): 0.4207\n",
      "convergence dfGP Run 1/2, Epoch 30/100, Training Loss (NLML): -927.8621, (RMSE): 0.4038\n",
      "convergence dfGP Run 1/2, Epoch 31/100, Training Loss (NLML): -928.5322, (RMSE): 0.4248\n",
      "convergence dfGP Run 1/2, Epoch 32/100, Training Loss (NLML): -929.1819, (RMSE): 0.4268\n",
      "convergence dfGP Run 1/2, Epoch 33/100, Training Loss (NLML): -929.8101, (RMSE): 0.4089\n",
      "convergence dfGP Run 1/2, Epoch 34/100, Training Loss (NLML): -930.4152, (RMSE): 0.4423\n",
      "convergence dfGP Run 1/2, Epoch 35/100, Training Loss (NLML): -930.9991, (RMSE): 0.4240\n",
      "convergence dfGP Run 1/2, Epoch 36/100, Training Loss (NLML): -931.5856, (RMSE): 0.4219\n",
      "convergence dfGP Run 1/2, Epoch 37/100, Training Loss (NLML): -932.1407, (RMSE): 0.4182\n",
      "convergence dfGP Run 1/2, Epoch 38/100, Training Loss (NLML): -932.6853, (RMSE): 0.4177\n",
      "convergence dfGP Run 1/2, Epoch 39/100, Training Loss (NLML): -933.1980, (RMSE): 0.4313\n",
      "convergence dfGP Run 1/2, Epoch 40/100, Training Loss (NLML): -933.7115, (RMSE): 0.4271\n",
      "convergence dfGP Run 1/2, Epoch 41/100, Training Loss (NLML): -934.2020, (RMSE): 0.4138\n",
      "convergence dfGP Run 1/2, Epoch 42/100, Training Loss (NLML): -934.6750, (RMSE): 0.4302\n",
      "convergence dfGP Run 1/2, Epoch 43/100, Training Loss (NLML): -935.1367, (RMSE): 0.4327\n",
      "convergence dfGP Run 1/2, Epoch 44/100, Training Loss (NLML): -935.5892, (RMSE): 0.4320\n",
      "convergence dfGP Run 1/2, Epoch 45/100, Training Loss (NLML): -936.0142, (RMSE): 0.4049\n",
      "convergence dfGP Run 1/2, Epoch 46/100, Training Loss (NLML): -936.4554, (RMSE): 0.4069\n",
      "convergence dfGP Run 1/2, Epoch 47/100, Training Loss (NLML): -936.8630, (RMSE): 0.4258\n",
      "convergence dfGP Run 1/2, Epoch 48/100, Training Loss (NLML): -937.2615, (RMSE): 0.4344\n",
      "convergence dfGP Run 1/2, Epoch 49/100, Training Loss (NLML): -937.6593, (RMSE): 0.4144\n",
      "convergence dfGP Run 1/2, Epoch 50/100, Training Loss (NLML): -938.0350, (RMSE): 0.4304\n",
      "convergence dfGP Run 1/2, Epoch 51/100, Training Loss (NLML): -938.4127, (RMSE): 0.4136\n",
      "convergence dfGP Run 1/2, Epoch 52/100, Training Loss (NLML): -938.7802, (RMSE): 0.3987\n",
      "convergence dfGP Run 1/2, Epoch 53/100, Training Loss (NLML): -939.1241, (RMSE): 0.4162\n",
      "convergence dfGP Run 1/2, Epoch 54/100, Training Loss (NLML): -939.4863, (RMSE): 0.4273\n",
      "convergence dfGP Run 1/2, Epoch 55/100, Training Loss (NLML): -939.8219, (RMSE): 0.4155\n",
      "convergence dfGP Run 1/2, Epoch 56/100, Training Loss (NLML): -940.1420, (RMSE): 0.4055\n",
      "convergence dfGP Run 1/2, Epoch 57/100, Training Loss (NLML): -940.4587, (RMSE): 0.4127\n",
      "convergence dfGP Run 1/2, Epoch 58/100, Training Loss (NLML): -940.7740, (RMSE): 0.4356\n",
      "convergence dfGP Run 1/2, Epoch 59/100, Training Loss (NLML): -941.0771, (RMSE): 0.4195\n",
      "convergence dfGP Run 1/2, Epoch 60/100, Training Loss (NLML): -941.3783, (RMSE): 0.4347\n",
      "convergence dfGP Run 1/2, Epoch 61/100, Training Loss (NLML): -941.6635, (RMSE): 0.4176\n",
      "convergence dfGP Run 1/2, Epoch 62/100, Training Loss (NLML): -941.9573, (RMSE): 0.4391\n",
      "convergence dfGP Run 1/2, Epoch 63/100, Training Loss (NLML): -942.2354, (RMSE): 0.4187\n",
      "convergence dfGP Run 1/2, Epoch 64/100, Training Loss (NLML): -942.5023, (RMSE): 0.4380\n",
      "convergence dfGP Run 1/2, Epoch 65/100, Training Loss (NLML): -942.7830, (RMSE): 0.4328\n",
      "convergence dfGP Run 1/2, Epoch 66/100, Training Loss (NLML): -943.0399, (RMSE): 0.4242\n",
      "convergence dfGP Run 1/2, Epoch 67/100, Training Loss (NLML): -943.2869, (RMSE): 0.3943\n",
      "convergence dfGP Run 1/2, Epoch 68/100, Training Loss (NLML): -943.5328, (RMSE): 0.3984\n",
      "convergence dfGP Run 1/2, Epoch 69/100, Training Loss (NLML): -943.7726, (RMSE): 0.4275\n",
      "convergence dfGP Run 1/2, Epoch 70/100, Training Loss (NLML): -944.0150, (RMSE): 0.4117\n",
      "convergence dfGP Run 1/2, Epoch 71/100, Training Loss (NLML): -944.2407, (RMSE): 0.4099\n",
      "convergence dfGP Run 1/2, Epoch 72/100, Training Loss (NLML): -944.4630, (RMSE): 0.4240\n",
      "convergence dfGP Run 1/2, Epoch 73/100, Training Loss (NLML): -944.6899, (RMSE): 0.4312\n",
      "convergence dfGP Run 1/2, Epoch 74/100, Training Loss (NLML): -944.9037, (RMSE): 0.4088\n",
      "convergence dfGP Run 1/2, Epoch 75/100, Training Loss (NLML): -945.1125, (RMSE): 0.3962\n",
      "convergence dfGP Run 1/2, Epoch 76/100, Training Loss (NLML): -945.3202, (RMSE): 0.4106\n",
      "convergence dfGP Run 1/2, Epoch 77/100, Training Loss (NLML): -945.5187, (RMSE): 0.4126\n",
      "convergence dfGP Run 1/2, Epoch 78/100, Training Loss (NLML): -945.7162, (RMSE): 0.4233\n",
      "convergence dfGP Run 1/2, Epoch 79/100, Training Loss (NLML): -945.9058, (RMSE): 0.4210\n",
      "convergence dfGP Run 1/2, Epoch 80/100, Training Loss (NLML): -946.0959, (RMSE): 0.4098\n",
      "convergence dfGP Run 1/2, Epoch 81/100, Training Loss (NLML): -946.2764, (RMSE): 0.4299\n",
      "convergence dfGP Run 1/2, Epoch 82/100, Training Loss (NLML): -946.4534, (RMSE): 0.4246\n",
      "convergence dfGP Run 1/2, Epoch 83/100, Training Loss (NLML): -946.6267, (RMSE): 0.4310\n",
      "convergence dfGP Run 1/2, Epoch 84/100, Training Loss (NLML): -946.8005, (RMSE): 0.4218\n",
      "convergence dfGP Run 1/2, Epoch 85/100, Training Loss (NLML): -946.9674, (RMSE): 0.4286\n",
      "convergence dfGP Run 1/2, Epoch 86/100, Training Loss (NLML): -947.1213, (RMSE): 0.4352\n",
      "convergence dfGP Run 1/2, Epoch 87/100, Training Loss (NLML): -947.2822, (RMSE): 0.3991\n",
      "convergence dfGP Run 1/2, Epoch 88/100, Training Loss (NLML): -947.4396, (RMSE): 0.4212\n",
      "convergence dfGP Run 1/2, Epoch 89/100, Training Loss (NLML): -947.5927, (RMSE): 0.4164\n",
      "convergence dfGP Run 1/2, Epoch 90/100, Training Loss (NLML): -947.7400, (RMSE): 0.4119\n",
      "convergence dfGP Run 1/2, Epoch 91/100, Training Loss (NLML): -947.8844, (RMSE): 0.4364\n",
      "convergence dfGP Run 1/2, Epoch 92/100, Training Loss (NLML): -948.0232, (RMSE): 0.4169\n",
      "convergence dfGP Run 1/2, Epoch 93/100, Training Loss (NLML): -948.1598, (RMSE): 0.4044\n",
      "convergence dfGP Run 1/2, Epoch 94/100, Training Loss (NLML): -948.3003, (RMSE): 0.4162\n",
      "convergence dfGP Run 1/2, Epoch 95/100, Training Loss (NLML): -948.4276, (RMSE): 0.4346\n",
      "convergence dfGP Run 1/2, Epoch 96/100, Training Loss (NLML): -948.5581, (RMSE): 0.4166\n",
      "convergence dfGP Run 1/2, Epoch 97/100, Training Loss (NLML): -948.6831, (RMSE): 0.4119\n",
      "convergence dfGP Run 1/2, Epoch 98/100, Training Loss (NLML): -948.8048, (RMSE): 0.4413\n",
      "convergence dfGP Run 1/2, Epoch 99/100, Training Loss (NLML): -948.9296, (RMSE): 0.4275\n",
      "convergence dfGP Run 1/2, Epoch 100/100, Training Loss (NLML): -949.0461, (RMSE): 0.4090\n",
      "\n",
      "--- Training Run 2/2 ---\n",
      "\n",
      "Start Training\n",
      "convergence dfGP Run 2/2, Epoch 1/100, Training Loss (NLML): -829.7519, (RMSE): 0.4258\n",
      "convergence dfGP Run 2/2, Epoch 2/100, Training Loss (NLML): -835.2526, (RMSE): 0.4361\n",
      "convergence dfGP Run 2/2, Epoch 3/100, Training Loss (NLML): -840.2003, (RMSE): 0.4328\n",
      "convergence dfGP Run 2/2, Epoch 4/100, Training Loss (NLML): -845.1125, (RMSE): 0.4110\n",
      "convergence dfGP Run 2/2, Epoch 5/100, Training Loss (NLML): -849.4910, (RMSE): 0.4277\n",
      "convergence dfGP Run 2/2, Epoch 6/100, Training Loss (NLML): -853.6262, (RMSE): 0.4095\n",
      "convergence dfGP Run 2/2, Epoch 7/100, Training Loss (NLML): -857.5273, (RMSE): 0.4384\n",
      "convergence dfGP Run 2/2, Epoch 8/100, Training Loss (NLML): -861.2812, (RMSE): 0.3992\n",
      "convergence dfGP Run 2/2, Epoch 9/100, Training Loss (NLML): -864.7391, (RMSE): 0.4213\n",
      "convergence dfGP Run 2/2, Epoch 10/100, Training Loss (NLML): -868.0209, (RMSE): 0.4085\n",
      "convergence dfGP Run 2/2, Epoch 11/100, Training Loss (NLML): -871.1947, (RMSE): 0.4308\n",
      "convergence dfGP Run 2/2, Epoch 12/100, Training Loss (NLML): -874.1350, (RMSE): 0.4026\n",
      "convergence dfGP Run 2/2, Epoch 13/100, Training Loss (NLML): -876.8701, (RMSE): 0.4164\n",
      "convergence dfGP Run 2/2, Epoch 14/100, Training Loss (NLML): -879.4963, (RMSE): 0.4195\n",
      "convergence dfGP Run 2/2, Epoch 15/100, Training Loss (NLML): -881.9031, (RMSE): 0.4280\n",
      "convergence dfGP Run 2/2, Epoch 16/100, Training Loss (NLML): -884.2618, (RMSE): 0.4221\n",
      "convergence dfGP Run 2/2, Epoch 17/100, Training Loss (NLML): -886.4343, (RMSE): 0.4035\n",
      "convergence dfGP Run 2/2, Epoch 18/100, Training Loss (NLML): -888.5369, (RMSE): 0.4168\n",
      "convergence dfGP Run 2/2, Epoch 19/100, Training Loss (NLML): -890.5457, (RMSE): 0.4310\n",
      "convergence dfGP Run 2/2, Epoch 20/100, Training Loss (NLML): -892.4310, (RMSE): 0.4178\n",
      "convergence dfGP Run 2/2, Epoch 21/100, Training Loss (NLML): -894.1913, (RMSE): 0.4193\n",
      "convergence dfGP Run 2/2, Epoch 22/100, Training Loss (NLML): -895.8887, (RMSE): 0.4268\n",
      "convergence dfGP Run 2/2, Epoch 23/100, Training Loss (NLML): -897.5317, (RMSE): 0.4151\n",
      "convergence dfGP Run 2/2, Epoch 24/100, Training Loss (NLML): -899.0884, (RMSE): 0.4291\n",
      "convergence dfGP Run 2/2, Epoch 25/100, Training Loss (NLML): -900.5757, (RMSE): 0.4361\n",
      "convergence dfGP Run 2/2, Epoch 26/100, Training Loss (NLML): -901.9907, (RMSE): 0.4383\n",
      "convergence dfGP Run 2/2, Epoch 27/100, Training Loss (NLML): -903.3599, (RMSE): 0.3979\n",
      "convergence dfGP Run 2/2, Epoch 28/100, Training Loss (NLML): -904.6594, (RMSE): 0.4214\n",
      "convergence dfGP Run 2/2, Epoch 29/100, Training Loss (NLML): -905.8816, (RMSE): 0.4098\n",
      "convergence dfGP Run 2/2, Epoch 30/100, Training Loss (NLML): -907.1035, (RMSE): 0.4189\n",
      "convergence dfGP Run 2/2, Epoch 31/100, Training Loss (NLML): -908.2388, (RMSE): 0.4146\n",
      "convergence dfGP Run 2/2, Epoch 32/100, Training Loss (NLML): -909.3450, (RMSE): 0.3952\n",
      "convergence dfGP Run 2/2, Epoch 33/100, Training Loss (NLML): -910.3881, (RMSE): 0.4122\n",
      "convergence dfGP Run 2/2, Epoch 34/100, Training Loss (NLML): -911.4005, (RMSE): 0.4374\n",
      "convergence dfGP Run 2/2, Epoch 35/100, Training Loss (NLML): -912.3907, (RMSE): 0.4182\n",
      "convergence dfGP Run 2/2, Epoch 36/100, Training Loss (NLML): -913.3289, (RMSE): 0.4058\n",
      "convergence dfGP Run 2/2, Epoch 37/100, Training Loss (NLML): -914.2302, (RMSE): 0.4253\n",
      "convergence dfGP Run 2/2, Epoch 38/100, Training Loss (NLML): -915.1392, (RMSE): 0.4043\n",
      "convergence dfGP Run 2/2, Epoch 39/100, Training Loss (NLML): -915.9723, (RMSE): 0.4070\n",
      "convergence dfGP Run 2/2, Epoch 40/100, Training Loss (NLML): -916.7715, (RMSE): 0.4187\n",
      "convergence dfGP Run 2/2, Epoch 41/100, Training Loss (NLML): -917.5957, (RMSE): 0.4421\n",
      "convergence dfGP Run 2/2, Epoch 42/100, Training Loss (NLML): -918.3727, (RMSE): 0.4258\n",
      "convergence dfGP Run 2/2, Epoch 43/100, Training Loss (NLML): -919.1031, (RMSE): 0.4303\n",
      "convergence dfGP Run 2/2, Epoch 44/100, Training Loss (NLML): -919.8276, (RMSE): 0.4082\n",
      "convergence dfGP Run 2/2, Epoch 45/100, Training Loss (NLML): -920.5529, (RMSE): 0.4314\n",
      "convergence dfGP Run 2/2, Epoch 46/100, Training Loss (NLML): -921.2262, (RMSE): 0.4231\n",
      "convergence dfGP Run 2/2, Epoch 47/100, Training Loss (NLML): -921.9093, (RMSE): 0.4115\n",
      "convergence dfGP Run 2/2, Epoch 48/100, Training Loss (NLML): -922.5345, (RMSE): 0.4324\n",
      "convergence dfGP Run 2/2, Epoch 49/100, Training Loss (NLML): -923.1825, (RMSE): 0.4089\n",
      "convergence dfGP Run 2/2, Epoch 50/100, Training Loss (NLML): -923.7882, (RMSE): 0.4163\n",
      "convergence dfGP Run 2/2, Epoch 51/100, Training Loss (NLML): -924.3688, (RMSE): 0.4241\n",
      "convergence dfGP Run 2/2, Epoch 52/100, Training Loss (NLML): -924.9449, (RMSE): 0.4249\n",
      "convergence dfGP Run 2/2, Epoch 53/100, Training Loss (NLML): -925.5210, (RMSE): 0.4333\n",
      "convergence dfGP Run 2/2, Epoch 54/100, Training Loss (NLML): -926.0731, (RMSE): 0.4318\n",
      "convergence dfGP Run 2/2, Epoch 55/100, Training Loss (NLML): -926.5951, (RMSE): 0.4399\n",
      "convergence dfGP Run 2/2, Epoch 56/100, Training Loss (NLML): -927.1310, (RMSE): 0.4249\n",
      "convergence dfGP Run 2/2, Epoch 57/100, Training Loss (NLML): -927.6454, (RMSE): 0.4321\n",
      "convergence dfGP Run 2/2, Epoch 58/100, Training Loss (NLML): -928.1382, (RMSE): 0.4166\n",
      "convergence dfGP Run 2/2, Epoch 59/100, Training Loss (NLML): -928.6207, (RMSE): 0.4149\n",
      "convergence dfGP Run 2/2, Epoch 60/100, Training Loss (NLML): -929.1079, (RMSE): 0.4103\n",
      "convergence dfGP Run 2/2, Epoch 61/100, Training Loss (NLML): -929.5645, (RMSE): 0.4150\n",
      "convergence dfGP Run 2/2, Epoch 62/100, Training Loss (NLML): -930.0093, (RMSE): 0.4090\n",
      "convergence dfGP Run 2/2, Epoch 63/100, Training Loss (NLML): -930.4509, (RMSE): 0.4330\n",
      "convergence dfGP Run 2/2, Epoch 64/100, Training Loss (NLML): -930.8796, (RMSE): 0.4153\n",
      "convergence dfGP Run 2/2, Epoch 65/100, Training Loss (NLML): -931.3065, (RMSE): 0.4316\n",
      "convergence dfGP Run 2/2, Epoch 66/100, Training Loss (NLML): -931.7230, (RMSE): 0.4240\n",
      "convergence dfGP Run 2/2, Epoch 67/100, Training Loss (NLML): -932.1298, (RMSE): 0.4292\n",
      "convergence dfGP Run 2/2, Epoch 68/100, Training Loss (NLML): -932.5242, (RMSE): 0.4078\n",
      "convergence dfGP Run 2/2, Epoch 69/100, Training Loss (NLML): -932.9169, (RMSE): 0.4159\n",
      "convergence dfGP Run 2/2, Epoch 70/100, Training Loss (NLML): -933.2959, (RMSE): 0.4317\n",
      "convergence dfGP Run 2/2, Epoch 71/100, Training Loss (NLML): -933.6636, (RMSE): 0.4306\n",
      "convergence dfGP Run 2/2, Epoch 72/100, Training Loss (NLML): -934.0396, (RMSE): 0.4363\n",
      "convergence dfGP Run 2/2, Epoch 73/100, Training Loss (NLML): -934.3929, (RMSE): 0.3985\n",
      "convergence dfGP Run 2/2, Epoch 74/100, Training Loss (NLML): -934.7520, (RMSE): 0.4344\n",
      "convergence dfGP Run 2/2, Epoch 75/100, Training Loss (NLML): -935.0966, (RMSE): 0.4280\n",
      "convergence dfGP Run 2/2, Epoch 76/100, Training Loss (NLML): -935.4406, (RMSE): 0.4328\n",
      "convergence dfGP Run 2/2, Epoch 77/100, Training Loss (NLML): -935.7645, (RMSE): 0.4368\n",
      "convergence dfGP Run 2/2, Epoch 78/100, Training Loss (NLML): -936.0940, (RMSE): 0.4074\n",
      "convergence dfGP Run 2/2, Epoch 79/100, Training Loss (NLML): -936.4005, (RMSE): 0.4230\n",
      "convergence dfGP Run 2/2, Epoch 80/100, Training Loss (NLML): -936.7212, (RMSE): 0.4241\n",
      "convergence dfGP Run 2/2, Epoch 81/100, Training Loss (NLML): -937.0251, (RMSE): 0.4193\n",
      "convergence dfGP Run 2/2, Epoch 82/100, Training Loss (NLML): -937.3302, (RMSE): 0.4287\n",
      "convergence dfGP Run 2/2, Epoch 83/100, Training Loss (NLML): -937.6227, (RMSE): 0.4417\n",
      "convergence dfGP Run 2/2, Epoch 84/100, Training Loss (NLML): -937.9104, (RMSE): 0.4352\n",
      "convergence dfGP Run 2/2, Epoch 85/100, Training Loss (NLML): -938.1875, (RMSE): 0.4218\n",
      "convergence dfGP Run 2/2, Epoch 86/100, Training Loss (NLML): -938.4789, (RMSE): 0.4367\n",
      "convergence dfGP Run 2/2, Epoch 87/100, Training Loss (NLML): -938.7482, (RMSE): 0.4255\n",
      "convergence dfGP Run 2/2, Epoch 88/100, Training Loss (NLML): -939.0164, (RMSE): 0.4017\n",
      "convergence dfGP Run 2/2, Epoch 89/100, Training Loss (NLML): -939.2778, (RMSE): 0.4046\n",
      "convergence dfGP Run 2/2, Epoch 90/100, Training Loss (NLML): -939.5369, (RMSE): 0.4198\n",
      "convergence dfGP Run 2/2, Epoch 91/100, Training Loss (NLML): -939.7876, (RMSE): 0.4150\n",
      "convergence dfGP Run 2/2, Epoch 92/100, Training Loss (NLML): -940.0343, (RMSE): 0.4264\n",
      "convergence dfGP Run 2/2, Epoch 93/100, Training Loss (NLML): -940.2747, (RMSE): 0.4071\n",
      "convergence dfGP Run 2/2, Epoch 94/100, Training Loss (NLML): -940.5104, (RMSE): 0.4298\n",
      "convergence dfGP Run 2/2, Epoch 95/100, Training Loss (NLML): -940.7426, (RMSE): 0.4090\n",
      "convergence dfGP Run 2/2, Epoch 96/100, Training Loss (NLML): -940.9694, (RMSE): 0.4017\n",
      "convergence dfGP Run 2/2, Epoch 97/100, Training Loss (NLML): -941.1915, (RMSE): 0.4232\n",
      "convergence dfGP Run 2/2, Epoch 98/100, Training Loss (NLML): -941.4093, (RMSE): 0.4367\n",
      "convergence dfGP Run 2/2, Epoch 99/100, Training Loss (NLML): -941.6226, (RMSE): 0.4205\n",
      "convergence dfGP Run 2/2, Epoch 100/100, Training Loss (NLML): -941.8315, (RMSE): 0.4388\n",
      "\n",
      "Results saved to results/DFGP/convergence_dfGP_metrics_per_run.csv\n",
      "\n",
      "Mean & Std saved to results/DFGP/convergence_dfGP_metrics_summary.csv\n",
      "\n",
      "Training for BRANCHING...\n",
      "\n",
      "--- Training Run 1/2 ---\n",
      "\n",
      "Start Training\n",
      "branching dfGP Run 1/2, Epoch 1/100, Training Loss (NLML): -911.4586, (RMSE): 0.3889\n",
      "branching dfGP Run 1/2, Epoch 2/100, Training Loss (NLML): -912.8103, (RMSE): 0.3707\n",
      "branching dfGP Run 1/2, Epoch 3/100, Training Loss (NLML): -914.0699, (RMSE): 0.3823\n",
      "branching dfGP Run 1/2, Epoch 4/100, Training Loss (NLML): -915.2975, (RMSE): 0.3867\n",
      "branching dfGP Run 1/2, Epoch 5/100, Training Loss (NLML): -916.4973, (RMSE): 0.3692\n",
      "branching dfGP Run 1/2, Epoch 6/100, Training Loss (NLML): -917.6721, (RMSE): 0.3846\n",
      "branching dfGP Run 1/2, Epoch 7/100, Training Loss (NLML): -918.7676, (RMSE): 0.3853\n",
      "branching dfGP Run 1/2, Epoch 8/100, Training Loss (NLML): -919.8350, (RMSE): 0.3688\n",
      "branching dfGP Run 1/2, Epoch 9/100, Training Loss (NLML): -920.8876, (RMSE): 0.3687\n",
      "branching dfGP Run 1/2, Epoch 10/100, Training Loss (NLML): -921.9259, (RMSE): 0.3817\n",
      "branching dfGP Run 1/2, Epoch 11/100, Training Loss (NLML): -922.8846, (RMSE): 0.3868\n",
      "branching dfGP Run 1/2, Epoch 12/100, Training Loss (NLML): -923.8257, (RMSE): 0.4095\n",
      "branching dfGP Run 1/2, Epoch 13/100, Training Loss (NLML): -924.7252, (RMSE): 0.3867\n",
      "branching dfGP Run 1/2, Epoch 14/100, Training Loss (NLML): -925.5884, (RMSE): 0.3943\n",
      "branching dfGP Run 1/2, Epoch 15/100, Training Loss (NLML): -926.4303, (RMSE): 0.3824\n",
      "branching dfGP Run 1/2, Epoch 16/100, Training Loss (NLML): -927.2180, (RMSE): 0.3779\n",
      "branching dfGP Run 1/2, Epoch 17/100, Training Loss (NLML): -928.0308, (RMSE): 0.3573\n",
      "branching dfGP Run 1/2, Epoch 18/100, Training Loss (NLML): -928.7804, (RMSE): 0.3917\n",
      "branching dfGP Run 1/2, Epoch 19/100, Training Loss (NLML): -929.5112, (RMSE): 0.3879\n",
      "branching dfGP Run 1/2, Epoch 20/100, Training Loss (NLML): -930.2218, (RMSE): 0.3966\n",
      "branching dfGP Run 1/2, Epoch 21/100, Training Loss (NLML): -930.8938, (RMSE): 0.3897\n",
      "branching dfGP Run 1/2, Epoch 22/100, Training Loss (NLML): -931.5649, (RMSE): 0.3973\n",
      "branching dfGP Run 1/2, Epoch 23/100, Training Loss (NLML): -932.1816, (RMSE): 0.3925\n",
      "branching dfGP Run 1/2, Epoch 24/100, Training Loss (NLML): -932.8029, (RMSE): 0.3987\n",
      "branching dfGP Run 1/2, Epoch 25/100, Training Loss (NLML): -933.3875, (RMSE): 0.3844\n",
      "branching dfGP Run 1/2, Epoch 26/100, Training Loss (NLML): -933.9717, (RMSE): 0.3746\n",
      "branching dfGP Run 1/2, Epoch 27/100, Training Loss (NLML): -934.5281, (RMSE): 0.3858\n",
      "branching dfGP Run 1/2, Epoch 28/100, Training Loss (NLML): -935.0580, (RMSE): 0.3653\n",
      "branching dfGP Run 1/2, Epoch 29/100, Training Loss (NLML): -935.5862, (RMSE): 0.3833\n",
      "branching dfGP Run 1/2, Epoch 30/100, Training Loss (NLML): -936.0903, (RMSE): 0.3734\n",
      "branching dfGP Run 1/2, Epoch 31/100, Training Loss (NLML): -936.5695, (RMSE): 0.3851\n",
      "branching dfGP Run 1/2, Epoch 32/100, Training Loss (NLML): -937.0486, (RMSE): 0.3810\n",
      "branching dfGP Run 1/2, Epoch 33/100, Training Loss (NLML): -937.5208, (RMSE): 0.3647\n",
      "branching dfGP Run 1/2, Epoch 34/100, Training Loss (NLML): -937.9692, (RMSE): 0.3621\n",
      "branching dfGP Run 1/2, Epoch 35/100, Training Loss (NLML): -938.3965, (RMSE): 0.3501\n",
      "branching dfGP Run 1/2, Epoch 36/100, Training Loss (NLML): -938.8181, (RMSE): 0.3641\n",
      "branching dfGP Run 1/2, Epoch 37/100, Training Loss (NLML): -939.2195, (RMSE): 0.3798\n",
      "branching dfGP Run 1/2, Epoch 38/100, Training Loss (NLML): -939.6106, (RMSE): 0.3804\n",
      "branching dfGP Run 1/2, Epoch 39/100, Training Loss (NLML): -939.9991, (RMSE): 0.3905\n",
      "branching dfGP Run 1/2, Epoch 40/100, Training Loss (NLML): -940.3705, (RMSE): 0.3831\n",
      "branching dfGP Run 1/2, Epoch 41/100, Training Loss (NLML): -940.7368, (RMSE): 0.3783\n",
      "branching dfGP Run 1/2, Epoch 42/100, Training Loss (NLML): -941.0822, (RMSE): 0.3768\n",
      "branching dfGP Run 1/2, Epoch 43/100, Training Loss (NLML): -941.4170, (RMSE): 0.3731\n",
      "branching dfGP Run 1/2, Epoch 44/100, Training Loss (NLML): -941.7483, (RMSE): 0.3712\n",
      "branching dfGP Run 1/2, Epoch 45/100, Training Loss (NLML): -942.0706, (RMSE): 0.4009\n",
      "branching dfGP Run 1/2, Epoch 46/100, Training Loss (NLML): -942.3767, (RMSE): 0.3610\n",
      "branching dfGP Run 1/2, Epoch 47/100, Training Loss (NLML): -942.6725, (RMSE): 0.3684\n",
      "branching dfGP Run 1/2, Epoch 48/100, Training Loss (NLML): -942.9667, (RMSE): 0.3718\n",
      "branching dfGP Run 1/2, Epoch 49/100, Training Loss (NLML): -943.2466, (RMSE): 0.3829\n",
      "branching dfGP Run 1/2, Epoch 50/100, Training Loss (NLML): -943.5201, (RMSE): 0.3694\n",
      "branching dfGP Run 1/2, Epoch 51/100, Training Loss (NLML): -943.7788, (RMSE): 0.3779\n",
      "branching dfGP Run 1/2, Epoch 52/100, Training Loss (NLML): -944.0377, (RMSE): 0.3892\n",
      "branching dfGP Run 1/2, Epoch 53/100, Training Loss (NLML): -944.2860, (RMSE): 0.3800\n",
      "branching dfGP Run 1/2, Epoch 54/100, Training Loss (NLML): -944.5236, (RMSE): 0.3709\n",
      "branching dfGP Run 1/2, Epoch 55/100, Training Loss (NLML): -944.7559, (RMSE): 0.3541\n",
      "branching dfGP Run 1/2, Epoch 56/100, Training Loss (NLML): -944.9789, (RMSE): 0.3817\n",
      "branching dfGP Run 1/2, Epoch 57/100, Training Loss (NLML): -945.1987, (RMSE): 0.3615\n",
      "branching dfGP Run 1/2, Epoch 58/100, Training Loss (NLML): -945.4021, (RMSE): 0.3700\n",
      "branching dfGP Run 1/2, Epoch 59/100, Training Loss (NLML): -945.6055, (RMSE): 0.3797\n",
      "branching dfGP Run 1/2, Epoch 60/100, Training Loss (NLML): -945.8008, (RMSE): 0.3882\n",
      "branching dfGP Run 1/2, Epoch 61/100, Training Loss (NLML): -945.9926, (RMSE): 0.3723\n",
      "branching dfGP Run 1/2, Epoch 62/100, Training Loss (NLML): -946.1766, (RMSE): 0.3702\n",
      "branching dfGP Run 1/2, Epoch 63/100, Training Loss (NLML): -946.3479, (RMSE): 0.3796\n",
      "branching dfGP Run 1/2, Epoch 64/100, Training Loss (NLML): -946.5178, (RMSE): 0.3799\n",
      "branching dfGP Run 1/2, Epoch 65/100, Training Loss (NLML): -946.6758, (RMSE): 0.3759\n",
      "branching dfGP Run 1/2, Epoch 66/100, Training Loss (NLML): -946.8318, (RMSE): 0.3774\n",
      "branching dfGP Run 1/2, Epoch 67/100, Training Loss (NLML): -946.9749, (RMSE): 0.3656\n",
      "branching dfGP Run 1/2, Epoch 68/100, Training Loss (NLML): -947.1202, (RMSE): 0.3712\n",
      "branching dfGP Run 1/2, Epoch 69/100, Training Loss (NLML): -947.2578, (RMSE): 0.3821\n",
      "branching dfGP Run 1/2, Epoch 70/100, Training Loss (NLML): -947.3881, (RMSE): 0.3726\n",
      "branching dfGP Run 1/2, Epoch 71/100, Training Loss (NLML): -947.5205, (RMSE): 0.3860\n",
      "branching dfGP Run 1/2, Epoch 72/100, Training Loss (NLML): -947.6378, (RMSE): 0.3722\n",
      "branching dfGP Run 1/2, Epoch 73/100, Training Loss (NLML): -947.7545, (RMSE): 0.3759\n",
      "branching dfGP Run 1/2, Epoch 74/100, Training Loss (NLML): -947.8623, (RMSE): 0.3804\n",
      "branching dfGP Run 1/2, Epoch 75/100, Training Loss (NLML): -947.9636, (RMSE): 0.3789\n",
      "branching dfGP Run 1/2, Epoch 76/100, Training Loss (NLML): -948.0619, (RMSE): 0.3782\n",
      "branching dfGP Run 1/2, Epoch 77/100, Training Loss (NLML): -948.1597, (RMSE): 0.3695\n",
      "branching dfGP Run 1/2, Epoch 78/100, Training Loss (NLML): -948.2465, (RMSE): 0.3764\n",
      "branching dfGP Run 1/2, Epoch 79/100, Training Loss (NLML): -948.3304, (RMSE): 0.3922\n",
      "branching dfGP Run 1/2, Epoch 80/100, Training Loss (NLML): -948.4097, (RMSE): 0.3798\n",
      "branching dfGP Run 1/2, Epoch 81/100, Training Loss (NLML): -948.4844, (RMSE): 0.3916\n",
      "branching dfGP Run 1/2, Epoch 82/100, Training Loss (NLML): -948.5576, (RMSE): 0.3848\n",
      "branching dfGP Run 1/2, Epoch 83/100, Training Loss (NLML): -948.6219, (RMSE): 0.3734\n",
      "branching dfGP Run 1/2, Epoch 84/100, Training Loss (NLML): -948.6877, (RMSE): 0.3946\n",
      "branching dfGP Run 1/2, Epoch 85/100, Training Loss (NLML): -948.7506, (RMSE): 0.3839\n",
      "branching dfGP Run 1/2, Epoch 86/100, Training Loss (NLML): -948.8059, (RMSE): 0.3762\n",
      "branching dfGP Run 1/2, Epoch 87/100, Training Loss (NLML): -948.8597, (RMSE): 0.3705\n",
      "branching dfGP Run 1/2, Epoch 88/100, Training Loss (NLML): -948.9081, (RMSE): 0.3792\n",
      "branching dfGP Run 1/2, Epoch 89/100, Training Loss (NLML): -948.9550, (RMSE): 0.3883\n",
      "branching dfGP Run 1/2, Epoch 90/100, Training Loss (NLML): -948.9951, (RMSE): 0.3787\n",
      "branching dfGP Run 1/2, Epoch 91/100, Training Loss (NLML): -949.0399, (RMSE): 0.3763\n",
      "branching dfGP Run 1/2, Epoch 92/100, Training Loss (NLML): -949.0818, (RMSE): 0.3602\n",
      "branching dfGP Run 1/2, Epoch 93/100, Training Loss (NLML): -949.1160, (RMSE): 0.3770\n",
      "branching dfGP Run 1/2, Epoch 94/100, Training Loss (NLML): -949.1492, (RMSE): 0.3727\n",
      "branching dfGP Run 1/2, Epoch 95/100, Training Loss (NLML): -949.1823, (RMSE): 0.3716\n",
      "branching dfGP Run 1/2, Epoch 96/100, Training Loss (NLML): -949.2107, (RMSE): 0.3827\n",
      "branching dfGP Run 1/2, Epoch 97/100, Training Loss (NLML): -949.2408, (RMSE): 0.3790\n",
      "branching dfGP Run 1/2, Epoch 98/100, Training Loss (NLML): -949.2643, (RMSE): 0.3810\n",
      "branching dfGP Run 1/2, Epoch 99/100, Training Loss (NLML): -949.2933, (RMSE): 0.3974\n",
      "branching dfGP Run 1/2, Epoch 100/100, Training Loss (NLML): -949.3164, (RMSE): 0.3760\n",
      "\n",
      "--- Training Run 2/2 ---\n",
      "\n",
      "Start Training\n",
      "branching dfGP Run 2/2, Epoch 1/100, Training Loss (NLML): -872.8188, (RMSE): 0.3834\n",
      "branching dfGP Run 2/2, Epoch 2/100, Training Loss (NLML): -875.8857, (RMSE): 0.3660\n",
      "branching dfGP Run 2/2, Epoch 3/100, Training Loss (NLML): -878.7886, (RMSE): 0.3873\n",
      "branching dfGP Run 2/2, Epoch 4/100, Training Loss (NLML): -881.5277, (RMSE): 0.3674\n",
      "branching dfGP Run 2/2, Epoch 5/100, Training Loss (NLML): -884.1547, (RMSE): 0.3794\n",
      "branching dfGP Run 2/2, Epoch 6/100, Training Loss (NLML): -886.6281, (RMSE): 0.3911\n",
      "branching dfGP Run 2/2, Epoch 7/100, Training Loss (NLML): -888.9513, (RMSE): 0.3731\n",
      "branching dfGP Run 2/2, Epoch 8/100, Training Loss (NLML): -891.1917, (RMSE): 0.3863\n",
      "branching dfGP Run 2/2, Epoch 9/100, Training Loss (NLML): -893.3252, (RMSE): 0.3898\n",
      "branching dfGP Run 2/2, Epoch 10/100, Training Loss (NLML): -895.4269, (RMSE): 0.3621\n",
      "branching dfGP Run 2/2, Epoch 11/100, Training Loss (NLML): -897.3657, (RMSE): 0.3880\n",
      "branching dfGP Run 2/2, Epoch 12/100, Training Loss (NLML): -899.1659, (RMSE): 0.3773\n",
      "branching dfGP Run 2/2, Epoch 13/100, Training Loss (NLML): -900.8903, (RMSE): 0.3757\n",
      "branching dfGP Run 2/2, Epoch 14/100, Training Loss (NLML): -902.5906, (RMSE): 0.3761\n",
      "branching dfGP Run 2/2, Epoch 15/100, Training Loss (NLML): -904.1398, (RMSE): 0.3905\n",
      "branching dfGP Run 2/2, Epoch 16/100, Training Loss (NLML): -905.6631, (RMSE): 0.3747\n",
      "branching dfGP Run 2/2, Epoch 17/100, Training Loss (NLML): -907.0768, (RMSE): 0.3954\n",
      "branching dfGP Run 2/2, Epoch 18/100, Training Loss (NLML): -908.4578, (RMSE): 0.3558\n",
      "branching dfGP Run 2/2, Epoch 19/100, Training Loss (NLML): -909.7781, (RMSE): 0.3653\n",
      "branching dfGP Run 2/2, Epoch 20/100, Training Loss (NLML): -911.0289, (RMSE): 0.3723\n",
      "branching dfGP Run 2/2, Epoch 21/100, Training Loss (NLML): -912.2035, (RMSE): 0.3661\n",
      "branching dfGP Run 2/2, Epoch 22/100, Training Loss (NLML): -913.3661, (RMSE): 0.3948\n",
      "branching dfGP Run 2/2, Epoch 23/100, Training Loss (NLML): -914.4447, (RMSE): 0.3556\n",
      "branching dfGP Run 2/2, Epoch 24/100, Training Loss (NLML): -915.4985, (RMSE): 0.3816\n",
      "branching dfGP Run 2/2, Epoch 25/100, Training Loss (NLML): -916.5057, (RMSE): 0.3791\n",
      "branching dfGP Run 2/2, Epoch 26/100, Training Loss (NLML): -917.5005, (RMSE): 0.3660\n",
      "branching dfGP Run 2/2, Epoch 27/100, Training Loss (NLML): -918.4310, (RMSE): 0.3685\n",
      "branching dfGP Run 2/2, Epoch 28/100, Training Loss (NLML): -919.3129, (RMSE): 0.3671\n",
      "branching dfGP Run 2/2, Epoch 29/100, Training Loss (NLML): -920.1982, (RMSE): 0.3886\n",
      "branching dfGP Run 2/2, Epoch 30/100, Training Loss (NLML): -921.0079, (RMSE): 0.3686\n",
      "branching dfGP Run 2/2, Epoch 31/100, Training Loss (NLML): -921.8079, (RMSE): 0.3689\n",
      "branching dfGP Run 2/2, Epoch 32/100, Training Loss (NLML): -922.5714, (RMSE): 0.3841\n",
      "branching dfGP Run 2/2, Epoch 33/100, Training Loss (NLML): -923.3149, (RMSE): 0.3579\n",
      "branching dfGP Run 2/2, Epoch 34/100, Training Loss (NLML): -924.0363, (RMSE): 0.3611\n",
      "branching dfGP Run 2/2, Epoch 35/100, Training Loss (NLML): -924.7147, (RMSE): 0.3869\n",
      "branching dfGP Run 2/2, Epoch 36/100, Training Loss (NLML): -925.3805, (RMSE): 0.3650\n",
      "branching dfGP Run 2/2, Epoch 37/100, Training Loss (NLML): -926.0125, (RMSE): 0.3832\n",
      "branching dfGP Run 2/2, Epoch 38/100, Training Loss (NLML): -926.6450, (RMSE): 0.3903\n",
      "branching dfGP Run 2/2, Epoch 39/100, Training Loss (NLML): -927.2559, (RMSE): 0.3835\n",
      "branching dfGP Run 2/2, Epoch 40/100, Training Loss (NLML): -927.8354, (RMSE): 0.4048\n",
      "branching dfGP Run 2/2, Epoch 41/100, Training Loss (NLML): -928.3894, (RMSE): 0.3776\n",
      "branching dfGP Run 2/2, Epoch 42/100, Training Loss (NLML): -928.9236, (RMSE): 0.3921\n",
      "branching dfGP Run 2/2, Epoch 43/100, Training Loss (NLML): -929.4497, (RMSE): 0.3764\n",
      "branching dfGP Run 2/2, Epoch 44/100, Training Loss (NLML): -929.9586, (RMSE): 0.3733\n",
      "branching dfGP Run 2/2, Epoch 45/100, Training Loss (NLML): -930.4528, (RMSE): 0.3607\n",
      "branching dfGP Run 2/2, Epoch 46/100, Training Loss (NLML): -930.9282, (RMSE): 0.3816\n",
      "branching dfGP Run 2/2, Epoch 47/100, Training Loss (NLML): -931.3761, (RMSE): 0.3697\n",
      "branching dfGP Run 2/2, Epoch 48/100, Training Loss (NLML): -931.8333, (RMSE): 0.3817\n",
      "branching dfGP Run 2/2, Epoch 49/100, Training Loss (NLML): -932.2910, (RMSE): 0.3834\n",
      "branching dfGP Run 2/2, Epoch 50/100, Training Loss (NLML): -932.7150, (RMSE): 0.3722\n",
      "branching dfGP Run 2/2, Epoch 51/100, Training Loss (NLML): -933.1224, (RMSE): 0.3602\n",
      "branching dfGP Run 2/2, Epoch 52/100, Training Loss (NLML): -933.5168, (RMSE): 0.3832\n",
      "branching dfGP Run 2/2, Epoch 53/100, Training Loss (NLML): -933.9004, (RMSE): 0.3740\n",
      "branching dfGP Run 2/2, Epoch 54/100, Training Loss (NLML): -934.2882, (RMSE): 0.3866\n",
      "branching dfGP Run 2/2, Epoch 55/100, Training Loss (NLML): -934.6536, (RMSE): 0.3911\n",
      "branching dfGP Run 2/2, Epoch 56/100, Training Loss (NLML): -935.0073, (RMSE): 0.3596\n",
      "branching dfGP Run 2/2, Epoch 57/100, Training Loss (NLML): -935.3503, (RMSE): 0.3892\n",
      "branching dfGP Run 2/2, Epoch 58/100, Training Loss (NLML): -935.6932, (RMSE): 0.3805\n",
      "branching dfGP Run 2/2, Epoch 59/100, Training Loss (NLML): -936.0284, (RMSE): 0.3642\n",
      "branching dfGP Run 2/2, Epoch 60/100, Training Loss (NLML): -936.3411, (RMSE): 0.3546\n",
      "branching dfGP Run 2/2, Epoch 61/100, Training Loss (NLML): -936.6418, (RMSE): 0.3864\n",
      "branching dfGP Run 2/2, Epoch 62/100, Training Loss (NLML): -936.9524, (RMSE): 0.3674\n",
      "branching dfGP Run 2/2, Epoch 63/100, Training Loss (NLML): -937.2533, (RMSE): 0.3861\n",
      "branching dfGP Run 2/2, Epoch 64/100, Training Loss (NLML): -937.5452, (RMSE): 0.3938\n",
      "branching dfGP Run 2/2, Epoch 65/100, Training Loss (NLML): -937.8248, (RMSE): 0.3678\n",
      "branching dfGP Run 2/2, Epoch 66/100, Training Loss (NLML): -938.1049, (RMSE): 0.3783\n",
      "branching dfGP Run 2/2, Epoch 67/100, Training Loss (NLML): -938.3706, (RMSE): 0.3719\n",
      "branching dfGP Run 2/2, Epoch 68/100, Training Loss (NLML): -938.6365, (RMSE): 0.3722\n",
      "branching dfGP Run 2/2, Epoch 69/100, Training Loss (NLML): -938.8926, (RMSE): 0.3591\n",
      "branching dfGP Run 2/2, Epoch 70/100, Training Loss (NLML): -939.1388, (RMSE): 0.3752\n",
      "branching dfGP Run 2/2, Epoch 71/100, Training Loss (NLML): -939.3931, (RMSE): 0.3694\n",
      "branching dfGP Run 2/2, Epoch 72/100, Training Loss (NLML): -939.6278, (RMSE): 0.3723\n",
      "branching dfGP Run 2/2, Epoch 73/100, Training Loss (NLML): -939.8661, (RMSE): 0.3858\n",
      "branching dfGP Run 2/2, Epoch 74/100, Training Loss (NLML): -940.0968, (RMSE): 0.3704\n",
      "branching dfGP Run 2/2, Epoch 75/100, Training Loss (NLML): -940.3204, (RMSE): 0.3480\n",
      "branching dfGP Run 2/2, Epoch 76/100, Training Loss (NLML): -940.5375, (RMSE): 0.4004\n",
      "branching dfGP Run 2/2, Epoch 77/100, Training Loss (NLML): -940.7472, (RMSE): 0.3719\n",
      "branching dfGP Run 2/2, Epoch 78/100, Training Loss (NLML): -940.9656, (RMSE): 0.3847\n",
      "branching dfGP Run 2/2, Epoch 79/100, Training Loss (NLML): -941.1678, (RMSE): 0.3651\n",
      "branching dfGP Run 2/2, Epoch 80/100, Training Loss (NLML): -941.3678, (RMSE): 0.3742\n",
      "branching dfGP Run 2/2, Epoch 81/100, Training Loss (NLML): -941.5657, (RMSE): 0.3913\n",
      "branching dfGP Run 2/2, Epoch 82/100, Training Loss (NLML): -941.7465, (RMSE): 0.3792\n",
      "branching dfGP Run 2/2, Epoch 83/100, Training Loss (NLML): -941.9374, (RMSE): 0.4003\n",
      "branching dfGP Run 2/2, Epoch 84/100, Training Loss (NLML): -942.1075, (RMSE): 0.3902\n",
      "branching dfGP Run 2/2, Epoch 85/100, Training Loss (NLML): -942.2887, (RMSE): 0.3691\n",
      "branching dfGP Run 2/2, Epoch 86/100, Training Loss (NLML): -942.4619, (RMSE): 0.3839\n",
      "branching dfGP Run 2/2, Epoch 87/100, Training Loss (NLML): -942.6304, (RMSE): 0.3950\n",
      "branching dfGP Run 2/2, Epoch 88/100, Training Loss (NLML): -942.7898, (RMSE): 0.3918\n",
      "branching dfGP Run 2/2, Epoch 89/100, Training Loss (NLML): -942.9507, (RMSE): 0.3858\n",
      "branching dfGP Run 2/2, Epoch 90/100, Training Loss (NLML): -943.1051, (RMSE): 0.3701\n",
      "branching dfGP Run 2/2, Epoch 91/100, Training Loss (NLML): -943.2588, (RMSE): 0.3634\n",
      "branching dfGP Run 2/2, Epoch 92/100, Training Loss (NLML): -943.4032, (RMSE): 0.4013\n",
      "branching dfGP Run 2/2, Epoch 93/100, Training Loss (NLML): -943.5442, (RMSE): 0.3785\n",
      "branching dfGP Run 2/2, Epoch 94/100, Training Loss (NLML): -943.6908, (RMSE): 0.3656\n",
      "branching dfGP Run 2/2, Epoch 95/100, Training Loss (NLML): -943.8260, (RMSE): 0.3718\n",
      "branching dfGP Run 2/2, Epoch 96/100, Training Loss (NLML): -943.9531, (RMSE): 0.3916\n",
      "branching dfGP Run 2/2, Epoch 97/100, Training Loss (NLML): -944.0829, (RMSE): 0.3893\n",
      "branching dfGP Run 2/2, Epoch 98/100, Training Loss (NLML): -944.2078, (RMSE): 0.3787\n",
      "branching dfGP Run 2/2, Epoch 99/100, Training Loss (NLML): -944.3298, (RMSE): 0.3990\n",
      "branching dfGP Run 2/2, Epoch 100/100, Training Loss (NLML): -944.4473, (RMSE): 0.3704\n",
      "\n",
      "Results saved to results/DFGP/branching_dfGP_metrics_per_run.csv\n",
      "\n",
      "Mean & Std saved to results/DFGP/branching_dfGP_metrics_summary.csv\n",
      "\n",
      "Training for MERGE...\n",
      "\n",
      "--- Training Run 1/2 ---\n",
      "\n",
      "Start Training\n",
      "merge dfGP Run 1/2, Epoch 1/100, Training Loss (NLML): -878.0396, (RMSE): 0.7180\n",
      "merge dfGP Run 1/2, Epoch 2/100, Training Loss (NLML): -880.8527, (RMSE): 0.6563\n",
      "merge dfGP Run 1/2, Epoch 3/100, Training Loss (NLML): -883.5741, (RMSE): 0.6998\n",
      "merge dfGP Run 1/2, Epoch 4/100, Training Loss (NLML): -886.0575, (RMSE): 0.6761\n",
      "merge dfGP Run 1/2, Epoch 5/100, Training Loss (NLML): -888.3868, (RMSE): 0.6897\n",
      "merge dfGP Run 1/2, Epoch 6/100, Training Loss (NLML): -890.5985, (RMSE): 0.6873\n",
      "merge dfGP Run 1/2, Epoch 7/100, Training Loss (NLML): -892.7537, (RMSE): 0.6656\n",
      "merge dfGP Run 1/2, Epoch 8/100, Training Loss (NLML): -894.7289, (RMSE): 0.7099\n",
      "merge dfGP Run 1/2, Epoch 9/100, Training Loss (NLML): -896.6241, (RMSE): 0.6629\n",
      "merge dfGP Run 1/2, Epoch 10/100, Training Loss (NLML): -898.4302, (RMSE): 0.6784\n",
      "merge dfGP Run 1/2, Epoch 11/100, Training Loss (NLML): -900.0972, (RMSE): 0.6775\n",
      "merge dfGP Run 1/2, Epoch 12/100, Training Loss (NLML): -901.7153, (RMSE): 0.7093\n",
      "merge dfGP Run 1/2, Epoch 13/100, Training Loss (NLML): -903.2522, (RMSE): 0.6763\n",
      "merge dfGP Run 1/2, Epoch 14/100, Training Loss (NLML): -904.7314, (RMSE): 0.7159\n",
      "merge dfGP Run 1/2, Epoch 15/100, Training Loss (NLML): -906.0885, (RMSE): 0.6564\n",
      "merge dfGP Run 1/2, Epoch 16/100, Training Loss (NLML): -907.4457, (RMSE): 0.6394\n",
      "merge dfGP Run 1/2, Epoch 17/100, Training Loss (NLML): -908.7036, (RMSE): 0.6582\n",
      "merge dfGP Run 1/2, Epoch 18/100, Training Loss (NLML): -909.8566, (RMSE): 0.6707\n",
      "merge dfGP Run 1/2, Epoch 19/100, Training Loss (NLML): -910.9756, (RMSE): 0.6816\n",
      "merge dfGP Run 1/2, Epoch 20/100, Training Loss (NLML): -912.0475, (RMSE): 0.6436\n",
      "merge dfGP Run 1/2, Epoch 21/100, Training Loss (NLML): -913.0698, (RMSE): 0.6681\n",
      "merge dfGP Run 1/2, Epoch 22/100, Training Loss (NLML): -914.0492, (RMSE): 0.6662\n",
      "merge dfGP Run 1/2, Epoch 23/100, Training Loss (NLML): -914.9702, (RMSE): 0.6732\n",
      "merge dfGP Run 1/2, Epoch 24/100, Training Loss (NLML): -915.8514, (RMSE): 0.6824\n",
      "merge dfGP Run 1/2, Epoch 25/100, Training Loss (NLML): -916.6942, (RMSE): 0.6968\n",
      "merge dfGP Run 1/2, Epoch 26/100, Training Loss (NLML): -917.4702, (RMSE): 0.7052\n",
      "merge dfGP Run 1/2, Epoch 27/100, Training Loss (NLML): -918.2269, (RMSE): 0.6481\n",
      "merge dfGP Run 1/2, Epoch 28/100, Training Loss (NLML): -918.9562, (RMSE): 0.6590\n",
      "merge dfGP Run 1/2, Epoch 29/100, Training Loss (NLML): -919.6149, (RMSE): 0.6512\n",
      "merge dfGP Run 1/2, Epoch 30/100, Training Loss (NLML): -920.2648, (RMSE): 0.6790\n",
      "merge dfGP Run 1/2, Epoch 31/100, Training Loss (NLML): -920.8821, (RMSE): 0.6906\n",
      "merge dfGP Run 1/2, Epoch 32/100, Training Loss (NLML): -921.4583, (RMSE): 0.6747\n",
      "merge dfGP Run 1/2, Epoch 33/100, Training Loss (NLML): -922.0059, (RMSE): 0.6650\n",
      "merge dfGP Run 1/2, Epoch 34/100, Training Loss (NLML): -922.5353, (RMSE): 0.6642\n",
      "merge dfGP Run 1/2, Epoch 35/100, Training Loss (NLML): -923.0339, (RMSE): 0.7087\n",
      "merge dfGP Run 1/2, Epoch 36/100, Training Loss (NLML): -923.5212, (RMSE): 0.6844\n",
      "merge dfGP Run 1/2, Epoch 37/100, Training Loss (NLML): -923.9531, (RMSE): 0.6549\n",
      "merge dfGP Run 1/2, Epoch 38/100, Training Loss (NLML): -924.3748, (RMSE): 0.6667\n",
      "merge dfGP Run 1/2, Epoch 39/100, Training Loss (NLML): -924.7740, (RMSE): 0.6453\n",
      "merge dfGP Run 1/2, Epoch 40/100, Training Loss (NLML): -925.1561, (RMSE): 0.6596\n",
      "merge dfGP Run 1/2, Epoch 41/100, Training Loss (NLML): -925.5132, (RMSE): 0.7026\n",
      "merge dfGP Run 1/2, Epoch 42/100, Training Loss (NLML): -925.8649, (RMSE): 0.6864\n",
      "merge dfGP Run 1/2, Epoch 43/100, Training Loss (NLML): -926.1836, (RMSE): 0.6579\n",
      "merge dfGP Run 1/2, Epoch 44/100, Training Loss (NLML): -926.4983, (RMSE): 0.6231\n",
      "merge dfGP Run 1/2, Epoch 45/100, Training Loss (NLML): -926.7811, (RMSE): 0.6964\n",
      "merge dfGP Run 1/2, Epoch 46/100, Training Loss (NLML): -927.0623, (RMSE): 0.7018\n",
      "merge dfGP Run 1/2, Epoch 47/100, Training Loss (NLML): -927.3318, (RMSE): 0.6831\n",
      "merge dfGP Run 1/2, Epoch 48/100, Training Loss (NLML): -927.5883, (RMSE): 0.6598\n",
      "merge dfGP Run 1/2, Epoch 49/100, Training Loss (NLML): -927.8330, (RMSE): 0.6724\n",
      "merge dfGP Run 1/2, Epoch 50/100, Training Loss (NLML): -928.0662, (RMSE): 0.6723\n",
      "merge dfGP Run 1/2, Epoch 51/100, Training Loss (NLML): -928.2861, (RMSE): 0.6932\n",
      "merge dfGP Run 1/2, Epoch 52/100, Training Loss (NLML): -928.5051, (RMSE): 0.6652\n",
      "merge dfGP Run 1/2, Epoch 53/100, Training Loss (NLML): -928.7024, (RMSE): 0.6662\n",
      "merge dfGP Run 1/2, Epoch 54/100, Training Loss (NLML): -928.9010, (RMSE): 0.6966\n",
      "merge dfGP Run 1/2, Epoch 55/100, Training Loss (NLML): -929.0879, (RMSE): 0.7177\n",
      "merge dfGP Run 1/2, Epoch 56/100, Training Loss (NLML): -929.2706, (RMSE): 0.6710\n",
      "merge dfGP Run 1/2, Epoch 57/100, Training Loss (NLML): -929.4479, (RMSE): 0.6728\n",
      "merge dfGP Run 1/2, Epoch 58/100, Training Loss (NLML): -929.6063, (RMSE): 0.6641\n",
      "merge dfGP Run 1/2, Epoch 59/100, Training Loss (NLML): -929.7728, (RMSE): 0.6638\n",
      "merge dfGP Run 1/2, Epoch 60/100, Training Loss (NLML): -929.9293, (RMSE): 0.6910\n",
      "merge dfGP Run 1/2, Epoch 61/100, Training Loss (NLML): -930.0813, (RMSE): 0.6456\n",
      "merge dfGP Run 1/2, Epoch 62/100, Training Loss (NLML): -930.2257, (RMSE): 0.6845\n",
      "merge dfGP Run 1/2, Epoch 63/100, Training Loss (NLML): -930.3678, (RMSE): 0.6846\n",
      "merge dfGP Run 1/2, Epoch 64/100, Training Loss (NLML): -930.5048, (RMSE): 0.6432\n",
      "merge dfGP Run 1/2, Epoch 65/100, Training Loss (NLML): -930.6318, (RMSE): 0.6979\n",
      "merge dfGP Run 1/2, Epoch 66/100, Training Loss (NLML): -930.7607, (RMSE): 0.6707\n",
      "merge dfGP Run 1/2, Epoch 67/100, Training Loss (NLML): -930.8856, (RMSE): 0.6378\n",
      "merge dfGP Run 1/2, Epoch 68/100, Training Loss (NLML): -931.0105, (RMSE): 0.6961\n",
      "merge dfGP Run 1/2, Epoch 69/100, Training Loss (NLML): -931.1304, (RMSE): 0.6277\n",
      "merge dfGP Run 1/2, Epoch 70/100, Training Loss (NLML): -931.2375, (RMSE): 0.6755\n",
      "merge dfGP Run 1/2, Epoch 71/100, Training Loss (NLML): -931.3481, (RMSE): 0.6651\n",
      "merge dfGP Run 1/2, Epoch 72/100, Training Loss (NLML): -931.4459, (RMSE): 0.6755\n",
      "merge dfGP Run 1/2, Epoch 73/100, Training Loss (NLML): -931.5460, (RMSE): 0.6969\n",
      "merge dfGP Run 1/2, Epoch 74/100, Training Loss (NLML): -931.6411, (RMSE): 0.6521\n",
      "merge dfGP Run 1/2, Epoch 75/100, Training Loss (NLML): -931.7267, (RMSE): 0.6856\n",
      "merge dfGP Run 1/2, Epoch 76/100, Training Loss (NLML): -931.8269, (RMSE): 0.7005\n",
      "merge dfGP Run 1/2, Epoch 77/100, Training Loss (NLML): -931.9081, (RMSE): 0.6747\n",
      "merge dfGP Run 1/2, Epoch 78/100, Training Loss (NLML): -931.9867, (RMSE): 0.6926\n",
      "merge dfGP Run 1/2, Epoch 79/100, Training Loss (NLML): -932.0615, (RMSE): 0.7024\n",
      "merge dfGP Run 1/2, Epoch 80/100, Training Loss (NLML): -932.1460, (RMSE): 0.6809\n",
      "merge dfGP Run 1/2, Epoch 81/100, Training Loss (NLML): -932.2159, (RMSE): 0.6724\n",
      "merge dfGP Run 1/2, Epoch 82/100, Training Loss (NLML): -932.2865, (RMSE): 0.6823\n",
      "merge dfGP Run 1/2, Epoch 83/100, Training Loss (NLML): -932.3566, (RMSE): 0.6466\n",
      "merge dfGP Run 1/2, Epoch 84/100, Training Loss (NLML): -932.4250, (RMSE): 0.7167\n",
      "merge dfGP Run 1/2, Epoch 85/100, Training Loss (NLML): -932.4825, (RMSE): 0.6638\n",
      "merge dfGP Run 1/2, Epoch 86/100, Training Loss (NLML): -932.5374, (RMSE): 0.6690\n",
      "merge dfGP Run 1/2, Epoch 87/100, Training Loss (NLML): -932.5994, (RMSE): 0.6949\n",
      "merge dfGP Run 1/2, Epoch 88/100, Training Loss (NLML): -932.6639, (RMSE): 0.6479\n",
      "merge dfGP Run 1/2, Epoch 89/100, Training Loss (NLML): -932.7114, (RMSE): 0.6421\n",
      "merge dfGP Run 1/2, Epoch 90/100, Training Loss (NLML): -932.7572, (RMSE): 0.7044\n",
      "merge dfGP Run 1/2, Epoch 91/100, Training Loss (NLML): -932.8127, (RMSE): 0.6853\n",
      "merge dfGP Run 1/2, Epoch 92/100, Training Loss (NLML): -932.8583, (RMSE): 0.6908\n",
      "merge dfGP Run 1/2, Epoch 93/100, Training Loss (NLML): -932.9026, (RMSE): 0.6814\n",
      "merge dfGP Run 1/2, Epoch 94/100, Training Loss (NLML): -932.9487, (RMSE): 0.6877\n",
      "merge dfGP Run 1/2, Epoch 95/100, Training Loss (NLML): -932.9893, (RMSE): 0.6810\n",
      "merge dfGP Run 1/2, Epoch 96/100, Training Loss (NLML): -933.0333, (RMSE): 0.7075\n",
      "merge dfGP Run 1/2, Epoch 97/100, Training Loss (NLML): -933.0651, (RMSE): 0.6893\n",
      "merge dfGP Run 1/2, Epoch 98/100, Training Loss (NLML): -933.1093, (RMSE): 0.7011\n",
      "merge dfGP Run 1/2, Epoch 99/100, Training Loss (NLML): -933.1439, (RMSE): 0.6653\n",
      "merge dfGP Run 1/2, Epoch 100/100, Training Loss (NLML): -933.1810, (RMSE): 0.6827\n",
      "\n",
      "--- Training Run 2/2 ---\n",
      "\n",
      "Start Training\n",
      "merge dfGP Run 2/2, Epoch 1/100, Training Loss (NLML): -889.6973, (RMSE): 0.6268\n",
      "merge dfGP Run 2/2, Epoch 2/100, Training Loss (NLML): -891.9885, (RMSE): 0.6497\n",
      "merge dfGP Run 2/2, Epoch 3/100, Training Loss (NLML): -894.1356, (RMSE): 0.6883\n",
      "merge dfGP Run 2/2, Epoch 4/100, Training Loss (NLML): -896.2228, (RMSE): 0.6613\n",
      "merge dfGP Run 2/2, Epoch 5/100, Training Loss (NLML): -898.2063, (RMSE): 0.6695\n",
      "merge dfGP Run 2/2, Epoch 6/100, Training Loss (NLML): -900.0662, (RMSE): 0.6819\n",
      "merge dfGP Run 2/2, Epoch 7/100, Training Loss (NLML): -901.8402, (RMSE): 0.6495\n",
      "merge dfGP Run 2/2, Epoch 8/100, Training Loss (NLML): -903.5491, (RMSE): 0.6865\n",
      "merge dfGP Run 2/2, Epoch 9/100, Training Loss (NLML): -905.1652, (RMSE): 0.6752\n",
      "merge dfGP Run 2/2, Epoch 10/100, Training Loss (NLML): -906.6892, (RMSE): 0.6636\n",
      "merge dfGP Run 2/2, Epoch 11/100, Training Loss (NLML): -908.1053, (RMSE): 0.6984\n",
      "merge dfGP Run 2/2, Epoch 12/100, Training Loss (NLML): -909.4728, (RMSE): 0.6470\n",
      "merge dfGP Run 2/2, Epoch 13/100, Training Loss (NLML): -910.7588, (RMSE): 0.6930\n",
      "merge dfGP Run 2/2, Epoch 14/100, Training Loss (NLML): -911.9562, (RMSE): 0.6885\n",
      "merge dfGP Run 2/2, Epoch 15/100, Training Loss (NLML): -913.1001, (RMSE): 0.6638\n",
      "merge dfGP Run 2/2, Epoch 16/100, Training Loss (NLML): -914.1493, (RMSE): 0.6813\n",
      "merge dfGP Run 2/2, Epoch 17/100, Training Loss (NLML): -915.1565, (RMSE): 0.6922\n",
      "merge dfGP Run 2/2, Epoch 18/100, Training Loss (NLML): -916.0741, (RMSE): 0.7224\n",
      "merge dfGP Run 2/2, Epoch 19/100, Training Loss (NLML): -916.9562, (RMSE): 0.7050\n",
      "merge dfGP Run 2/2, Epoch 20/100, Training Loss (NLML): -917.7457, (RMSE): 0.6803\n",
      "merge dfGP Run 2/2, Epoch 21/100, Training Loss (NLML): -918.5219, (RMSE): 0.6665\n",
      "merge dfGP Run 2/2, Epoch 22/100, Training Loss (NLML): -919.2205, (RMSE): 0.7259\n",
      "merge dfGP Run 2/2, Epoch 23/100, Training Loss (NLML): -919.8617, (RMSE): 0.6912\n",
      "merge dfGP Run 2/2, Epoch 24/100, Training Loss (NLML): -920.4730, (RMSE): 0.6541\n",
      "merge dfGP Run 2/2, Epoch 25/100, Training Loss (NLML): -921.0575, (RMSE): 0.6756\n",
      "merge dfGP Run 2/2, Epoch 26/100, Training Loss (NLML): -921.5632, (RMSE): 0.6939\n",
      "merge dfGP Run 2/2, Epoch 27/100, Training Loss (NLML): -922.0396, (RMSE): 0.6843\n",
      "merge dfGP Run 2/2, Epoch 28/100, Training Loss (NLML): -922.4915, (RMSE): 0.6886\n",
      "merge dfGP Run 2/2, Epoch 29/100, Training Loss (NLML): -922.9091, (RMSE): 0.6578\n",
      "merge dfGP Run 2/2, Epoch 30/100, Training Loss (NLML): -923.2765, (RMSE): 0.6875\n",
      "merge dfGP Run 2/2, Epoch 31/100, Training Loss (NLML): -923.6318, (RMSE): 0.6694\n",
      "merge dfGP Run 2/2, Epoch 32/100, Training Loss (NLML): -923.9454, (RMSE): 0.6811\n",
      "merge dfGP Run 2/2, Epoch 33/100, Training Loss (NLML): -924.2645, (RMSE): 0.6871\n",
      "merge dfGP Run 2/2, Epoch 34/100, Training Loss (NLML): -924.5422, (RMSE): 0.6461\n",
      "merge dfGP Run 2/2, Epoch 35/100, Training Loss (NLML): -924.8057, (RMSE): 0.6495\n",
      "merge dfGP Run 2/2, Epoch 36/100, Training Loss (NLML): -925.0601, (RMSE): 0.7218\n",
      "merge dfGP Run 2/2, Epoch 37/100, Training Loss (NLML): -925.3070, (RMSE): 0.6793\n",
      "merge dfGP Run 2/2, Epoch 38/100, Training Loss (NLML): -925.5422, (RMSE): 0.6780\n",
      "merge dfGP Run 2/2, Epoch 39/100, Training Loss (NLML): -925.7572, (RMSE): 0.6892\n",
      "merge dfGP Run 2/2, Epoch 40/100, Training Loss (NLML): -925.9658, (RMSE): 0.6632\n",
      "merge dfGP Run 2/2, Epoch 41/100, Training Loss (NLML): -926.1779, (RMSE): 0.6994\n",
      "merge dfGP Run 2/2, Epoch 42/100, Training Loss (NLML): -926.3750, (RMSE): 0.6656\n",
      "merge dfGP Run 2/2, Epoch 43/100, Training Loss (NLML): -926.5758, (RMSE): 0.6737\n",
      "merge dfGP Run 2/2, Epoch 44/100, Training Loss (NLML): -926.7584, (RMSE): 0.6710\n",
      "merge dfGP Run 2/2, Epoch 45/100, Training Loss (NLML): -926.9399, (RMSE): 0.7116\n",
      "merge dfGP Run 2/2, Epoch 46/100, Training Loss (NLML): -927.1224, (RMSE): 0.6820\n",
      "merge dfGP Run 2/2, Epoch 47/100, Training Loss (NLML): -927.2927, (RMSE): 0.6887\n",
      "merge dfGP Run 2/2, Epoch 48/100, Training Loss (NLML): -927.4615, (RMSE): 0.6150\n",
      "merge dfGP Run 2/2, Epoch 49/100, Training Loss (NLML): -927.6292, (RMSE): 0.6939\n",
      "merge dfGP Run 2/2, Epoch 50/100, Training Loss (NLML): -927.7831, (RMSE): 0.6362\n",
      "merge dfGP Run 2/2, Epoch 51/100, Training Loss (NLML): -927.9453, (RMSE): 0.6845\n",
      "merge dfGP Run 2/2, Epoch 52/100, Training Loss (NLML): -928.0978, (RMSE): 0.6772\n",
      "merge dfGP Run 2/2, Epoch 53/100, Training Loss (NLML): -928.2473, (RMSE): 0.6386\n",
      "merge dfGP Run 2/2, Epoch 54/100, Training Loss (NLML): -928.3849, (RMSE): 0.7043\n",
      "merge dfGP Run 2/2, Epoch 55/100, Training Loss (NLML): -928.5255, (RMSE): 0.6823\n",
      "merge dfGP Run 2/2, Epoch 56/100, Training Loss (NLML): -928.6534, (RMSE): 0.6728\n",
      "merge dfGP Run 2/2, Epoch 57/100, Training Loss (NLML): -928.7784, (RMSE): 0.6928\n",
      "merge dfGP Run 2/2, Epoch 58/100, Training Loss (NLML): -928.9014, (RMSE): 0.6759\n",
      "merge dfGP Run 2/2, Epoch 59/100, Training Loss (NLML): -929.0245, (RMSE): 0.7013\n",
      "merge dfGP Run 2/2, Epoch 60/100, Training Loss (NLML): -929.1370, (RMSE): 0.6406\n",
      "merge dfGP Run 2/2, Epoch 61/100, Training Loss (NLML): -929.2377, (RMSE): 0.6968\n",
      "merge dfGP Run 2/2, Epoch 62/100, Training Loss (NLML): -929.3407, (RMSE): 0.6852\n",
      "merge dfGP Run 2/2, Epoch 63/100, Training Loss (NLML): -929.4443, (RMSE): 0.6952\n",
      "merge dfGP Run 2/2, Epoch 64/100, Training Loss (NLML): -929.5463, (RMSE): 0.6605\n",
      "merge dfGP Run 2/2, Epoch 65/100, Training Loss (NLML): -929.6401, (RMSE): 0.6853\n",
      "merge dfGP Run 2/2, Epoch 66/100, Training Loss (NLML): -929.7312, (RMSE): 0.6797\n",
      "merge dfGP Run 2/2, Epoch 67/100, Training Loss (NLML): -929.8165, (RMSE): 0.6780\n",
      "merge dfGP Run 2/2, Epoch 68/100, Training Loss (NLML): -929.9050, (RMSE): 0.6889\n",
      "merge dfGP Run 2/2, Epoch 69/100, Training Loss (NLML): -929.9917, (RMSE): 0.6724\n",
      "merge dfGP Run 2/2, Epoch 70/100, Training Loss (NLML): -930.0713, (RMSE): 0.7003\n",
      "merge dfGP Run 2/2, Epoch 71/100, Training Loss (NLML): -930.1477, (RMSE): 0.6476\n",
      "merge dfGP Run 2/2, Epoch 72/100, Training Loss (NLML): -930.2267, (RMSE): 0.6912\n",
      "merge dfGP Run 2/2, Epoch 73/100, Training Loss (NLML): -930.3055, (RMSE): 0.6807\n",
      "merge dfGP Run 2/2, Epoch 74/100, Training Loss (NLML): -930.3772, (RMSE): 0.7166\n",
      "merge dfGP Run 2/2, Epoch 75/100, Training Loss (NLML): -930.4424, (RMSE): 0.7152\n",
      "merge dfGP Run 2/2, Epoch 76/100, Training Loss (NLML): -930.5165, (RMSE): 0.6499\n",
      "merge dfGP Run 2/2, Epoch 77/100, Training Loss (NLML): -930.5847, (RMSE): 0.7078\n",
      "merge dfGP Run 2/2, Epoch 78/100, Training Loss (NLML): -930.6517, (RMSE): 0.6873\n",
      "merge dfGP Run 2/2, Epoch 79/100, Training Loss (NLML): -930.7152, (RMSE): 0.6763\n",
      "merge dfGP Run 2/2, Epoch 80/100, Training Loss (NLML): -930.7747, (RMSE): 0.6779\n",
      "merge dfGP Run 2/2, Epoch 81/100, Training Loss (NLML): -930.8492, (RMSE): 0.6755\n",
      "merge dfGP Run 2/2, Epoch 82/100, Training Loss (NLML): -930.9027, (RMSE): 0.6881\n",
      "merge dfGP Run 2/2, Epoch 83/100, Training Loss (NLML): -930.9573, (RMSE): 0.6143\n",
      "merge dfGP Run 2/2, Epoch 84/100, Training Loss (NLML): -931.0188, (RMSE): 0.6705\n",
      "merge dfGP Run 2/2, Epoch 85/100, Training Loss (NLML): -931.0763, (RMSE): 0.6755\n",
      "merge dfGP Run 2/2, Epoch 86/100, Training Loss (NLML): -931.1340, (RMSE): 0.6871\n",
      "merge dfGP Run 2/2, Epoch 87/100, Training Loss (NLML): -931.1882, (RMSE): 0.7255\n",
      "merge dfGP Run 2/2, Epoch 88/100, Training Loss (NLML): -931.2362, (RMSE): 0.6428\n",
      "merge dfGP Run 2/2, Epoch 89/100, Training Loss (NLML): -931.2913, (RMSE): 0.6866\n",
      "merge dfGP Run 2/2, Epoch 90/100, Training Loss (NLML): -931.3458, (RMSE): 0.6952\n",
      "merge dfGP Run 2/2, Epoch 91/100, Training Loss (NLML): -931.3887, (RMSE): 0.6282\n",
      "merge dfGP Run 2/2, Epoch 92/100, Training Loss (NLML): -931.4380, (RMSE): 0.6802\n",
      "merge dfGP Run 2/2, Epoch 93/100, Training Loss (NLML): -931.4889, (RMSE): 0.6711\n",
      "merge dfGP Run 2/2, Epoch 94/100, Training Loss (NLML): -931.5315, (RMSE): 0.6944\n",
      "merge dfGP Run 2/2, Epoch 95/100, Training Loss (NLML): -931.5779, (RMSE): 0.6865\n",
      "merge dfGP Run 2/2, Epoch 96/100, Training Loss (NLML): -931.6206, (RMSE): 0.6892\n",
      "merge dfGP Run 2/2, Epoch 97/100, Training Loss (NLML): -931.6670, (RMSE): 0.6512\n",
      "merge dfGP Run 2/2, Epoch 98/100, Training Loss (NLML): -931.7054, (RMSE): 0.6557\n",
      "merge dfGP Run 2/2, Epoch 99/100, Training Loss (NLML): -931.7438, (RMSE): 0.6414\n",
      "merge dfGP Run 2/2, Epoch 100/100, Training Loss (NLML): -931.7809, (RMSE): 0.6534\n",
      "\n",
      "Results saved to results/DFGP/merge_dfGP_metrics_per_run.csv\n",
      "\n",
      "Mean & Std saved to results/DFGP/merge_dfGP_metrics_summary.csv\n",
      "\n",
      "Training for DEFLECTION...\n",
      "\n",
      "--- Training Run 1/2 ---\n",
      "\n",
      "Start Training\n",
      "deflection dfGP Run 1/2, Epoch 1/100, Training Loss (NLML): -861.0963, (RMSE): 0.6700\n",
      "deflection dfGP Run 1/2, Epoch 2/100, Training Loss (NLML): -864.4003, (RMSE): 0.6247\n",
      "deflection dfGP Run 1/2, Epoch 3/100, Training Loss (NLML): -867.4315, (RMSE): 0.6602\n",
      "deflection dfGP Run 1/2, Epoch 4/100, Training Loss (NLML): -870.2808, (RMSE): 0.6713\n",
      "deflection dfGP Run 1/2, Epoch 5/100, Training Loss (NLML): -872.9473, (RMSE): 0.6523\n",
      "deflection dfGP Run 1/2, Epoch 6/100, Training Loss (NLML): -875.5374, (RMSE): 0.6772\n",
      "deflection dfGP Run 1/2, Epoch 7/100, Training Loss (NLML): -877.7919, (RMSE): 0.6626\n",
      "deflection dfGP Run 1/2, Epoch 8/100, Training Loss (NLML): -879.8948, (RMSE): 0.6629\n",
      "deflection dfGP Run 1/2, Epoch 9/100, Training Loss (NLML): -881.8561, (RMSE): 0.6449\n",
      "deflection dfGP Run 1/2, Epoch 10/100, Training Loss (NLML): -883.6852, (RMSE): 0.6587\n",
      "deflection dfGP Run 1/2, Epoch 11/100, Training Loss (NLML): -885.4215, (RMSE): 0.6140\n",
      "deflection dfGP Run 1/2, Epoch 12/100, Training Loss (NLML): -886.9850, (RMSE): 0.6748\n",
      "deflection dfGP Run 1/2, Epoch 13/100, Training Loss (NLML): -888.4625, (RMSE): 0.6784\n",
      "deflection dfGP Run 1/2, Epoch 14/100, Training Loss (NLML): -889.8145, (RMSE): 0.6276\n",
      "deflection dfGP Run 1/2, Epoch 15/100, Training Loss (NLML): -891.0951, (RMSE): 0.6647\n",
      "deflection dfGP Run 1/2, Epoch 16/100, Training Loss (NLML): -892.2572, (RMSE): 0.7048\n",
      "deflection dfGP Run 1/2, Epoch 17/100, Training Loss (NLML): -893.3698, (RMSE): 0.6734\n",
      "deflection dfGP Run 1/2, Epoch 18/100, Training Loss (NLML): -894.3732, (RMSE): 0.6393\n",
      "deflection dfGP Run 1/2, Epoch 19/100, Training Loss (NLML): -895.2617, (RMSE): 0.6437\n",
      "deflection dfGP Run 1/2, Epoch 20/100, Training Loss (NLML): -896.1007, (RMSE): 0.6315\n",
      "deflection dfGP Run 1/2, Epoch 21/100, Training Loss (NLML): -896.8485, (RMSE): 0.6587\n",
      "deflection dfGP Run 1/2, Epoch 22/100, Training Loss (NLML): -897.5159, (RMSE): 0.6362\n",
      "deflection dfGP Run 1/2, Epoch 23/100, Training Loss (NLML): -898.1104, (RMSE): 0.6635\n",
      "deflection dfGP Run 1/2, Epoch 24/100, Training Loss (NLML): -898.6094, (RMSE): 0.6898\n",
      "deflection dfGP Run 1/2, Epoch 25/100, Training Loss (NLML): -899.0580, (RMSE): 0.6688\n",
      "deflection dfGP Run 1/2, Epoch 26/100, Training Loss (NLML): -899.4575, (RMSE): 0.6796\n",
      "deflection dfGP Run 1/2, Epoch 27/100, Training Loss (NLML): -899.8059, (RMSE): 0.6449\n",
      "deflection dfGP Run 1/2, Epoch 28/100, Training Loss (NLML): -900.0972, (RMSE): 0.6921\n",
      "deflection dfGP Run 1/2, Epoch 29/100, Training Loss (NLML): -900.3389, (RMSE): 0.6557\n",
      "deflection dfGP Run 1/2, Epoch 30/100, Training Loss (NLML): -900.5789, (RMSE): 0.6755\n",
      "deflection dfGP Run 1/2, Epoch 31/100, Training Loss (NLML): -900.7711, (RMSE): 0.6969\n",
      "deflection dfGP Run 1/2, Epoch 32/100, Training Loss (NLML): -900.9362, (RMSE): 0.6274\n",
      "deflection dfGP Run 1/2, Epoch 33/100, Training Loss (NLML): -901.0833, (RMSE): 0.6374\n",
      "deflection dfGP Run 1/2, Epoch 34/100, Training Loss (NLML): -901.2318, (RMSE): 0.6497\n",
      "deflection dfGP Run 1/2, Epoch 35/100, Training Loss (NLML): -901.3506, (RMSE): 0.6720\n",
      "deflection dfGP Run 1/2, Epoch 36/100, Training Loss (NLML): -901.4677, (RMSE): 0.6219\n",
      "deflection dfGP Run 1/2, Epoch 37/100, Training Loss (NLML): -901.6018, (RMSE): 0.6721\n",
      "deflection dfGP Run 1/2, Epoch 38/100, Training Loss (NLML): -901.7218, (RMSE): 0.6384\n",
      "deflection dfGP Run 1/2, Epoch 39/100, Training Loss (NLML): -901.8344, (RMSE): 0.6629\n",
      "deflection dfGP Run 1/2, Epoch 40/100, Training Loss (NLML): -901.9397, (RMSE): 0.6590\n",
      "deflection dfGP Run 1/2, Epoch 41/100, Training Loss (NLML): -902.0608, (RMSE): 0.6668\n",
      "deflection dfGP Run 1/2, Epoch 42/100, Training Loss (NLML): -902.1835, (RMSE): 0.6852\n",
      "deflection dfGP Run 1/2, Epoch 43/100, Training Loss (NLML): -902.3142, (RMSE): 0.6602\n",
      "deflection dfGP Run 1/2, Epoch 44/100, Training Loss (NLML): -902.4250, (RMSE): 0.6641\n",
      "deflection dfGP Run 1/2, Epoch 45/100, Training Loss (NLML): -902.5433, (RMSE): 0.6619\n",
      "deflection dfGP Run 1/2, Epoch 46/100, Training Loss (NLML): -902.6791, (RMSE): 0.6653\n",
      "deflection dfGP Run 1/2, Epoch 47/100, Training Loss (NLML): -902.7709, (RMSE): 0.6694\n",
      "deflection dfGP Run 1/2, Epoch 48/100, Training Loss (NLML): -902.8788, (RMSE): 0.6683\n",
      "deflection dfGP Run 1/2, Epoch 49/100, Training Loss (NLML): -903.0035, (RMSE): 0.6685\n",
      "deflection dfGP Run 1/2, Epoch 50/100, Training Loss (NLML): -903.1289, (RMSE): 0.6942\n",
      "deflection dfGP Run 1/2, Epoch 51/100, Training Loss (NLML): -903.2260, (RMSE): 0.6301\n",
      "deflection dfGP Run 1/2, Epoch 52/100, Training Loss (NLML): -903.3320, (RMSE): 0.6912\n",
      "deflection dfGP Run 1/2, Epoch 53/100, Training Loss (NLML): -903.4532, (RMSE): 0.6678\n",
      "deflection dfGP Run 1/2, Epoch 54/100, Training Loss (NLML): -903.5428, (RMSE): 0.6655\n",
      "deflection dfGP Run 1/2, Epoch 55/100, Training Loss (NLML): -903.6132, (RMSE): 0.6925\n",
      "deflection dfGP Run 1/2, Epoch 56/100, Training Loss (NLML): -903.7173, (RMSE): 0.6574\n",
      "deflection dfGP Run 1/2, Epoch 57/100, Training Loss (NLML): -903.8062, (RMSE): 0.6528\n",
      "deflection dfGP Run 1/2, Epoch 58/100, Training Loss (NLML): -903.8785, (RMSE): 0.6427\n",
      "deflection dfGP Run 1/2, Epoch 59/100, Training Loss (NLML): -903.9679, (RMSE): 0.6642\n",
      "deflection dfGP Run 1/2, Epoch 60/100, Training Loss (NLML): -904.0459, (RMSE): 0.6802\n",
      "deflection dfGP Run 1/2, Epoch 61/100, Training Loss (NLML): -904.1239, (RMSE): 0.6516\n",
      "deflection dfGP Run 1/2, Epoch 62/100, Training Loss (NLML): -904.1975, (RMSE): 0.6728\n",
      "deflection dfGP Run 1/2, Epoch 63/100, Training Loss (NLML): -904.2670, (RMSE): 0.6549\n",
      "deflection dfGP Run 1/2, Epoch 64/100, Training Loss (NLML): -904.3456, (RMSE): 0.6485\n",
      "deflection dfGP Run 1/2, Epoch 65/100, Training Loss (NLML): -904.3834, (RMSE): 0.6801\n",
      "deflection dfGP Run 1/2, Epoch 66/100, Training Loss (NLML): -904.4359, (RMSE): 0.6455\n",
      "deflection dfGP Run 1/2, Epoch 67/100, Training Loss (NLML): -904.5044, (RMSE): 0.6432\n",
      "deflection dfGP Run 1/2, Epoch 68/100, Training Loss (NLML): -904.5737, (RMSE): 0.6964\n",
      "deflection dfGP Run 1/2, Epoch 69/100, Training Loss (NLML): -904.6221, (RMSE): 0.6345\n",
      "deflection dfGP Run 1/2, Epoch 70/100, Training Loss (NLML): -904.6719, (RMSE): 0.6584\n",
      "deflection dfGP Run 1/2, Epoch 71/100, Training Loss (NLML): -904.7258, (RMSE): 0.6766\n",
      "deflection dfGP Run 1/2, Epoch 72/100, Training Loss (NLML): -904.7675, (RMSE): 0.6400\n",
      "deflection dfGP Run 1/2, Epoch 73/100, Training Loss (NLML): -904.8182, (RMSE): 0.6404\n",
      "deflection dfGP Run 1/2, Epoch 74/100, Training Loss (NLML): -904.8766, (RMSE): 0.6585\n",
      "deflection dfGP Run 1/2, Epoch 75/100, Training Loss (NLML): -904.9343, (RMSE): 0.6345\n",
      "deflection dfGP Run 1/2, Epoch 76/100, Training Loss (NLML): -904.9744, (RMSE): 0.6292\n",
      "deflection dfGP Run 1/2, Epoch 77/100, Training Loss (NLML): -905.0311, (RMSE): 0.6408\n",
      "deflection dfGP Run 1/2, Epoch 78/100, Training Loss (NLML): -905.0815, (RMSE): 0.6474\n",
      "deflection dfGP Run 1/2, Epoch 79/100, Training Loss (NLML): -905.1223, (RMSE): 0.6750\n",
      "deflection dfGP Run 1/2, Epoch 80/100, Training Loss (NLML): -905.1620, (RMSE): 0.6338\n",
      "deflection dfGP Run 1/2, Epoch 81/100, Training Loss (NLML): -905.1918, (RMSE): 0.6374\n",
      "deflection dfGP Run 1/2, Epoch 82/100, Training Loss (NLML): -905.2355, (RMSE): 0.6412\n",
      "deflection dfGP Run 1/2, Epoch 83/100, Training Loss (NLML): -905.2832, (RMSE): 0.6953\n",
      "deflection dfGP Run 1/2, Epoch 84/100, Training Loss (NLML): -905.3297, (RMSE): 0.6425\n",
      "deflection dfGP Run 1/2, Epoch 85/100, Training Loss (NLML): -905.3611, (RMSE): 0.6625\n",
      "deflection dfGP Run 1/2, Epoch 86/100, Training Loss (NLML): -905.4117, (RMSE): 0.6337\n",
      "deflection dfGP Run 1/2, Epoch 87/100, Training Loss (NLML): -905.4528, (RMSE): 0.6560\n",
      "deflection dfGP Run 1/2, Epoch 88/100, Training Loss (NLML): -905.5118, (RMSE): 0.6507\n",
      "deflection dfGP Run 1/2, Epoch 89/100, Training Loss (NLML): -905.5551, (RMSE): 0.6702\n",
      "deflection dfGP Run 1/2, Epoch 90/100, Training Loss (NLML): -905.5811, (RMSE): 0.6823\n",
      "deflection dfGP Run 1/2, Epoch 91/100, Training Loss (NLML): -905.6306, (RMSE): 0.6815\n",
      "deflection dfGP Run 1/2, Epoch 92/100, Training Loss (NLML): -905.6825, (RMSE): 0.6709\n",
      "deflection dfGP Run 1/2, Epoch 93/100, Training Loss (NLML): -905.7009, (RMSE): 0.6837\n",
      "deflection dfGP Run 1/2, Epoch 94/100, Training Loss (NLML): -905.7407, (RMSE): 0.6532\n",
      "deflection dfGP Run 1/2, Epoch 95/100, Training Loss (NLML): -905.7826, (RMSE): 0.6503\n",
      "deflection dfGP Run 1/2, Epoch 96/100, Training Loss (NLML): -905.8442, (RMSE): 0.6591\n",
      "deflection dfGP Run 1/2, Epoch 97/100, Training Loss (NLML): -905.8690, (RMSE): 0.6500\n",
      "deflection dfGP Run 1/2, Epoch 98/100, Training Loss (NLML): -905.8979, (RMSE): 0.6790\n",
      "deflection dfGP Run 1/2, Epoch 99/100, Training Loss (NLML): -905.9540, (RMSE): 0.6867\n",
      "deflection dfGP Run 1/2, Epoch 100/100, Training Loss (NLML): -905.9720, (RMSE): 0.6489\n",
      "\n",
      "--- Training Run 2/2 ---\n",
      "\n",
      "Start Training\n",
      "deflection dfGP Run 2/2, Epoch 1/100, Training Loss (NLML): -896.2969, (RMSE): 0.6384\n",
      "deflection dfGP Run 2/2, Epoch 2/100, Training Loss (NLML): -897.4454, (RMSE): 0.6452\n",
      "deflection dfGP Run 2/2, Epoch 3/100, Training Loss (NLML): -898.4537, (RMSE): 0.6535\n",
      "deflection dfGP Run 2/2, Epoch 4/100, Training Loss (NLML): -899.3909, (RMSE): 0.6907\n",
      "deflection dfGP Run 2/2, Epoch 5/100, Training Loss (NLML): -900.2086, (RMSE): 0.6652\n",
      "deflection dfGP Run 2/2, Epoch 6/100, Training Loss (NLML): -900.9135, (RMSE): 0.6436\n",
      "deflection dfGP Run 2/2, Epoch 7/100, Training Loss (NLML): -901.4758, (RMSE): 0.6753\n",
      "deflection dfGP Run 2/2, Epoch 8/100, Training Loss (NLML): -902.0071, (RMSE): 0.6927\n",
      "deflection dfGP Run 2/2, Epoch 9/100, Training Loss (NLML): -902.3987, (RMSE): 0.6362\n",
      "deflection dfGP Run 2/2, Epoch 10/100, Training Loss (NLML): -902.7289, (RMSE): 0.6346\n",
      "deflection dfGP Run 2/2, Epoch 11/100, Training Loss (NLML): -902.9867, (RMSE): 0.6697\n",
      "deflection dfGP Run 2/2, Epoch 12/100, Training Loss (NLML): -903.1816, (RMSE): 0.6501\n",
      "deflection dfGP Run 2/2, Epoch 13/100, Training Loss (NLML): -903.3206, (RMSE): 0.6763\n",
      "deflection dfGP Run 2/2, Epoch 14/100, Training Loss (NLML): -903.4543, (RMSE): 0.6457\n",
      "deflection dfGP Run 2/2, Epoch 15/100, Training Loss (NLML): -903.5481, (RMSE): 0.6340\n",
      "deflection dfGP Run 2/2, Epoch 16/100, Training Loss (NLML): -903.6404, (RMSE): 0.6778\n",
      "deflection dfGP Run 2/2, Epoch 17/100, Training Loss (NLML): -903.7174, (RMSE): 0.6349\n",
      "deflection dfGP Run 2/2, Epoch 18/100, Training Loss (NLML): -903.7743, (RMSE): 0.6447\n",
      "deflection dfGP Run 2/2, Epoch 19/100, Training Loss (NLML): -903.8844, (RMSE): 0.6708\n",
      "deflection dfGP Run 2/2, Epoch 20/100, Training Loss (NLML): -903.9360, (RMSE): 0.6559\n",
      "deflection dfGP Run 2/2, Epoch 21/100, Training Loss (NLML): -904.0558, (RMSE): 0.6868\n",
      "deflection dfGP Run 2/2, Epoch 22/100, Training Loss (NLML): -904.1578, (RMSE): 0.6650\n",
      "deflection dfGP Run 2/2, Epoch 23/100, Training Loss (NLML): -904.2408, (RMSE): 0.6611\n",
      "deflection dfGP Run 2/2, Epoch 24/100, Training Loss (NLML): -904.3153, (RMSE): 0.6388\n",
      "deflection dfGP Run 2/2, Epoch 25/100, Training Loss (NLML): -904.4263, (RMSE): 0.6458\n",
      "deflection dfGP Run 2/2, Epoch 26/100, Training Loss (NLML): -904.5133, (RMSE): 0.6622\n",
      "deflection dfGP Run 2/2, Epoch 27/100, Training Loss (NLML): -904.6307, (RMSE): 0.6622\n",
      "deflection dfGP Run 2/2, Epoch 28/100, Training Loss (NLML): -904.7251, (RMSE): 0.6766\n",
      "deflection dfGP Run 2/2, Epoch 29/100, Training Loss (NLML): -904.7963, (RMSE): 0.6740\n",
      "deflection dfGP Run 2/2, Epoch 30/100, Training Loss (NLML): -904.9189, (RMSE): 0.6504\n",
      "deflection dfGP Run 2/2, Epoch 31/100, Training Loss (NLML): -905.0310, (RMSE): 0.6218\n",
      "deflection dfGP Run 2/2, Epoch 32/100, Training Loss (NLML): -905.0824, (RMSE): 0.6497\n",
      "deflection dfGP Run 2/2, Epoch 33/100, Training Loss (NLML): -905.1591, (RMSE): 0.6593\n",
      "deflection dfGP Run 2/2, Epoch 34/100, Training Loss (NLML): -905.2352, (RMSE): 0.6589\n",
      "deflection dfGP Run 2/2, Epoch 35/100, Training Loss (NLML): -905.2770, (RMSE): 0.6886\n",
      "deflection dfGP Run 2/2, Epoch 36/100, Training Loss (NLML): -905.3563, (RMSE): 0.6873\n",
      "deflection dfGP Run 2/2, Epoch 37/100, Training Loss (NLML): -905.4512, (RMSE): 0.6668\n",
      "deflection dfGP Run 2/2, Epoch 38/100, Training Loss (NLML): -905.4729, (RMSE): 0.6477\n",
      "deflection dfGP Run 2/2, Epoch 39/100, Training Loss (NLML): -905.5538, (RMSE): 0.6730\n",
      "deflection dfGP Run 2/2, Epoch 40/100, Training Loss (NLML): -905.5847, (RMSE): 0.6861\n",
      "deflection dfGP Run 2/2, Epoch 41/100, Training Loss (NLML): -905.6274, (RMSE): 0.6723\n",
      "deflection dfGP Run 2/2, Epoch 42/100, Training Loss (NLML): -905.7303, (RMSE): 0.6607\n",
      "deflection dfGP Run 2/2, Epoch 43/100, Training Loss (NLML): -905.7557, (RMSE): 0.6723\n",
      "deflection dfGP Run 2/2, Epoch 44/100, Training Loss (NLML): -905.7747, (RMSE): 0.6674\n",
      "deflection dfGP Run 2/2, Epoch 45/100, Training Loss (NLML): -905.8440, (RMSE): 0.6702\n",
      "deflection dfGP Run 2/2, Epoch 46/100, Training Loss (NLML): -905.9081, (RMSE): 0.6475\n",
      "deflection dfGP Run 2/2, Epoch 47/100, Training Loss (NLML): -905.9572, (RMSE): 0.6673\n",
      "deflection dfGP Run 2/2, Epoch 48/100, Training Loss (NLML): -905.9868, (RMSE): 0.6649\n",
      "deflection dfGP Run 2/2, Epoch 49/100, Training Loss (NLML): -906.0532, (RMSE): 0.6647\n",
      "deflection dfGP Run 2/2, Epoch 50/100, Training Loss (NLML): -906.1000, (RMSE): 0.6417\n",
      "deflection dfGP Run 2/2, Epoch 51/100, Training Loss (NLML): -906.1848, (RMSE): 0.6824\n",
      "deflection dfGP Run 2/2, Epoch 52/100, Training Loss (NLML): -906.2156, (RMSE): 0.6490\n",
      "deflection dfGP Run 2/2, Epoch 53/100, Training Loss (NLML): -906.2504, (RMSE): 0.6538\n",
      "deflection dfGP Run 2/2, Epoch 54/100, Training Loss (NLML): -906.2894, (RMSE): 0.6649\n",
      "deflection dfGP Run 2/2, Epoch 55/100, Training Loss (NLML): -906.3420, (RMSE): 0.6784\n",
      "deflection dfGP Run 2/2, Epoch 56/100, Training Loss (NLML): -906.3804, (RMSE): 0.6822\n",
      "deflection dfGP Run 2/2, Epoch 57/100, Training Loss (NLML): -906.4427, (RMSE): 0.6678\n",
      "deflection dfGP Run 2/2, Epoch 58/100, Training Loss (NLML): -906.4983, (RMSE): 0.6628\n",
      "deflection dfGP Run 2/2, Epoch 59/100, Training Loss (NLML): -906.5280, (RMSE): 0.6477\n",
      "deflection dfGP Run 2/2, Epoch 60/100, Training Loss (NLML): -906.5839, (RMSE): 0.6436\n",
      "deflection dfGP Run 2/2, Epoch 61/100, Training Loss (NLML): -906.6182, (RMSE): 0.6816\n",
      "deflection dfGP Run 2/2, Epoch 62/100, Training Loss (NLML): -906.6652, (RMSE): 0.6804\n",
      "deflection dfGP Run 2/2, Epoch 63/100, Training Loss (NLML): -906.7173, (RMSE): 0.6411\n",
      "deflection dfGP Run 2/2, Epoch 64/100, Training Loss (NLML): -906.7606, (RMSE): 0.6836\n",
      "deflection dfGP Run 2/2, Epoch 65/100, Training Loss (NLML): -906.8035, (RMSE): 0.6465\n",
      "deflection dfGP Run 2/2, Epoch 66/100, Training Loss (NLML): -906.8259, (RMSE): 0.6524\n",
      "deflection dfGP Run 2/2, Epoch 67/100, Training Loss (NLML): -906.8654, (RMSE): 0.6497\n",
      "deflection dfGP Run 2/2, Epoch 68/100, Training Loss (NLML): -906.9200, (RMSE): 0.6515\n",
      "deflection dfGP Run 2/2, Epoch 69/100, Training Loss (NLML): -906.9458, (RMSE): 0.6573\n",
      "deflection dfGP Run 2/2, Epoch 70/100, Training Loss (NLML): -906.9860, (RMSE): 0.6657\n",
      "deflection dfGP Run 2/2, Epoch 71/100, Training Loss (NLML): -907.0457, (RMSE): 0.6677\n",
      "deflection dfGP Run 2/2, Epoch 72/100, Training Loss (NLML): -907.0763, (RMSE): 0.6819\n",
      "deflection dfGP Run 2/2, Epoch 73/100, Training Loss (NLML): -907.1041, (RMSE): 0.6627\n",
      "deflection dfGP Run 2/2, Epoch 74/100, Training Loss (NLML): -907.1472, (RMSE): 0.6902\n",
      "deflection dfGP Run 2/2, Epoch 75/100, Training Loss (NLML): -907.1877, (RMSE): 0.6442\n",
      "deflection dfGP Run 2/2, Epoch 76/100, Training Loss (NLML): -907.2294, (RMSE): 0.6927\n",
      "deflection dfGP Run 2/2, Epoch 77/100, Training Loss (NLML): -907.2778, (RMSE): 0.6310\n",
      "deflection dfGP Run 2/2, Epoch 78/100, Training Loss (NLML): -907.3101, (RMSE): 0.6799\n",
      "deflection dfGP Run 2/2, Epoch 79/100, Training Loss (NLML): -907.3456, (RMSE): 0.6794\n",
      "deflection dfGP Run 2/2, Epoch 80/100, Training Loss (NLML): -907.3796, (RMSE): 0.6691\n",
      "deflection dfGP Run 2/2, Epoch 81/100, Training Loss (NLML): -907.4071, (RMSE): 0.6481\n",
      "deflection dfGP Run 2/2, Epoch 82/100, Training Loss (NLML): -907.4537, (RMSE): 0.6588\n",
      "deflection dfGP Run 2/2, Epoch 83/100, Training Loss (NLML): -907.4546, (RMSE): 0.6848\n",
      "deflection dfGP Run 2/2, Epoch 84/100, Training Loss (NLML): -907.5194, (RMSE): 0.6983\n",
      "deflection dfGP Run 2/2, Epoch 85/100, Training Loss (NLML): -907.5481, (RMSE): 0.6519\n",
      "deflection dfGP Run 2/2, Epoch 86/100, Training Loss (NLML): -907.5900, (RMSE): 0.6577\n",
      "deflection dfGP Run 2/2, Epoch 87/100, Training Loss (NLML): -907.6274, (RMSE): 0.7112\n",
      "deflection dfGP Run 2/2, Epoch 88/100, Training Loss (NLML): -907.6561, (RMSE): 0.6382\n",
      "deflection dfGP Run 2/2, Epoch 89/100, Training Loss (NLML): -907.6904, (RMSE): 0.6409\n",
      "deflection dfGP Run 2/2, Epoch 90/100, Training Loss (NLML): -907.7205, (RMSE): 0.6459\n",
      "deflection dfGP Run 2/2, Epoch 91/100, Training Loss (NLML): -907.7739, (RMSE): 0.6637\n",
      "deflection dfGP Run 2/2, Epoch 92/100, Training Loss (NLML): -907.7697, (RMSE): 0.6704\n",
      "deflection dfGP Run 2/2, Epoch 93/100, Training Loss (NLML): -907.8027, (RMSE): 0.6922\n",
      "deflection dfGP Run 2/2, Epoch 94/100, Training Loss (NLML): -907.8416, (RMSE): 0.6794\n",
      "deflection dfGP Run 2/2, Epoch 95/100, Training Loss (NLML): -907.9062, (RMSE): 0.6451\n",
      "deflection dfGP Run 2/2, Epoch 96/100, Training Loss (NLML): -907.9191, (RMSE): 0.6306\n",
      "deflection dfGP Run 2/2, Epoch 97/100, Training Loss (NLML): -907.9371, (RMSE): 0.6722\n",
      "deflection dfGP Run 2/2, Epoch 98/100, Training Loss (NLML): -907.9835, (RMSE): 0.6630\n",
      "deflection dfGP Run 2/2, Epoch 99/100, Training Loss (NLML): -907.9883, (RMSE): 0.6622\n",
      "deflection dfGP Run 2/2, Epoch 100/100, Training Loss (NLML): -908.0212, (RMSE): 0.6640\n",
      "\n",
      "Results saved to results/DFGP/deflection_dfGP_metrics_per_run.csv\n",
      "\n",
      "Mean & Std saved to results/DFGP/deflection_dfGP_metrics_summary.csv\n",
      "\n",
      "Training for RIDGE...\n",
      "\n",
      "--- Training Run 1/2 ---\n",
      "\n",
      "Start Training\n",
      "ridge dfGP Run 1/2, Epoch 1/100, Training Loss (NLML): -539.7969, (RMSE): 0.8107\n",
      "ridge dfGP Run 1/2, Epoch 2/100, Training Loss (NLML): -565.2105, (RMSE): 0.7829\n",
      "ridge dfGP Run 1/2, Epoch 3/100, Training Loss (NLML): -591.2576, (RMSE): 0.8010\n",
      "ridge dfGP Run 1/2, Epoch 4/100, Training Loss (NLML): -614.0786, (RMSE): 0.8219\n",
      "ridge dfGP Run 1/2, Epoch 5/100, Training Loss (NLML): -636.1077, (RMSE): 0.8310\n",
      "ridge dfGP Run 1/2, Epoch 6/100, Training Loss (NLML): -657.1219, (RMSE): 0.7836\n",
      "ridge dfGP Run 1/2, Epoch 7/100, Training Loss (NLML): -675.8033, (RMSE): 0.8150\n",
      "ridge dfGP Run 1/2, Epoch 8/100, Training Loss (NLML): -692.2990, (RMSE): 0.8659\n",
      "ridge dfGP Run 1/2, Epoch 9/100, Training Loss (NLML): -706.5955, (RMSE): 0.8032\n",
      "ridge dfGP Run 1/2, Epoch 10/100, Training Loss (NLML): -718.7683, (RMSE): 0.7916\n",
      "ridge dfGP Run 1/2, Epoch 11/100, Training Loss (NLML): -728.8456, (RMSE): 0.7914\n",
      "ridge dfGP Run 1/2, Epoch 12/100, Training Loss (NLML): -736.8157, (RMSE): 0.7785\n",
      "ridge dfGP Run 1/2, Epoch 13/100, Training Loss (NLML): -742.9941, (RMSE): 0.7977\n",
      "ridge dfGP Run 1/2, Epoch 14/100, Training Loss (NLML): -746.9856, (RMSE): 0.7908\n",
      "ridge dfGP Run 1/2, Epoch 15/100, Training Loss (NLML): -749.5298, (RMSE): 0.7673\n",
      "ridge dfGP Run 1/2, Epoch 16/100, Training Loss (NLML): -751.1132, (RMSE): 0.8575\n",
      "ridge dfGP Run 1/2, Epoch 17/100, Training Loss (NLML): -751.9318, (RMSE): 0.8536\n",
      "ridge dfGP Run 1/2, Epoch 18/100, Training Loss (NLML): -752.4514, (RMSE): 0.7897\n",
      "ridge dfGP Run 1/2, Epoch 19/100, Training Loss (NLML): -753.0920, (RMSE): 0.8062\n",
      "ridge dfGP Run 1/2, Epoch 20/100, Training Loss (NLML): -753.8397, (RMSE): 0.8354\n",
      "ridge dfGP Run 1/2, Epoch 21/100, Training Loss (NLML): -755.0146, (RMSE): 0.8014\n",
      "ridge dfGP Run 1/2, Epoch 22/100, Training Loss (NLML): -756.5437, (RMSE): 0.7997\n",
      "ridge dfGP Run 1/2, Epoch 23/100, Training Loss (NLML): -758.4191, (RMSE): 0.8216\n",
      "ridge dfGP Run 1/2, Epoch 24/100, Training Loss (NLML): -760.8481, (RMSE): 0.7659\n",
      "ridge dfGP Run 1/2, Epoch 25/100, Training Loss (NLML): -763.6801, (RMSE): 0.8370\n",
      "ridge dfGP Run 1/2, Epoch 26/100, Training Loss (NLML): -766.3275, (RMSE): 0.8467\n",
      "ridge dfGP Run 1/2, Epoch 27/100, Training Loss (NLML): -769.3179, (RMSE): 0.8316\n",
      "ridge dfGP Run 1/2, Epoch 28/100, Training Loss (NLML): -772.1824, (RMSE): 0.7831\n",
      "ridge dfGP Run 1/2, Epoch 29/100, Training Loss (NLML): -774.8860, (RMSE): 0.8044\n",
      "ridge dfGP Run 1/2, Epoch 30/100, Training Loss (NLML): -777.5748, (RMSE): 0.8201\n",
      "ridge dfGP Run 1/2, Epoch 31/100, Training Loss (NLML): -780.2598, (RMSE): 0.8167\n",
      "ridge dfGP Run 1/2, Epoch 32/100, Training Loss (NLML): -782.4797, (RMSE): 0.8060\n",
      "ridge dfGP Run 1/2, Epoch 33/100, Training Loss (NLML): -784.5617, (RMSE): 0.8089\n",
      "ridge dfGP Run 1/2, Epoch 34/100, Training Loss (NLML): -786.1292, (RMSE): 0.8057\n",
      "ridge dfGP Run 1/2, Epoch 35/100, Training Loss (NLML): -787.3882, (RMSE): 0.8208\n",
      "ridge dfGP Run 1/2, Epoch 36/100, Training Loss (NLML): -788.3585, (RMSE): 0.8138\n",
      "ridge dfGP Run 1/2, Epoch 37/100, Training Loss (NLML): -789.2543, (RMSE): 0.8264\n",
      "ridge dfGP Run 1/2, Epoch 38/100, Training Loss (NLML): -789.8942, (RMSE): 0.7863\n",
      "ridge dfGP Run 1/2, Epoch 39/100, Training Loss (NLML): -790.6899, (RMSE): 0.8330\n",
      "ridge dfGP Run 1/2, Epoch 40/100, Training Loss (NLML): -791.3878, (RMSE): 0.8697\n",
      "ridge dfGP Run 1/2, Epoch 41/100, Training Loss (NLML): -792.2001, (RMSE): 0.8442\n",
      "ridge dfGP Run 1/2, Epoch 42/100, Training Loss (NLML): -793.1295, (RMSE): 0.8288\n",
      "ridge dfGP Run 1/2, Epoch 43/100, Training Loss (NLML): -793.8764, (RMSE): 0.8149\n",
      "ridge dfGP Run 1/2, Epoch 44/100, Training Loss (NLML): -795.1359, (RMSE): 0.7862\n",
      "ridge dfGP Run 1/2, Epoch 45/100, Training Loss (NLML): -796.4092, (RMSE): 0.8269\n",
      "ridge dfGP Run 1/2, Epoch 46/100, Training Loss (NLML): -797.2058, (RMSE): 0.8232\n",
      "ridge dfGP Run 1/2, Epoch 47/100, Training Loss (NLML): -798.3047, (RMSE): 0.8097\n",
      "ridge dfGP Run 1/2, Epoch 48/100, Training Loss (NLML): -799.3944, (RMSE): 0.8010\n",
      "ridge dfGP Run 1/2, Epoch 49/100, Training Loss (NLML): -800.4034, (RMSE): 0.8008\n",
      "ridge dfGP Run 1/2, Epoch 50/100, Training Loss (NLML): -801.1279, (RMSE): 0.8419\n",
      "ridge dfGP Run 1/2, Epoch 51/100, Training Loss (NLML): -801.9728, (RMSE): 0.7875\n",
      "ridge dfGP Run 1/2, Epoch 52/100, Training Loss (NLML): -802.6986, (RMSE): 0.8405\n",
      "ridge dfGP Run 1/2, Epoch 53/100, Training Loss (NLML): -803.3959, (RMSE): 0.7525\n",
      "ridge dfGP Run 1/2, Epoch 54/100, Training Loss (NLML): -804.1250, (RMSE): 0.8264\n",
      "ridge dfGP Run 1/2, Epoch 55/100, Training Loss (NLML): -804.7935, (RMSE): 0.8243\n",
      "ridge dfGP Run 1/2, Epoch 56/100, Training Loss (NLML): -805.3207, (RMSE): 0.8702\n",
      "ridge dfGP Run 1/2, Epoch 57/100, Training Loss (NLML): -805.8521, (RMSE): 0.8167\n",
      "ridge dfGP Run 1/2, Epoch 58/100, Training Loss (NLML): -806.5476, (RMSE): 0.8131\n",
      "ridge dfGP Run 1/2, Epoch 59/100, Training Loss (NLML): -807.1102, (RMSE): 0.8033\n",
      "ridge dfGP Run 1/2, Epoch 60/100, Training Loss (NLML): -807.6317, (RMSE): 0.8063\n",
      "ridge dfGP Run 1/2, Epoch 61/100, Training Loss (NLML): -808.2971, (RMSE): 0.8009\n",
      "ridge dfGP Run 1/2, Epoch 62/100, Training Loss (NLML): -808.9250, (RMSE): 0.8134\n",
      "ridge dfGP Run 1/2, Epoch 63/100, Training Loss (NLML): -809.5701, (RMSE): 0.8195\n",
      "ridge dfGP Run 1/2, Epoch 64/100, Training Loss (NLML): -810.0424, (RMSE): 0.8367\n",
      "ridge dfGP Run 1/2, Epoch 65/100, Training Loss (NLML): -810.7658, (RMSE): 0.7928\n",
      "ridge dfGP Run 1/2, Epoch 66/100, Training Loss (NLML): -811.2952, (RMSE): 0.8381\n",
      "ridge dfGP Run 1/2, Epoch 67/100, Training Loss (NLML): -811.8112, (RMSE): 0.7693\n",
      "ridge dfGP Run 1/2, Epoch 68/100, Training Loss (NLML): -812.3459, (RMSE): 0.8132\n",
      "ridge dfGP Run 1/2, Epoch 69/100, Training Loss (NLML): -812.8316, (RMSE): 0.8245\n",
      "ridge dfGP Run 1/2, Epoch 70/100, Training Loss (NLML): -813.3892, (RMSE): 0.8512\n",
      "ridge dfGP Run 1/2, Epoch 71/100, Training Loss (NLML): -813.8170, (RMSE): 0.8021\n",
      "ridge dfGP Run 1/2, Epoch 72/100, Training Loss (NLML): -814.1688, (RMSE): 0.8126\n",
      "ridge dfGP Run 1/2, Epoch 73/100, Training Loss (NLML): -814.6946, (RMSE): 0.8368\n",
      "ridge dfGP Run 1/2, Epoch 74/100, Training Loss (NLML): -815.1593, (RMSE): 0.7914\n",
      "ridge dfGP Run 1/2, Epoch 75/100, Training Loss (NLML): -815.4669, (RMSE): 0.8315\n",
      "ridge dfGP Run 1/2, Epoch 76/100, Training Loss (NLML): -816.0460, (RMSE): 0.8399\n",
      "ridge dfGP Run 1/2, Epoch 77/100, Training Loss (NLML): -816.4551, (RMSE): 0.7859\n",
      "ridge dfGP Run 1/2, Epoch 78/100, Training Loss (NLML): -816.8627, (RMSE): 0.7675\n",
      "ridge dfGP Run 1/2, Epoch 79/100, Training Loss (NLML): -817.2832, (RMSE): 0.7679\n",
      "ridge dfGP Run 1/2, Epoch 80/100, Training Loss (NLML): -817.5741, (RMSE): 0.8065\n",
      "ridge dfGP Run 1/2, Epoch 81/100, Training Loss (NLML): -818.1039, (RMSE): 0.7749\n",
      "ridge dfGP Run 1/2, Epoch 82/100, Training Loss (NLML): -818.5355, (RMSE): 0.8310\n",
      "ridge dfGP Run 1/2, Epoch 83/100, Training Loss (NLML): -818.8922, (RMSE): 0.8014\n",
      "ridge dfGP Run 1/2, Epoch 84/100, Training Loss (NLML): -819.2788, (RMSE): 0.8063\n",
      "ridge dfGP Run 1/2, Epoch 85/100, Training Loss (NLML): -819.6511, (RMSE): 0.8405\n",
      "ridge dfGP Run 1/2, Epoch 86/100, Training Loss (NLML): -819.9554, (RMSE): 0.8260\n",
      "ridge dfGP Run 1/2, Epoch 87/100, Training Loss (NLML): -820.3380, (RMSE): 0.8021\n",
      "ridge dfGP Run 1/2, Epoch 88/100, Training Loss (NLML): -820.7179, (RMSE): 0.7950\n",
      "ridge dfGP Run 1/2, Epoch 89/100, Training Loss (NLML): -821.0128, (RMSE): 0.7706\n",
      "ridge dfGP Run 1/2, Epoch 90/100, Training Loss (NLML): -821.4133, (RMSE): 0.8259\n",
      "ridge dfGP Run 1/2, Epoch 91/100, Training Loss (NLML): -821.6876, (RMSE): 0.7802\n",
      "ridge dfGP Run 1/2, Epoch 92/100, Training Loss (NLML): -822.0024, (RMSE): 0.8203\n",
      "ridge dfGP Run 1/2, Epoch 93/100, Training Loss (NLML): -822.2200, (RMSE): 0.8062\n",
      "ridge dfGP Run 1/2, Epoch 94/100, Training Loss (NLML): -822.6008, (RMSE): 0.7858\n",
      "ridge dfGP Run 1/2, Epoch 95/100, Training Loss (NLML): -822.7977, (RMSE): 0.8328\n",
      "ridge dfGP Run 1/2, Epoch 96/100, Training Loss (NLML): -823.1595, (RMSE): 0.7770\n",
      "ridge dfGP Run 1/2, Epoch 97/100, Training Loss (NLML): -823.4999, (RMSE): 0.8671\n",
      "ridge dfGP Run 1/2, Epoch 98/100, Training Loss (NLML): -823.7571, (RMSE): 0.8169\n",
      "ridge dfGP Run 1/2, Epoch 99/100, Training Loss (NLML): -823.9775, (RMSE): 0.8055\n",
      "ridge dfGP Run 1/2, Epoch 100/100, Training Loss (NLML): -824.2340, (RMSE): 0.7981\n",
      "\n",
      "--- Training Run 2/2 ---\n",
      "\n",
      "Start Training\n",
      "ridge dfGP Run 2/2, Epoch 1/100, Training Loss (NLML): -656.0981, (RMSE): 0.8380\n",
      "ridge dfGP Run 2/2, Epoch 2/100, Training Loss (NLML): -679.9719, (RMSE): 0.8168\n",
      "ridge dfGP Run 2/2, Epoch 3/100, Training Loss (NLML): -702.8912, (RMSE): 0.7950\n",
      "ridge dfGP Run 2/2, Epoch 4/100, Training Loss (NLML): -724.4689, (RMSE): 0.8136\n",
      "ridge dfGP Run 2/2, Epoch 5/100, Training Loss (NLML): -743.0184, (RMSE): 0.8413\n",
      "ridge dfGP Run 2/2, Epoch 6/100, Training Loss (NLML): -758.6315, (RMSE): 0.8309\n",
      "ridge dfGP Run 2/2, Epoch 7/100, Training Loss (NLML): -772.3351, (RMSE): 0.7977\n",
      "ridge dfGP Run 2/2, Epoch 8/100, Training Loss (NLML): -783.9194, (RMSE): 0.7980\n",
      "ridge dfGP Run 2/2, Epoch 9/100, Training Loss (NLML): -792.7106, (RMSE): 0.8072\n",
      "ridge dfGP Run 2/2, Epoch 10/100, Training Loss (NLML): -798.9771, (RMSE): 0.8434\n",
      "ridge dfGP Run 2/2, Epoch 11/100, Training Loss (NLML): -803.1294, (RMSE): 0.7973\n",
      "ridge dfGP Run 2/2, Epoch 12/100, Training Loss (NLML): -805.1005, (RMSE): 0.7981\n",
      "ridge dfGP Run 2/2, Epoch 13/100, Training Loss (NLML): -805.8314, (RMSE): 0.8108\n",
      "ridge dfGP Run 2/2, Epoch 14/100, Training Loss (NLML): -805.0468, (RMSE): 0.8346\n",
      "ridge dfGP Run 2/2, Epoch 15/100, Training Loss (NLML): -804.1563, (RMSE): 0.8172\n",
      "ridge dfGP Run 2/2, Epoch 16/100, Training Loss (NLML): -803.2257, (RMSE): 0.7977\n",
      "ridge dfGP Run 2/2, Epoch 17/100, Training Loss (NLML): -802.6550, (RMSE): 0.8356\n",
      "ridge dfGP Run 2/2, Epoch 18/100, Training Loss (NLML): -802.0529, (RMSE): 0.8226\n",
      "ridge dfGP Run 2/2, Epoch 19/100, Training Loss (NLML): -802.0756, (RMSE): 0.7825\n",
      "ridge dfGP Run 2/2, Epoch 20/100, Training Loss (NLML): -802.3635, (RMSE): 0.8103\n",
      "ridge dfGP Run 2/2, Epoch 21/100, Training Loss (NLML): -803.2518, (RMSE): 0.7792\n",
      "ridge dfGP Run 2/2, Epoch 22/100, Training Loss (NLML): -804.5235, (RMSE): 0.8237\n",
      "ridge dfGP Run 2/2, Epoch 23/100, Training Loss (NLML): -805.7728, (RMSE): 0.7899\n",
      "ridge dfGP Run 2/2, Epoch 24/100, Training Loss (NLML): -807.6661, (RMSE): 0.8039\n",
      "ridge dfGP Run 2/2, Epoch 25/100, Training Loss (NLML): -809.5209, (RMSE): 0.8771\n",
      "ridge dfGP Run 2/2, Epoch 26/100, Training Loss (NLML): -811.3700, (RMSE): 0.8216\n",
      "ridge dfGP Run 2/2, Epoch 27/100, Training Loss (NLML): -813.4257, (RMSE): 0.8064\n",
      "ridge dfGP Run 2/2, Epoch 28/100, Training Loss (NLML): -815.2147, (RMSE): 0.7926\n",
      "ridge dfGP Run 2/2, Epoch 29/100, Training Loss (NLML): -817.2447, (RMSE): 0.7695\n",
      "ridge dfGP Run 2/2, Epoch 30/100, Training Loss (NLML): -818.8063, (RMSE): 0.8246\n",
      "ridge dfGP Run 2/2, Epoch 31/100, Training Loss (NLML): -820.0342, (RMSE): 0.8076\n",
      "ridge dfGP Run 2/2, Epoch 32/100, Training Loss (NLML): -821.1151, (RMSE): 0.8151\n",
      "ridge dfGP Run 2/2, Epoch 33/100, Training Loss (NLML): -821.8391, (RMSE): 0.7958\n",
      "ridge dfGP Run 2/2, Epoch 34/100, Training Loss (NLML): -822.3322, (RMSE): 0.8395\n",
      "ridge dfGP Run 2/2, Epoch 35/100, Training Loss (NLML): -822.6181, (RMSE): 0.8162\n",
      "ridge dfGP Run 2/2, Epoch 36/100, Training Loss (NLML): -822.9461, (RMSE): 0.8238\n",
      "ridge dfGP Run 2/2, Epoch 37/100, Training Loss (NLML): -823.1467, (RMSE): 0.7955\n",
      "ridge dfGP Run 2/2, Epoch 38/100, Training Loss (NLML): -823.4883, (RMSE): 0.7843\n",
      "ridge dfGP Run 2/2, Epoch 39/100, Training Loss (NLML): -823.6010, (RMSE): 0.7650\n",
      "ridge dfGP Run 2/2, Epoch 40/100, Training Loss (NLML): -824.0137, (RMSE): 0.7804\n",
      "ridge dfGP Run 2/2, Epoch 41/100, Training Loss (NLML): -824.4500, (RMSE): 0.7745\n",
      "ridge dfGP Run 2/2, Epoch 42/100, Training Loss (NLML): -824.9447, (RMSE): 0.7685\n",
      "ridge dfGP Run 2/2, Epoch 43/100, Training Loss (NLML): -825.6052, (RMSE): 0.8171\n",
      "ridge dfGP Run 2/2, Epoch 44/100, Training Loss (NLML): -826.0884, (RMSE): 0.7366\n",
      "ridge dfGP Run 2/2, Epoch 45/100, Training Loss (NLML): -826.8777, (RMSE): 0.7802\n",
      "ridge dfGP Run 2/2, Epoch 46/100, Training Loss (NLML): -827.3164, (RMSE): 0.7788\n",
      "ridge dfGP Run 2/2, Epoch 47/100, Training Loss (NLML): -827.8835, (RMSE): 0.8218\n",
      "ridge dfGP Run 2/2, Epoch 48/100, Training Loss (NLML): -828.3690, (RMSE): 0.7961\n",
      "ridge dfGP Run 2/2, Epoch 49/100, Training Loss (NLML): -828.7659, (RMSE): 0.8254\n",
      "ridge dfGP Run 2/2, Epoch 50/100, Training Loss (NLML): -829.2579, (RMSE): 0.8317\n",
      "ridge dfGP Run 2/2, Epoch 51/100, Training Loss (NLML): -829.4974, (RMSE): 0.7956\n",
      "ridge dfGP Run 2/2, Epoch 52/100, Training Loss (NLML): -829.6868, (RMSE): 0.7740\n",
      "ridge dfGP Run 2/2, Epoch 53/100, Training Loss (NLML): -830.0449, (RMSE): 0.8327\n",
      "ridge dfGP Run 2/2, Epoch 54/100, Training Loss (NLML): -830.3056, (RMSE): 0.8225\n",
      "ridge dfGP Run 2/2, Epoch 55/100, Training Loss (NLML): -830.4495, (RMSE): 0.8561\n",
      "ridge dfGP Run 2/2, Epoch 56/100, Training Loss (NLML): -830.7799, (RMSE): 0.7452\n",
      "ridge dfGP Run 2/2, Epoch 57/100, Training Loss (NLML): -831.0347, (RMSE): 0.8094\n",
      "ridge dfGP Run 2/2, Epoch 58/100, Training Loss (NLML): -831.4347, (RMSE): 0.7972\n",
      "ridge dfGP Run 2/2, Epoch 59/100, Training Loss (NLML): -831.5620, (RMSE): 0.8399\n",
      "ridge dfGP Run 2/2, Epoch 60/100, Training Loss (NLML): -831.7997, (RMSE): 0.8012\n",
      "ridge dfGP Run 2/2, Epoch 61/100, Training Loss (NLML): -832.2222, (RMSE): 0.7794\n",
      "ridge dfGP Run 2/2, Epoch 62/100, Training Loss (NLML): -832.4822, (RMSE): 0.7792\n",
      "ridge dfGP Run 2/2, Epoch 63/100, Training Loss (NLML): -832.6709, (RMSE): 0.8239\n",
      "ridge dfGP Run 2/2, Epoch 64/100, Training Loss (NLML): -832.8164, (RMSE): 0.7694\n",
      "ridge dfGP Run 2/2, Epoch 65/100, Training Loss (NLML): -833.0456, (RMSE): 0.7855\n",
      "ridge dfGP Run 2/2, Epoch 66/100, Training Loss (NLML): -833.3842, (RMSE): 0.8100\n",
      "ridge dfGP Run 2/2, Epoch 67/100, Training Loss (NLML): -833.6037, (RMSE): 0.7948\n",
      "ridge dfGP Run 2/2, Epoch 68/100, Training Loss (NLML): -833.7888, (RMSE): 0.8342\n",
      "ridge dfGP Run 2/2, Epoch 69/100, Training Loss (NLML): -833.9151, (RMSE): 0.8029\n",
      "ridge dfGP Run 2/2, Epoch 70/100, Training Loss (NLML): -834.0604, (RMSE): 0.8061\n",
      "ridge dfGP Run 2/2, Epoch 71/100, Training Loss (NLML): -834.1237, (RMSE): 0.8185\n",
      "ridge dfGP Run 2/2, Epoch 72/100, Training Loss (NLML): -834.2825, (RMSE): 0.8160\n",
      "ridge dfGP Run 2/2, Epoch 73/100, Training Loss (NLML): -834.4788, (RMSE): 0.8267\n",
      "ridge dfGP Run 2/2, Epoch 74/100, Training Loss (NLML): -834.7380, (RMSE): 0.7970\n",
      "ridge dfGP Run 2/2, Epoch 75/100, Training Loss (NLML): -834.8283, (RMSE): 0.8133\n",
      "ridge dfGP Run 2/2, Epoch 76/100, Training Loss (NLML): -835.0089, (RMSE): 0.8252\n",
      "ridge dfGP Run 2/2, Epoch 77/100, Training Loss (NLML): -835.1111, (RMSE): 0.8266\n",
      "ridge dfGP Run 2/2, Epoch 78/100, Training Loss (NLML): -835.2521, (RMSE): 0.7910\n",
      "ridge dfGP Run 2/2, Epoch 79/100, Training Loss (NLML): -835.4594, (RMSE): 0.8306\n",
      "ridge dfGP Run 2/2, Epoch 80/100, Training Loss (NLML): -835.4533, (RMSE): 0.8491\n",
      "ridge dfGP Run 2/2, Epoch 81/100, Training Loss (NLML): -835.7142, (RMSE): 0.8341\n",
      "ridge dfGP Run 2/2, Epoch 82/100, Training Loss (NLML): -835.8708, (RMSE): 0.8379\n",
      "ridge dfGP Run 2/2, Epoch 83/100, Training Loss (NLML): -835.8866, (RMSE): 0.8122\n",
      "ridge dfGP Run 2/2, Epoch 84/100, Training Loss (NLML): -836.0735, (RMSE): 0.8191\n",
      "ridge dfGP Run 2/2, Epoch 85/100, Training Loss (NLML): -836.1583, (RMSE): 0.8237\n",
      "ridge dfGP Run 2/2, Epoch 86/100, Training Loss (NLML): -836.1503, (RMSE): 0.8391\n",
      "ridge dfGP Run 2/2, Epoch 87/100, Training Loss (NLML): -836.2953, (RMSE): 0.8124\n",
      "ridge dfGP Run 2/2, Epoch 88/100, Training Loss (NLML): -836.3546, (RMSE): 0.8158\n",
      "ridge dfGP Run 2/2, Epoch 89/100, Training Loss (NLML): -836.4866, (RMSE): 0.8112\n",
      "ridge dfGP Run 2/2, Epoch 90/100, Training Loss (NLML): -836.5966, (RMSE): 0.7819\n",
      "ridge dfGP Run 2/2, Epoch 91/100, Training Loss (NLML): -836.6889, (RMSE): 0.8181\n",
      "ridge dfGP Run 2/2, Epoch 92/100, Training Loss (NLML): -836.7526, (RMSE): 0.8464\n",
      "ridge dfGP Run 2/2, Epoch 93/100, Training Loss (NLML): -836.9147, (RMSE): 0.8035\n",
      "ridge dfGP Run 2/2, Epoch 94/100, Training Loss (NLML): -836.9040, (RMSE): 0.8302\n",
      "ridge dfGP Run 2/2, Epoch 95/100, Training Loss (NLML): -837.0023, (RMSE): 0.8041\n",
      "ridge dfGP Run 2/2, Epoch 96/100, Training Loss (NLML): -837.0666, (RMSE): 0.7556\n",
      "ridge dfGP Run 2/2, Epoch 97/100, Training Loss (NLML): -837.1229, (RMSE): 0.8019\n",
      "ridge dfGP Run 2/2, Epoch 98/100, Training Loss (NLML): -837.3145, (RMSE): 0.8092\n",
      "ridge dfGP Run 2/2, Epoch 99/100, Training Loss (NLML): -837.3948, (RMSE): 0.7975\n",
      "ridge dfGP Run 2/2, Epoch 100/100, Training Loss (NLML): -837.3423, (RMSE): 0.8075\n",
      "\n",
      "Results saved to results/DFGP/ridge_dfGP_metrics_per_run.csv\n",
      "\n",
      "Mean & Std saved to results/DFGP/ridge_dfGP_metrics_summary.csv\n"
     ]
    }
   ],
   "source": [
    "from GP_models import GP_predict, optimise_hypers_on_train\n",
    "from simulate import simulate_convergence, simulate_branching, simulate_ridge, simulate_merge, simulate_deflection\n",
    "from metrics import compute_RMSE, compute_MAE, log_likelihood_test\n",
    "from utils import set_seed\n",
    "from metrics import compute_RMSE\n",
    "\n",
    "# Global file for training configs\n",
    "from configs import PATIENCE, MAX_NUM_EPOCHS, NUM_RUNS, GP_LEARNING_RATE, WEIGHT_DECAY, BATCH_SIZE, N_SIDE, DFGP_RESULTS_DIR, SIGMA_F_RANGE, L_RANGE\n",
    "\n",
    "import torch\n",
    "from torch.func import vmap, jacfwd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "model_name = \"dfGP\"\n",
    "\n",
    "#########################\n",
    "### x_train & y_train ###\n",
    "#########################\n",
    "\n",
    "# Import all simulation functions\n",
    "from simulate import (\n",
    "    simulate_convergence,\n",
    "    simulate_branching,\n",
    "    simulate_merge,\n",
    "    simulate_deflection,\n",
    "    simulate_ridge,\n",
    ")\n",
    "\n",
    "# Define simulations as a dictionary with names as keys to function objects\n",
    "simulations = {\n",
    "    \"convergence\": simulate_convergence,\n",
    "    \"branching\": simulate_branching,\n",
    "    \"merge\": simulate_merge,\n",
    "    \"deflection\": simulate_deflection,\n",
    "    \"ridge\": simulate_ridge,\n",
    "}\n",
    "\n",
    "# Load training inputs\n",
    "x_train = torch.load(\"data/sim_data/x_train_lines_discretised_0to1.pt\", weights_only = False).float()\n",
    "\n",
    "# Storage dictionaries\n",
    "y_train_dict = {}\n",
    "\n",
    "# Make y_train_dict: Iterate over all simulation functions\n",
    "for sim_name, sim_func in simulations.items():\n",
    "\n",
    "    # Generate training observations\n",
    "    y_train = sim_func(x_train)\n",
    "    y_train_dict[sim_name] = y_train  # Store training outputs\n",
    "\n",
    "    # Print details\n",
    "    print(f\"=== {sim_name.upper()} ===\")\n",
    "    print(f\"Training inputs shape: {x_train.shape}\")\n",
    "    print(f\"Training observations shape: {y_train.shape}\")\n",
    "    print(f\"Training inputs dtype: {x_train.dtype}\")\n",
    "    print()\n",
    "\n",
    "#######################\n",
    "### x_test & y_test ###\n",
    "#######################\n",
    "\n",
    "print(\"=== Generating test data ===\")\n",
    "\n",
    "# Choose discretisation that is good for simulations and also for quiver plotting\n",
    "N_SIDE = N_SIDE\n",
    "\n",
    "side_array = torch.linspace(start = 0.0, end = 1.0, steps = N_SIDE)\n",
    "XX, YY = torch.meshgrid(side_array, side_array, indexing = \"xy\")\n",
    "x_test_grid = torch.cat([XX.unsqueeze(-1), YY.unsqueeze(-1)], dim = -1)\n",
    "# long format\n",
    "x_test = x_test_grid.reshape(-1, 2)\n",
    "\n",
    "# Storage dictionaries\n",
    "y_test_dict = {}\n",
    "\n",
    "# Make y_test_dict: Iterate over all simulation functions\n",
    "for sim_name, sim_func in simulations.items():\n",
    "\n",
    "    # Generate test observations\n",
    "    y_test = sim_func(x_test)\n",
    "    y_test_dict[sim_name] = y_test  # Store test outputs\n",
    "\n",
    "    # Print details\n",
    "    print(f\"=== {sim_name.upper()} ===\")\n",
    "    print(f\"Test inputs shape: {x_test.shape}\")\n",
    "    print(f\"Test observations shape: {y_test.shape}\")\n",
    "    print(f\"Test inputs dtype: {x_test.dtype}\")\n",
    "    print()\n",
    "\n",
    "    # visualise_v_quiver(y_test, x_test, title_string = name)\n",
    "\n",
    "#####################\n",
    "### Training loop ###\n",
    "#####################\n",
    "\n",
    "# Early stopping parameters\n",
    "PATIENCE = PATIENCE\n",
    "MAX_NUM_EPOCHS = MAX_NUM_EPOCHS\n",
    "MAX_NUM_EPOCHS = 100\n",
    "\n",
    "# Number of training runs for mean and std of metrics\n",
    "NUM_RUNS = NUM_RUNS\n",
    "NUM_RUNS = 2\n",
    "LEARNING_RATE = GP_LEARNING_RATE\n",
    "WEIGHT_DECAY = WEIGHT_DECAY\n",
    "\n",
    "# Pass in all the training data\n",
    "# BATCH_SIZE = BATCH_SIZE\n",
    "\n",
    "# Ensure the results folder exists\n",
    "RESULTS_DIR = DFGP_RESULTS_DIR\n",
    "os.makedirs(RESULTS_DIR, exist_ok = True)\n",
    "\n",
    "### LOOP OVER SIMULATIONS ###\n",
    "for sim_name, sim_func in simulations.items():\n",
    "    print(f\"\\nTraining for {sim_name.upper()}...\")\n",
    "\n",
    "    # Store metrics for the current simulation\n",
    "    simulation_results = []\n",
    "\n",
    "    # x_train is the same, select y_train\n",
    "    x_train = x_train.to(device)\n",
    "\n",
    "    y_train = y_train_dict[sim_name].to(device)\n",
    "    # select the correct y_test (PREVIOUS ERROR)\n",
    "    y_test = y_test_dict[sim_name].to(device)\n",
    "\n",
    "    ### LOOP OVER RUNS ###\n",
    "    for run in range(NUM_RUNS):\n",
    "        print(f\"\\n--- Training Run {run + 1}/{NUM_RUNS} ---\")\n",
    "\n",
    "        # Sample from uniform distributions to initialise hyperparameters\n",
    "        sigma_n = torch.tensor([0.05], requires_grad = False).to(device) # no optimisation for noise, no sampling\n",
    "        sigma_f = torch.tensor([torch.empty(1).uniform_(* SIGMA_F_RANGE)], requires_grad = True) # Trainable\n",
    "        l = torch.tensor(torch.empty(2).uniform_(* L_RANGE), requires_grad = True) # Trainable\n",
    "\n",
    "        learnable_hyperparameters_init = [sigma_f, l]\n",
    "        learnable_hyperparameters = [param.detach().clone().to(device).requires_grad_() for param in learnable_hyperparameters_init]\n",
    "\n",
    "        all_hyperparameters = [sigma_n] + learnable_hyperparameters\n",
    "        \n",
    "        # We do not need to \"initialse\" the GP model\n",
    "        # We don't need a criterion either\n",
    "\n",
    "        # Define optimizer (e.g., AdamW)\n",
    "        optimizer = optim.AdamW([sigma_f, l], lr = LEARNING_RATE, weight_decay = WEIGHT_DECAY)\n",
    "\n",
    "        # Initialise tensors to store losses for this run\n",
    "        epoch_train_losses = torch.zeros(MAX_NUM_EPOCHS)\n",
    "        epoch_train_RMSE_losses = torch.zeros(MAX_NUM_EPOCHS)\n",
    "        epoch_test_RMSE_losses = torch.zeros(MAX_NUM_EPOCHS)\n",
    "\n",
    "        # Early stopping variables\n",
    "        best_loss = float('inf')\n",
    "        epochs_no_improve = 0\n",
    "\n",
    "        # Convert to DataLoader for batching\n",
    "        dataset = TensorDataset(x_train, y_train)\n",
    "        # Pass in full training dataset as batch!!\n",
    "        dataloader = DataLoader(dataset, batch_size = x_train.shape[0], shuffle = True)\n",
    "\n",
    "        ### LOOP OVER EPOCHS ###\n",
    "        print(\"\\nStart Training\")\n",
    "        for epoch in range(MAX_NUM_EPOCHS):\n",
    "\n",
    "            epoch_train_loss = 0.0  # Accumulate batch losses within epoch\n",
    "            epoch_test_loss = 0.0\n",
    "\n",
    "            for batch in dataloader:\n",
    "\n",
    "                x_batch, y_batch = batch\n",
    "                # put on GPU\n",
    "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "                x_batch.requires_grad_()\n",
    "\n",
    "                # THIS DOES NOT WORK\n",
    "                mean_pred_train, _, lml_batch = GP_predict(\n",
    "                     x_batch,\n",
    "                     y_batch,\n",
    "                     x_batch, # have predictions for training data again\n",
    "                     [sigma_n, sigma_f, l], # initial hyperparameters\n",
    "                     # no mean\n",
    "                     divergence_free_bool = True)\n",
    "\n",
    "                # - lml because loss in minimised\n",
    "                loss = - lml_batch\n",
    "                epoch_train_loss += loss.item()\n",
    "\n",
    "                # Backpropagation\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            ### END BATCH LOOP ###\n",
    "            # Compute test loss for loss convergence plot\n",
    "            mean_pred_test, _, _ = GP_predict(\n",
    "                     x_train,\n",
    "                     y_train,\n",
    "                     x_test.to(device), # have predictions for training data again\n",
    "                     all_hyperparameters, # initial hyperparameters\n",
    "                     # no mean\n",
    "                     divergence_free_bool = True)\n",
    "            \n",
    "            test_loss_RMSE = compute_RMSE(y_test, mean_pred_test)\n",
    "            \n",
    "            # Compute average loss for the epoch (e.g. #batches/epoch)\n",
    "            avg_train_loss = epoch_train_loss / len(dataloader)\n",
    "\n",
    "            epoch_train_losses[epoch] = avg_train_loss\n",
    "            epoch_train_RMSE_losses[epoch] = compute_RMSE(y_train, mean_pred_train)\n",
    "            # epoch_test_RMSE_losses[epoch] = test_loss_RMSE\n",
    "\n",
    "            print(f\"{sim_name} {model_name} Run {run + 1}/{NUM_RUNS}, Epoch {epoch + 1}/{MAX_NUM_EPOCHS}, Training Loss (NLML): {avg_train_loss:.4f}, (RMSE): {epoch_train_RMSE_losses[epoch]:.4f}\")\n",
    "\n",
    "            # Early stopping check\n",
    "            if avg_train_loss < best_loss:\n",
    "                best_loss = avg_train_loss\n",
    "                epochs_no_improve = 0  # Reset counter\n",
    "                # best_model_state = dfNN_model.state_dict()  # Save best model\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "\n",
    "            if epochs_no_improve >= PATIENCE:\n",
    "                print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
    "                break\n",
    "\n",
    "        ################\n",
    "        ### EVALUATE ###\n",
    "        ################\n",
    "\n",
    "        # Evaluate the trained model after all epochs are finished/early stopping\n",
    "\n",
    "        mean_pred_test, covar_pred_test, _ = GP_predict(\n",
    "                     x_train,\n",
    "                     y_train,\n",
    "                     x_test.to(device),\n",
    "                     [sigma_n, sigma_f, l], # optimal hypers\n",
    "                     # no mean\n",
    "                     divergence_free_bool = True)\n",
    "\n",
    "        # Only save things for one run\n",
    "        if run == 0:\n",
    "            #(1) Save predictions from first run so we can visualise them later\n",
    "            torch.save(mean_pred_test, f\"{RESULTS_DIR}/{sim_name}_{model_name}_test_mean_predictions.pt\")\n",
    "            torch.save(covar_pred_test, f\"{RESULTS_DIR}/{sim_name}_{model_name}_test_covar_predictions.pt\")\n",
    "\n",
    "            #(2) Save loss over epochs\n",
    "            df_losses = pd.DataFrame({\n",
    "                'Epoch': list(range(epoch_train_losses.shape[0])), # pythonic\n",
    "                'Train Loss RMSE': epoch_train_losses.tolist(), \n",
    "                'Test Loss RMSE': epoch_test_RMSE_losses.tolist()\n",
    "                })\n",
    "            \n",
    "            df_losses.to_csv(f\"{RESULTS_DIR}/{sim_name}_{model_name}_losses_over_epochs.csv\", index = False)\n",
    "\n",
    "        # Compute Divergence (convert tensor to float)\n",
    "        # dfNN_train_div = torch.diagonal(vmap(jacfwd(dfNN_model))(x_train.to(device)), dim1 = -2, dim2 = -1).detach().sum().item()\n",
    "        # dfNN_test_div = torch.diagonal(vmap(jacfwd(dfNN_model))(x_test.to(device)), dim1 = -2, dim2 = -1).detach().sum().item()\n",
    "\n",
    "        # Compute metrics (convert tensors to float)\n",
    "        dfNN_test_RMSE = compute_RMSE(y_test, mean_pred_test).item()\n",
    "        dfNN_test_MAE = compute_MAE(y_test, mean_pred_test).item()\n",
    "        dfNN_test_NLL = - log_likelihood_test(mean_pred_test, covar_pred_test, y_test).item()\n",
    "\n",
    "        # dfNN_test_RMSE = compute_RMSE(y_test, y_test_dfNN_predicted.cpu()).item()\n",
    "        # dfNN_test_MAE = compute_MAE(y_test, y_test_dfNN_predicted.cpu()).item()\n",
    "\n",
    "        # BEST HYPERS \n",
    "        best_hypers = {\n",
    "            \"sigma_n\": sigma_n.detach().cpu(),\n",
    "            \"sigma_f\": sigma_f.detach().cpu(),\n",
    "            \"l\": l.detach().cpu(),\n",
    "        }\n",
    "\n",
    "        simulation_results.append([\n",
    "            run + 1, dfNN_test_RMSE, dfNN_test_MAE, dfNN_test_NLL\n",
    "        ])\n",
    "\n",
    "    ### FINISH LOOP OVER RUNS ###\n",
    "    # Convert results to a Pandas DataFrame\n",
    "    df = pd.DataFrame(\n",
    "        simulation_results, \n",
    "        columns = [\"Run\", \"Test RMSE\", \"Test MAE\", \"Test NLL\"])\n",
    "\n",
    "    # Compute mean and standard deviation for each metric\n",
    "    mean_std_df = df.iloc[:, 1:].agg([\"mean\", \"std\"])  # Exclude \"Run\" column\n",
    "\n",
    "    # Save results to CSV\n",
    "    results_file = os.path.join(RESULTS_DIR, f\"{sim_name}_{model_name}_metrics_per_run.csv\")\n",
    "    df.to_csv(results_file, index = False)\n",
    "    print(f\"\\nResults saved to {results_file}\")\n",
    "\n",
    "    # Save mean and standard deviation to CSV\n",
    "    mean_std_file = os.path.join(RESULTS_DIR, f\"{sim_name}_{model_name}_metrics_summary.csv\")\n",
    "    mean_std_df.to_csv(mean_std_file)\n",
    "    print(f\"\\nMean & Std saved to {mean_std_file}\")\n",
    "    # Only train for one simulation for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Run",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Test RMSE",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Test MAE",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Test NLL",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "181d3ffb-2857-4a4e-a319-80c7225aaefc",
       "rows": [
        [
         "0",
         "1",
         "0.006257400847971439",
         "0.003660879097878933",
         "-2900.965087890625"
        ],
        [
         "1",
         "2",
         "0.005000976845622063",
         "0.0027325712144374847",
         "-2838.84228515625"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 2
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Run</th>\n",
       "      <th>Test RMSE</th>\n",
       "      <th>Test MAE</th>\n",
       "      <th>Test NLL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.006257</td>\n",
       "      <td>0.003661</td>\n",
       "      <td>-2900.965088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.005001</td>\n",
       "      <td>0.002733</td>\n",
       "      <td>-2838.842285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Run  Test RMSE  Test MAE     Test NLL\n",
       "0    1   0.006257  0.003661 -2900.965088\n",
       "1    2   0.005001  0.002733 -2838.842285"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# torch.sqrt(torch.mean(torch.square(mean_pred_train - y_train)))\n",
    "torch.sqrt(torch.mean(torch.square(mean_pred_test - y_test)))\n",
    "\n",
    "visualise_v_quiver(mean_pred_train.cpu().detach(), x_train.cpu(), title_string = \"Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([1.9411], device='cuda:0', requires_grad=True),\n",
       " tensor([0.7575, 0.4914], device='cuda:0', requires_grad=True)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learnable_hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GP_models import predict_GP, optimise_hypers_on_train\n",
    "from simulate import simulate_convergence, simulate_branching, simulate_ridge, simulate_merge, simulate_deflection\n",
    "from metrics import compute_RMSE, compute_MAE\n",
    "from utils import set_seed\n",
    "\n",
    "# Global file for training configs\n",
    "from configs import PATIENCE, MAX_NUM_EPOCHS, NUM_RUNS, LEARNING_RATE, WEIGHT_DECAY, BATCH_SIZE, N_SIDE, DFGP_RESULTS_DIR, SIGMA_F_RANGE, L_RANGE\n",
    "\n",
    "import torch\n",
    "from torch.func import vmap, jacfwd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "model_name = \"dfNN\"\n",
    "\n",
    "#########################\n",
    "### x_train & y_train ###\n",
    "#########################\n",
    "\n",
    "# Import all simulation functions\n",
    "from simulate import (\n",
    "    simulate_convergence,\n",
    "    simulate_branching,\n",
    "    simulate_merge,\n",
    "    simulate_deflection,\n",
    "    simulate_ridge,\n",
    ")\n",
    "\n",
    "# Define simulations as a dictionary with names as keys to function objects\n",
    "simulations = {\n",
    "    \"convergence\": simulate_convergence,\n",
    "    \"branching\": simulate_branching,\n",
    "    \"merge\": simulate_merge,\n",
    "    \"deflection\": simulate_deflection,\n",
    "    \"ridge\": simulate_ridge,\n",
    "}\n",
    "\n",
    "# Load training inputs\n",
    "x_train = torch.load(\"data/sim_data/x_train_lines_discretised_0to1.pt\").float()\n",
    "\n",
    "# Storage dictionaries\n",
    "y_train_dict = {}\n",
    "\n",
    "# Make y_train_dict: Iterate over all simulation functions\n",
    "for sim_name, sim_func in simulations.items():\n",
    "\n",
    "    # Generate training observations\n",
    "    y_train = sim_func(x_train)\n",
    "    y_train_dict[sim_name] = y_train  # Store training outputs\n",
    "\n",
    "    # Print details\n",
    "    print(f\"=== {sim_name.upper()} ===\")\n",
    "    print(f\"Training inputs shape: {x_train.shape}\")\n",
    "    print(f\"Training observations shape: {y_train.shape}\")\n",
    "    print(f\"Training inputs dtype: {x_train.dtype}\")\n",
    "    print()\n",
    "\n",
    "#######################\n",
    "### x_test & y_test ###\n",
    "#######################\n",
    "\n",
    "print(\"=== Generating test data ===\")\n",
    "\n",
    "# Choose discretisation that is good for simulations and also for quiver plotting\n",
    "N_SIDE = N_SIDE\n",
    "\n",
    "side_array = torch.linspace(start = 0.0, end = 1.0, steps = N_SIDE)\n",
    "XX, YY = torch.meshgrid(side_array, side_array, indexing = \"xy\")\n",
    "x_test_grid = torch.cat([XX.unsqueeze(-1), YY.unsqueeze(-1)], dim = -1)\n",
    "# long format\n",
    "x_test = x_test_grid.reshape(-1, 2)\n",
    "\n",
    "# Storage dictionaries\n",
    "y_test_dict = {}\n",
    "\n",
    "# Make y_test_dict: Iterate over all simulation functions\n",
    "for sim_name, sim_func in simulations.items():\n",
    "\n",
    "    # Generate test observations\n",
    "    y_test = sim_func(x_test)\n",
    "    y_test_dict[sim_name] = y_test  # Store test outputs\n",
    "\n",
    "    # Print details\n",
    "    print(f\"=== {sim_name.upper()} ===\")\n",
    "    print(f\"Test inputs shape: {x_test.shape}\")\n",
    "    print(f\"Test observations shape: {y_test.shape}\")\n",
    "    print(f\"Test inputs dtype: {x_test.dtype}\")\n",
    "    print()\n",
    "\n",
    "    # visualise_v_quiver(y_test, x_test, title_string = name)\n",
    "\n",
    "#####################\n",
    "### Training loop ###\n",
    "#####################\n",
    "\n",
    "# Early stopping parameters\n",
    "PATIENCE = PATIENCE\n",
    "MAX_NUM_EPOCHS = MAX_NUM_EPOCHS\n",
    "\n",
    "# Number of training runs for mean and std of metrics\n",
    "NUM_RUNS = NUM_RUNS\n",
    "LEARNING_RATE = LEARNING_RATE\n",
    "WEIGHT_DECAY = WEIGHT_DECAY\n",
    "\n",
    "# Pass in all the training data\n",
    "# BATCH_SIZE = BATCH_SIZE\n",
    "\n",
    "# Ensure the results folder exists\n",
    "RESULTS_DIR = DFGP_RESULTS_DIR\n",
    "os.makedirs(RESULTS_DIR, exist_ok = True)\n",
    "\n",
    "### LOOP OVER SIMULATIONS ###\n",
    "for sim_name, sim_func in simulations.items():\n",
    "    print(f\"\\nTraining for {sim_name.upper()}...\")\n",
    "\n",
    "    # Store metrics for the current simulation\n",
    "    simulation_results = []\n",
    "\n",
    "    # x_train is the same, select y_train\n",
    "    y_train = y_train_dict[sim_name]\n",
    "    # select the correct y_test (PREVIOUS ERROR)\n",
    "    y_test = y_test_dict[sim_name]\n",
    "\n",
    "    ### LOOP OVER RUNS ###\n",
    "    for run in range(NUM_RUNS):\n",
    "        print(f\"\\n--- Training Run {run + 1}/{NUM_RUNS} ---\")\n",
    "\n",
    "        # Sample from uniform distributions\n",
    "        sigma_n = torch.tensor([0.05], requires_grad = False) # no optimisation for noise, no sampling\n",
    "        sigma_f = torch.tensor([torch.empty(1).uniform_(* SIGMA_F_RANGE)], requires_grad = True)   # Trainable\n",
    "        l = torch.tensor(torch.empty(2).uniform_(* L_RANGE), requires_grad = True)  # Trainable\n",
    "\n",
    "        print(f\"sigma_n: {sigma_n.item():.4f}, sigma_f: {sigma_f.item():.4f}, l: {l[0].item():.4f}, {l[1].item():.4f}\")\n",
    "\n",
    "        # Convert to DataLoader for batching\n",
    "        dataset = TensorDataset(x_train, y_train)\n",
    "        # Pass in full training dataset as batch\n",
    "        dataloader = DataLoader(dataset, batch_size = x_train.shape[0], shuffle = True)\n",
    "\n",
    "        # Initialise fresh model\n",
    "        # we seeded so this is reproducible\n",
    "        dfNN_model = dfNN_for_vmap().to(device)\n",
    "\n",
    "        # Define loss function (e.g., MSE for regression)\n",
    "        criterion = torch.nn.MSELoss()\n",
    "\n",
    "        # Define optimizer (e.g., AdamW)\n",
    "        optimizer = optim.AdamW(dfNN_model.parameters(), lr = LEARNING_RATE, weight_decay = WEIGHT_DECAY)\n",
    "\n",
    "        # Initialise tensors to store losses for this run\n",
    "        epoch_train_losses = torch.zeros(MAX_NUM_EPOCHS)\n",
    "        epoch_test_losses = torch.zeros(MAX_NUM_EPOCHS)\n",
    "\n",
    "        # Early stopping variables\n",
    "        best_loss = float('inf')\n",
    "        epochs_no_improve = 0\n",
    "\n",
    "        ### LOOP OVER EPOCHS ###\n",
    "        print(\"\\nStart Training\")\n",
    "        for epoch in range(MAX_NUM_EPOCHS):\n",
    "\n",
    "            epoch_train_loss = 0.0  # Accumulate batch losses within epoch\n",
    "            epoch_test_loss = 0.0\n",
    "\n",
    "            for batch in dataloader:\n",
    "                # assure model is in training mode \n",
    "                dfNN_model.train()\n",
    "\n",
    "                x_batch, y_batch = batch\n",
    "                # put on GPU\n",
    "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "                x_batch.requires_grad_()\n",
    "\n",
    "                # Forward pass\n",
    "                y_pred = vmap(dfNN_model)(x_batch)\n",
    "\n",
    "                # Compute loss (RMSE for same units as data) - criterion(pred, target)\n",
    "                loss = torch.sqrt(criterion(y_pred, y_batch))\n",
    "                epoch_train_loss += loss.item()\n",
    "\n",
    "                # Backpropagation\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            ### END BATCH LOOP ###\n",
    "            dfNN_model.eval()\n",
    "            # Compute test loss for loss convergence plot\n",
    "            y_test_pred = vmap(dfNN_model)(x_test.to(device))\n",
    "            test_loss = torch.sqrt(criterion(y_test_pred, y_test.to(device))).item()\n",
    "            \n",
    "            # Compute average loss for the epoch (e.g. 7 batches/epoch)\n",
    "            avg_train_loss = epoch_train_loss / len(dataloader)\n",
    "\n",
    "            epoch_train_losses[epoch] = avg_train_loss\n",
    "            epoch_test_losses[epoch] = test_loss\n",
    "\n",
    "            print(f\"{sim_name} {model_name} Run {run + 1}/{NUM_RUNS}, Epoch {epoch + 1}/{MAX_NUM_EPOCHS}, Training Loss (RMSE): {avg_train_loss:.4f}\")\n",
    "\n",
    "            # debug\n",
    "            # Early stopping check\n",
    "            if avg_train_loss < best_loss:\n",
    "                best_loss = avg_train_loss\n",
    "                epochs_no_improve = 0  # Reset counter\n",
    "                best_model_state = dfNN_model.state_dict()  # Save best model\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "\n",
    "            if epochs_no_improve >= PATIENCE:\n",
    "                print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
    "                break\n",
    "        \n",
    "        ### END EPOCH LOOP ###\n",
    "        # Load the best model before stopping for this \"run\"\n",
    "        dfNN_model.load_state_dict(best_model_state)\n",
    "        print(f\"Run {run + 1}/{NUM_RUNS}, Training of {model_name} complete for {sim_name.upper()}. Restored best model.\")\n",
    "\n",
    "        ################\n",
    "        ### EVALUATE ###\n",
    "        ################\n",
    "\n",
    "        # Evaluate the trained model after epochs are finished\n",
    "        dfNN_model.eval()\n",
    "\n",
    "        y_train_dfNN_predicted = vmap(dfNN_model)(x_train.to(device)).detach()\n",
    "        y_test_dfNN_predicted = vmap(dfNN_model)(x_test.to(device)).detach()\n",
    "\n",
    "        # Only save things for one run\n",
    "        if run == 0:\n",
    "            #(1) Save predictions from first run so we can visualise them later\n",
    "            torch.save(y_test_dfNN_predicted, f\"{RESULTS_DIR}/{sim_name}_{model_name}_test_predictions.pt\")\n",
    "\n",
    "            #(2) Save loss over epochs\n",
    "            df_losses = pd.DataFrame({\n",
    "                'Epoch': list(range(epoch_train_losses.shape[0])), # pythonic\n",
    "                'Train Loss RMSE': epoch_train_losses.tolist(), \n",
    "                'Test Loss RMSE': epoch_test_losses.tolist()\n",
    "                })\n",
    "            \n",
    "            df_losses.to_csv(f\"{RESULTS_DIR}/{sim_name}_{model_name}_losses_over_epochs.csv\", index = False)\n",
    "\n",
    "        # Compute Divergence (convert tensor to float)\n",
    "        dfNN_train_div = torch.diagonal(vmap(jacfwd(dfNN_model))(x_train.to(device)), dim1 = -2, dim2 = -1).detach().sum().item()\n",
    "        dfNN_test_div = torch.diagonal(vmap(jacfwd(dfNN_model))(x_test.to(device)), dim1 = -2, dim2 = -1).detach().sum().item()\n",
    "\n",
    "        # Compute metrics (convert tensors to float)\n",
    "        dfNN_train_RMSE = compute_RMSE(y_train, y_train_dfNN_predicted.cpu()).item()\n",
    "        dfNN_train_MAE = compute_MAE(y_train, y_train_dfNN_predicted.cpu()).item()\n",
    "\n",
    "        dfNN_test_RMSE = compute_RMSE(y_test, y_test_dfNN_predicted.cpu()).item()\n",
    "        dfNN_test_MAE = compute_MAE(y_test, y_test_dfNN_predicted.cpu()).item()\n",
    "\n",
    "        # Store results in list\n",
    "        simulation_results.append([\n",
    "            run + 1, dfNN_train_RMSE, dfNN_train_MAE, dfNN_train_div,\n",
    "            dfNN_test_RMSE, dfNN_test_MAE, dfNN_test_div\n",
    "        ])\n",
    "\n",
    "    ### FINISH LOOP OVER RUNS ###\n",
    "    # Convert results to a Pandas DataFrame\n",
    "    df = pd.DataFrame(\n",
    "        simulation_results, \n",
    "        columns = [\"Run\", \"Train RMSE\", \"Train MAE\", \"Train Divergence\",\n",
    "                   \"Test RMSE\", \"Test MAE\", \"Test Divergence\"])\n",
    "\n",
    "    # Compute mean and standard deviation for each metric\n",
    "    mean_std_df = df.iloc[:, 1:].agg([\"mean\", \"std\"])  # Exclude \"Run\" column\n",
    "\n",
    "    # Save results to CSV\n",
    "    results_file = os.path.join(RESULTS_DIR, f\"{sim_name}_{model_name}_metrics_per_run.csv\")\n",
    "    df.to_csv(results_file, index = False)\n",
    "    print(f\"\\nResults saved to {results_file}\")\n",
    "\n",
    "    # Save mean and standard deviation to CSV\n",
    "    mean_std_file = os.path.join(RESULTS_DIR, f\"{sim_name}_{model_name}_metrics_summary.csv\")\n",
    "    mean_std_df.to_csv(mean_std_file)\n",
    "    print(f\"\\nMean & Std saved to {mean_std_file}\")\n",
    "    # Only train for one simulation for now"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
