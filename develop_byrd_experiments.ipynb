{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c517244c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from GP_models import GP_predict\n",
    "from metrics import compute_RMSE, compute_MAE, compute_NLL, compute_NLL_full\n",
    "from utils import set_seed\n",
    "\n",
    "# Global file for training configs\n",
    "from configs import PATIENCE, MAX_NUM_EPOCHS, NUM_RUNS, GP_LEARNING_RATE, WEIGHT_DECAY, SIGMA_N_RANGE, SIGMA_F_RANGE, L_RANGE, B_DIAGONAL_RANGE, B_OFFDIAGONAL_RANGE, GP_REAL_RESULTS_DIR\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "model_name = \"GP\"\n",
    "\n",
    "#########################\n",
    "### x_train & y_train ###\n",
    "#########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46575c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TIMING ###\n",
    "import time\n",
    "start_time = time.time()  # Start timing after imports\n",
    "\n",
    "#########################\n",
    "### Loop over regions ###\n",
    "#########################\n",
    "\n",
    "# What used to be region_name is now region name\n",
    "# For region_name in [\"regiona\", \"regionb\", \"regionc\"]:\n",
    "for region_name in [\"regionc\"]:\n",
    "\n",
    "    print(f\"\\nTraining for {region_name.upper()}...\")\n",
    "\n",
    "    # Store metrics for the current simulation\n",
    "    region_results = []\n",
    "\n",
    "    #########################\n",
    "    ### x_train & y_train ###\n",
    "    #########################\n",
    "\n",
    "    path_to_training_tensor = \"data/real_data/\" + region_name + \"_train_tensor.pt\"\n",
    "    path_to_test_tensor = \"data/real_data/\" + region_name + \"_test_tensor.pt\"\n",
    "\n",
    "    train = torch.load(path_to_training_tensor, weights_only = False).T # we need to transpose the tensor to have observations in the first dimension\n",
    "    test = torch.load(path_to_test_tensor, weights_only = False).T\n",
    "\n",
    "    # The train and test tensors have the following columns:\n",
    "    # [:, 0] = x\n",
    "    # [:, 1] = y\n",
    "    # [:, 2] = surface elevation (s)\n",
    "    # [:, 3] = ice flux in x direction (u)\n",
    "    # [:, 4] = ice flux in y direction (v)\n",
    "    # [:, 5] = ice flux error in x direction (u_err)\n",
    "    # [:, 6] = ice flux error in y direction (v_err)\n",
    "\n",
    "    x_train = train[:, [0, 1]].to(device)\n",
    "    y_train = train[:, [3, 4]].to(device)\n",
    "\n",
    "    x_test = test[:, [0, 1]].to(device)\n",
    "    y_test = test[:, [3, 4]].to(device)\n",
    "\n",
    "    train_noise_diag = torch.concat((train[:, 5], train[:, 5]), dim = 0).to(device)\n",
    "    train_noise_diagmatrix = torch.eye(len(train_noise_diag)).to(device) * train_noise_diag\n",
    "\n",
    "    # Print details\n",
    "    print(f\"=== {region_name.upper()} ===\")\n",
    "    print(f\"Training inputs shape: {x_train.shape}\")\n",
    "    print(f\"Training observations shape: {y_train.shape}\")\n",
    "    print(f\"Training inputs dtype: {x_train.dtype}\")\n",
    "    print()\n",
    "\n",
    "    # Print details\n",
    "    print(f\"=== {region_name.upper()} ===\")\n",
    "    print(f\"Test inputs shape: {x_test.shape}\")\n",
    "    print(f\"Test observations shape: {y_test.shape}\")\n",
    "    print(f\"Test inputs dtype: {x_test.dtype}\")\n",
    "    print()\n",
    "\n",
    "    #####################\n",
    "    ### Training loop ###\n",
    "    #####################\n",
    "\n",
    "    # Early stopping parameters\n",
    "    PATIENCE = PATIENCE\n",
    "    MAX_NUM_EPOCHS = MAX_NUM_EPOCHS\n",
    "    MAX_NUM_EPOCHS = 1000\n",
    "\n",
    "    # Number of training runs for mean and std of metrics\n",
    "    NUM_RUNS = NUM_RUNS\n",
    "    NUM_RUNS = 1\n",
    "    LEARNING_RATE = GP_LEARNING_RATE\n",
    "    LEARNING_RATE = 0.001\n",
    "    WEIGHT_DECAY = WEIGHT_DECAY\n",
    "\n",
    "    # Pass in all the training data for GPs\n",
    "    # Don't need dataloader either\n",
    "    # BATCH_SIZE = BATCH_SIZE\n",
    "\n",
    "    # Ensure the results folder exists\n",
    "    RESULTS_DIR = GP_REAL_RESULTS_DIR\n",
    "    os.makedirs(RESULTS_DIR, exist_ok = True)\n",
    "    ### LOOP OVER RUNS ###\n",
    "    for run in range(NUM_RUNS):\n",
    "        print(f\"\\n--- Training Run {run + 1}/{NUM_RUNS} ---\")\n",
    "\n",
    "        # Sample from uniform distributions to initialise hyperparameters\n",
    "        # We could inform this \n",
    "        # sigma_n = torch.tensor([0.05], requires_grad = False).to(device) # no optimisation for noise, no sampling\n",
    "        # sigma_n = nn.Parameter(torch.empty(1, device = device).uniform_( * SIGMA_N_RANGE)) # Not Trainable\n",
    "        sigma_n = torch.tensor([0.005], requires_grad = False).to(device)\n",
    "\n",
    "        sigma_f = torch.tensor([1.0], requires_grad = False).to(device) # Fixed because we tune B (and sigma_f would just scale B)\n",
    "        # Initialising l from a uniform distribution as nn.Param to avoid leaf variable error\n",
    "        l = nn.Parameter(torch.empty(2, device = device).uniform_( * L_RANGE))\n",
    "\n",
    "        # Trainable B matrix components\n",
    "        B_diag = nn.Parameter(torch.empty(1, device = device).uniform_( * B_DIAGONAL_RANGE))  \n",
    "        B_off_diag = nn.Parameter(torch.empty(1, device = device).uniform_( * B_OFFDIAGONAL_RANGE)) \n",
    "\n",
    "        # Construct B using a proper tensor operation (not `torch.tensor()`)\n",
    "        # squeeze to make 2 x 2 \n",
    "        B = torch.stack([\n",
    "            torch.stack([B_diag, B_off_diag], dim = 0),\n",
    "            torch.stack([B_off_diag, B_diag], dim = 0)\n",
    "        ], dim = 0).squeeze()\n",
    "\n",
    "        B = nn.Parameter(B)\n",
    "        \n",
    "        # We do not need to \"initialse\" the GP model\n",
    "        # We don't need a criterion either\n",
    "\n",
    "        # Define optimizer (e.g., AdamW)\n",
    "        optimizer = optim.AdamW([l, B], lr = LEARNING_RATE, weight_decay = WEIGHT_DECAY)\n",
    "\n",
    "        # Initialise tensors to store losses over epochs (for convergence plot)\n",
    "        epoch_train_NLML_losses = torch.zeros(MAX_NUM_EPOCHS)\n",
    "        epoch_train_RMSE_losses = torch.zeros(MAX_NUM_EPOCHS)\n",
    "        epoch_test_RMSE_losses = torch.zeros(MAX_NUM_EPOCHS)\n",
    "\n",
    "        epoch_b_diag = torch.zeros(MAX_NUM_EPOCHS)\n",
    "        epoch_b_offdiag = torch.zeros(MAX_NUM_EPOCHS)\n",
    "        epoch_l1 = torch.zeros(MAX_NUM_EPOCHS)\n",
    "        epoch_l2 = torch.zeros(MAX_NUM_EPOCHS)\n",
    "\n",
    "        # Early stopping variables\n",
    "        best_loss = float('inf')\n",
    "        epochs_no_improve = 0\n",
    "\n",
    "        # We do not need to add extra noise to real data\n",
    "\n",
    "        ### LOOP OVER EPOCHS ###\n",
    "        print(\"\\nStart Training\")\n",
    "        for epoch in range(MAX_NUM_EPOCHS):\n",
    "\n",
    "            # No batching - full epoch pass in one\n",
    "            if run == 0:\n",
    "                mean_pred_train, _, lml_train = GP_predict(\n",
    "                        x_train,\n",
    "                        y_train,\n",
    "                        x_train, # have predictions for training data again\n",
    "                        [sigma_n, sigma_f, l, B], # initial hyperparameters\n",
    "                        # no mean\n",
    "                        divergence_free_bool = False,\n",
    "                        train_noise_input = train_noise_diagmatrix)\n",
    "                \n",
    "                loss = - lml_train\n",
    "                # Backpropagation\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Compute test loss for loss convergence plot\n",
    "                mean_pred_test, _, _ = GP_predict(\n",
    "                        x_train,\n",
    "                        y_train,\n",
    "                        x_test.to(device), # have predictions for training data again\n",
    "                        [sigma_n, sigma_f, l, B], # initial hyperparameters\n",
    "                        # no mean\n",
    "                        divergence_free_bool = False,\n",
    "                        train_noise_input = train_noise_diagmatrix)\n",
    "                \n",
    "                train_RMSE = compute_RMSE(y_train, mean_pred_train)\n",
    "                test_RMSE = compute_RMSE(y_test, mean_pred_test)\n",
    "\n",
    "                epoch_train_NLML_losses[epoch] = - lml_train\n",
    "                epoch_train_RMSE_losses[epoch] = train_RMSE\n",
    "                # epoch_test_NLML_losses[epoch] =  # train NLML\n",
    "                epoch_test_RMSE_losses[epoch] = test_RMSE\n",
    "\n",
    "                epoch_b_diag[epoch] = B[0, 0]\n",
    "                epoch_b_offdiag[epoch] = B[0, 1]\n",
    "                epoch_l1[epoch] = l[0]\n",
    "                epoch_l2[epoch] = l[1]\n",
    "\n",
    "                print(f\"{region_name} {model_name} Run {run + 1}/{NUM_RUNS}, Epoch {epoch + 1}/{MAX_NUM_EPOCHS}, Training Loss (NLML): {loss:.4f}, (RMSE): {train_RMSE:.4f}\")\n",
    "            \n",
    "            else:\n",
    "                # Save compute after run 1\n",
    "                _, _, lml_train = GP_predict(\n",
    "                        x_train,\n",
    "                        y_train,\n",
    "                        x_train[0:2], # have predictions for training data again\n",
    "                        [sigma_n, sigma_f, l, B], # initial hyperparameters\n",
    "                        # no mean\n",
    "                        divergence_free_bool = False, \n",
    "                        train_noise_input = train_noise_diagmatrix)\n",
    "                \n",
    "                loss = - lml_train\n",
    "                # Backpropagation\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                print(f\"{region_name} {model_name} Run {run + 1}/{NUM_RUNS}, Epoch {epoch + 1}/{MAX_NUM_EPOCHS}, Training Loss (NLML): {loss:.4f}\")\n",
    "\n",
    "            # Early stopping check\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                epochs_no_improve = 0  # Reset counter\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "\n",
    "            if epochs_no_improve >= PATIENCE:\n",
    "                print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
    "                break\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        ################\n",
    "        ### EVALUATE ###\n",
    "        ################\n",
    "\n",
    "        # Now HPs should be tuned\n",
    "        # Evaluate the trained model after all epochs are finished/early stopping\n",
    "\n",
    "        # Need gradients for autograd divergence\n",
    "        x_test_grad = x_test.to(device).requires_grad_(True)\n",
    "\n",
    "        mean_pred_test, covar_pred_test, _ = GP_predict(\n",
    "                     x_train,\n",
    "                     y_train,\n",
    "                     x_test_grad,\n",
    "                     [sigma_n, sigma_f, l, B], # optimal hypers\n",
    "                     # no mean\n",
    "                     divergence_free_bool = False, \n",
    "                     train_noise_input = train_noise_diagmatrix)\n",
    "\n",
    "        # Only save things for one run\n",
    "        if run == 0:\n",
    "            #(1) Save predictions from first run so we can visualise them later\n",
    "            torch.save(mean_pred_test, f\"{RESULTS_DIR}/{region_name}_{model_name}_test_mean_predictions.pt\")\n",
    "            torch.save(covar_pred_test, f\"{RESULTS_DIR}/{region_name}_{model_name}_test_covar_predictions.pt\")\n",
    "\n",
    "            #(2) Save best hyperparameters from run 1\n",
    "            # Stack tensors into a single tensor\n",
    "            best_hypers_tensor = torch.cat([\n",
    "                sigma_n.reshape(-1),  # Ensure 1D shape\n",
    "                sigma_f.reshape(-1),\n",
    "                l.reshape(-1),\n",
    "                B.reshape(-1)\n",
    "            ])\n",
    "\n",
    "            # Save the tensor\n",
    "            torch.save(best_hypers_tensor, f\"{RESULTS_DIR}/{region_name}_{model_name}_best_hypers.pt\")\n",
    "\n",
    "            #(3) Save loss over epochs\n",
    "            df_losses = pd.DataFrame({\n",
    "                'Epoch': list(range(epoch_train_NLML_losses.shape[0])), # pythonic\n",
    "                'Train Loss NLML': epoch_train_NLML_losses.tolist(),\n",
    "                'Train Loss RMSE': epoch_train_RMSE_losses.tolist(),\n",
    "                'Test Loss RMSE': epoch_test_RMSE_losses.tolist(),\n",
    "                'B_diag': epoch_b_diag.tolist(),\n",
    "                'B_offdiag': epoch_b_offdiag.tolist(),\n",
    "                'l1': epoch_l1.tolist(),\n",
    "                'l2': epoch_l2.tolist()\n",
    "                })\n",
    "            \n",
    "            df_losses.to_csv(f\"{RESULTS_DIR}/{region_name}_{model_name}_losses_over_epochs.csv\", index = False, float_format = \"%.5f\") # reduce to 5 decimals\n",
    "\n",
    "            # #(4) Save divergence field\n",
    "            u_indicator_test, v_indicator_test = torch.zeros_like(mean_pred_test), torch.zeros_like(mean_pred_test)\n",
    "            u_indicator_test[:, 0] = 1.0 # output column u selected\n",
    "            v_indicator_test[:, 1] = 1.0 # output column v selected\n",
    "\n",
    "            # divergence field (positive and negative divergences)\n",
    "            GP_test_div_field = (torch.autograd.grad(\n",
    "                outputs = mean_pred_test,\n",
    "                inputs = x_test_grad,\n",
    "                grad_outputs = u_indicator_test,\n",
    "                create_graph = True\n",
    "            )[0][:, 0] + torch.autograd.grad(\n",
    "                outputs = mean_pred_test,\n",
    "                inputs = x_test_grad,\n",
    "                grad_outputs = v_indicator_test,\n",
    "                create_graph = True\n",
    "            )[0][:, 1])\n",
    "\n",
    "            # Save as test predition divergence field\n",
    "            torch.save(GP_test_div_field, f\"{RESULTS_DIR}/{region_name}_{model_name}_test_prediction_divergence_field.pt\")\n",
    "\n",
    "        x_train_grad = x_train.to(device).requires_grad_(True)\n",
    "\n",
    "        mean_pred_train, covar_pred_train, _ = GP_predict(\n",
    "                     x_train,\n",
    "                     y_train,\n",
    "                     x_train_grad,\n",
    "                     [sigma_n, sigma_f, l, B], # optimal hypers\n",
    "                     # no mean\n",
    "                     divergence_free_bool = False, \n",
    "                     train_noise_input = train_noise_diagmatrix)\n",
    "\n",
    "        ### Divergence: Total absolute divergence (sum divergence at each point, after summing dims)\n",
    "        # autograd div test\n",
    "        u_indicator_test, v_indicator_test = torch.zeros_like(mean_pred_test), torch.zeros_like(mean_pred_test)\n",
    "        u_indicator_test[:, 0] = 1.0 # output column u selected\n",
    "        v_indicator_test[:, 1] = 1.0 # output column v selected\n",
    "\n",
    "        GP_test_div = (torch.autograd.grad(\n",
    "            outputs = mean_pred_test,\n",
    "            inputs = x_test_grad,\n",
    "            grad_outputs = u_indicator_test,\n",
    "            create_graph = True\n",
    "        )[0][:, 0] + torch.autograd.grad(\n",
    "            outputs = mean_pred_test,\n",
    "            inputs = x_test_grad,\n",
    "            grad_outputs = v_indicator_test,\n",
    "            create_graph = True\n",
    "        )[0][:, 1]).abs().mean().item() # v with respect to y\n",
    "\n",
    "        # autograd div train\n",
    "        u_indicator_train, v_indicator_train = torch.zeros_like(mean_pred_train), torch.zeros_like(mean_pred_train)\n",
    "        u_indicator_train[:, 0] = 1.0 # output column u selected\n",
    "        v_indicator_train[:, 1] = 1.0 # output column v selected\n",
    "\n",
    "        GP_train_div = (torch.autograd.grad(\n",
    "            outputs = mean_pred_train,\n",
    "            inputs = x_train_grad,\n",
    "            grad_outputs = u_indicator_train,\n",
    "            create_graph = True\n",
    "        )[0][:, 0] + torch.autograd.grad(\n",
    "            outputs = mean_pred_train,\n",
    "            inputs = x_train_grad,\n",
    "            grad_outputs = v_indicator_train,\n",
    "            create_graph = True\n",
    "        )[0][:, 1]).abs().mean().item() # v with respect to y\n",
    "\n",
    "        # Compute metrics (convert tensors to float) for every run's tuned model\n",
    "        GP_train_RMSE = compute_RMSE(y_train, mean_pred_train).item()\n",
    "        GP_train_MAE = compute_MAE(y_train, mean_pred_train).item()\n",
    "        GP_train_NLL = compute_NLL(y_train, mean_pred_train, covar_pred_train).item()\n",
    "\n",
    "        GP_test_RMSE = compute_RMSE(y_test, mean_pred_test).item()\n",
    "        GP_test_MAE = compute_MAE(y_test, mean_pred_test).item()\n",
    "        # full NLL has caused instability issues due to the logdet\n",
    "        # now we use sparse\n",
    "        GP_test_NLL = compute_NLL(y_test, mean_pred_test, covar_pred_test).item()\n",
    "\n",
    "        region_results.append([\n",
    "            run + 1,\n",
    "            GP_train_RMSE, GP_train_MAE, GP_train_NLL, GP_train_div,\n",
    "            GP_test_RMSE, GP_test_MAE, GP_test_NLL, GP_test_div\n",
    "        ])\n",
    "\n",
    "    ### FINISH LOOP OVER RUNS ###\n",
    "    # Convert results to a Pandas DataFrame\n",
    "    df = pd.DataFrame(\n",
    "        region_results, \n",
    "        columns = [\"Run\", \n",
    "                   \"Train RMSE\", \"Train MAE\", \"Train NLL\", \"Train MAD\",\n",
    "                   \"Test RMSE\", \"Test MAE\", \"Test NLL\", \"Test MAD\"])\n",
    "\n",
    "    # Compute mean and standard deviation for each metric\n",
    "    mean_std_df = df.iloc[:, 1:].agg([\"mean\", \"std\"])  # Exclude \"Run\" column\n",
    "\n",
    "    # Add region_name and model_name as columns in the DataFrame _metrics_summary\n",
    "    mean_std_df[\"region name\"] = region_name\n",
    "    mean_std_df[\"model name\"] = model_name\n",
    "\n",
    "    # Save results to CSV\n",
    "    results_file = os.path.join(RESULTS_DIR, f\"{region_name}_{model_name}_metrics_per_run.csv\")\n",
    "    df.to_csv(results_file, index = False, float_format = \"%.5f\") # reduce to 5 decimals\n",
    "    print(f\"\\nResults saved to {results_file}\")\n",
    "\n",
    "    # Save mean and standard deviation to CSV\n",
    "    mean_std_file = os.path.join(RESULTS_DIR, f\"{region_name}_{model_name}_metrics_summary.csv\")\n",
    "    mean_std_df.to_csv(mean_std_file, float_format = \"%.5f\") # reduce to 5 decimals\n",
    "    print(f\"\\nMean & Std saved to {mean_std_file}\")\n",
    "    # Only train for one simulation for now\n",
    "\n",
    "### End timing ###\n",
    "end_time = time.time()  # End timing\n",
    "elapsed_time = end_time - start_time  # Compute elapsed time\n",
    "# Convert elapsed time to minutes\n",
    "elapsed_time_minutes = elapsed_time / 60\n",
    "\n",
    "if device == \"cuda\":\n",
    "    gpu_name = torch.cuda.get_device_name(0)  # Get GPU model\n",
    "else:\n",
    "    gpu_name = \"N/A\"\n",
    "\n",
    "print(f\"Elapsed wall time: {elapsed_time:.4f} seconds\")\n",
    "\n",
    "# Define full path for the file\n",
    "wall_time_path = os.path.join(RESULTS_DIR, model_name + \"_run_\" \"wall_time.txt\")\n",
    "\n",
    "# Save to the correct folder with both seconds and minutes\n",
    "with open(wall_time_path, \"w\") as f:\n",
    "    f.write(f\"Elapsed wall time: {elapsed_time:.4f} seconds\\n\")\n",
    "    f.write(f\"Elapsed wall time: {elapsed_time_minutes:.2f} minutes\\n\")\n",
    "    f.write(f\"Device used: {device}\\n\")\n",
    "    f.write(f\"GPU model: {gpu_name}\\n\")\n",
    "\n",
    "print(f\"Wall time saved to {wall_time_path}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
