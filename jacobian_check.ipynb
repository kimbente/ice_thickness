{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 2])\n"
     ]
    }
   ],
   "source": [
    "N_side = 8\n",
    "\n",
    "x_vec = torch.linspace(0, 1, N_side)\n",
    "\n",
    "# xx colum is all zeros\n",
    "xx, yy  = torch.meshgrid(x_vec, x_vec, indexing = 'xy')\n",
    "\n",
    "# flatten and concat\n",
    "x_full = torch.cat([xx.reshape(-1, 1), yy.reshape(-1, 1)], dim = 1)\n",
    "print(x_full.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understand order of outputs:\n",
    "\n",
    "Stregtegy: All inputs/outputs have different dimensionality so that we can differentite them in the shape of the output.\n",
    "\n",
    "## Case 1: \n",
    "\n",
    "- N inputs/outputs: 5\n",
    "- (x) input dimensionality: 2D\n",
    "- (y) output dimensionality: 3D "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 5, 2])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_N5 = x_full[[3, 18, 11, 60, 61], :]\n",
    "\n",
    "def f_2dto3d(x):\n",
    "    # Define the function f(x)\n",
    "    y1 = x[:, 0] ** 2 + x[:, 1] ** 2\n",
    "    # print(y1.shape)\n",
    "    y2 = torch.sin(x[:, 0]) + torch.cos(x[:, 1])\n",
    "    y3 = x[:, 0] * x[:, 1]\n",
    "    return torch.concatenate([y1.unsqueeze(-1), y2.unsqueeze(-1), y3.unsqueeze(-1)], dim = -1)\n",
    "\n",
    "autograd_jac = torch.autograd.functional.jacobian(f_2dto3d, x_N5)\n",
    "autograd_jac.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The output is [N, output_dim, N, input_dim].\n",
    "- From the [documentation](https://pytorch.org/docs/stable/generated/torch.autograd.functional.jacobian.html) \"Jacobian[i][j] will contain the Jacobian of the **ith output** and **jth input** and will have as size the concatenation of the sizes of the corresponding output and the corresponding input\"\n",
    "- rows i.e. [i] are the output dims, as in the proper Jacobian definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 5, 2])\n",
      "torch.Size([5, 3, 5, 2])\n"
     ]
    }
   ],
   "source": [
    "print(torch.func.jacfwd(f_2dto3d)(x_N5).shape)\n",
    "print(torch.func.jacrev(f_2dto3d)(x_N5).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 2: \n",
    "\n",
    "- N inputs/outputs: 5\n",
    "- (x) input dimensionality: 2D\n",
    "- (y) output dimensionality: 3x4D "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of output:\n",
      "torch.Size([5, 3, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 4, 5, 2])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_N5 = x_full[[3, 18, 11, 60, 61], :]\n",
    "\n",
    "def f_2dto3x4d(x):\n",
    "    # Define the function f(x)\n",
    "    y1 = x[:, 0] ** 2 + x[:, 1] ** 2\n",
    "    # print(y1.shape)\n",
    "    y2 = torch.sin(x[:, 0]) + torch.cos(x[:, 1])\n",
    "    y3 = x[:, 0] * x[:, 1]\n",
    "    out1 = torch.concatenate([y1.unsqueeze(-1), y2.unsqueeze(-1), y3.unsqueeze(-1)], dim = -1).unsqueeze(-1)\n",
    "    return torch.concatenate([out1, 2 * out1, 3 * out1, 4 * out1], dim = -1)\n",
    "\n",
    "print(\"Shape of output:\")\n",
    "print(f_2dto3x4d(x_N5).shape)\n",
    "\n",
    "autograd_jac = torch.autograd.functional.jacobian(f_2dto3x4d, x_N5)\n",
    "autograd_jac.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The output is [N, output_dim1, output_dim2, N, input_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 4, 5, 2])\n",
      "torch.Size([5, 3, 4, 5, 2])\n"
     ]
    }
   ],
   "source": [
    "print(torch.func.jacfwd(f_2dto3x4d)(x_N5).shape)\n",
    "print(torch.func.jacrev(f_2dto3x4d)(x_N5).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 3:\n",
    "\n",
    "This has is most like ours because our input dim (of A) is N x 2 x 2 \n",
    "\n",
    "- N inputs/outputs: 5\n",
    "- (x) input dimensionality: 2x4D\n",
    "- (y) output dimensionality: 3D "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2, 4])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_N5 = x_full[[3, 18, 11, 60, 61], :]\n",
    "x_N5_2x4 = torch.cat([x_N5.unsqueeze(-1), x_N5.unsqueeze(-1), x_N5.unsqueeze(-1), x_N5.unsqueeze(-1)], dim = -1)\n",
    "x_N5_2x4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 4, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 5, 2, 4])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f_2x4dto3d(x):\n",
    "    # Define the function f(x)\n",
    "    y1 = x[:, 0].mean(-1) ** 2 + x[:, 1].mean(-1) ** 2\n",
    "    # print(y1.shape)\n",
    "    y2 = torch.sin(x[:, 0].mean(-1)) + torch.cos(x[:, 1].mean(-1))\n",
    "    y3 = x[:, 0].mean(-1) * x[:, 1].mean(-1)\n",
    "    return torch.concatenate([y1.unsqueeze(-1), y2.unsqueeze(-1), y3.unsqueeze(-1)], dim = -1)\n",
    "\n",
    "print(f_2dto3d(x_N5_2x4).shape)\n",
    "\n",
    "autograd_jac = torch.autograd.functional.jacobian(f_2x4dto3d, x_N5_2x4)\n",
    "autograd_jac.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The output is [N, output_dim1, N, input_dim1, input_dim2]\n",
    "- So autograd returns all output dims first, followed by N, and then all input dims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 5, 2, 4])\n",
      "torch.Size([5, 3, 5, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "print(torch.func.jacfwd(f_2x4dto3d)(x_N5_2x4).shape)\n",
    "print(torch.func.jacrev(f_2x4dto3d)(x_N5_2x4).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    # Define the function f(x), where x is a tensor\n",
    "    y1 = x[:, 0] ** 2 + x[:, 1] ** 2\n",
    "    # print(y1.shape)\n",
    "    y2 = torch.sin(x[:, 0]) + torch.cos(x[:, 1])\n",
    "    return torch.concatenate([y1.unsqueeze(-1), y2.unsqueeze(-1)], dim = -1)\n",
    "\n",
    "def analytical_jacobian_f(x):\n",
    "    # Define the Jacobian of f(x)\n",
    "    df1_dx = 2 * x[:, 0]\n",
    "    df1_dy = 2 * x[:, 1]\n",
    "    df2_dx = torch.cos(x[:, 0])\n",
    "    df2_dy = - torch.sin(x[:, 1])\n",
    "    # This is the correct concat\n",
    "    row1 = torch.stack([df1_dx, df1_dy], dim = -1)\n",
    "    row2 = torch.stack([df2_dx, df2_dy], dim = -1)\n",
    "    # unqueeze at 1 because 0 is batch dim\n",
    "    return torch.cat([row1.unsqueeze(1), row2.unsqueeze(1)], dim = 1)\n",
    "\n",
    "torch.func.vmap(torch.func.jacfwd(f))(x_N5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_single(x_single):\n",
    "    # Define the function f(x), where x is a tensor\n",
    "    y1 = x_single[0] ** 2 + x_single[1] ** 2\n",
    "    # print(y1.shape)\n",
    "    y2 = torch.sin(x_single[0]) + torch.cos(x_single[1])\n",
    "    return torch.concatenate([y1.unsqueeze(-1), y2.unsqueeze(-1)], dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.8571,  0.0000],\n",
       "         [ 0.9096,  0.0000]],\n",
       "\n",
       "        [[ 0.5714,  0.5714],\n",
       "         [ 0.9595, -0.2818]],\n",
       "\n",
       "        [[ 0.8571,  0.2857],\n",
       "         [ 0.9096, -0.1424]],\n",
       "\n",
       "        [[ 1.1429,  2.0000],\n",
       "         [ 0.8411, -0.8415]],\n",
       "\n",
       "        [[ 1.4286,  2.0000],\n",
       "         [ 0.7556, -0.8415]]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract one value for single value formulation\n",
    "x_N5_single = x_N5[3, :]\n",
    "# pass through\n",
    "f_single(x_N5_single)\n",
    "\n",
    "torch.func.vmap(torch.func.jacfwd(f_single))(x_N5)\n",
    "\n",
    "# torch.func.vmap(torch.func.jacfwd(f_single))(x_N5) == analytical_jacobian_f(x_N5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.8571,  0.0000],\n",
       "         [ 0.9096, -0.0000]],\n",
       "\n",
       "        [[ 0.5714,  0.5714],\n",
       "         [ 0.9595, -0.2818]],\n",
       "\n",
       "        [[ 0.8571,  0.2857],\n",
       "         [ 0.9096, -0.1424]],\n",
       "\n",
       "        [[ 1.1429,  2.0000],\n",
       "         [ 0.8411, -0.8415]],\n",
       "\n",
       "        [[ 1.4286,  2.0000],\n",
       "         [ 0.7556, -0.8415]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analytical_jacobian_f(x_N5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4286, 0.0000],\n",
       "        [0.2857, 0.2857],\n",
       "        [0.4286, 0.1429],\n",
       "        [0.5714, 1.0000],\n",
       "        [0.7143, 1.0000]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select 5 points\n",
    "x_N5 = x_full[[3, 18, 11, 60, 61], :]\n",
    "x_N5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.8571,  0.0000],\n",
       "         [ 0.9096, -0.0000]],\n",
       "\n",
       "        [[ 0.5714,  0.5714],\n",
       "         [ 0.9595, -0.2818]],\n",
       "\n",
       "        [[ 0.8571,  0.2857],\n",
       "         [ 0.9096, -0.1424]],\n",
       "\n",
       "        [[ 1.1429,  2.0000],\n",
       "         [ 0.8411, -0.8415]],\n",
       "\n",
       "        [[ 1.4286,  2.0000],\n",
       "         [ 0.7556, -0.8415]]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analytical_jacobian_f(x_N5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.8571,  0.0000],\n",
       "         [ 0.9096,  0.0000]],\n",
       "\n",
       "        [[ 0.5714,  0.5714],\n",
       "         [ 0.9595, -0.2818]],\n",
       "\n",
       "        [[ 0.8571,  0.2857],\n",
       "         [ 0.9096, -0.1424]],\n",
       "\n",
       "        [[ 1.1429,  2.0000],\n",
       "         [ 0.8411, -0.8415]],\n",
       "\n",
       "        [[ 1.4286,  2.0000],\n",
       "         [ 0.7556, -0.8415]]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jac_autograd = torch.autograd.functional.jacobian(f, x_N5)\n",
    "torch.einsum(\"bobi -> boi\", jac_autograd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.func import jacrev, jacfwd, vmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2, 5, 2])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jacfwd(f)(x_N5).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jacobian\n",
    "\n",
    "Row 1: f1, row2: f2\n",
    "\"Jacobian[i][j] will contain the Jacobian of the ith output and jth input\"\n",
    "\n",
    "## Learnings:\n",
    "- **torch.autograd.functional.jacobian**: \n",
    "    - default: create_graph = False! We probably need to turn this on?!\n",
    "    - function is quite slow\n",
    "    - [N, output_dim, N, input_dim(s)]\n",
    "    - vectorize does not work\n",
    "- **torch.func.jacref**\n",
    "    - the function passed into jacref is meant to handle a single input.\n",
    "    - the vmap formulation - which saves a lot of times - is designed to take in a function that takes in one item at the time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.3938, 2.5333, 2.2979])\n",
      "torch.Size([3, 3, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.6762, 1.7176],\n",
       "        [1.0402, 1.4931],\n",
       "        [1.1861, 1.1117]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def exp_reducer(x):\n",
    "    return x.exp().sum(dim = 1)\n",
    "\n",
    "inputs = torch.rand(3, 2)\n",
    "print(exp_reducer(inputs)) # only f1 (i.e. y1) 1D output\n",
    "\n",
    "# torch.autograd.functional.jacobian(exp_reducer, inputs)[0].shape\n",
    "print(torch.autograd.functional.jacobian(exp_reducer, inputs).shape)\n",
    "jac = torch.autograd.functional.jacobian(exp_reducer, inputs)\n",
    "\n",
    "# Only one row expected f1/x1 f1/x2\n",
    "jac[:, :, :]\n",
    "\n",
    "torch.einsum(\"bbi -> bi\", jac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
