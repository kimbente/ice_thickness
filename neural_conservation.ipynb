{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.func import jacrev, vmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kim/eccv/.conda/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: \n",
    "\n",
    "- torch.autograd.functional.jacobian calculates cross-terms which we don't need\n",
    "- the otehr version is faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5 # number of points along x and y axes\n",
    "D = 2 # dimensions\n",
    "\n",
    "# each point on 5 x 5 2D grid maps to a 2 x 2 matrix\n",
    "any_square_matrix = torch.randn(size = (N, N, D, D))\n",
    "\n",
    "# construct the antisymmetric (skew-symmetric) matrix (no scaling needed) \n",
    "# we transpose just the last two dimensions\n",
    "A = any_square_matrix - any_square_matrix.transpose(-1, -2)\n",
    "\n",
    "# assert raises an error if the condition is not met\n",
    "assert torch.allclose(A, - A.transpose(-1, -2)), \"Matrix is not antisymmetric!\"\n",
    "\n",
    "# https://github.com/facebookresearch/neural-conservation-law/blob/main/pytorch/divfree.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.diagonal(dim1 = - 1, dim2 = -2).sum(dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jax'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mjnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m jit, vmap\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'jax'"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import jit, vmap\n",
    "from jax import jacfwd, jacrev, grad, jvp\n",
    "\n",
    "import flax.linen as nn\n",
    "from einops import rearrange\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_flat = A.reshape(-1) # flattens A into a 1D tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.nelement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from NCL\n",
    "def div(u):\n",
    "    \"\"\"Accepts a function u:R^D -> R^D.\"\"\"\n",
    "    J = jacrev(u)\n",
    "    return lambda x: torch.trace(J(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected Jacobian shape: torch.Size([5, 2, 2])\n",
      "Jacobian for the first input:\n",
      " tensor([[ 4.5247,  1.0000],\n",
      "        [-0.6377,  0.8643]], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Define vector function f: R^2 -> R^2\n",
    "def f(x):\n",
    "    x1, x2 = x[..., 0], x[..., 1]\n",
    "    return torch.stack([\n",
    "        x1**2 + x2,\n",
    "        torch.sin(x1) + x2**3\n",
    "    ], dim = -1)  # Keep vector output shape\n",
    "\n",
    "# Generate a batch of 5 random vectors in R^2\n",
    "x = torch.randn(5, 2, requires_grad=True)\n",
    "\n",
    "# Compute Jacobian correctly: Per-sample, not across batch\n",
    "J_fn = vmap(jacrev(f))  # Vectorized Jacobian function\n",
    "J = J_fn(x)  # Apply to batch\n",
    "\n",
    "# Print output shape\n",
    "print(\"Corrected Jacobian shape:\", J.shape)  # Should be (5, 2, 2)\n",
    "\n",
    "# Print an example Jacobian for the first input\n",
    "print(\"Jacobian for the first input:\\n\", J[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Got argnum=1, but only 1 positional inputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# https://pytorch.org/functorch/nightly/generated/functorch.jacrev.html\u001b[39;00m\n\u001b[1;32m      2\u001b[0m J_fn \u001b[38;5;241m=\u001b[39m jacrev(func \u001b[38;5;241m=\u001b[39m f, argnums \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mJ_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ice_thickness/.conda/lib/python3.9/site-packages/torch/_functorch/eager_transforms.py:604\u001b[0m, in \u001b[0;36mjacrev.<locals>.wrapper_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper_fn\u001b[39m(\u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m    603\u001b[0m     error_if_complex(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjacrev\u001b[39m\u001b[38;5;124m\"\u001b[39m, args, is_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 604\u001b[0m     vjp_out \u001b[38;5;241m=\u001b[39m \u001b[43m_vjp_with_argnums\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margnums\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margnums\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_aux\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    605\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_aux:\n\u001b[1;32m    606\u001b[0m         output, vjp_fn, aux \u001b[38;5;241m=\u001b[39m vjp_out\n",
      "File \u001b[0;32m~/ice_thickness/.conda/lib/python3.9/site-packages/torch/_functorch/vmap.py:48\u001b[0m, in \u001b[0;36mdoesnt_support_saved_tensors_hooks.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mdisable_saved_tensors_hooks(message):\n\u001b[0;32m---> 48\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ice_thickness/.conda/lib/python3.9/site-packages/torch/_functorch/eager_transforms.py:397\u001b[0m, in \u001b[0;36m_vjp_with_argnums\u001b[0;34m(func, argnums, has_aux, *primals)\u001b[0m\n\u001b[1;32m    395\u001b[0m     diff_primals \u001b[38;5;241m=\u001b[39m _create_differentiable(primals, level)\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 397\u001b[0m     diff_primals \u001b[38;5;241m=\u001b[39m \u001b[43m_slice_argnums\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprimals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margnums\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_tuple\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m     tree_map_(partial(_create_differentiable, level\u001b[38;5;241m=\u001b[39mlevel), diff_primals)\n\u001b[1;32m    399\u001b[0m primals_out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39mprimals)\n",
      "File \u001b[0;32m~/ice_thickness/.conda/lib/python3.9/site-packages/torch/_functorch/eager_transforms.py:924\u001b[0m, in \u001b[0;36m_slice_argnums\u001b[0;34m(args, argnums, as_tuple)\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(argnums, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(argnums, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    922\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margnums must be int or Tuple[int, ...], got: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(argnums)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    923\u001b[0m     )\n\u001b[0;32m--> 924\u001b[0m argnums \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_and_wrap_argnums\u001b[49m\u001b[43m(\u001b[49m\u001b[43margnums\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    925\u001b[0m _check_unique_non_empty(argnums)\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(argnums, \u001b[38;5;28mint\u001b[39m):\n",
      "File \u001b[0;32m~/ice_thickness/.conda/lib/python3.9/site-packages/torch/_functorch/eager_transforms.py:913\u001b[0m, in \u001b[0;36m_validate_and_wrap_argnums\u001b[0;34m(argnums, num_args)\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_and_wrap_argnums\u001b[39m(argnums, num_args):\n\u001b[1;32m    912\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(argnums, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m--> 913\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_validate_and_wrap_argnum\u001b[49m\u001b[43m(\u001b[49m\u001b[43margnums\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(argnums, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    915\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(_validate_and_wrap_argnum(argnum, num_args) \u001b[38;5;28;01mfor\u001b[39;00m argnum \u001b[38;5;129;01min\u001b[39;00m argnums)\n",
      "File \u001b[0;32m~/ice_thickness/.conda/lib/python3.9/site-packages/torch/_functorch/eager_transforms.py:877\u001b[0m, in \u001b[0;36m_validate_and_wrap_argnum\u001b[0;34m(argnum, num_args)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m argnum \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m argnum \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mnum_args:\n\u001b[1;32m    876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m argnum \u001b[38;5;241m+\u001b[39m num_args\n\u001b[0;32m--> 877\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot argnum=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margnum\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but only \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_args\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m positional inputs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Got argnum=1, but only 1 positional inputs"
     ]
    }
   ],
   "source": [
    "# https://pytorch.org/functorch/nightly/generated/functorch.jacrev.html\n",
    "J_fn = jacrev(func = f, argnums = 1)\n",
    "J_fn(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My annotation of the code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from functorch import make_functional\n",
    "from functorch import vmap\n",
    "from functorch import jacrev\n",
    "\n",
    "\n",
    "def div(u):\n",
    "    \"\"\"Accepts a function u:R^D -> R^D.\"\"\"\n",
    "    J = jacrev(u)\n",
    "    return lambda x: torch.trace(J(x))\n",
    "\n",
    "\n",
    "def build_divfree_vector_field(module):\n",
    "    \"\"\"Returns an unbatched vector field, i.e. assumes input is a 1D tensor.\"\"\"\n",
    "\n",
    "    F_fn, params = make_functional(module)\n",
    "\n",
    "    J_fn = jacrev(F_fn, argnums=1)\n",
    "\n",
    "    def A_fn(params, x):\n",
    "        J = J_fn(params, x)\n",
    "        A = J - J.T\n",
    "        return A\n",
    "\n",
    "    def A_flat_fn(params, x):\n",
    "        A = A_fn(params, x)\n",
    "        A_flat = A.reshape(-1)\n",
    "        return A_flat\n",
    "\n",
    "    def ddF(params, x):\n",
    "        D = x.nelement() # dimension of the input (counts all elements across dimensions)\n",
    "        dA_flat = jacrev(A_flat_fn, argnums = 1)(params, x)\n",
    "        Jac_all = dA_flat.reshape(D, D, D)\n",
    "        ddF = vmap(torch.trace)(Jac_all)\n",
    "        return ddF\n",
    "\n",
    "    return ddF, params, A_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jac rev is faster"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
