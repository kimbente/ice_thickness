{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def divergence_free_se_kernel_multi_input(X1_rows,\n",
    "                              X2_rows, \n",
    "                              X1_columns, \n",
    "                              X2_columns, \n",
    "                              hyperparameters):\n",
    "    \n",
    "    # Extract hyperparameters\n",
    "    sigma_f = hyperparameters[0]\n",
    "    l = hyperparameters[1]\n",
    "    \n",
    "    # Calculate the (not quite Euclidean) distance between all pairs of points\n",
    "    # This approach yields negative values as well\n",
    "    # X1_dist: torch.Size([n_rows, n_columns])\n",
    "    X1_dist = (X1_rows.unsqueeze(1) - X1_columns.unsqueeze(0)).squeeze()\n",
    "    # ALTERNATIVE (pos.) torch.cdist(X1_rows, X1_columns): this is just torch.abs(X1_dist)\n",
    "\n",
    "    # X2_dist: torch.Size([n_rows, n_columns])\n",
    "    X2_dist = (X2_rows.unsqueeze(1) - X2_columns.unsqueeze(0)).squeeze()\n",
    "\n",
    "    # torch.Size([n_rows, n_columns])\n",
    "    upper_left = (1 - X2_dist.square().div(l**2)).div(l**2)\n",
    "\n",
    "    # elementwise multiplication and division by scalar\n",
    "    # Matlab version has negative values here! \n",
    "    upper_right = torch.mul(X1_dist, X2_dist).div(l**4)\n",
    "    lower_left = upper_right\n",
    "    lower_right = (1 - X1_dist.square().div(l**2)).div(l**2)\n",
    "\n",
    "    # Concatenate upper and lower blocks column-wise, and then concatenate them row-wise\n",
    "    # torch.Size([2 * n_train, 2 * n_test])\n",
    "    block = torch.cat((torch.cat((upper_left, upper_right), 1), torch.cat((lower_left, lower_right), 1)), 0)\n",
    "\n",
    "    # torch.Size([2 * n_train, 2 * n_test])\n",
    "    # elementwise multiplication\n",
    "    K = sigma_f.square() * block.mul((X1_dist.square() + X2_dist.square()).div(-2 * l**2).exp().tile(2, 2))\n",
    "\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# seperate grid inputs and outputs: now tensors\n",
    "def simulate_convergence(X1, X2):\n",
    "    U = X2\n",
    "    V = X1\n",
    "    return U, V\n",
    "\n",
    "def simulate_merge(X1, X2):\n",
    "    U = (X2 + 0.5)**2\n",
    "    V = np.sin(X1 * math.pi)\n",
    "    return U, V\n",
    "\n",
    "def simulate_branching(X1, X2):\n",
    "    U = X1 * X2\n",
    "    V = - 0.5 * X2**2 + (X1 - 0.8)\n",
    "    return U, V\n",
    "\n",
    "def simulate_deflection(X1, X2):\n",
    "    U = (X2 * 6 - 3)**2 + (X1 * 6 - 3)**2 + 3\n",
    "    V = -2 * (X2 * 6 - 3) * (X1 * 6 - 3)\n",
    "    return U, V\n",
    "\n",
    "def simulate_ridge(X1, X2):\n",
    "    U = X2 + 1\n",
    "    V = - np.cos(3 * X1**3 * math.pi)\n",
    "    return U, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "##############################################################\n",
    "### OPTIMIZATION ###\n",
    "def optimize_hypers_on_train_df(\n",
    "        hyperparameters_df_initial, \n",
    "        X_train, \n",
    "        Y_train_noisy, \n",
    "        X_test,\n",
    "        max_optimisation_iterations = 1000,\n",
    "        patience = 20,\n",
    "        learning_rate = 0.0001):\n",
    "\n",
    "        # Clone hyperparameters to avoid modifying the original tensor\n",
    "        hyperparameters_df = hyperparameters_df_initial\n",
    "\n",
    "        _, _, lml_initial = predict(\n",
    "                X_train,\n",
    "                Y_train_noisy,\n",
    "                X_test,\n",
    "                hyperparameters_df,\n",
    "                divergence_free_bool = True)\n",
    "        \n",
    "        print(f\"Initial hyperparameters: {hyperparameters_df[0].item():.3f}, \"\n",
    "              f\"{hyperparameters_df[1].item():.3f}, \"\n",
    "              f\"{hyperparameters_df[2].item():.3f}\")\n",
    "        print(f\"Initial LML: {lml_initial.item():.2f}\")\n",
    "        \n",
    "        # doesn't matter if we leave hyperparameters_df[0] in here\n",
    "        optimizer = optim.Adam([hyperparameters_df[0], hyperparameters_df[1], hyperparameters_df[2]], lr = learning_rate)\n",
    "\n",
    "        best_loss = float('inf') # initialse as infinity\n",
    "        best_hypers = None\n",
    "        no_improvement_count = 0\n",
    "\n",
    "        for trial in range(max_optimisation_iterations):\n",
    "                \n",
    "                # Compute nlml\n",
    "                _, _, lml = predict(\n",
    "                X_train,\n",
    "                Y_train_noisy,\n",
    "                X_test,\n",
    "                hyperparameters_df,\n",
    "                divergence_free_bool = True)\n",
    "                \n",
    "                # We are minimising the negative log marginal likelihood, like a loss function\n",
    "                loss = - lml # NLML\n",
    "\n",
    "                # Check for improvement\n",
    "                if loss < best_loss:\n",
    "                        best_loss = loss.item() # If better than current, save loss and hypers\n",
    "                        # we need to clone and not reference\n",
    "                        best_hypers = [h.clone().detach() for h in hyperparameters_df]\n",
    "                \n",
    "                        no_improvement_count = 0  # Reset counter\n",
    "\n",
    "                else:\n",
    "                        no_improvement_count += 1  # Increase counter\n",
    "\n",
    "                # Stop if loss has stagnated\n",
    "                if no_improvement_count >= patience:\n",
    "                # Printing current state\n",
    "                        print(f\"Stopping early after {trial+1} iterations.\")\n",
    "                        print(f\"Best hyperparameters: {best_hypers[0].detach().numpy()[0]:.3f}, \"\n",
    "                              f\"{best_hypers[1].detach().numpy()[0]:.3f}, \"\n",
    "                              f\"{best_hypers[2].detach().numpy()[0]:.3f}\")\n",
    "                        print(f\"Best LML: {(- best_loss):.2f}\")\n",
    "                        \n",
    "                        break\n",
    "                \n",
    "                optimizer.zero_grad()  # Reset gradients\n",
    "                loss.backward()  # Compute gradients\n",
    "                optimizer.step()  # Update hypers\n",
    "\n",
    "                # if trial % 10 == 0:  # Print every 10 iterations\n",
    "                #        print(f\"Current hyperparameters: {hyperparameters_df[0].detach().numpy()[0]:.3f}, \"\n",
    "                #              f\"{hyperparameters_df[1].detach().numpy()[0]:.3f}, \"\n",
    "                #              f\"{hyperparameters_df[2].detach().numpy()[0]:.3f}\")\n",
    "                #        print(f\"Current NLML: {nlml.item():.2f}\")\n",
    "        \n",
    "        return best_hypers\n",
    "\n",
    "### OPTIMIZATION ###\n",
    "def optimize_hypers_on_train_bd(\n",
    "        hyperparameters_bd_initial, \n",
    "        X_train, \n",
    "        Y_train_noisy, \n",
    "        X_test,\n",
    "        max_optimisation_iterations = 1000,\n",
    "        patience = 20,\n",
    "        learning_rate = 0.0001):\n",
    "\n",
    "        # Clone hyperparameters to avoid modifying the original tensor\n",
    "        hyperparameters_bd = hyperparameters_bd_initial\n",
    "        \n",
    "        optimizer = optim.Adam([hyperparameters_bd[0], hyperparameters_bd[1], hyperparameters_bd[2], hyperparameters_bd[3]], lr = learning_rate)\n",
    "\n",
    "        best_loss = float('inf')\n",
    "        best_hypers_bd = None\n",
    "        no_improvement_count = 0\n",
    "\n",
    "        for trial in range(max_optimisation_iterations):\n",
    "                \n",
    "                # Compute nlml\n",
    "                _, _, nlml = predict(\n",
    "                X_train,\n",
    "                Y_train_noisy,\n",
    "                X_test,\n",
    "                hyperparameters_bd,\n",
    "                divergence_free_bool = True)\n",
    "                \n",
    "                loss = - nlml\n",
    "\n",
    "                # Check for improvement\n",
    "                if loss < best_loss:\n",
    "                        best_loss = loss.item()\n",
    "                        best_hypers_bd = [h.clone().detach() for h in hyperparameters_bd]\n",
    "                \n",
    "                        no_improvement_count = 0  # Reset counter\n",
    "\n",
    "                else:\n",
    "                        no_improvement_count += 1  # Increase counter\n",
    "\n",
    "                # Stop if loss has stagnated\n",
    "                if no_improvement_count >= patience:\n",
    "                # Printing current state\n",
    "\n",
    "                        print(f\"Stopping early after {trial+1} iterations.\")\n",
    "                        print(f\"Best hyperparameters: {best_hypers_bd[0].item():.3f}, \"\n",
    "                              f\"{best_hypers_bd[1].item():.3f}, \"\n",
    "                              f\"{best_hypers_bd[2].item():.3f}\")\n",
    "                        print(f\"Best NLML: {best_loss:.2f}\")\n",
    "                        \n",
    "                        break\n",
    "\n",
    "                optimizer.zero_grad()  # Reset gradients\n",
    "                loss.backward()  # Compute gradients\n",
    "                optimizer.step()  # Update hypers\n",
    "\n",
    "                # if trial % 10 == 0:  # Print every 10 iterations\n",
    "                #        print(f\"Current hyperparameters: {hyperparameters_bd[0].detach().numpy()[0]:.3f}, \"\n",
    "                #              f\"{hyperparameters_bd[1].detach().numpy()[0]:.3f}, \"\n",
    "                #              f\"{hyperparameters_bd[2].detach().numpy()[0]:.3f}\")\n",
    "                #        print(f\"Current NLML: {nlml.item():.2f}\")\n",
    "        \n",
    "        return best_hypers_bd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def divergence_free_se_kernel_l2d(\n",
    "        row_tensor, # torch.Size([n_rows, 2])\n",
    "        column_tensor, # torch.Size([n_columns, 2])\n",
    "        hyperparameters):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculate the divergence-free SE kernel for two sets of points in 2D space.\n",
    "    R^2 -> R^2\n",
    "\n",
    "    Inputs:\n",
    "        row_tensor: torch.Size([n_rows, 2])\n",
    "        column_tensor: torch.Size([n_columns, 2])\n",
    "        hyperparameters: list of length 3 containing sigma_n, sigma_f and l\n",
    "\n",
    "    Returns:\n",
    "        K: torch.Size([n_rows * 2, n_columns * 2])\n",
    "    \"\"\"\n",
    "    \n",
    "    # We calculate the kernel for each pair of points\n",
    "    \n",
    "    # Extract hyperparameters (except for sigma_n)\n",
    "    # sigma_f_squared = torch.exp(hyperparameters[1]) # torch.exp(log_sigma_f_squared)\n",
    "    # sigma_f_squared = hyperparameters[1]\n",
    "    sigma_f = hyperparameters[1] \n",
    "    l = hyperparameters[2]\n",
    "\n",
    "    # Accommodate for single or double lengthscale\n",
    "    if l.shape == torch.Size([1]):\n",
    "        l1 = l\n",
    "        l2 = l\n",
    "    else:\n",
    "        l1 = l[0]\n",
    "        l2 = l[1]\n",
    "\n",
    "    # Add dimension (broadcasting) for difference calculation\n",
    "    # torch.Size([n_rows, 1, 2]) - 1 is for n_columns\n",
    "    row_tensor_expanded = row_tensor[:, None, :]\n",
    "    # torch.Size([1, n_columns, 2]) - 1 is for n_rowns\n",
    "    column_tensor_expanded = column_tensor[None, :, :]\n",
    "\n",
    "    # Calculate differences for x-coordinate \"features\" as well as y-coordinate \"features\"\n",
    "    # [:, :, 0] are the x1 differences and [:, :, 1] are the x2 differences\n",
    "    # yields negative values as well\n",
    "    diff = row_tensor_expanded - column_tensor_expanded\n",
    "\n",
    "    ### 2x2 BLOCKS ###\n",
    "    # x2 diffs: torch.Size([n_rows, n_columns])\n",
    "    upper_left = (1 - diff[:, :, 1].square().div(l2.square())).div(l2.square())\n",
    "\n",
    "    # x1 diffs: torch.Size([n_rows, n_columns])\n",
    "    lower_right = (1 - diff[:, :, 0].square().div(l1.square())).div(l1.square())\n",
    "\n",
    "    # Elementwise multiplication of x1 and x2 diffs and division by scalar\n",
    "    # Matlab version has negative values here!\n",
    "    # Combined at x1 and x2 diffs\n",
    "    # l4 squared\n",
    "    upper_right = torch.prod(diff, dim = -1).div(l1.square() * l2.square())\n",
    "\n",
    "    # same as other off-diagonal block\n",
    "    lower_left = upper_right\n",
    "\n",
    "    # Concatenate upper and lower blocks column-wise, and then concatenate them row-wise\n",
    "    # torch.Size([2 * n_train, 2 * n_test])\n",
    "    blocks = torch.cat((\n",
    "        torch.cat((upper_left, upper_right), 1), \n",
    "        torch.cat((lower_left, lower_right), 1)\n",
    "        ), 0)\n",
    "\n",
    "    # torch.Size([2 * n_row, 2 * n_column])\n",
    "    # elementwise multiplication\n",
    "    # sum squared difference over x1 and x2, divide by -2 * l^2, and exponentiate. Tile for blocks\n",
    "    K = sigma_f.square() * blocks.mul(diff.square().sum(dim = -1).div(-2 * l1 * l2).exp().tile(2, 2))\n",
    "\n",
    "    return K"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
