{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix field reconstruction\n",
    "\n",
    "The matrix field reconstruction has one less Jacobian step than the vector field reconstruction which is why it is computationally more efficient.\n",
    "\n",
    "We use the following deterministic transformations to get from the NN output to a divergence-free vector field v:\n",
    "1. Parameterise the Skew-Symmetric decomposition of A\n",
    "    - U = NN(x) (non-zero values of the Upper Triangular U are of size N(N - 1)/(2) (x2))\n",
    "2. Construct anti-symmetric matrix A\n",
    "    - A = U - U.T \n",
    "3. Attain divergence-free vector field v via\n",
    "    - v = (div(A1), div(A2)), trace of the Jacobian\n",
    "\n",
    "see https://github.com/facebookresearch/neural-conservation-law/blob/main/pytorch/divfree.py\n",
    "\n",
    "Dimensionalities:\n",
    "- If our input in (4 x 4, 2) so flat that is (16, 2) coordinate pairs, U should be (6, 2, 2)\n",
    "- 6 = (sqrt(N)(sqrt(N) - 1)/2)\n",
    "\n",
    "NN batch-wise or not\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "- Do we use a NN that processes a batch or points individually? (Batch)\n",
    "- What is the output shape of u_v?\n",
    "    - For the dim = 2 case, do it is always (dim * (dim - 1) / 2), which is 1 because we only estimate the upper right corner of every (N) 2 x 2 matrix\n",
    "    - Can we build this directly into the net?\n",
    "\n",
    "The model seems to only be implemented under [jax > models.py > Divfree()](https://github.com/facebookresearch/neural-conservation-law/blob/20a403d00affad905d1c47b041bc60d0ff0ea360/jax/models.py#L118). DivfreeSparse() and DivFreeImplicit() are not used anywhere.\n",
    "\n",
    "The model is used in [jax > hh_experiment_DivFree.py](https://github.com/facebookresearch/neural-conservation-law/blob/20a403d00affad905d1c47b041bc60d0ff0ea360/jax/hh_experiment_DivFree.py#L53). Hodge decomp.\n",
    "\n",
    "dim = 10.  \n",
    "mlp = MLP(depth = layers, width = width, act = act, out_dim = **dim * (dim-1) // 2**, std = 1, bias = True)\n",
    "\n",
    "For dim = 2, at each point, each matrix A (2 x 2) is antisymm. So we only have to estimate a scalar for each input point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "u_fn, params, _ = build_divfree_vector_field(self.module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim = 2, hidden_dim = 32):\n",
    "        super().__init__()\n",
    "        output_dim = int((input_dim * (input_dim - 1)) / 2)\n",
    "        print(output_dim)\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)  # Output shape: (4,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 2])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLP()\n",
    "\n",
    "N = 16  # N should be a perfect square\n",
    "N_side = int(N ** 0.5)\n",
    "dims = 2\n",
    "inputs = torch.randn(N, dims)  # Random (N, 2) inputs\n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# U_fill shape is (N, 1)\n",
    "U_fill = model(inputs)\n",
    "U = torch.zeros(N, dims, dims)\n",
    "# Fills all top right corners of the (N, 2, 2) tensor\n",
    "U[:, 0, 1] = U_fill.squeeze()\n",
    "# U is (N, 2, 2), so we need to swap the last two dims and then subtract\n",
    "A = U - U.transpose(1, 2)\n",
    "\n",
    "def compute_A(inputs):\n",
    "    # U_fill is (N, 1)\n",
    "    U_fill = model(inputs)\n",
    "    # This version works with vmap\n",
    "    U = torch.triu(torch.ones(N, dims, dims), diagonal = 1)\n",
    "    U = U * U_fill.unsqueeze(1)\n",
    "    A = U - U.transpose(1, 2)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# U = U.index_add(1, torch.tensor([0, 1]), U_fill.squeeze().unsqueeze(1).repeat(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jacobian\n",
    "\n",
    "- Jacobian is probably among the most expensive functions\n",
    "- [torch.func.jacrev](https://pytorch.org/docs/stable/generated/torch.func.jacrev.html#torch.func.jacrev)\n",
    "    - The implementation goes forward\n",
    "    - torch.func.jacobian chooses based on efficiency\n",
    "- batched Jacobians via vmap\n",
    "- torch.autograd.functional.jacobian(f, x)\n",
    "    - not as fast as func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.func import jacrev, jacfwd, vmap\n",
    "x = torch.randn(5, 2, 2)\n",
    "jacobian = vmap(jacrev(torch.sin))(x)\n",
    "jacobian.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 2, 2, 2])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Issue with how we construct U \n",
    "# Jacobian: torch.Size([16, 16, 2, 2, 2])\n",
    "# without vmap it was also torch.Size([16, 2, 2, 16, 2])\n",
    "jacobian = vmap(jacrev(compute_A))(inputs)\n",
    "\n",
    "# Remove redundant dim\n",
    "# (jacobian[:, 0, : , :, :] == jacobian[:, 15, : , :, :]).any()\n",
    "jacobian_sq = jacobian[:, 0, : , :, :]\n",
    "jacobian_sq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.diagonal(jacobian_sq, dim1 = 2, dim2 = 3).sum(dim = 1)\n",
    "# torch.diagonal(jacobian_sq, dim1 = 1, dim2 = 3).sum(dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = torch.diagonal(jacobian_sq, dim1 = 2, dim2 = 3).sum(dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0344],\n",
       "        [ 0.0000, -0.0620],\n",
       "        [ 0.0000, -0.0434],\n",
       "        [ 0.0000, -0.0447],\n",
       "        [ 0.0000, -0.0142],\n",
       "        [ 0.0000,  0.0077],\n",
       "        [ 0.0000,  0.0283],\n",
       "        [ 0.0000,  0.0057],\n",
       "        [ 0.0000,  0.0440],\n",
       "        [ 0.0000,  0.0528],\n",
       "        [ 0.0000, -0.0360],\n",
       "        [ 0.0000,  0.0541],\n",
       "        [ 0.0000,  0.0662],\n",
       "        [ 0.0000,  0.0424],\n",
       "        [ 0.0000,  0.1336],\n",
       "        [ 0.0000,  0.0620]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jacobian[:, :, 1, 0, 0]\n",
    "jacobian[:, 0, 1, 0, 0] # second d is unnessary\n",
    "jacobian[:, 0, 0, 1, 0]\n",
    "jacobian[:, 0, 0, :, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the Jacobian using autograd\n",
    "# Takes in function & input\n",
    "# torch.Size([16, 2, 2, 16, 2])\n",
    "# jacobian = torch.autograd.functional.jacobian(compute_A, inputs, vectorize = True)\n",
    "jacobian_func = torch.func.vmap(torch.func.jacfwd(compute_A))(inputs)\n",
    "# trace_result = torch.trace(jacobian, dim1 = 1, dim2 = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 16, 2, 2, 2])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vmap(torch.func.jacfwd(compute_A))(inputs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  0.0000],\n",
       "         [ 0.0273,  0.0344]],\n",
       "\n",
       "        [[-0.0273, -0.0344],\n",
       "         [ 0.0000,  0.0000]]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.func.jacrev(compute_A)(inputs).diagonal(dim1 = 2, dim2 = 3).shape\n",
    "torch.func.jacfwd(compute_A)(inputs)[0, :, :, 0]\n",
    "# torch.func.jacrev(compute_A)(inputs)[0, 0, 1, 0, 0]\n",
    "# torch.func.jacrev(compute_A)(inputs)[0, 1, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'diagonal'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mjacobian_func\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiagonal\u001b[49m(dim1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, dim2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39msum()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'diagonal'"
     ]
    }
   ],
   "source": [
    "jacobian_func.diagonal(dim1 = 1, dim2 = 2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jacobian.diagonal(dim1 = 1, dim2 = 2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 2, 2, 16, 2])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jacobian.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U_empty[:, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = jnp.zeros((N_side, N_side))\n",
    "idx = jnp.triu_indices(N, 1)\n",
    "U = U.at[idx].set(b) # go through via row\n",
    "A = U - U.T # miuns now multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([16, 2])\n",
      "Output shape: torch.Size([120, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import itertools\n",
    "import math\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim = 2, hidden_dim = 32, output_dim = 4):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)  # Output shape: (4,)\n",
    "\n",
    "def generate_pairwise_matrices(inputs, model):\n",
    "    \"\"\"\n",
    "    Takes (N, 2) inputs, applies the model, and outputs (sqrt(N)(sqrt(N)-1)/2, 2, 2) shaped tensor.\n",
    "    \"\"\"\n",
    "    N = inputs.shape[0]\n",
    "    \n",
    "    # Check that N is a perfect square for √N pairs\n",
    "    sqrt_N = math.isqrt(N)\n",
    "    assert sqrt_N ** 2 == N, \"N should be a perfect square!\"\n",
    "\n",
    "    # Compute the number of pairwise combinations: sqrt(N) * (sqrt(N) - 1) / 2\n",
    "    num_pairs = (sqrt_N * (sqrt_N - 1)) // 2\n",
    "\n",
    "    pairs = list(itertools.combinations(range(N), 2))  # Generate all unique (i, j) pairs\n",
    "\n",
    "    outputs = []\n",
    "    for i, j in pairs:\n",
    "        pair_input = (inputs[i] + inputs[j]) / 2  # Combine inputs (simple average)\n",
    "        matrix_flat = model(pair_input)  # Get (4,) shaped output\n",
    "        matrix = matrix_flat.view(2, 2)  # Reshape to (2, 2)\n",
    "        outputs.append(matrix)\n",
    "\n",
    "    return torch.stack(outputs)  # Shape: (num_pairs, 2, 2)\n",
    "\n",
    "# Example usage\n",
    "N = 16  # N should be a perfect square\n",
    "inputs = torch.randn(N, 2)  # Random (N, 2) inputs\n",
    "\n",
    "model = MLP()\n",
    "output_matrices = generate_pairwise_matrices(inputs, model)\n",
    "\n",
    "print(\"Input shape:\", inputs.shape)  # (N, 2)\n",
    "print(\"Output shape:\", output_matrices.shape)  # (12, 2, 2) for N = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
