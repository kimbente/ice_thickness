{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "\n",
    "import torch\n",
    "import gpytorch\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel, InducingPointKernel\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# subset data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "byrd_bedmap_corner = byrd_bedmap_points_pixel[(byrd_bedmap_points_pixel[\"x\"] > 525000) & (byrd_bedmap_points_pixel[\"y\"] > - 825000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_span, y_span = byrd_bedmap_corner.x.max() - byrd_bedmap_corner.x.min(), byrd_bedmap_corner.y.max() - byrd_bedmap_corner.y.min()\n",
    "x_span/500\n",
    "y_span/500\n",
    "\n",
    "# inferene is for 2300\n",
    "48*48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inducing point kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "        # CONSTANT MEAN\n",
    "        self.mean_module = ConstantMean()\n",
    "        # RBF KERNEL WITH ARD\n",
    "        self.base_covar_module = ScaleKernel(RBFKernel(ard_num_dims = 2))\n",
    "        self.covar_module = InducingPointKernel(self.base_covar_module, inducing_points = initial_inducing_points, likelihood = likelihood)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "N_size = 5000\n",
    "N_induce = 500\n",
    "\n",
    "train_x = torch.tensor(np.array(byrd_bedmap_points_pixel[[\"x\", \"y\"]]).astype(int)/1000000, dtype = torch.float32)\n",
    "# generate random order\n",
    "order = torch.randperm(train_x.size()[0])\n",
    "# reorder and select first N_size\n",
    "train_x = train_x[order][:N_size].to(device)\n",
    "\n",
    "train_y = torch.tensor(np.array(byrd_bedmap_points_pixel[[\"t_mean\"]]), dtype = torch.float32)\n",
    "train_y = train_y[order][:N_size, 0].to(device)\n",
    "\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "\n",
    "# train_x_small = train_x[:1000].to(device)\n",
    "# train_y_small = train_x[:1000].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "initial_inducing_points = train_x[:N_induce]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Gaussian Process Model (SGPR) (inducing point kernel)\n",
    "\n",
    "Scalable kernel approximations.\n",
    "\n",
    "Inducing point locations should be learned?!\n",
    "\n",
    "Sparse Gaussian Process Regression (SGPR) (proposed by Titsias, 2009) which approximates kernels using a set of inducing points. This is a general purpose approximation\n",
    "\n",
    "- scaled RBF kernel as base kernel\n",
    "- wraped in\n",
    "- https://docs.gpytorch.ai/en/stable/kernels.html#kernels-for-scalable-gp-regression-methods \n",
    "    - Documentation (missing): https://docs.gpytorch.ai/en/stable/kernels.html#gpytorch.kernels.InducingPointKernel\n",
    "    - Source code: https://docs.gpytorch.ai/en/stable/_modules/gpytorch/kernels/inducing_point_kernel.html#InducingPointKernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "        # CONSTANT MEAN\n",
    "        self.mean_module = ConstantMean()\n",
    "        # RBF KERNEL WITH ARD\n",
    "        self.base_covar_module = ScaleKernel(RBFKernel(ard_num_dims = 2))\n",
    "        # INDUCING POINTS\n",
    "        # Randomly selects 500 first points from the training data (x locations?!)\n",
    "        # Initialise Inducing points (this is a parameter)\n",
    "        self.covar_module = InducingPointKernel(self.base_covar_module, inducing_points = initial_inducing_points, likelihood = likelihood)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = GPRegressionModel(train_x, train_y, likelihood)\n",
    "\n",
    "# Put on cuda\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    likelihood = likelihood.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "training_iterations = 1000\n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "def train():\n",
    "    iterator = tqdm.tqdm(range(training_iterations), desc = \"Train\")\n",
    "\n",
    "    for i in iterator:\n",
    "        # Zero backprop gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Get output from model\n",
    "        output = model(train_x)\n",
    "        # Calc loss and backprop derivatives\n",
    "        loss = - mll(output, train_y)\n",
    "        loss.backward()\n",
    "        iterator.set_postfix(loss = loss.item())\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "%time train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"Parameter name: {name}\")\n",
    "    print(f\"Parameter value: {param}\")\n",
    "    print(f\"Requires gradient: {param.requires_grad}\")\n",
    "    print(\"-----------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why do the inducing points not move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# XX, YY = np.meshgrid(byrd_bedmachine.x, byrd_bedmachine.y)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10, 10))\n",
    "\n",
    "# Plot bed topography mesh\n",
    "# ax.pcolormesh(XX, YY, byrd_bedmachine.thickness_ellipsoid_true, cmap = icethickness_cmap, vmin = 0, vmax = 3500)\n",
    "\n",
    "# Plot data points\n",
    "ax.scatter(initial_inducing_points[:, 0].cpu().detach().numpy(), \n",
    "           initial_inducing_points[:, 1].cpu().detach().numpy(), \n",
    "           c = \"green\", \n",
    "           alpha = 0.5,\n",
    "           s = 10,\n",
    "           edgecolors = \"green\",\n",
    "           linewidth = 0.15)\n",
    "\n",
    "ax.scatter(model.covar_module.inducing_points[:, 0].cpu().detach().numpy(), \n",
    "           model.covar_module.inducing_points[:, 1].cpu().detach().numpy(), \n",
    "           c = \"red\", \n",
    "           s = 10,\n",
    "           alpha = 0.5,\n",
    "           edgecolors = \"red\",\n",
    "           linewidth = 0.15)\n",
    "\n",
    "fig.colorbar(mappable = ax.collections[0], ax = ax)\n",
    "ax.set_aspect('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# XX, YY = np.meshgrid(byrd_bedmachine.x, byrd_bedmachine.y)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10, 10))\n",
    "\n",
    "# Plot bed topography mesh\n",
    "# ax.pcolormesh(XX, YY, byrd_bedmachine.thickness_ellipsoid_true, cmap = icethickness_cmap, vmin = 0, vmax = 3500)\n",
    "\n",
    "# Plot data points\n",
    "ax.scatter(initial_inducing_points[:, 0].cpu().detach().numpy(), \n",
    "           initial_inducing_points[:, 1].cpu().detach().numpy(), \n",
    "           c = \"green\", \n",
    "           s = 10,\n",
    "           edgecolors = \"green\",\n",
    "           linewidth = 0.15)\n",
    "\n",
    "\n",
    "fig.colorbar(mappable = ax.collections[0], ax = ax)\n",
    "ax.set_aspect('equal')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
