{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dfGP\n",
    "\n",
    "- write the functions that matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma_n: 0.0500, sigma_f: 1.9411, l: 0.7575, 0.4914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1112449/1334247450.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  l = torch.tensor(torch.empty(2).uniform_(*l_range), requires_grad = True)  # Trainable\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from utils import set_seed\n",
    "set_seed(42)\n",
    "\n",
    "# Define ranges\n",
    "# sigma_n_range = (0.01, 0.1)  # Example range for sigma_n\n",
    "sigma_f_range = (1.5, 2.0)   # Example range for sigma_f\n",
    "l_range = (0.3, 0.8)         # Example range for each lengthscale\n",
    "\n",
    "# Sample from uniform distributions\n",
    "sigma_n = torch.tensor([0.05], requires_grad = False) # no optimisation for noise, no sampling\n",
    "sigma_f = torch.tensor([torch.empty(1).uniform_(* sigma_f_range)], requires_grad = True)   # Trainable\n",
    "l = torch.tensor(torch.empty(2).uniform_(* l_range), requires_grad = True)  # Trainable\n",
    "\n",
    "print(f\"sigma_n: {sigma_n.item():.4f}, sigma_f: {sigma_f.item():.4f}, l: {l[0].item():.4f}, {l[1].item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GP_models import predict_GP, optimise_hypers_on_train\n",
    "from simulate import simulate_convergence, simulate_branching, simulate_ridge, simulate_merge, simulate_deflection\n",
    "from metrics import compute_RMSE, compute_MAE\n",
    "from utils import set_seed\n",
    "\n",
    "# Global file for training configs\n",
    "from configs import PATIENCE, MAX_NUM_EPOCHS, NUM_RUNS, LEARNING_RATE, WEIGHT_DECAY, BATCH_SIZE, N_SIDE, DFGP_RESULTS_DIR\n",
    "\n",
    "import torch\n",
    "from torch.func import vmap, jacfwd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "model_name = \"dfNN\"\n",
    "\n",
    "#########################\n",
    "### x_train & y_train ###\n",
    "#########################\n",
    "\n",
    "# Import all simulation functions\n",
    "from simulate import (\n",
    "    simulate_convergence,\n",
    "    simulate_branching,\n",
    "    simulate_merge,\n",
    "    simulate_deflection,\n",
    "    simulate_ridge,\n",
    ")\n",
    "\n",
    "# Define simulations as a dictionary with names as keys to function objects\n",
    "simulations = {\n",
    "    \"convergence\": simulate_convergence,\n",
    "    \"branching\": simulate_branching,\n",
    "    \"merge\": simulate_merge,\n",
    "    \"deflection\": simulate_deflection,\n",
    "    \"ridge\": simulate_ridge,\n",
    "}\n",
    "\n",
    "# Load training inputs\n",
    "x_train = torch.load(\"data/sim_data/x_train_lines_discretised_0to1.pt\").float()\n",
    "\n",
    "# Storage dictionaries\n",
    "y_train_dict = {}\n",
    "\n",
    "# Make y_train_dict: Iterate over all simulation functions\n",
    "for sim_name, sim_func in simulations.items():\n",
    "\n",
    "    # Generate training observations\n",
    "    y_train = sim_func(x_train)\n",
    "    y_train_dict[sim_name] = y_train  # Store training outputs\n",
    "\n",
    "    # Print details\n",
    "    print(f\"=== {sim_name.upper()} ===\")\n",
    "    print(f\"Training inputs shape: {x_train.shape}\")\n",
    "    print(f\"Training observations shape: {y_train.shape}\")\n",
    "    print(f\"Training inputs dtype: {x_train.dtype}\")\n",
    "    print()\n",
    "\n",
    "#######################\n",
    "### x_test & y_test ###\n",
    "#######################\n",
    "\n",
    "print(\"=== Generating test data ===\")\n",
    "\n",
    "# Choose discretisation that is good for simulations and also for quiver plotting\n",
    "N_SIDE = N_SIDE\n",
    "\n",
    "side_array = torch.linspace(start = 0.0, end = 1.0, steps = N_SIDE)\n",
    "XX, YY = torch.meshgrid(side_array, side_array, indexing = \"xy\")\n",
    "x_test_grid = torch.cat([XX.unsqueeze(-1), YY.unsqueeze(-1)], dim = -1)\n",
    "# long format\n",
    "x_test = x_test_grid.reshape(-1, 2)\n",
    "\n",
    "# Storage dictionaries\n",
    "y_test_dict = {}\n",
    "\n",
    "# Make y_test_dict: Iterate over all simulation functions\n",
    "for sim_name, sim_func in simulations.items():\n",
    "\n",
    "    # Generate test observations\n",
    "    y_test = sim_func(x_test)\n",
    "    y_test_dict[sim_name] = y_test  # Store test outputs\n",
    "\n",
    "    # Print details\n",
    "    print(f\"=== {sim_name.upper()} ===\")\n",
    "    print(f\"Test inputs shape: {x_test.shape}\")\n",
    "    print(f\"Test observations shape: {y_test.shape}\")\n",
    "    print(f\"Test inputs dtype: {x_test.dtype}\")\n",
    "    print()\n",
    "\n",
    "    # visualise_v_quiver(y_test, x_test, title_string = name)\n",
    "\n",
    "#####################\n",
    "### Training loop ###\n",
    "#####################\n",
    "\n",
    "# Early stopping parameters\n",
    "PATIENCE = PATIENCE\n",
    "MAX_NUM_EPOCHS = MAX_NUM_EPOCHS\n",
    "\n",
    "# Number of training runs for mean and std of metrics\n",
    "NUM_RUNS = NUM_RUNS\n",
    "LEARNING_RATE = LEARNING_RATE\n",
    "WEIGHT_DECAY = WEIGHT_DECAY\n",
    "\n",
    "# Pass in all the training data\n",
    "# BATCH_SIZE = BATCH_SIZE\n",
    "\n",
    "# Ensure the results folder exists\n",
    "RESULTS_DIR = DFGP_RESULTS_DIR\n",
    "os.makedirs(RESULTS_DIR, exist_ok = True)\n",
    "\n",
    "### LOOP OVER SIMULATIONS ###\n",
    "for sim_name, sim_func in simulations.items():\n",
    "    print(f\"\\nTraining for {sim_name.upper()}...\")\n",
    "\n",
    "    # Store metrics for the current simulation\n",
    "    simulation_results = []\n",
    "\n",
    "    # x_train is the same, select y_train\n",
    "    y_train = y_train_dict[sim_name]\n",
    "    # select the correct y_test (PREVIOUS ERROR)\n",
    "    y_test = y_test_dict[sim_name]\n",
    "\n",
    "    ### LOOP OVER RUNS ###\n",
    "    for run in range(NUM_RUNS):\n",
    "        print(f\"\\n--- Training Run {run + 1}/{NUM_RUNS} ---\")\n",
    "\n",
    "        # Convert to DataLoader for batching\n",
    "        dataset = TensorDataset(x_train, y_train)\n",
    "        # Pass in full training dataset as batch\n",
    "        dataloader = DataLoader(dataset, batch_size = x_train.shape[0], shuffle = True)\n",
    "\n",
    "        # Initialise fresh model\n",
    "        # we seeded so this is reproducible\n",
    "        dfNN_model = dfNN_for_vmap().to(device)\n",
    "\n",
    "        # Define loss function (e.g., MSE for regression)\n",
    "        criterion = torch.nn.MSELoss()\n",
    "\n",
    "        # Define optimizer (e.g., AdamW)\n",
    "        optimizer = optim.AdamW(dfNN_model.parameters(), lr = LEARNING_RATE, weight_decay = WEIGHT_DECAY)\n",
    "\n",
    "        # Initialise tensors to store losses for this run\n",
    "        epoch_train_losses = torch.zeros(MAX_NUM_EPOCHS)\n",
    "        epoch_test_losses = torch.zeros(MAX_NUM_EPOCHS)\n",
    "\n",
    "        # Early stopping variables\n",
    "        best_loss = float('inf')\n",
    "        epochs_no_improve = 0\n",
    "\n",
    "        ### LOOP OVER EPOCHS ###\n",
    "        print(\"\\nStart Training\")\n",
    "        for epoch in range(MAX_NUM_EPOCHS):\n",
    "\n",
    "            epoch_train_loss = 0.0  # Accumulate batch losses within epoch\n",
    "            epoch_test_loss = 0.0\n",
    "\n",
    "            for batch in dataloader:\n",
    "                # assure model is in training mode \n",
    "                dfNN_model.train()\n",
    "\n",
    "                x_batch, y_batch = batch\n",
    "                # put on GPU\n",
    "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "                x_batch.requires_grad_()\n",
    "\n",
    "                # Forward pass\n",
    "                y_pred = vmap(dfNN_model)(x_batch)\n",
    "\n",
    "                # Compute loss (RMSE for same units as data) - criterion(pred, target)\n",
    "                loss = torch.sqrt(criterion(y_pred, y_batch))\n",
    "                epoch_train_loss += loss.item()\n",
    "\n",
    "                # Backpropagation\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            ### END BATCH LOOP ###\n",
    "            dfNN_model.eval()\n",
    "            # Compute test loss for loss convergence plot\n",
    "            y_test_pred = vmap(dfNN_model)(x_test.to(device))\n",
    "            test_loss = torch.sqrt(criterion(y_test_pred, y_test.to(device))).item()\n",
    "            \n",
    "            # Compute average loss for the epoch (e.g. 7 batches/epoch)\n",
    "            avg_train_loss = epoch_train_loss / len(dataloader)\n",
    "\n",
    "            epoch_train_losses[epoch] = avg_train_loss\n",
    "            epoch_test_losses[epoch] = test_loss\n",
    "\n",
    "            print(f\"{sim_name} {model_name} Run {run + 1}/{NUM_RUNS}, Epoch {epoch + 1}/{MAX_NUM_EPOCHS}, Training Loss (RMSE): {avg_train_loss:.4f}\")\n",
    "\n",
    "            # debug\n",
    "            # Early stopping check\n",
    "            if avg_train_loss < best_loss:\n",
    "                best_loss = avg_train_loss\n",
    "                epochs_no_improve = 0  # Reset counter\n",
    "                best_model_state = dfNN_model.state_dict()  # Save best model\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "\n",
    "            if epochs_no_improve >= PATIENCE:\n",
    "                print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
    "                break\n",
    "        \n",
    "        ### END EPOCH LOOP ###\n",
    "        # Load the best model before stopping for this \"run\"\n",
    "        dfNN_model.load_state_dict(best_model_state)\n",
    "        print(f\"Run {run + 1}/{NUM_RUNS}, Training of {model_name} complete for {sim_name.upper()}. Restored best model.\")\n",
    "\n",
    "        ################\n",
    "        ### EVALUATE ###\n",
    "        ################\n",
    "\n",
    "        # Evaluate the trained model after epochs are finished\n",
    "        dfNN_model.eval()\n",
    "\n",
    "        y_train_dfNN_predicted = vmap(dfNN_model)(x_train.to(device)).detach()\n",
    "        y_test_dfNN_predicted = vmap(dfNN_model)(x_test.to(device)).detach()\n",
    "\n",
    "        # Only save things for one run\n",
    "        if run == 0:\n",
    "            #(1) Save predictions from first run so we can visualise them later\n",
    "            torch.save(y_test_dfNN_predicted, f\"{RESULTS_DIR}/{sim_name}_{model_name}_test_predictions.pt\")\n",
    "\n",
    "            #(2) Save loss over epochs\n",
    "            df_losses = pd.DataFrame({\n",
    "                'Epoch': list(range(epoch_train_losses.shape[0])), # pythonic\n",
    "                'Train Loss RMSE': epoch_train_losses.tolist(), \n",
    "                'Test Loss RMSE': epoch_test_losses.tolist()\n",
    "                })\n",
    "            \n",
    "            df_losses.to_csv(f\"{RESULTS_DIR}/{sim_name}_{model_name}_losses_over_epochs.csv\", index = False)\n",
    "\n",
    "        # Compute Divergence (convert tensor to float)\n",
    "        dfNN_train_div = torch.diagonal(vmap(jacfwd(dfNN_model))(x_train.to(device)), dim1 = -2, dim2 = -1).detach().sum().item()\n",
    "        dfNN_test_div = torch.diagonal(vmap(jacfwd(dfNN_model))(x_test.to(device)), dim1 = -2, dim2 = -1).detach().sum().item()\n",
    "\n",
    "        # Compute metrics (convert tensors to float)\n",
    "        dfNN_train_RMSE = compute_RMSE(y_train, y_train_dfNN_predicted.cpu()).item()\n",
    "        dfNN_train_MAE = compute_MAE(y_train, y_train_dfNN_predicted.cpu()).item()\n",
    "\n",
    "        dfNN_test_RMSE = compute_RMSE(y_test, y_test_dfNN_predicted.cpu()).item()\n",
    "        dfNN_test_MAE = compute_MAE(y_test, y_test_dfNN_predicted.cpu()).item()\n",
    "\n",
    "        # Store results in list\n",
    "        simulation_results.append([\n",
    "            run + 1, dfNN_train_RMSE, dfNN_train_MAE, dfNN_train_div,\n",
    "            dfNN_test_RMSE, dfNN_test_MAE, dfNN_test_div\n",
    "        ])\n",
    "\n",
    "    ### FINISH LOOP OVER RUNS ###\n",
    "    # Convert results to a Pandas DataFrame\n",
    "    df = pd.DataFrame(\n",
    "        simulation_results, \n",
    "        columns = [\"Run\", \"Train RMSE\", \"Train MAE\", \"Train Divergence\",\n",
    "                   \"Test RMSE\", \"Test MAE\", \"Test Divergence\"])\n",
    "\n",
    "    # Compute mean and standard deviation for each metric\n",
    "    mean_std_df = df.iloc[:, 1:].agg([\"mean\", \"std\"])  # Exclude \"Run\" column\n",
    "\n",
    "    # Save results to CSV\n",
    "    results_file = os.path.join(RESULTS_DIR, f\"{sim_name}_{model_name}_metrics_per_run.csv\")\n",
    "    df.to_csv(results_file, index = False)\n",
    "    print(f\"\\nResults saved to {results_file}\")\n",
    "\n",
    "    # Save mean and standard deviation to CSV\n",
    "    mean_std_file = os.path.join(RESULTS_DIR, f\"{sim_name}_{model_name}_metrics_summary.csv\")\n",
    "    mean_std_df.to_csv(mean_std_file)\n",
    "    print(f\"\\nMean & Std saved to {mean_std_file}\")\n",
    "    # Only train for one simulation for now"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
