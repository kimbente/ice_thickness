{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9e74d1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "=== CONVERGENCE_DTL ===\n",
      "Training inputs shape: torch.Size([196, 2])\n",
      "Training observations shape: torch.Size([196, 2])\n",
      "Training inputs dtype: torch.float32\n",
      "\n",
      "=== Generating test data ===\n",
      "=== CONVERGENCE_DTL ===\n",
      "Test inputs shape: torch.Size([400, 2])\n",
      "Test observations shape: torch.Size([400, 2])\n",
      "Test inputs dtype: torch.float32\n",
      "\n",
      "\n",
      "Training for CONVERGENCE_DTL...\n",
      "\n",
      "--- Training Run 1/1 ---\n",
      "\n",
      "Start Training\n",
      "convergence_dtl PINN Run 1/1, Epoch 1/2000, Training Loss (RMSE): 0.6746\n",
      "convergence_dtl PINN Run 1/1, Epoch 2/2000, Training Loss (RMSE): 0.4724\n",
      "convergence_dtl PINN Run 1/1, Epoch 3/2000, Training Loss (RMSE): 0.5190\n",
      "convergence_dtl PINN Run 1/1, Epoch 4/2000, Training Loss (RMSE): 0.4592\n",
      "convergence_dtl PINN Run 1/1, Epoch 5/2000, Training Loss (RMSE): 0.4411\n",
      "convergence_dtl PINN Run 1/1, Epoch 6/2000, Training Loss (RMSE): 0.3808\n",
      "convergence_dtl PINN Run 1/1, Epoch 7/2000, Training Loss (RMSE): 0.4188\n",
      "convergence_dtl PINN Run 1/1, Epoch 8/2000, Training Loss (RMSE): 0.3958\n",
      "convergence_dtl PINN Run 1/1, Epoch 9/2000, Training Loss (RMSE): 0.3789\n",
      "convergence_dtl PINN Run 1/1, Epoch 10/2000, Training Loss (RMSE): 0.3569\n",
      "convergence_dtl PINN Run 1/1, Epoch 11/2000, Training Loss (RMSE): 0.3735\n",
      "convergence_dtl PINN Run 1/1, Epoch 12/2000, Training Loss (RMSE): 0.3413\n",
      "convergence_dtl PINN Run 1/1, Epoch 13/2000, Training Loss (RMSE): 0.3234\n",
      "convergence_dtl PINN Run 1/1, Epoch 14/2000, Training Loss (RMSE): 0.3349\n",
      "convergence_dtl PINN Run 1/1, Epoch 15/2000, Training Loss (RMSE): 0.3109\n",
      "convergence_dtl PINN Run 1/1, Epoch 16/2000, Training Loss (RMSE): 0.2972\n",
      "convergence_dtl PINN Run 1/1, Epoch 17/2000, Training Loss (RMSE): 0.3111\n",
      "convergence_dtl PINN Run 1/1, Epoch 18/2000, Training Loss (RMSE): 0.3016\n",
      "convergence_dtl PINN Run 1/1, Epoch 19/2000, Training Loss (RMSE): 0.3276\n",
      "convergence_dtl PINN Run 1/1, Epoch 20/2000, Training Loss (RMSE): 0.3295\n",
      "convergence_dtl PINN Run 1/1, Epoch 21/2000, Training Loss (RMSE): 0.3029\n",
      "convergence_dtl PINN Run 1/1, Epoch 22/2000, Training Loss (RMSE): 0.3438\n",
      "convergence_dtl PINN Run 1/1, Epoch 23/2000, Training Loss (RMSE): 0.2979\n",
      "convergence_dtl PINN Run 1/1, Epoch 24/2000, Training Loss (RMSE): 0.3004\n",
      "convergence_dtl PINN Run 1/1, Epoch 25/2000, Training Loss (RMSE): 0.2955\n",
      "convergence_dtl PINN Run 1/1, Epoch 26/2000, Training Loss (RMSE): 0.3084\n",
      "convergence_dtl PINN Run 1/1, Epoch 27/2000, Training Loss (RMSE): 0.2924\n",
      "convergence_dtl PINN Run 1/1, Epoch 28/2000, Training Loss (RMSE): 0.2688\n",
      "convergence_dtl PINN Run 1/1, Epoch 29/2000, Training Loss (RMSE): 0.2480\n",
      "convergence_dtl PINN Run 1/1, Epoch 30/2000, Training Loss (RMSE): 0.2837\n",
      "convergence_dtl PINN Run 1/1, Epoch 31/2000, Training Loss (RMSE): 0.2490\n",
      "convergence_dtl PINN Run 1/1, Epoch 32/2000, Training Loss (RMSE): 0.2340\n",
      "convergence_dtl PINN Run 1/1, Epoch 33/2000, Training Loss (RMSE): 0.2483\n",
      "convergence_dtl PINN Run 1/1, Epoch 34/2000, Training Loss (RMSE): 0.2277\n",
      "convergence_dtl PINN Run 1/1, Epoch 35/2000, Training Loss (RMSE): 0.2488\n",
      "convergence_dtl PINN Run 1/1, Epoch 36/2000, Training Loss (RMSE): 0.2529\n",
      "convergence_dtl PINN Run 1/1, Epoch 37/2000, Training Loss (RMSE): 0.2505\n",
      "convergence_dtl PINN Run 1/1, Epoch 38/2000, Training Loss (RMSE): 0.2645\n",
      "convergence_dtl PINN Run 1/1, Epoch 39/2000, Training Loss (RMSE): 0.2212\n",
      "convergence_dtl PINN Run 1/1, Epoch 40/2000, Training Loss (RMSE): 0.2330\n",
      "convergence_dtl PINN Run 1/1, Epoch 41/2000, Training Loss (RMSE): 0.2703\n",
      "convergence_dtl PINN Run 1/1, Epoch 42/2000, Training Loss (RMSE): 0.2612\n",
      "convergence_dtl PINN Run 1/1, Epoch 43/2000, Training Loss (RMSE): 0.3396\n",
      "convergence_dtl PINN Run 1/1, Epoch 44/2000, Training Loss (RMSE): 0.2760\n",
      "convergence_dtl PINN Run 1/1, Epoch 45/2000, Training Loss (RMSE): 0.2869\n",
      "convergence_dtl PINN Run 1/1, Epoch 46/2000, Training Loss (RMSE): 0.2890\n",
      "convergence_dtl PINN Run 1/1, Epoch 47/2000, Training Loss (RMSE): 0.2487\n",
      "convergence_dtl PINN Run 1/1, Epoch 48/2000, Training Loss (RMSE): 0.2755\n",
      "convergence_dtl PINN Run 1/1, Epoch 49/2000, Training Loss (RMSE): 0.3629\n",
      "convergence_dtl PINN Run 1/1, Epoch 50/2000, Training Loss (RMSE): 0.2536\n",
      "convergence_dtl PINN Run 1/1, Epoch 51/2000, Training Loss (RMSE): 0.2330\n",
      "convergence_dtl PINN Run 1/1, Epoch 52/2000, Training Loss (RMSE): 0.2365\n",
      "convergence_dtl PINN Run 1/1, Epoch 53/2000, Training Loss (RMSE): 0.2438\n",
      "convergence_dtl PINN Run 1/1, Epoch 54/2000, Training Loss (RMSE): 0.2619\n",
      "convergence_dtl PINN Run 1/1, Epoch 55/2000, Training Loss (RMSE): 0.2349\n",
      "convergence_dtl PINN Run 1/1, Epoch 56/2000, Training Loss (RMSE): 0.2964\n",
      "convergence_dtl PINN Run 1/1, Epoch 57/2000, Training Loss (RMSE): 0.2255\n",
      "convergence_dtl PINN Run 1/1, Epoch 58/2000, Training Loss (RMSE): 0.2172\n",
      "convergence_dtl PINN Run 1/1, Epoch 59/2000, Training Loss (RMSE): 0.2133\n",
      "convergence_dtl PINN Run 1/1, Epoch 60/2000, Training Loss (RMSE): 0.2229\n",
      "convergence_dtl PINN Run 1/1, Epoch 61/2000, Training Loss (RMSE): 0.2241\n",
      "convergence_dtl PINN Run 1/1, Epoch 62/2000, Training Loss (RMSE): 0.2395\n",
      "convergence_dtl PINN Run 1/1, Epoch 63/2000, Training Loss (RMSE): 0.2348\n",
      "convergence_dtl PINN Run 1/1, Epoch 64/2000, Training Loss (RMSE): 0.2285\n",
      "convergence_dtl PINN Run 1/1, Epoch 65/2000, Training Loss (RMSE): 0.2212\n",
      "convergence_dtl PINN Run 1/1, Epoch 66/2000, Training Loss (RMSE): 0.2317\n",
      "convergence_dtl PINN Run 1/1, Epoch 67/2000, Training Loss (RMSE): 0.2341\n",
      "convergence_dtl PINN Run 1/1, Epoch 68/2000, Training Loss (RMSE): 0.2370\n",
      "convergence_dtl PINN Run 1/1, Epoch 69/2000, Training Loss (RMSE): 0.3360\n",
      "convergence_dtl PINN Run 1/1, Epoch 70/2000, Training Loss (RMSE): 0.2640\n",
      "convergence_dtl PINN Run 1/1, Epoch 71/2000, Training Loss (RMSE): 0.2624\n",
      "convergence_dtl PINN Run 1/1, Epoch 72/2000, Training Loss (RMSE): 0.2450\n",
      "convergence_dtl PINN Run 1/1, Epoch 73/2000, Training Loss (RMSE): 0.2367\n",
      "convergence_dtl PINN Run 1/1, Epoch 74/2000, Training Loss (RMSE): 0.2290\n",
      "convergence_dtl PINN Run 1/1, Epoch 75/2000, Training Loss (RMSE): 0.2215\n",
      "convergence_dtl PINN Run 1/1, Epoch 76/2000, Training Loss (RMSE): 0.2397\n",
      "convergence_dtl PINN Run 1/1, Epoch 77/2000, Training Loss (RMSE): 0.2278\n",
      "convergence_dtl PINN Run 1/1, Epoch 78/2000, Training Loss (RMSE): 0.2362\n",
      "convergence_dtl PINN Run 1/1, Epoch 79/2000, Training Loss (RMSE): 0.2111\n",
      "convergence_dtl PINN Run 1/1, Epoch 80/2000, Training Loss (RMSE): 0.2109\n",
      "convergence_dtl PINN Run 1/1, Epoch 81/2000, Training Loss (RMSE): 0.2216\n",
      "convergence_dtl PINN Run 1/1, Epoch 82/2000, Training Loss (RMSE): 0.2242\n",
      "convergence_dtl PINN Run 1/1, Epoch 83/2000, Training Loss (RMSE): 0.2326\n",
      "convergence_dtl PINN Run 1/1, Epoch 84/2000, Training Loss (RMSE): 0.2259\n",
      "convergence_dtl PINN Run 1/1, Epoch 85/2000, Training Loss (RMSE): 0.2335\n",
      "convergence_dtl PINN Run 1/1, Epoch 86/2000, Training Loss (RMSE): 0.2591\n",
      "convergence_dtl PINN Run 1/1, Epoch 87/2000, Training Loss (RMSE): 0.2259\n",
      "convergence_dtl PINN Run 1/1, Epoch 88/2000, Training Loss (RMSE): 0.2433\n",
      "convergence_dtl PINN Run 1/1, Epoch 89/2000, Training Loss (RMSE): 0.2184\n",
      "convergence_dtl PINN Run 1/1, Epoch 90/2000, Training Loss (RMSE): 0.2185\n",
      "convergence_dtl PINN Run 1/1, Epoch 91/2000, Training Loss (RMSE): 0.2209\n",
      "convergence_dtl PINN Run 1/1, Epoch 92/2000, Training Loss (RMSE): 0.2140\n",
      "convergence_dtl PINN Run 1/1, Epoch 93/2000, Training Loss (RMSE): 0.2202\n",
      "convergence_dtl PINN Run 1/1, Epoch 94/2000, Training Loss (RMSE): 0.2031\n",
      "convergence_dtl PINN Run 1/1, Epoch 95/2000, Training Loss (RMSE): 0.2224\n",
      "convergence_dtl PINN Run 1/1, Epoch 96/2000, Training Loss (RMSE): 0.2339\n",
      "convergence_dtl PINN Run 1/1, Epoch 97/2000, Training Loss (RMSE): 0.2765\n",
      "convergence_dtl PINN Run 1/1, Epoch 98/2000, Training Loss (RMSE): 0.2193\n",
      "convergence_dtl PINN Run 1/1, Epoch 99/2000, Training Loss (RMSE): 0.2006\n",
      "convergence_dtl PINN Run 1/1, Epoch 100/2000, Training Loss (RMSE): 0.2377\n",
      "convergence_dtl PINN Run 1/1, Epoch 101/2000, Training Loss (RMSE): 0.2174\n",
      "convergence_dtl PINN Run 1/1, Epoch 102/2000, Training Loss (RMSE): 0.2151\n",
      "convergence_dtl PINN Run 1/1, Epoch 103/2000, Training Loss (RMSE): 0.2381\n",
      "convergence_dtl PINN Run 1/1, Epoch 104/2000, Training Loss (RMSE): 0.2487\n",
      "convergence_dtl PINN Run 1/1, Epoch 105/2000, Training Loss (RMSE): 0.2338\n",
      "convergence_dtl PINN Run 1/1, Epoch 106/2000, Training Loss (RMSE): 0.2061\n",
      "convergence_dtl PINN Run 1/1, Epoch 107/2000, Training Loss (RMSE): 0.2087\n",
      "convergence_dtl PINN Run 1/1, Epoch 108/2000, Training Loss (RMSE): 0.2123\n",
      "convergence_dtl PINN Run 1/1, Epoch 109/2000, Training Loss (RMSE): 0.2091\n",
      "convergence_dtl PINN Run 1/1, Epoch 110/2000, Training Loss (RMSE): 0.2151\n",
      "convergence_dtl PINN Run 1/1, Epoch 111/2000, Training Loss (RMSE): 0.2048\n",
      "convergence_dtl PINN Run 1/1, Epoch 112/2000, Training Loss (RMSE): 0.2168\n",
      "convergence_dtl PINN Run 1/1, Epoch 113/2000, Training Loss (RMSE): 0.2075\n",
      "convergence_dtl PINN Run 1/1, Epoch 114/2000, Training Loss (RMSE): 0.2249\n",
      "convergence_dtl PINN Run 1/1, Epoch 115/2000, Training Loss (RMSE): 0.2228\n",
      "convergence_dtl PINN Run 1/1, Epoch 116/2000, Training Loss (RMSE): 0.2068\n",
      "convergence_dtl PINN Run 1/1, Epoch 117/2000, Training Loss (RMSE): 0.2128\n",
      "convergence_dtl PINN Run 1/1, Epoch 118/2000, Training Loss (RMSE): 0.2050\n",
      "convergence_dtl PINN Run 1/1, Epoch 119/2000, Training Loss (RMSE): 0.2114\n",
      "convergence_dtl PINN Run 1/1, Epoch 120/2000, Training Loss (RMSE): 0.2125\n",
      "convergence_dtl PINN Run 1/1, Epoch 121/2000, Training Loss (RMSE): 0.2233\n",
      "convergence_dtl PINN Run 1/1, Epoch 122/2000, Training Loss (RMSE): 0.2081\n",
      "convergence_dtl PINN Run 1/1, Epoch 123/2000, Training Loss (RMSE): 0.2188\n",
      "convergence_dtl PINN Run 1/1, Epoch 124/2000, Training Loss (RMSE): 0.2328\n",
      "convergence_dtl PINN Run 1/1, Epoch 125/2000, Training Loss (RMSE): 0.2150\n",
      "convergence_dtl PINN Run 1/1, Epoch 126/2000, Training Loss (RMSE): 0.2099\n",
      "convergence_dtl PINN Run 1/1, Epoch 127/2000, Training Loss (RMSE): 0.2018\n",
      "convergence_dtl PINN Run 1/1, Epoch 128/2000, Training Loss (RMSE): 0.2092\n",
      "convergence_dtl PINN Run 1/1, Epoch 129/2000, Training Loss (RMSE): 0.2081\n",
      "convergence_dtl PINN Run 1/1, Epoch 130/2000, Training Loss (RMSE): 0.2138\n",
      "convergence_dtl PINN Run 1/1, Epoch 131/2000, Training Loss (RMSE): 0.2044\n",
      "convergence_dtl PINN Run 1/1, Epoch 132/2000, Training Loss (RMSE): 0.2083\n",
      "convergence_dtl PINN Run 1/1, Epoch 133/2000, Training Loss (RMSE): 0.2086\n",
      "convergence_dtl PINN Run 1/1, Epoch 134/2000, Training Loss (RMSE): 0.2165\n",
      "convergence_dtl PINN Run 1/1, Epoch 135/2000, Training Loss (RMSE): 0.2193\n",
      "convergence_dtl PINN Run 1/1, Epoch 136/2000, Training Loss (RMSE): 0.2245\n",
      "convergence_dtl PINN Run 1/1, Epoch 137/2000, Training Loss (RMSE): 0.2224\n",
      "convergence_dtl PINN Run 1/1, Epoch 138/2000, Training Loss (RMSE): 0.2144\n",
      "convergence_dtl PINN Run 1/1, Epoch 139/2000, Training Loss (RMSE): 0.2316\n",
      "convergence_dtl PINN Run 1/1, Epoch 140/2000, Training Loss (RMSE): 0.2327\n",
      "convergence_dtl PINN Run 1/1, Epoch 141/2000, Training Loss (RMSE): 0.2086\n",
      "convergence_dtl PINN Run 1/1, Epoch 142/2000, Training Loss (RMSE): 0.2136\n",
      "convergence_dtl PINN Run 1/1, Epoch 143/2000, Training Loss (RMSE): 0.2119\n",
      "convergence_dtl PINN Run 1/1, Epoch 144/2000, Training Loss (RMSE): 0.2185\n",
      "convergence_dtl PINN Run 1/1, Epoch 145/2000, Training Loss (RMSE): 0.2130\n",
      "convergence_dtl PINN Run 1/1, Epoch 146/2000, Training Loss (RMSE): 0.2011\n",
      "convergence_dtl PINN Run 1/1, Epoch 147/2000, Training Loss (RMSE): 0.2152\n",
      "convergence_dtl PINN Run 1/1, Epoch 148/2000, Training Loss (RMSE): 0.2067\n",
      "convergence_dtl PINN Run 1/1, Epoch 149/2000, Training Loss (RMSE): 0.2071\n",
      "Early stopping triggered after 149 epochs.\n",
      "Run 1/1, Training of PINN complete for CONVERGENCE_DTL. Restored best model.\n"
     ]
    }
   ],
   "source": [
    "from NN_models import PINN_backbone\n",
    "# from simulate import simulate_convergence, simulate_branching, simulate_ridge, simulate_merge, simulate_deflection\n",
    "from metrics import compute_RMSE, compute_MAE\n",
    "from utils import set_seed\n",
    "\n",
    "# Global file for training configs\n",
    "from configs import PATIENCE, MAX_NUM_EPOCHS, NUM_RUNS, LEARNING_RATE, WEIGHT_DECAY, BATCH_SIZE, N_SIDE, PINN_RESULTS_DIR, W_PINN_DIV_WEIGHT, PINN_LEARNING_RATE\n",
    "\n",
    "import torch\n",
    "from torch.func import vmap, jacfwd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "### TIMING ###\n",
    "import time\n",
    "start_time = time.time()  # Start timing after imports\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "model_name = \"PINN\"\n",
    "\n",
    "#########################\n",
    "### x_train & y_train ###\n",
    "#########################\n",
    "\n",
    "# Import all simulation functions\n",
    "from simulate import (\n",
    "    simulate_detailed_convergence,\n",
    "    simulate_detailed_deflection,\n",
    "    simulate_detailed_curve,\n",
    "    simulate_detailed_ridges,\n",
    "    simulate_detailed_branching,\n",
    ")\n",
    "\n",
    "# Define simulations as a dictionary with names as keys to function objects\n",
    "simulations = {\n",
    "    \"convergence_dtl\": simulate_detailed_convergence,\n",
    "    #\"deflection_dtl\": simulate_detailed_deflection,\n",
    "    #\"curve_dtl\": simulate_detailed_curve,\n",
    "    #\"ridges_dtl\": simulate_detailed_ridges,\n",
    "    #\"branching_dtl\": simulate_detailed_branching,\n",
    "}\n",
    "\n",
    "# Load training inputs, not weights_only = True\n",
    "x_train = torch.load(\"data/sim_data/x_train_lines_discretised_0to1.pt\", weights_only = False).float()\n",
    "\n",
    "# Storage dictionaries\n",
    "y_train_dict = {}\n",
    "\n",
    "# Make y_train_dict: Iterate over all simulation functions\n",
    "for sim_name, sim_func in simulations.items():\n",
    "\n",
    "    # Generate training observations\n",
    "    y_train = sim_func(x_train)\n",
    "    y_train_dict[sim_name] = y_train  # Store training outputs\n",
    "\n",
    "    # Print details\n",
    "    print(f\"=== {sim_name.upper()} ===\")\n",
    "    print(f\"Training inputs shape: {x_train.shape}\")\n",
    "    print(f\"Training observations shape: {y_train.shape}\")\n",
    "    print(f\"Training inputs dtype: {x_train.dtype}\")\n",
    "    print()\n",
    "\n",
    "#######################\n",
    "### x_test & y_test ###\n",
    "#######################\n",
    "\n",
    "print(\"=== Generating test data ===\")\n",
    "\n",
    "# Choose discretisation that is good for simulations and also for quiver plotting\n",
    "N_SIDE = N_SIDE\n",
    "\n",
    "side_array = torch.linspace(start = 0.0, end = 1.0, steps = N_SIDE)\n",
    "XX, YY = torch.meshgrid(side_array, side_array, indexing = \"xy\")\n",
    "x_test_grid = torch.cat([XX.unsqueeze(-1), YY.unsqueeze(-1)], dim = -1)\n",
    "# long format\n",
    "x_test = x_test_grid.reshape(-1, 2)\n",
    "\n",
    "# Storage dictionaries\n",
    "y_test_dict = {}\n",
    "\n",
    "# Make y_test_dict: Iterate over all simulation functions\n",
    "for sim_name, sim_func in simulations.items():\n",
    "\n",
    "    # Generate test observations\n",
    "    y_test = sim_func(x_test)\n",
    "    y_test_dict[sim_name] = y_test  # Store test outputs\n",
    "\n",
    "    # Print details\n",
    "    print(f\"=== {sim_name.upper()} ===\")\n",
    "    print(f\"Test inputs shape: {x_test.shape}\")\n",
    "    print(f\"Test observations shape: {y_test.shape}\")\n",
    "    print(f\"Test inputs dtype: {x_test.dtype}\")\n",
    "    print()\n",
    "\n",
    "#####################\n",
    "### Training loop ###\n",
    "#####################\n",
    "\n",
    "# Early stopping parameters\n",
    "PATIENCE = PATIENCE  # Stop after 50 epochs with no improvement\n",
    "\n",
    "MAX_NUM_EPOCHS = MAX_NUM_EPOCHS # 2000\n",
    "\n",
    "# Number of training runs for mean and std of metrics\n",
    "NUM_RUNS = 1 # 10\n",
    "\n",
    "# higher lr for PINN\n",
    "LEARNING_RATE = PINN_LEARNING_RATE\n",
    "WEIGHT_DECAY = WEIGHT_DECAY\n",
    "\n",
    "BATCH_SIZE = BATCH_SIZE\n",
    "\n",
    "w = W_PINN_DIV_WEIGHT\n",
    "\n",
    "# Ensure the results folder exists\n",
    "RESULTS_DIR = PINN_RESULTS_DIR # Change this to \"results\" for full training\n",
    "\n",
    "os.makedirs(RESULTS_DIR, exist_ok = True)\n",
    "\n",
    "### LOOP OVER SIMULATIONS ###\n",
    "for sim_name, sim_func in simulations.items():\n",
    "    print(f\"\\nTraining for {sim_name.upper()}...\")\n",
    "\n",
    "    # Store metrics for the current simulation\n",
    "    simulation_results = []\n",
    "\n",
    "    # x_train, x_test stays the same but select y_train\n",
    "    y_train = y_train_dict[sim_name]\n",
    "    # select the correct y_test (PREVIOUS ERROR)\n",
    "    y_test = y_test_dict[sim_name]\n",
    "\n",
    "    ### LOOP OVER RUNS ###\n",
    "    for run in range(NUM_RUNS):\n",
    "        print(f\"\\n--- Training Run {run + 1}/{NUM_RUNS} ---\")\n",
    "\n",
    "        # Convert to DataLoader for batching\n",
    "        dataset = TensorDataset(x_train, y_train)\n",
    "        dataloader = DataLoader(dataset, batch_size = BATCH_SIZE, shuffle = True)\n",
    "\n",
    "        # Initialise fresh model\n",
    "        # we seeded so this is reproducible\n",
    "        PINN_model = PINN_backbone().to(device)\n",
    "        PINN_model.train()\n",
    "\n",
    "        # Define loss function (e.g., MSE for regression)\n",
    "        criterion = torch.nn.MSELoss()\n",
    "\n",
    "        # Define optimizer (e.g., AdamW)\n",
    "        optimizer = optim.AdamW(PINN_model.parameters(), lr = LEARNING_RATE, weight_decay = WEIGHT_DECAY)\n",
    "\n",
    "        # Initialise tensor to store losses\n",
    "        epoch_train_losses = torch.zeros(MAX_NUM_EPOCHS)\n",
    "        epoch_train_rmse_losses = torch.zeros(MAX_NUM_EPOCHS)\n",
    "        epoch_test_losses = torch.zeros(MAX_NUM_EPOCHS)\n",
    "        epoch_test_rmse_losses = torch.zeros(MAX_NUM_EPOCHS)\n",
    "\n",
    "        # Early stopping variables\n",
    "        best_loss = float('inf')\n",
    "        epochs_no_improve = 0\n",
    "\n",
    "        ### LOOP OVER EPOCHS ###\n",
    "        print(\"\\nStart Training\")\n",
    "        for epoch in range(MAX_NUM_EPOCHS):\n",
    "\n",
    "            epoch_train_loss = 0.0  # Accumulate batch losses\n",
    "            epoch_train_rmse_loss = 0.0\n",
    "\n",
    "            for batch in dataloader:\n",
    "                PINN_model.train()\n",
    "\n",
    "                x_batch, y_batch = batch\n",
    "                # put on GPU\n",
    "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "                # inplace\n",
    "                x_batch.requires_grad_()\n",
    "\n",
    "                # Forward pass\n",
    "                y_pred = vmap(PINN_model)(x_batch)\n",
    "                # torch.Size([32 (batch_dim), 2 (out_dim), 2 (in_dim)])\n",
    "                batch_divergence = vmap(jacfwd(PINN_model))(x_batch)\n",
    "                # sum: f1/x1 + f2/x2, square to account for negative\n",
    "                batch_divergence_loss = torch.square(torch.diagonal(batch_divergence, dim1 = -2, dim2 = -1).sum())\n",
    "\n",
    "                # Compute loss (RMSE for same units as data) + divergence loss\n",
    "                loss = (1 - w) * torch.sqrt(criterion(y_pred, y_batch)) + w * batch_divergence_loss\n",
    "                epoch_train_loss += loss.item()\n",
    "                epoch_train_rmse_loss += torch.sqrt(criterion(y_pred, y_batch)).item()\n",
    "\n",
    "                # Backpropagation\n",
    "                # AFTER\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            ### END BATCH LOOP ###\n",
    "            PINN_model.eval()\n",
    "\n",
    "            # Test loss outside of batch loop - once per epoch\n",
    "            y_test_pred = vmap(PINN_model)(x_test.to(device))\n",
    "            # compute just once\n",
    "            epoch_test_rmse_loss = torch.sqrt(criterion(y_test_pred, y_test.to(device))).item()\n",
    "            epoch_test_loss = (1 - w) * torch.sqrt(criterion(y_test_pred, y_test.to(device))) + w * torch.square(torch.diagonal(vmap(jacfwd(PINN_model))(x_test.to(device)), dim1 = -2, dim2 = -1).sum()).item()\n",
    "\n",
    "            # Compute average loss for the epoch and store\n",
    "            epoch_train_losses[epoch] = epoch_train_loss / len(dataloader)\n",
    "            epoch_train_rmse_losses[epoch] = epoch_train_rmse_loss / len(dataloader)\n",
    "            epoch_test_losses[epoch] = epoch_test_loss # old calc once per epoch over all\n",
    "            epoch_test_rmse_losses[epoch] = epoch_test_rmse_loss\n",
    "\n",
    "            print(f\"{sim_name} {model_name} Run {run + 1}/{NUM_RUNS}, Epoch {epoch + 1}/{MAX_NUM_EPOCHS}, Training Loss (RMSE): {epoch_train_losses[epoch]:.4f}\")\n",
    "\n",
    "            # Early stopping check\n",
    "            if epoch_train_losses[epoch] < best_loss:\n",
    "                best_loss = epoch_train_losses[epoch]\n",
    "                epochs_no_improve = 0  # Reset counter\n",
    "                best_model_state = PINN_model.state_dict()  # Save best model\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "\n",
    "            if epochs_no_improve >= PATIENCE:\n",
    "                print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
    "                break\n",
    "\n",
    "        ### END EPOCH LOOP ###\n",
    "        # Load the best model before stopping\n",
    "        PINN_model.load_state_dict(best_model_state)\n",
    "        print(f\"Run {run + 1}/{NUM_RUNS}, Training of {model_name} complete for {sim_name.upper()}. Restored best model.\")\n",
    "\n",
    "        ################\n",
    "        ### EVALUATE ###\n",
    "        ################\n",
    "\n",
    "        # Evaluate the trained model for each run\n",
    "        PINN_model.eval()\n",
    "\n",
    "        y_train_PINN_predicted = vmap(PINN_model)(x_train.to(device)).detach()\n",
    "        y_test_PINN_predicted = vmap(PINN_model)(x_test.to(device)).detach()\n",
    "\n",
    "        # Compute Divergence (convert tensor to float)\n",
    "        PINN_train_div = torch.diagonal(vmap(jacfwd(PINN_model))(x_train.to(device)), dim1 = -2, dim2 = -1).detach().sum().item()\n",
    "        PINN_test_div = torch.diagonal(vmap(jacfwd(PINN_model))(x_test.to(device)), dim1 = -2, dim2 = -1).detach().sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "18c3aec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PINN divergence (train): -0.3034\n"
     ]
    }
   ],
   "source": [
    "print(f\"PINN divergence (train): {PINN_train_div:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4ef0f620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.0124, device='cuda:0')"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.diagonal(vmap(jacfwd(PINN_model))(x_train.to(device)), dim1 = -2, dim2 = -1).detach().sum(dim = -1).abs().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "768de11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_grad = x_train.to(device).requires_grad_()\n",
    "y_train_PINN_predicted = vmap(PINN_model)(x_train_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c37526c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# autograd div train\n",
    "u_indicator_train, v_indicator_train = torch.zeros_like(y_train_PINN_predicted), torch.zeros_like(y_train_PINN_predicted)\n",
    "u_indicator_train[:, 0] = 1.0 # output column u selected\n",
    "v_indicator_train[:, 1] = 1.0 # output column v selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d916a54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "GP_train_div = (torch.autograd.grad(\n",
    "            outputs = y_train_PINN_predicted,\n",
    "            inputs = x_train_grad,\n",
    "            grad_outputs = u_indicator_train,\n",
    "            create_graph = True\n",
    "        )[0][:, 0] + torch.autograd.grad(\n",
    "            outputs = y_train_PINN_predicted,\n",
    "            inputs = x_train_grad,\n",
    "            grad_outputs = v_indicator_train,\n",
    "            create_graph = True\n",
    "        )[0][:, 1]).abs().sum().item() # v with respect to y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9cbee71c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.012362003326416"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GP_train_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c4c3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functional div test\n",
    "jac_autograd_test = torch.autograd.functional.jacobian(apply_GP, \n",
    "                                        x_test.to(device))\n",
    "jac_autograd_test = torch.einsum(\"bobi -> boi\", jac_autograd_test) # batch out batch in\n",
    "GP_test_div = torch.diagonal(jac_autograd_test, dim1 = -2, dim2 = -1).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "71975e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-12.526342391967773\n"
     ]
    }
   ],
   "source": [
    "print(GP_test_div)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4248e66d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.7544e-01,  5.3905e-01],\n",
       "        [-3.6514e-01,  3.5680e-01],\n",
       "        [-1.8622e-01,  1.2825e-01],\n",
       "        [ 4.1965e-02, -1.2784e-01],\n",
       "        [ 2.9171e-01, -3.8863e-01],\n",
       "        [ 5.3086e-01, -6.2973e-01],\n",
       "        [ 7.2783e-01, -8.2798e-01],\n",
       "        [ 8.5671e-01, -9.6408e-01],\n",
       "        [ 9.0161e-01, -1.0248e+00],\n",
       "        [ 8.5936e-01, -1.0044e+00],\n",
       "        [ 7.4000e-01, -9.0492e-01],\n",
       "        [ 5.6497e-01, -7.3582e-01],\n",
       "        [ 3.6322e-01, -5.1233e-01],\n",
       "        [ 1.6616e-01, -2.5351e-01],\n",
       "        [ 2.2838e-03,  2.0085e-02],\n",
       "        [-1.0754e-01,  2.8869e-01],\n",
       "        [-1.5314e-01,  5.3522e-01],\n",
       "        [-1.3611e-01,  7.4656e-01],\n",
       "        [-6.8718e-02,  9.1426e-01],\n",
       "        [ 2.8924e-02,  1.0346e+00],\n",
       "        [-4.9582e-01,  5.4583e-01],\n",
       "        [-4.1540e-01,  3.9960e-01],\n",
       "        [-2.5884e-01,  2.0148e-01],\n",
       "        [-4.2740e-02, -3.2286e-02],\n",
       "        [ 2.0676e-01, -2.8075e-01],\n",
       "        [ 4.5766e-01, -5.2047e-01],\n",
       "        [ 6.7709e-01, -7.2820e-01],\n",
       "        [ 8.3655e-01, -8.8359e-01],\n",
       "        [ 9.1666e-01, -9.7152e-01],\n",
       "        [ 9.1030e-01, -9.8374e-01],\n",
       "        [ 8.2363e-01, -9.1963e-01],\n",
       "        [ 6.7474e-01, -7.8594e-01],\n",
       "        [ 4.9019e-01, -5.9567e-01],\n",
       "        [ 3.0011e-01, -3.6625e-01],\n",
       "        [ 1.3290e-01, -1.1727e-01],\n",
       "        [ 1.0379e-02,  1.3172e-01],\n",
       "        [-5.5529e-02,  3.6328e-01],\n",
       "        [-6.4170e-02,  5.6355e-01],\n",
       "        [-2.5426e-02,  7.2314e-01],\n",
       "        [ 4.2892e-02,  8.3740e-01],\n",
       "        [-4.4196e-01,  4.9165e-01],\n",
       "        [-4.0692e-01,  3.9775e-01],\n",
       "        [-2.9240e-01,  2.4978e-01],\n",
       "        [-1.1107e-01,  6.0247e-02],\n",
       "        [ 1.1402e-01, -1.5319e-01],\n",
       "        [ 3.5277e-01, -3.6959e-01],\n",
       "        [ 5.7273e-01, -5.6715e-01],\n",
       "        [ 7.4435e-01, -7.2575e-01],\n",
       "        [ 8.4591e-01, -8.2932e-01],\n",
       "        [ 8.6695e-01, -8.6770e-01],\n",
       "        [ 8.0990e-01, -8.3763e-01],\n",
       "        [ 6.8909e-01, -7.4287e-01],\n",
       "        [ 5.2782e-01, -5.9345e-01],\n",
       "        [ 3.5387e-01, -4.0416e-01],\n",
       "        [ 1.9427e-01, -1.9255e-01],\n",
       "        [ 7.0620e-02,  2.3199e-02],\n",
       "        [-4.4112e-03,  2.2632e-01],\n",
       "        [-2.8660e-02,  4.0303e-01],\n",
       "        [-1.0015e-02,  5.4362e-01],\n",
       "        [ 3.5877e-02,  6.4290e-01],\n",
       "        [-3.0823e-01,  3.6405e-01],\n",
       "        [-3.3098e-01,  3.3474e-01],\n",
       "        [-2.7555e-01,  2.5322e-01],\n",
       "        [-1.4976e-01,  1.2731e-01],\n",
       "        [ 2.7814e-02, -2.9872e-02],\n",
       "        [ 2.3059e-01, -2.0134e-01],\n",
       "        [ 4.2825e-01, -3.6820e-01],\n",
       "        [ 5.9182e-01, -5.1195e-01],\n",
       "        [ 6.9854e-01, -6.1673e-01],\n",
       "        [ 7.3558e-01, -6.7121e-01],\n",
       "        [ 7.0195e-01, -6.6983e-01],\n",
       "        [ 6.0812e-01, -6.1327e-01],\n",
       "        [ 4.7354e-01, -5.0812e-01],\n",
       "        [ 3.2264e-01, -3.6576e-01],\n",
       "        [ 1.8002e-01, -2.0070e-01],\n",
       "        [ 6.5866e-02, -2.8736e-02],\n",
       "        [-7.4365e-03,  1.3503e-01],\n",
       "        [-3.7035e-02,  2.7775e-01],\n",
       "        [-2.9346e-02,  3.9003e-01],\n",
       "        [ 1.8857e-03,  4.6652e-01],\n",
       "        [-9.7568e-02,  1.5940e-01],\n",
       "        [-1.8585e-01,  2.0118e-01],\n",
       "        [-2.0188e-01,  1.9687e-01],\n",
       "        [-1.4803e-01,  1.4904e-01],\n",
       "        [-3.7373e-02,  6.5312e-02],\n",
       "        [ 1.0835e-01, -4.2445e-02],\n",
       "        [ 2.6252e-01, -1.5960e-01],\n",
       "        [ 3.9828e-01, -2.7058e-01],\n",
       "        [ 4.9322e-01, -3.6090e-01],\n",
       "        [ 5.3317e-01, -4.1898e-01],\n",
       "        [ 5.1434e-01, -4.3752e-01],\n",
       "        [ 4.4348e-01, -4.1431e-01],\n",
       "        [ 3.3588e-01, -3.5230e-01],\n",
       "        [ 2.1197e-01, -2.5896e-01],\n",
       "        [ 9.2945e-02, -1.4506e-01],\n",
       "        [-3.3907e-03, -2.3153e-02],\n",
       "        [-6.5906e-02,  9.4157e-02],\n",
       "        [-9.1830e-02,  1.9572e-01],\n",
       "        [-8.6712e-02,  2.7305e-01],\n",
       "        [-6.2683e-02,  3.2105e-01],\n",
       "        [ 1.7817e-01, -1.1484e-01],\n",
       "        [ 2.2281e-02, -2.3255e-03],\n",
       "        [-7.1370e-02,  7.4281e-02],\n",
       "        [-9.9534e-02,  1.1213e-01],\n",
       "        [-6.9264e-02,  1.1284e-01],\n",
       "        [ 3.5267e-03,  8.2373e-02],\n",
       "        [ 9.7106e-02,  3.0081e-02],\n",
       "        [ 1.8804e-01, -3.2531e-02],\n",
       "        [ 2.5557e-01, -9.3435e-02],\n",
       "        [ 2.8523e-01, -1.4174e-01],\n",
       "        [ 2.7125e-01, -1.6915e-01],\n",
       "        [ 2.1701e-01, -1.7099e-01],\n",
       "        [ 1.3364e-01, -1.4671e-01],\n",
       "        [ 3.7182e-02, -9.9679e-02],\n",
       "        [-5.5145e-02, -3.6553e-02],\n",
       "        [-1.2868e-01,  3.3919e-02],\n",
       "        [-1.7432e-01,  1.0224e-01],\n",
       "        [-1.9006e-01,  1.5954e-01],\n",
       "        [-1.8109e-01,  1.9873e-01],\n",
       "        [-1.5828e-01,  2.1537e-01],\n",
       "        [ 4.9900e-01, -4.3968e-01],\n",
       "        [ 2.7939e-01, -2.6381e-01],\n",
       "        [ 1.0892e-01, -1.1031e-01],\n",
       "        [-3.8624e-03,  1.2838e-02],\n",
       "        [-5.9909e-02,  1.0128e-01],\n",
       "        [-6.8911e-02,  1.5470e-01],\n",
       "        [-4.6971e-02,  1.7660e-01],\n",
       "        [-1.3119e-02,  1.7363e-01],\n",
       "        [ 1.4533e-02,  1.5447e-01],\n",
       "        [ 2.2264e-02,  1.2841e-01],\n",
       "        [ 3.1289e-03,  1.0396e-01],\n",
       "        [-4.2294e-02,  8.7598e-02],\n",
       "        [-1.0677e-01,  8.3015e-02],\n",
       "        [-1.7863e-01,  9.0707e-02],\n",
       "        [-2.4493e-01,  1.0821e-01],\n",
       "        [-2.9458e-01,  1.3075e-01],\n",
       "        [-3.2097e-01,  1.5227e-01],\n",
       "        [-3.2337e-01,  1.6653e-01],\n",
       "        [-3.0701e-01,  1.6819e-01],\n",
       "        [-2.8179e-01,  1.5371e-01],\n",
       "        [ 8.3878e-01, -7.8614e-01],\n",
       "        [ 5.6482e-01, -5.6059e-01],\n",
       "        [ 3.2511e-01, -3.4163e-01],\n",
       "        [ 1.3281e-01, -1.4162e-01],\n",
       "        [-7.3738e-03,  2.9587e-02],\n",
       "        [-9.9046e-02,  1.6559e-01],\n",
       "        [-1.5247e-01,  2.6394e-01],\n",
       "        [-1.8179e-01,  3.2607e-01],\n",
       "        [-2.0177e-01,  3.5670e-01],\n",
       "        [-2.2458e-01,  3.6279e-01],\n",
       "        [-2.5758e-01,  3.5226e-01],\n",
       "        [-3.0229e-01,  3.3269e-01],\n",
       "        [-3.5488e-01,  3.1021e-01],\n",
       "        [-4.0778e-01,  2.8872e-01],\n",
       "        [-4.5220e-01,  2.6961e-01],\n",
       "        [-4.8071e-01,  2.5197e-01],\n",
       "        [-4.8940e-01,  2.3313e-01],\n",
       "        [-4.7902e-01,  2.0959e-01],\n",
       "        [-4.5509e-01,  1.7791e-01],\n",
       "        [-4.2671e-01,  1.3559e-01],\n",
       "        [ 1.1682e+00, -1.1187e+00],\n",
       "        [ 8.5347e-01, -8.6187e-01],\n",
       "        [ 5.5784e-01, -5.9500e-01],\n",
       "        [ 2.9805e-01, -3.3372e-01],\n",
       "        [ 8.3563e-02, -9.2466e-02],\n",
       "        [-8.3744e-02,  1.1698e-01],\n",
       "        [-2.0865e-01,  2.8662e-01],\n",
       "        [-3.0044e-01,  4.1273e-01],\n",
       "        [-3.7018e-01,  4.9583e-01],\n",
       "        [-4.2804e-01,  5.4007e-01],\n",
       "        [-4.8110e-01,  5.5210e-01],\n",
       "        [-5.3231e-01,  5.3981e-01],\n",
       "        [-5.8059e-01,  5.1096e-01],\n",
       "        [-6.2192e-01,  4.7214e-01],\n",
       "        [-6.5127e-01,  4.2797e-01],\n",
       "        [-6.6463e-01,  3.8090e-01],\n",
       "        [-6.6063e-01,  3.3139e-01],\n",
       "        [-6.4152e-01,  2.7847e-01],\n",
       "        [-6.1309e-01,  2.2054e-01],\n",
       "        [-5.8374e-01,  1.5619e-01],\n",
       "        [ 1.4584e+00, -1.4000e+00],\n",
       "        [ 1.1188e+00, -1.1330e+00],\n",
       "        [ 7.8443e-01, -8.3986e-01],\n",
       "        [ 4.7442e-01, -5.3822e-01],\n",
       "        [ 2.0174e-01, -2.4587e-01],\n",
       "        [-2.7352e-02,  2.1149e-02],\n",
       "        [-2.1292e-01,  2.5008e-01],\n",
       "        [-3.5987e-01,  4.3258e-01],\n",
       "        [-4.7574e-01,  5.6512e-01],\n",
       "        [-5.6845e-01,  6.4882e-01],\n",
       "        [-6.4441e-01,  6.8857e-01],\n",
       "        [-7.0735e-01,  6.9181e-01],\n",
       "        [-7.5823e-01,  6.6714e-01],\n",
       "        [-7.9590e-01,  6.2290e-01],\n",
       "        [-8.1849e-01,  5.6610e-01],\n",
       "        [-8.2481e-01,  5.0186e-01],\n",
       "        [-8.1570e-01,  4.3317e-01],\n",
       "        [-7.9459e-01,  3.6123e-01],\n",
       "        [-7.6742e-01,  2.8603e-01],\n",
       "        [-7.4180e-01,  2.0712e-01],\n",
       "        [ 1.6841e+00, -1.5968e+00],\n",
       "        [ 1.3358e+00, -1.3408e+00],\n",
       "        [ 9.8169e-01, -1.0445e+00],\n",
       "        [ 6.4169e-01, -7.2623e-01],\n",
       "        [ 3.3100e-01, -4.0572e-01],\n",
       "        [ 5.8911e-02, -1.0188e-01],\n",
       "        [-1.7110e-01,  1.6900e-01],\n",
       "        [-3.6040e-01,  3.9479e-01],\n",
       "        [-5.1351e-01,  5.6843e-01],\n",
       "        [-6.3627e-01,  6.8810e-01],\n",
       "        [-7.3418e-01,  7.5662e-01],\n",
       "        [-8.1130e-01,  7.8044e-01],\n",
       "        [-8.6997e-01,  7.6813e-01],\n",
       "        [-9.1115e-01,  7.2886e-01],\n",
       "        [-9.3533e-01,  6.7113e-01],\n",
       "        [-9.4351e-01,  6.0181e-01],\n",
       "        [-9.3810e-01,  5.2571e-01],\n",
       "        [-9.2328e-01,  4.4561e-01],\n",
       "        [-9.0482e-01,  3.6274e-01],\n",
       "        [-8.8935e-01,  2.7741e-01],\n",
       "        [ 1.8267e+00, -1.6843e+00],\n",
       "        [ 1.4844e+00, -1.4582e+00],\n",
       "        [ 1.1287e+00, -1.1809e+00],\n",
       "        [ 7.7944e-01, -8.7002e-01],\n",
       "        [ 4.5243e-01, -5.4563e-01],\n",
       "        [ 1.5863e-01, -2.2808e-01],\n",
       "        [-9.6319e-02,  6.4126e-02],\n",
       "        [-3.1135e-01,  3.1623e-01],\n",
       "        [-4.8880e-01,  5.1840e-01],\n",
       "        [-6.3283e-01,  6.6622e-01],\n",
       "        [-7.4808e-01,  7.6047e-01],\n",
       "        [-8.3866e-01,  8.0614e-01],\n",
       "        [-9.0777e-01,  8.1109e-01],\n",
       "        [-9.5774e-01,  7.8449e-01],\n",
       "        [-9.9066e-01,  7.3532e-01],\n",
       "        [-1.0089e+00,  6.7133e-01],\n",
       "        [-1.0159e+00,  5.9833e-01],\n",
       "        [-1.0158e+00,  5.2008e-01],\n",
       "        [-1.0140e+00,  4.3855e-01],\n",
       "        [-1.0157e+00,  3.5454e-01],\n",
       "        [ 1.8766e+00, -1.6507e+00],\n",
       "        [ 1.5515e+00, -1.4693e+00],\n",
       "        [ 1.2097e+00, -1.2297e+00],\n",
       "        [ 8.6979e-01, -9.4759e-01],\n",
       "        [ 5.4713e-01, -6.4227e-01],\n",
       "        [ 2.5282e-01, -3.3395e-01],\n",
       "        [-6.6750e-03, -4.1907e-02],\n",
       "        [-2.2913e-01,  2.1768e-01],\n",
       "        [-4.1562e-01,  4.3316e-01],\n",
       "        [-5.6930e-01,  5.9812e-01],\n",
       "        [-6.9424e-01,  7.1143e-01],\n",
       "        [-7.9450e-01,  7.7650e-01],\n",
       "        [-8.7376e-01,  7.9994e-01],\n",
       "        [-9.3516e-01,  7.9017e-01],\n",
       "        [-9.8165e-01,  7.5586e-01],\n",
       "        [-1.0164e+00,  7.0477e-01],\n",
       "        [-1.0428e+00,  6.4296e-01],\n",
       "        [-1.0649e+00,  5.7449e-01],\n",
       "        [-1.0868e+00,  5.0165e-01],\n",
       "        [-1.1123e+00,  4.2542e-01],\n",
       "        [ 1.8333e+00, -1.4983e+00],\n",
       "        [ 1.5323e+00, -1.3709e+00],\n",
       "        [ 1.2156e+00, -1.1825e+00],\n",
       "        [ 8.9988e-01, -9.4625e-01],\n",
       "        [ 5.9893e-01, -6.7928e-01],\n",
       "        [ 3.2291e-01, -4.0054e-01],\n",
       "        [ 7.7809e-02, -1.2871e-01],\n",
       "        [-1.3418e-01,  1.1981e-01],\n",
       "        [-3.1391e-01,  3.3253e-01],\n",
       "        [-4.6427e-01,  5.0179e-01],\n",
       "        [-5.8917e-01,  6.2492e-01],\n",
       "        [-6.9278e-01,  7.0369e-01],\n",
       "        [-7.7904e-01,  7.4326e-01],\n",
       "        [-8.5152e-01,  7.5075e-01],\n",
       "        [-9.1353e-01,  7.3390e-01],\n",
       "        [-9.6822e-01,  6.9977e-01],\n",
       "        [-1.0187e+00,  6.5400e-01],\n",
       "        [-1.0681e+00,  6.0045e-01],\n",
       "        [-1.1190e+00,  5.4127e-01],\n",
       "        [-1.1734e+00,  4.7739e-01],\n",
       "        [ 1.7055e+00, -1.2433e+00],\n",
       "        [ 1.4309e+00, -1.1734e+00],\n",
       "        [ 1.1454e+00, -1.0437e+00],\n",
       "        [ 8.6347e-01, -8.6440e-01],\n",
       "        [ 5.9681e-01, -6.4978e-01],\n",
       "        [ 3.5364e-01, -4.1648e-01],\n",
       "        [ 1.3849e-01, -1.8155e-01],\n",
       "        [-4.7515e-02,  3.9504e-02],\n",
       "        [-2.0594e-01,  2.3434e-01],\n",
       "        [-3.4017e-01,  3.9476e-01],\n",
       "        [-4.5450e-01,  5.1703e-01],\n",
       "        [-5.5343e-01,  6.0152e-01],\n",
       "        [-6.4122e-01,  6.5185e-01],\n",
       "        [-7.2166e-01,  6.7361e-01],\n",
       "        [-7.9800e-01,  6.7314e-01],\n",
       "        [-8.7307e-01,  6.5637e-01],\n",
       "        [-9.4918e-01,  6.2806e-01],\n",
       "        [-1.0281e+00,  5.9142e-01],\n",
       "        [-1.1107e+00,  5.4820e-01],\n",
       "        [-1.1969e+00,  4.9903e-01],\n",
       "        [ 1.5091e+00, -9.1320e-01],\n",
       "        [ 1.2588e+00, -8.9855e-01],\n",
       "        [ 1.0056e+00, -8.2896e-01],\n",
       "        [ 7.6174e-01, -7.1143e-01],\n",
       "        [ 5.3634e-01, -5.5704e-01],\n",
       "        [ 3.3530e-01, -3.7950e-01],\n",
       "        [ 1.6099e-01, -1.9343e-01],\n",
       "        [ 1.2688e-02, -1.2617e-02],\n",
       "        [-1.1264e-01,  1.5158e-01],\n",
       "        [-2.1954e-01,  2.9111e-01],\n",
       "        [-3.1322e-01,  4.0174e-01],\n",
       "        [-3.9889e-01,  4.8276e-01],\n",
       "        [-4.8124e-01,  5.3640e-01],\n",
       "        [-5.6426e-01,  5.6669e-01],\n",
       "        [-6.5106e-01,  5.7841e-01],\n",
       "        [-7.4386e-01,  5.7614e-01],\n",
       "        [-8.4394e-01,  5.6348e-01],\n",
       "        [-9.5149e-01,  5.4280e-01],\n",
       "        [-1.0655e+00,  5.1523e-01],\n",
       "        [-1.1838e+00,  4.8099e-01],\n",
       "        [ 1.2650e+00, -5.4261e-01],\n",
       "        [ 1.0337e+00, -5.7631e-01],\n",
       "        [ 8.0962e-01, -5.6271e-01],\n",
       "        [ 6.0288e-01, -5.0578e-01],\n",
       "        [ 4.2026e-01, -4.1339e-01],\n",
       "        [ 2.6508e-01, -2.9606e-01],\n",
       "        [ 1.3723e-01, -1.6553e-01],\n",
       "        [ 3.3749e-02, -3.3259e-02],\n",
       "        [-5.0433e-02,  9.0909e-02],\n",
       "        [-1.2154e-01,  1.9974e-01],\n",
       "        [-1.8614e-01,  2.8897e-01],\n",
       "        [-2.5048e-01,  3.5729e-01],\n",
       "        [-3.1997e-01,  4.0579e-01],\n",
       "        [-3.9890e-01,  4.3711e-01],\n",
       "        [-4.9027e-01,  4.5460e-01],\n",
       "        [-5.9570e-01,  4.6145e-01],\n",
       "        [-7.1534e-01,  4.6011e-01],\n",
       "        [-8.4784e-01,  4.5201e-01],\n",
       "        [-9.9027e-01,  4.3763e-01],\n",
       "        [-1.1382e+00,  4.1675e-01],\n",
       "        [ 9.9605e-01, -1.6807e-01],\n",
       "        [ 7.7658e-01, -2.4000e-01],\n",
       "        [ 5.7539e-01, -2.7405e-01],\n",
       "        [ 4.0091e-01, -2.7166e-01],\n",
       "        [ 2.5788e-01, -2.3770e-01],\n",
       "        [ 1.4723e-01, -1.7958e-01],\n",
       "        [ 6.6406e-02, -1.0607e-01],\n",
       "        [ 1.0022e-02, -2.6061e-02],\n",
       "        [-2.9194e-02,  5.2571e-02],\n",
       "        [-5.9464e-02,  1.2379e-01],\n",
       "        [-8.9058e-02,  1.8382e-01],\n",
       "        [-1.2556e-01,  2.3115e-01],\n",
       "        [-1.7535e-01,  2.6614e-01],\n",
       "        [-2.4325e-01,  2.9040e-01],\n",
       "        [-3.3230e-01,  3.0608e-01],\n",
       "        [-4.4365e-01,  3.1521e-01],\n",
       "        [-5.7646e-01,  3.1920e-01],\n",
       "        [-7.2795e-01,  3.1871e-01],\n",
       "        [-8.9337e-01,  3.1363e-01],\n",
       "        [-1.0663e+00,  3.0331e-01],\n",
       "        [ 7.2468e-01,  1.7665e-01],\n",
       "        [ 5.0944e-01,  7.8256e-02],\n",
       "        [ 3.2331e-01,  7.5158e-03],\n",
       "        [ 1.7364e-01, -3.5091e-02],\n",
       "        [ 6.3580e-02, -5.1940e-02],\n",
       "        [-7.8719e-03, -4.7653e-02],\n",
       "        [-4.5455e-02, -2.8172e-02],\n",
       "        [-5.6788e-02,  1.5044e-04],\n",
       "        [-5.1352e-02,  3.1546e-02],\n",
       "        [-3.9422e-02,  6.1462e-02],\n",
       "        [-3.1108e-02,  8.6972e-02],\n",
       "        [-3.5547e-02,  1.0677e-01],\n",
       "        [-6.0304e-02,  1.2094e-01],\n",
       "        [-1.1096e-01,  1.3048e-01],\n",
       "        [-1.9084e-01,  1.3673e-01],\n",
       "        [-3.0085e-01,  1.4089e-01],\n",
       "        [-4.3942e-01,  1.4367e-01],\n",
       "        [-6.0255e-01,  1.4514e-01],\n",
       "        [-7.8400e-01,  1.4476e-01],\n",
       "        [-9.7557e-01,  1.4162e-01],\n",
       "        [ 4.7024e-01,  4.6462e-01],\n",
       "        [ 2.5258e-01,  3.5161e-01],\n",
       "        [ 7.3668e-02,  2.5611e-01],\n",
       "        [-5.9652e-02,  1.7986e-01],\n",
       "        [-1.4522e-01,  1.2236e-01],\n",
       "        [-1.8545e-01,  8.1270e-02],\n",
       "        [-1.8675e-01,  5.3054e-02],\n",
       "        [-1.5859e-01,  3.3722e-02],\n",
       "        [-1.1240e-01,  1.9547e-02],\n",
       "        [-6.0350e-02,  7.5747e-03],\n",
       "        [-1.4374e-02, -4.0580e-03],\n",
       "        [ 1.4791e-02, -1.6066e-02],\n",
       "        [ 1.8251e-02, -2.8235e-02],\n",
       "        [-1.0516e-02, -3.9754e-02],\n",
       "        [-7.5349e-02, -4.9625e-02],\n",
       "        [-1.7720e-01, -5.7038e-02],\n",
       "        [-3.1407e-01, -6.1651e-02],\n",
       "        [-4.8110e-01, -6.3671e-02],\n",
       "        [-6.7087e-01, -6.3828e-02],\n",
       "        [-8.7378e-01, -6.3176e-02]], device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jac_autograd_test.shape\n",
    "jac_autograd_test_per_item.shape\n",
    "jac_autograd_test_per_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ab6558f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(800.2223, device='cuda:0')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_pred_test\n",
    "\n",
    "autograd = torch.autograd.grad(\n",
    "    outputs = mean_pred_test.sum(),\n",
    "    inputs = x_test_grad,\n",
    ")[0]\n",
    "\n",
    "autograd.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a4fa8810",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_indicator_test, v_indicator_test = torch.zeros_like(mean_pred_test), torch.zeros_like(mean_pred_test)\n",
    "u_indicator_test[:, 0] = 1.0 # output column u selected\n",
    "v_indicator_test[:, 1] = 1.0 # output column v selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e880b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "autograd_u = torch.autograd.grad(\n",
    "    outputs = mean_pred_test,\n",
    "    inputs = x_test_grad,\n",
    "    grad_outputs = u_indicator_test,\n",
    "    create_graph = True\n",
    ")[0][:, 0] # u with respect to x\n",
    "\n",
    "autograd_v = torch.autograd.grad(\n",
    "    outputs = mean_pred_test,\n",
    "    inputs = x_test_grad,\n",
    "    grad_outputs = v_indicator,\n",
    "    create_graph = True\n",
    ")[0][:, 1] # v with respect to y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4a61824c",
   "metadata": {},
   "outputs": [],
   "source": [
    "divergence = torch.autograd.grad(\n",
    "    outputs = mean_pred_test,\n",
    "    inputs = x_test_grad,\n",
    "    grad_outputs = u_indicator_test,\n",
    "    create_graph = True\n",
    ")[0][:, 0] + torch.autograd.grad(\n",
    "    outputs = mean_pred_test,\n",
    "    inputs = x_test_grad,\n",
    "    grad_outputs = v_indicator_test,\n",
    "    create_graph = True\n",
    ")[0][:, 1] # v with respect to y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bdcc7b49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66.52590942382812"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "divergence.abs().sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7a6b38f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-12.526344299316406"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "divergence.sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fa74016f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-12.5263, device='cuda:0', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined = autograd_u[:, 0] + autograd_v[:, 1]\n",
    "combined.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dc46d2cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.7544e-01,  5.2412e-01],\n",
       "        [-3.6514e-01,  4.5890e-01],\n",
       "        [-1.8622e-01,  3.7656e-01],\n",
       "        [ 4.1965e-02,  2.8763e-01],\n",
       "        [ 2.9171e-01,  2.0410e-01],\n",
       "        [ 5.3086e-01,  1.3799e-01],\n",
       "        [ 7.2783e-01,  9.9891e-02],\n",
       "        [ 8.5671e-01,  9.7717e-02],\n",
       "        [ 9.0161e-01,  1.3569e-01],\n",
       "        [ 8.5936e-01,  2.1391e-01],\n",
       "        [ 7.4000e-01,  3.2843e-01],\n",
       "        [ 5.6497e-01,  4.7182e-01],\n",
       "        [ 3.6322e-01,  6.3414e-01],\n",
       "        [ 1.6616e-01,  8.0415e-01],\n",
       "        [ 2.2838e-03,  9.7053e-01],\n",
       "        [-1.0754e-01,  1.1230e+00],\n",
       "        [-1.5314e-01,  1.2533e+00],\n",
       "        [-1.3611e-01,  1.3554e+00],\n",
       "        [-6.8718e-02,  1.4260e+00],\n",
       "        [ 2.8924e-02,  1.4640e+00],\n",
       "        [-4.9582e-01,  1.2426e+00],\n",
       "        [-4.1540e-01,  1.2385e+00],\n",
       "        [-2.5884e-01,  1.1988e+00],\n",
       "        [-4.2740e-02,  1.1309e+00],\n",
       "        [ 2.0676e-01,  1.0449e+00],\n",
       "        [ 4.5766e-01,  9.5280e-01],\n",
       "        [ 6.7709e-01,  8.6669e-01],\n",
       "        [ 8.3655e-01,  7.9764e-01],\n",
       "        [ 9.1666e-01,  7.5424e-01],\n",
       "        [ 9.1030e-01,  7.4171e-01],\n",
       "        [ 8.2363e-01,  7.6146e-01],\n",
       "        [ 6.7474e-01,  8.1115e-01],\n",
       "        [ 4.9019e-01,  8.8516e-01],\n",
       "        [ 3.0011e-01,  9.7551e-01],\n",
       "        [ 1.3290e-01,  1.0729e+00],\n",
       "        [ 1.0379e-02,  1.1680e+00],\n",
       "        [-5.5529e-02,  1.2521e+00],\n",
       "        [-6.4170e-02,  1.3182e+00],\n",
       "        [-2.5426e-02,  1.3615e+00],\n",
       "        [ 4.2892e-02,  1.3792e+00],\n",
       "        [-4.4196e-01,  1.9484e+00],\n",
       "        [-4.0692e-01,  2.0159e+00],\n",
       "        [-2.9240e-01,  2.0314e+00],\n",
       "        [-1.1107e-01,  1.9980e+00],\n",
       "        [ 1.1402e-01,  1.9233e+00],\n",
       "        [ 3.5277e-01,  1.8181e+00],\n",
       "        [ 5.7273e-01,  1.6953e+00],\n",
       "        [ 7.4435e-01,  1.5682e+00],\n",
       "        [ 8.4591e-01,  1.4491e+00],\n",
       "        [ 8.6695e-01,  1.3481e+00],\n",
       "        [ 8.0990e-01,  1.2720e+00],\n",
       "        [ 6.8909e-01,  1.2238e+00],\n",
       "        [ 5.2782e-01,  1.2026e+00],\n",
       "        [ 3.5387e-01,  1.2043e+00],\n",
       "        [ 1.9427e-01,  1.2225e+00],\n",
       "        [ 7.0620e-02,  1.2493e+00],\n",
       "        [-4.4112e-03,  1.2765e+00],\n",
       "        [-2.8660e-02,  1.2966e+00],\n",
       "        [-1.0015e-02,  1.3037e+00],\n",
       "        [ 3.5877e-02,  1.2935e+00],\n",
       "        [-3.0823e-01,  2.5594e+00],\n",
       "        [-3.3098e-01,  2.7021e+00],\n",
       "        [-2.7555e-01,  2.7799e+00],\n",
       "        [-1.4976e-01,  2.7919e+00],\n",
       "        [ 2.7814e-02,  2.7422e+00],\n",
       "        [ 2.3059e-01,  2.6396e+00],\n",
       "        [ 4.2825e-01,  2.4964e+00],\n",
       "        [ 5.9182e-01,  2.3275e+00],\n",
       "        [ 6.9854e-01,  2.1478e+00],\n",
       "        [ 7.3558e-01,  1.9716e+00],\n",
       "        [ 7.0195e-01,  1.8101e+00],\n",
       "        [ 6.0812e-01,  1.6714e+00],\n",
       "        [ 4.7354e-01,  1.5594e+00],\n",
       "        [ 3.2264e-01,  1.4741e+00],\n",
       "        [ 1.8002e-01,  1.4123e+00],\n",
       "        [ 6.5866e-02,  1.3681e+00],\n",
       "        [-7.4365e-03,  1.3343e+00],\n",
       "        [-3.7035e-02,  1.3035e+00],\n",
       "        [-2.9346e-02,  1.2690e+00],\n",
       "        [ 1.8857e-03,  1.2258e+00],\n",
       "        [-9.7568e-02,  3.0028e+00],\n",
       "        [-1.8585e-01,  3.2159e+00],\n",
       "        [-2.0188e-01,  3.3568e+00],\n",
       "        [-1.4803e-01,  3.4204e+00],\n",
       "        [-3.7373e-02,  3.4070e+00],\n",
       "        [ 1.0835e-01,  3.3227e+00],\n",
       "        [ 2.6252e-01,  3.1787e+00],\n",
       "        [ 3.9828e-01,  2.9896e+00],\n",
       "        [ 4.9322e-01,  2.7720e+00],\n",
       "        [ 5.3317e-01,  2.5426e+00],\n",
       "        [ 5.1434e-01,  2.3166e+00],\n",
       "        [ 4.4348e-01,  2.1060e+00],\n",
       "        [ 3.3588e-01,  1.9188e+00],\n",
       "        [ 2.1197e-01,  1.7589e+00],\n",
       "        [ 9.2945e-02,  1.6261e+00],\n",
       "        [-3.3907e-03,  1.5168e+00],\n",
       "        [-6.5906e-02,  1.4252e+00],\n",
       "        [-9.1830e-02,  1.3446e+00],\n",
       "        [-8.6712e-02,  1.2681e+00],\n",
       "        [-6.2683e-02,  1.1901e+00],\n",
       "        [ 1.7817e-01,  3.2274e+00],\n",
       "        [ 2.2281e-02,  3.4975e+00],\n",
       "        [-7.1370e-02,  3.6946e+00],\n",
       "        [-9.9534e-02,  3.8098e+00],\n",
       "        [-6.9264e-02,  3.8397e+00],\n",
       "        [ 3.5267e-03,  3.7875e+00],\n",
       "        [ 9.7106e-02,  3.6620e+00],\n",
       "        [ 1.8804e-01,  3.4766e+00],\n",
       "        [ 2.5557e-01,  3.2480e+00],\n",
       "        [ 2.8523e-01,  2.9941e+00],\n",
       "        [ 2.7125e-01,  2.7320e+00],\n",
       "        [ 2.1701e-01,  2.4765e+00],\n",
       "        [ 1.3364e-01,  2.2387e+00],\n",
       "        [ 3.7182e-02,  2.0256e+00],\n",
       "        [-5.5145e-02,  1.8396e+00],\n",
       "        [-1.2868e-01,  1.6792e+00],\n",
       "        [-1.7432e-01,  1.5405e+00],\n",
       "        [-1.9006e-01,  1.4172e+00],\n",
       "        [-1.8109e-01,  1.3032e+00],\n",
       "        [-1.5828e-01,  1.1927e+00],\n",
       "        [ 4.9900e-01,  3.2115e+00],\n",
       "        [ 2.7939e-01,  3.5180e+00],\n",
       "        [ 1.0892e-01,  3.7571e+00],\n",
       "        [-3.8624e-03,  3.9170e+00],\n",
       "        [-5.9909e-02,  3.9915e+00],\n",
       "        [-6.8911e-02,  3.9804e+00],\n",
       "        [-4.6971e-02,  3.8896e+00],\n",
       "        [-1.3119e-02,  3.7305e+00],\n",
       "        [ 1.4533e-02,  3.5183e+00],\n",
       "        [ 2.2264e-02,  3.2704e+00],\n",
       "        [ 3.1289e-03,  3.0042e+00],\n",
       "        [-4.2294e-02,  2.7357e+00],\n",
       "        [-1.0677e-01,  2.4777e+00],\n",
       "        [-1.7863e-01,  2.2390e+00],\n",
       "        [-2.4493e-01,  2.0239e+00],\n",
       "        [-2.9458e-01,  1.8329e+00],\n",
       "        [-3.2097e-01,  1.6632e+00],\n",
       "        [-3.2337e-01,  1.5100e+00],\n",
       "        [-3.0701e-01,  1.3675e+00],\n",
       "        [-2.8179e-01,  1.2307e+00],\n",
       "        [ 8.3878e-01,  2.9669e+00],\n",
       "        [ 5.6482e-01,  3.2838e+00],\n",
       "        [ 3.2511e-01,  3.5447e+00],\n",
       "        [ 1.3281e-01,  3.7362e+00],\n",
       "        [-7.3738e-03,  3.8497e+00],\n",
       "        [-9.9046e-02,  3.8823e+00],\n",
       "        [-1.5247e-01,  3.8370e+00],\n",
       "        [-1.8179e-01,  3.7220e+00],\n",
       "        [-2.0177e-01,  3.5499e+00],\n",
       "        [-2.2458e-01,  3.3361e+00],\n",
       "        [-2.5758e-01,  3.0969e+00],\n",
       "        [-3.0229e-01,  2.8476e+00],\n",
       "        [-3.5488e-01,  2.6011e+00],\n",
       "        [-4.0778e-01,  2.3667e+00],\n",
       "        [-4.5220e-01,  2.1498e+00],\n",
       "        [-4.8071e-01,  1.9518e+00],\n",
       "        [-4.8940e-01,  1.7712e+00],\n",
       "        [-4.7902e-01,  1.6042e+00],\n",
       "        [-4.5509e-01,  1.4460e+00],\n",
       "        [-4.2671e-01,  1.2923e+00],\n",
       "        [ 1.1682e+00,  2.5362e+00],\n",
       "        [ 8.5347e-01,  2.8354e+00],\n",
       "        [ 5.5784e-01,  3.0940e+00],\n",
       "        [ 2.9805e-01,  3.2986e+00],\n",
       "        [ 8.3563e-02,  3.4395e+00],\n",
       "        [-8.3744e-02,  3.5115e+00],\n",
       "        [-2.0865e-01,  3.5147e+00],\n",
       "        [-3.0044e-01,  3.4541e+00],\n",
       "        [-3.7018e-01,  3.3388e+00],\n",
       "        [-4.2804e-01,  3.1811e+00],\n",
       "        [-4.8110e-01,  2.9945e+00],\n",
       "        [-5.3231e-01,  2.7923e+00],\n",
       "        [-5.8059e-01,  2.5859e+00],\n",
       "        [-6.2192e-01,  2.3838e+00],\n",
       "        [-6.5127e-01,  2.1914e+00],\n",
       "        [-6.6463e-01,  2.0104e+00],\n",
       "        [-6.6063e-01,  1.8399e+00],\n",
       "        [-6.4152e-01,  1.6769e+00],\n",
       "        [-6.1309e-01,  1.5177e+00],\n",
       "        [-5.8374e-01,  1.3590e+00],\n",
       "        [ 1.4584e+00,  1.9853e+00],\n",
       "        [ 1.1188e+00,  2.2398e+00],\n",
       "        [ 7.8443e-01,  2.4711e+00],\n",
       "        [ 4.7442e-01,  2.6674e+00],\n",
       "        [ 2.0174e-01,  2.8190e+00],\n",
       "        [-2.7352e-02,  2.9194e+00],\n",
       "        [-2.1292e-01,  2.9663e+00],\n",
       "        [-3.5987e-01,  2.9615e+00],\n",
       "        [-4.7574e-01,  2.9104e+00],\n",
       "        [-5.6845e-01,  2.8214e+00],\n",
       "        [-6.4441e-01,  2.7042e+00],\n",
       "        [-7.0735e-01,  2.5689e+00],\n",
       "        [-7.5823e-01,  2.4242e+00],\n",
       "        [-7.9590e-01,  2.2769e+00],\n",
       "        [-8.1849e-01,  2.1310e+00],\n",
       "        [-8.2481e-01,  1.9878e+00],\n",
       "        [-8.1570e-01,  1.8465e+00],\n",
       "        [-7.9459e-01,  1.7046e+00],\n",
       "        [-7.6742e-01,  1.5592e+00],\n",
       "        [-7.4180e-01,  1.4080e+00],\n",
       "        [ 1.6841e+00,  1.3918e+00],\n",
       "        [ 1.3358e+00,  1.5787e+00],\n",
       "        [ 9.8169e-01,  1.7600e+00],\n",
       "        [ 6.4169e-01,  1.9262e+00],\n",
       "        [ 3.3100e-01,  2.0690e+00],\n",
       "        [ 5.8911e-02,  2.1816e+00],\n",
       "        [-1.7110e-01,  2.2602e+00],\n",
       "        [-3.6040e-01,  2.3038e+00],\n",
       "        [-5.1351e-01,  2.3143e+00],\n",
       "        [-6.3627e-01,  2.2958e+00],\n",
       "        [-7.3418e-01,  2.2541e+00],\n",
       "        [-8.1130e-01,  2.1952e+00],\n",
       "        [-8.6997e-01,  2.1244e+00],\n",
       "        [-9.1115e-01,  2.0457e+00],\n",
       "        [-9.3533e-01,  1.9612e+00],\n",
       "        [-9.4351e-01,  1.8709e+00],\n",
       "        [-9.3810e-01,  1.7736e+00],\n",
       "        [-9.2328e-01,  1.6668e+00],\n",
       "        [-9.0482e-01,  1.5484e+00],\n",
       "        [-8.8935e-01,  1.4166e+00],\n",
       "        [ 1.8267e+00,  8.3089e-01],\n",
       "        [ 1.4844e+00,  9.3453e-01],\n",
       "        [ 1.1287e+00,  1.0478e+00],\n",
       "        [ 7.7944e-01,  1.1647e+00],\n",
       "        [ 4.5243e-01,  1.2791e+00],\n",
       "        [ 1.5863e-01,  1.3851e+00],\n",
       "        [-9.6319e-02,  1.4782e+00],\n",
       "        [-3.1135e-01,  1.5554e+00],\n",
       "        [-4.8880e-01,  1.6157e+00],\n",
       "        [-6.3283e-01,  1.6596e+00],\n",
       "        [-7.4808e-01,  1.6885e+00],\n",
       "        [-8.3866e-01,  1.7045e+00],\n",
       "        [-9.0777e-01,  1.7091e+00],\n",
       "        [-9.5774e-01,  1.7031e+00],\n",
       "        [-9.9066e-01,  1.6861e+00],\n",
       "        [-1.0089e+00,  1.6565e+00],\n",
       "        [-1.0159e+00,  1.6120e+00],\n",
       "        [-1.0158e+00,  1.5500e+00],\n",
       "        [-1.0140e+00,  1.4683e+00],\n",
       "        [-1.0157e+00,  1.3659e+00],\n",
       "        [ 1.8766e+00,  3.6351e-01],\n",
       "        [ 1.5515e+00,  3.7680e-01],\n",
       "        [ 1.2097e+00,  4.1113e-01],\n",
       "        [ 8.6979e-01,  4.6450e-01],\n",
       "        [ 5.4713e-01,  5.3356e-01],\n",
       "        [ 2.5282e-01,  6.1424e-01],\n",
       "        [-6.6750e-03,  7.0229e-01],\n",
       "        [-2.2913e-01,  7.9383e-01],\n",
       "        [-4.1562e-01,  8.8567e-01],\n",
       "        [-5.6930e-01,  9.7528e-01],\n",
       "        [-6.9424e-01,  1.0607e+00],\n",
       "        [-7.9450e-01,  1.1402e+00],\n",
       "        [-8.7376e-01,  1.2118e+00],\n",
       "        [-9.3516e-01,  1.2730e+00],\n",
       "        [-9.8165e-01,  1.3209e+00],\n",
       "        [-1.0164e+00,  1.3518e+00],\n",
       "        [-1.0428e+00,  1.3623e+00],\n",
       "        [-1.0649e+00,  1.3490e+00],\n",
       "        [-1.0868e+00,  1.3096e+00],\n",
       "        [-1.1123e+00,  1.2433e+00],\n",
       "        [ 1.8333e+00,  2.6977e-02],\n",
       "        [ 1.5323e+00, -4.8031e-02],\n",
       "        [ 1.2156e+00, -9.5355e-02],\n",
       "        [ 8.9988e-01, -1.1303e-01],\n",
       "        [ 5.9893e-01, -1.0116e-01],\n",
       "        [ 3.2291e-01, -6.1540e-02],\n",
       "        [ 7.7809e-02,  2.7709e-03],\n",
       "        [-1.3418e-01,  8.7928e-02],\n",
       "        [-3.1391e-01,  1.8967e-01],\n",
       "        [-4.6427e-01,  3.0354e-01],\n",
       "        [-5.8917e-01,  4.2501e-01],\n",
       "        [-6.9278e-01,  5.4937e-01],\n",
       "        [-7.7904e-01,  6.7169e-01],\n",
       "        [-8.5152e-01,  7.8676e-01],\n",
       "        [-9.1353e-01,  8.8915e-01],\n",
       "        [-9.6822e-01,  9.7343e-01],\n",
       "        [-1.0187e+00,  1.0346e+00],\n",
       "        [-1.0681e+00,  1.0685e+00],\n",
       "        [-1.1190e+00,  1.0723e+00],\n",
       "        [-1.1734e+00,  1.0452e+00],\n",
       "        [ 1.7055e+00, -1.6917e-01],\n",
       "        [ 1.4309e+00, -3.2209e-01],\n",
       "        [ 1.1454e+00, -4.4556e-01],\n",
       "        [ 8.6347e-01, -5.3411e-01],\n",
       "        [ 5.9681e-01, -5.8469e-01],\n",
       "        [ 3.5364e-01, -5.9653e-01],\n",
       "        [ 1.3849e-01, -5.7089e-01],\n",
       "        [-4.7515e-02, -5.1074e-01],\n",
       "        [-2.0594e-01, -4.2034e-01],\n",
       "        [-3.4017e-01, -3.0494e-01],\n",
       "        [-4.5450e-01, -1.7056e-01],\n",
       "        [-5.5343e-01, -2.3752e-02],\n",
       "        [-6.4122e-01,  1.2845e-01],\n",
       "        [-7.2166e-01,  2.7877e-01],\n",
       "        [-7.9800e-01,  4.1988e-01],\n",
       "        [-8.7307e-01,  5.4476e-01],\n",
       "        [-9.4918e-01,  6.4713e-01],\n",
       "        [-1.0281e+00,  7.2191e-01],\n",
       "        [-1.1107e+00,  7.6568e-01],\n",
       "        [-1.1969e+00,  7.7707e-01],\n",
       "        [ 1.5091e+00, -2.4197e-01],\n",
       "        [ 1.2588e+00, -4.5579e-01],\n",
       "        [ 1.0056e+00, -6.4271e-01],\n",
       "        [ 7.6174e-01, -7.9458e-01],\n",
       "        [ 5.3634e-01, -9.0561e-01],\n",
       "        [ 3.3530e-01, -9.7253e-01],\n",
       "        [ 1.6099e-01, -9.9455e-01],\n",
       "        [ 1.2688e-02, -9.7311e-01],\n",
       "        [-1.1264e-01, -9.1160e-01],\n",
       "        [-2.1954e-01, -8.1503e-01],\n",
       "        [-3.1322e-01, -6.8972e-01],\n",
       "        [-3.9889e-01, -5.4300e-01],\n",
       "        [-4.8124e-01, -3.8293e-01],\n",
       "        [-5.6426e-01, -2.1797e-01],\n",
       "        [-6.5106e-01, -5.6603e-02],\n",
       "        [-7.4386e-01,  9.3040e-02],\n",
       "        [-8.4394e-01,  2.2373e-01],\n",
       "        [-9.5149e-01,  3.2955e-01],\n",
       "        [-1.0655e+00,  4.0641e-01],\n",
       "        [-1.1838e+00,  4.5227e-01],\n",
       "        [ 1.2650e+00, -2.2894e-01],\n",
       "        [ 1.0337e+00, -4.8248e-01],\n",
       "        [ 8.0962e-01, -7.1489e-01],\n",
       "        [ 6.0288e-01, -9.1631e-01],\n",
       "        [ 4.2026e-01, -1.0790e+00],\n",
       "        [ 2.6508e-01, -1.1975e+00],\n",
       "        [ 1.3723e-01, -1.2690e+00],\n",
       "        [ 3.3749e-02, -1.2933e+00],\n",
       "        [-5.0433e-02, -1.2721e+00],\n",
       "        [-1.2154e-01, -1.2094e+00],\n",
       "        [-1.8614e-01, -1.1109e+00],\n",
       "        [-2.5048e-01, -9.8354e-01],\n",
       "        [-3.1997e-01, -8.3547e-01],\n",
       "        [-3.9890e-01, -6.7536e-01],\n",
       "        [-4.9027e-01, -5.1207e-01],\n",
       "        [-5.9570e-01, -3.5418e-01],\n",
       "        [-7.1534e-01, -2.0944e-01],\n",
       "        [-8.4784e-01, -8.4288e-02],\n",
       "        [-9.9027e-01,  1.6543e-02],\n",
       "        [-1.1382e+00,  9.0357e-02],\n",
       "        [ 9.9605e-01, -1.7889e-01],\n",
       "        [ 7.7658e-01, -4.4955e-01],\n",
       "        [ 5.7539e-01, -7.0660e-01],\n",
       "        [ 4.0091e-01, -9.3955e-01],\n",
       "        [ 2.5788e-01, -1.1395e+00],\n",
       "        [ 1.4723e-01, -1.2998e+00],\n",
       "        [ 6.6406e-02, -1.4157e+00],\n",
       "        [ 1.0022e-02, -1.4852e+00],\n",
       "        [-2.9194e-02, -1.5084e+00],\n",
       "        [-5.9464e-02, -1.4877e+00],\n",
       "        [-8.9058e-02, -1.4272e+00],\n",
       "        [-1.2556e-01, -1.3330e+00],\n",
       "        [-1.7535e-01, -1.2121e+00],\n",
       "        [-2.4325e-01, -1.0728e+00],\n",
       "        [-3.3230e-01, -9.2330e-01],\n",
       "        [-4.4365e-01, -7.7213e-01],\n",
       "        [-5.7646e-01, -6.2697e-01],\n",
       "        [-7.2795e-01, -4.9446e-01],\n",
       "        [-8.9337e-01, -3.7968e-01],\n",
       "        [-1.0663e+00, -2.8593e-01],\n",
       "        [ 7.2468e-01, -1.4148e-01],\n",
       "        [ 5.0944e-01, -4.0783e-01],\n",
       "        [ 3.2331e-01, -6.6831e-01],\n",
       "        [ 1.7364e-01, -9.1287e-01],\n",
       "        [ 6.3580e-02, -1.1325e+00],\n",
       "        [-7.8719e-03, -1.3198e+00],\n",
       "        [-4.5455e-02, -1.4691e+00],\n",
       "        [-5.6788e-02, -1.5767e+00],\n",
       "        [-5.1352e-02, -1.6412e+00],\n",
       "        [-3.9422e-02, -1.6630e+00],\n",
       "        [-3.1108e-02, -1.6446e+00],\n",
       "        [-3.5547e-02, -1.5904e+00],\n",
       "        [-6.0304e-02, -1.5059e+00],\n",
       "        [-1.1096e-01, -1.3979e+00],\n",
       "        [-1.9084e-01, -1.2738e+00],\n",
       "        [-3.0085e-01, -1.1411e+00],\n",
       "        [-4.3942e-01, -1.0069e+00],\n",
       "        [-6.0255e-01, -8.7780e-01],\n",
       "        [-7.8400e-01, -7.5877e-01],\n",
       "        [-9.7557e-01, -6.5352e-01],\n",
       "        [ 4.7024e-01, -1.5778e-01],\n",
       "        [ 2.5258e-01, -4.0169e-01],\n",
       "        [ 7.3668e-02, -6.4637e-01],\n",
       "        [-5.9652e-02, -8.8312e-01],\n",
       "        [-1.4522e-01, -1.1037e+00],\n",
       "        [-1.8545e-01, -1.3008e+00],\n",
       "        [-1.8675e-01, -1.4684e+00],\n",
       "        [-1.5859e-01, -1.6019e+00],\n",
       "        [-1.1240e-01, -1.6984e+00],\n",
       "        [-6.0350e-02, -1.7569e+00],\n",
       "        [-1.4374e-02, -1.7779e+00],\n",
       "        [ 1.4791e-02, -1.7638e+00],\n",
       "        [ 1.8251e-02, -1.7183e+00],\n",
       "        [-1.0516e-02, -1.6465e+00],\n",
       "        [-7.5349e-02, -1.5542e+00],\n",
       "        [-1.7720e-01, -1.4476e+00],\n",
       "        [-3.1407e-01, -1.3327e+00],\n",
       "        [-4.8110e-01, -1.2154e+00],\n",
       "        [-6.7087e-01, -1.1005e+00],\n",
       "        [-8.7378e-01, -9.9165e-01]], device='cuda:0')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d387d00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "divergence = 0.0\n",
    "for i in range(mean_pred_test.shape[-1]):  # loop over output dimensions (e.g., u and v)\n",
    "    grad_outputs = torch.zeros_like(mean_pred_test)\n",
    "    grad_outputs[:, i] = 1.0  # pick out derivative w.r.t. output i\n",
    "\n",
    "    grad = torch.autograd.grad(\n",
    "        outputs = mean_pred_test,\n",
    "        inputs = x_test_grad,\n",
    "        grad_outputs = grad_outputs,\n",
    "        create_graph = True\n",
    "    )[0]  # shape (batch, input_dim)\n",
    "\n",
    "    divergence += grad[:, i]  # take derivative of output i w.r.t. input i\n",
    "\n",
    "GP_test_div_manual = divergence.sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3b0482c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6.3601e-02, -8.3437e-03, -5.7976e-02, -8.5873e-02, -9.6913e-02,\n",
       "        -9.8862e-02, -1.0014e-01, -1.0737e-01, -1.2319e-01, -1.4502e-01,\n",
       "        -1.6492e-01, -1.7085e-01, -1.4911e-01, -8.7354e-02,  2.2368e-02,\n",
       "         1.8115e-01,  3.8208e-01,  6.1045e-01,  8.4554e-01,  1.0635e+00,\n",
       "         5.0009e-02, -1.5807e-02, -5.7362e-02, -7.5025e-02, -7.3992e-02,\n",
       "        -6.2808e-02, -5.1117e-02, -4.7046e-02, -5.4862e-02, -7.3443e-02,\n",
       "        -9.5995e-02, -1.1119e-01, -1.0548e-01, -6.6133e-02,  1.5629e-02,\n",
       "         1.4210e-01,  3.0776e-01,  4.9938e-01,  6.9772e-01,  8.8029e-01,\n",
       "         4.9696e-02, -9.1718e-03, -4.2615e-02, -5.0822e-02, -3.9174e-02,\n",
       "        -1.6823e-02,  5.5739e-03,  1.8605e-02,  1.6584e-02, -7.4810e-04,\n",
       "        -2.7729e-02, -5.3777e-02, -6.5623e-02, -5.0288e-02,  1.7172e-03,\n",
       "         9.3819e-02,  2.2190e-01,  3.7437e-01,  5.3361e-01,  6.7877e-01,\n",
       "         5.5814e-02,  3.7631e-03, -2.2331e-02, -2.2447e-02, -2.0576e-03,\n",
       "         2.9247e-02,  6.0052e-02,  7.9869e-02,  8.1809e-02,  6.4371e-02,\n",
       "         3.2124e-02, -5.1576e-03, -3.4588e-02, -4.3115e-02, -2.0686e-02,\n",
       "         3.7130e-02,  1.2759e-01,  2.4071e-01,  3.6068e-01,  4.6840e-01,\n",
       "         6.1835e-02,  1.5323e-02, -5.0114e-03,  1.0142e-03,  2.7940e-02,\n",
       "         6.5901e-02,  1.0292e-01,  1.2770e-01,  1.3232e-01,  1.1419e-01,\n",
       "         7.6826e-02,  2.9171e-02, -1.6416e-02, -4.6984e-02, -5.2116e-02,\n",
       "        -2.6544e-02,  2.8252e-02,  1.0389e-01,  1.8633e-01,  2.5837e-01,\n",
       "         6.3334e-02,  1.9955e-02,  2.9114e-03,  1.2592e-02,  4.3580e-02,\n",
       "         8.5900e-02,  1.2719e-01,  1.5551e-01,  1.6213e-01,  1.4348e-01,\n",
       "         1.0210e-01,  4.6011e-02, -1.3073e-02, -6.2497e-02, -9.1697e-02,\n",
       "        -9.4765e-02, -7.2078e-02, -3.0526e-02,  1.7643e-02,  5.7096e-02,\n",
       "         5.9316e-02,  1.5581e-02, -1.3863e-03,  8.9754e-03,  4.1376e-02,\n",
       "         8.5792e-02,  1.2962e-01,  1.6051e-01,  1.6901e-01,  1.5068e-01,\n",
       "         1.0708e-01,  4.5304e-02, -2.3752e-02, -8.7925e-02, -1.3672e-01,\n",
       "        -1.6383e-01, -1.6869e-01, -1.5684e-01, -1.3882e-01, -1.2807e-01,\n",
       "         5.2640e-02,  4.2255e-03, -1.6515e-02, -8.8058e-03,  2.2213e-02,\n",
       "         6.6540e-02,  1.1147e-01,  1.4428e-01,  1.5493e-01,  1.3821e-01,\n",
       "         9.4679e-02,  3.0396e-02, -4.4673e-02, -1.1907e-01, -1.8259e-01,\n",
       "        -2.2875e-01, -2.5627e-01, -2.6943e-01, -2.7718e-01, -2.9112e-01,\n",
       "         4.9591e-02, -8.4063e-03, -3.7165e-02, -3.5671e-02, -8.9025e-03,\n",
       "         3.3240e-02,  7.7972e-02,  1.1229e-01,  1.2565e-01,  1.1203e-01,\n",
       "         7.1002e-02,  7.4922e-03, -6.9627e-02, -1.4978e-01, -2.2330e-01,\n",
       "        -2.8373e-01, -3.2924e-01, -3.6305e-01, -3.9254e-01, -4.2755e-01,\n",
       "         5.8394e-02, -1.4265e-02, -5.5430e-02, -6.3797e-02, -4.4128e-02,\n",
       "        -6.2034e-03,  3.7157e-02,  7.2711e-02,  8.9385e-02,  8.0367e-02,\n",
       "         4.4165e-02, -1.5532e-02, -9.1084e-02, -1.7300e-01, -2.5238e-01,\n",
       "        -3.2294e-01, -3.8252e-01, -4.3335e-01, -4.8139e-01, -5.3468e-01,\n",
       "         8.7303e-02, -4.9452e-03, -6.2775e-02, -8.4536e-02, -7.4727e-02,\n",
       "        -4.2964e-02, -2.1024e-03,  3.4394e-02,  5.4920e-02,  5.1822e-02,\n",
       "         2.2437e-02, -3.0855e-02, -1.0184e-01, -1.8229e-01, -2.6420e-01,\n",
       "        -3.4170e-01, -4.1239e-01, -4.7766e-01, -5.4208e-01, -6.1194e-01,\n",
       "         1.4241e-01,  2.6201e-02, -5.2170e-02, -9.0582e-02, -9.3199e-02,\n",
       "        -6.9457e-02, -3.2192e-02,  4.8823e-03,  2.9598e-02,  3.3389e-02,\n",
       "         1.2388e-02, -3.2527e-02, -9.6674e-02, -1.7326e-01, -2.5533e-01,\n",
       "        -3.3759e-01, -4.1752e-01, -4.9574e-01, -5.7544e-01, -6.6118e-01,\n",
       "         2.2591e-01,  8.2198e-02, -1.9931e-02, -7.7802e-02, -9.5142e-02,\n",
       "        -8.1135e-02, -4.8582e-02, -1.1448e-02,  1.7536e-02,  2.8813e-02,\n",
       "         1.7193e-02, -1.8001e-02, -7.3818e-02, -1.4499e-01, -2.2579e-01,\n",
       "        -3.1158e-01, -3.9981e-01, -4.9037e-01, -5.8512e-01, -6.8687e-01,\n",
       "         3.3507e-01,  1.6142e-01,  3.3142e-02, -4.6374e-02, -8.0344e-02,\n",
       "        -7.7625e-02, -5.0902e-02, -1.4371e-02,  1.8614e-02,  3.7523e-02,\n",
       "         3.5747e-02,  1.0913e-02, -3.5781e-02, -1.0077e-01, -1.7963e-01,\n",
       "        -2.6845e-01, -3.6472e-01, -4.6763e-01, -5.7774e-01, -6.9605e-01,\n",
       "         4.6227e-01,  2.5750e-01,  1.0169e-01, -9.2995e-04, -5.2965e-02,\n",
       "        -6.2841e-02, -4.3066e-02, -8.0111e-03,  2.8401e-02,  5.4593e-02,\n",
       "         6.2525e-02,  4.8095e-02,  1.0626e-02, -4.8050e-02, -1.2486e-01,\n",
       "        -2.1670e-01, -3.2112e-01, -4.3665e-01, -5.6248e-01, -6.9788e-01,\n",
       "         5.9592e-01,  3.6023e-01,  1.7667e-01,  5.0303e-02, -2.0694e-02,\n",
       "        -4.4192e-02, -3.2448e-02,  7.1049e-05,  3.8943e-02,  7.1571e-02,\n",
       "         8.8513e-02,  8.3874e-02,  5.5160e-02,  2.4241e-03, -7.2647e-02,\n",
       "        -1.6772e-01, -2.8046e-01, -4.0869e-01, -5.5032e-01, -7.0282e-01,\n",
       "         7.2236e-01,  4.5735e-01,  2.4692e-01,  9.7098e-02,  6.8696e-03,\n",
       "        -3.0973e-02, -2.8292e-02,  4.8923e-04,  4.0475e-02,  7.8197e-02,\n",
       "         1.0282e-01,  1.0681e-01,  8.5817e-02,  3.8209e-02, -3.5667e-02,\n",
       "        -1.3424e-01, -2.5523e-01, -3.9582e-01, -5.5264e-01, -7.2144e-01,\n",
       "         8.2798e-01,  5.3658e-01,  3.0135e-01,  1.2925e-01,  2.0173e-02,\n",
       "        -3.2350e-02, -3.9664e-02, -1.6039e-02,  2.3377e-02,  6.4326e-02,\n",
       "         9.4763e-02,  1.0558e-01,  9.0786e-02,  4.7154e-02, -2.6215e-02,\n",
       "        -1.2844e-01, -2.5726e-01, -4.0923e-01, -5.7975e-01, -7.6299e-01,\n",
       "         9.0133e-01,  5.8770e-01,  3.3083e-01,  1.3854e-01,  1.1640e-02,\n",
       "        -5.5525e-02, -7.3627e-02, -5.6638e-02, -1.9806e-02,  2.2039e-02,\n",
       "         5.5864e-02,  7.1222e-02,  6.0639e-02,  1.9520e-02, -5.4107e-02,\n",
       "        -1.5996e-01, -2.9575e-01, -4.5742e-01, -6.3925e-01, -8.3395e-01,\n",
       "         9.3486e-01,  6.0419e-01,  3.2977e-01,  1.2021e-01, -2.2857e-02,\n",
       "        -1.0418e-01, -1.3369e-01, -1.2487e-01, -9.2851e-02, -5.2776e-02,\n",
       "        -1.8432e-02, -1.2748e-03, -9.9839e-03, -5.0270e-02, -1.2497e-01,\n",
       "        -2.3424e-01, -3.7572e-01, -5.4478e-01, -7.3469e-01, -9.3696e-01],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "744396f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-12.526344299316406"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GP_test_div_manual"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
