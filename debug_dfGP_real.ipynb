{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66bf128c",
   "metadata": {},
   "source": [
    "# Start with dfGP for real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69774e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "\n",
      "Training for REGION_LOWER_BYRD...\n",
      "=== REGION_LOWER_BYRD ===\n",
      "Training inputs shape: torch.Size([536, 2])\n",
      "Training observations shape: torch.Size([536, 2])\n",
      "Training inputs dtype: torch.float32\n",
      "\n",
      "=== REGION_LOWER_BYRD ===\n",
      "Test inputs shape: torch.Size([293, 2])\n",
      "Test observations shape: torch.Size([293, 2])\n",
      "Test inputs dtype: torch.float32\n",
      "\n",
      "\n",
      "--- Training Run 1/1 ---\n",
      "\n",
      "Start Training\n",
      "region_lower_byrd dfGP Run 1/1, Epoch 1/1, Training Loss (NLML): 4.5669, Training RMSE: 0.2931\n",
      "Test RMSE: 0.6080, Test NLL: 2.0080\n",
      "\n",
      "Training for REGION_MID_BYRD...\n",
      "=== REGION_MID_BYRD ===\n",
      "Training inputs shape: torch.Size([501, 2])\n",
      "Training observations shape: torch.Size([501, 2])\n",
      "Training inputs dtype: torch.float32\n",
      "\n",
      "=== REGION_MID_BYRD ===\n",
      "Test inputs shape: torch.Size([221, 2])\n",
      "Test observations shape: torch.Size([221, 2])\n",
      "Test inputs dtype: torch.float32\n",
      "\n",
      "\n",
      "--- Training Run 1/1 ---\n",
      "\n",
      "Start Training\n",
      "region_mid_byrd dfGP Run 1/1, Epoch 1/1, Training Loss (NLML): 9.2241, Training RMSE: 0.4982\n",
      "Test RMSE: 2.0993, Test NLL: 42.4609\n",
      "\n",
      "Training for REGION_UPPER_BYRD...\n",
      "=== REGION_UPPER_BYRD ===\n",
      "Training inputs shape: torch.Size([504, 2])\n",
      "Training observations shape: torch.Size([504, 2])\n",
      "Training inputs dtype: torch.float32\n",
      "\n",
      "=== REGION_UPPER_BYRD ===\n",
      "Test inputs shape: torch.Size([242, 2])\n",
      "Test observations shape: torch.Size([242, 2])\n",
      "Test inputs dtype: torch.float32\n",
      "\n",
      "\n",
      "--- Training Run 1/1 ---\n",
      "\n",
      "Start Training\n",
      "region_upper_byrd dfGP Run 1/1, Epoch 1/1, Training Loss (NLML): 12.8592, Training RMSE: 0.7411\n",
      "Test RMSE: 2.2314, Test NLL: 39.9017\n"
     ]
    }
   ],
   "source": [
    "# REAL DATA EXPERIMENTS\n",
    "# RUN WITH python run_real_experiments_dfGP.py\n",
    "#               _                 _   _      \n",
    "#              | |               | | (_)     \n",
    "#    __ _ _ __ | |_ __ _ _ __ ___| |_ _  ___ \n",
    "#   / _` | '_ \\| __/ _` | '__/ __| __| |/ __|\n",
    "#  | (_| | | | | || (_| | | | (__| |_| | (__ \n",
    "#   \\__,_|_| |_|\\__\\__,_|_|  \\___|\\__|_|\\___|\n",
    "# \n",
    "model_name = \"dfGP\"\n",
    "from gpytorch_models import dfGP\n",
    "\n",
    "# import configs to we can access the hypers with getattr\n",
    "import configs\n",
    "from configs import PATIENCE, MAX_NUM_EPOCHS, NUM_RUNS, WEIGHT_DECAY\n",
    "from configs import TRACK_EMISSIONS_BOOL\n",
    "from configs import REAL_L_RANGE, REAL_OUTPUTSCALE_VAR_RANGE, REAL_NOISE_VAR_RANGE\n",
    "from configs import SCALE_INPUT_region_lower_byrd, SCALE_INPUT_region_mid_byrd, SCALE_INPUT_region_upper_byrd\n",
    "from configs import REAL_L_RANGE, REAL_NOISE_VAR_RANGE, REAL_OUTPUTSCALE_VAR_RANGE\n",
    "\n",
    "SCALE_INPUT = {\n",
    "    \"region_lower_byrd\": SCALE_INPUT_region_lower_byrd,\n",
    "    \"region_mid_byrd\": SCALE_INPUT_region_mid_byrd,\n",
    "    \"region_upper_byrd\": SCALE_INPUT_region_upper_byrd,\n",
    "}\n",
    "\n",
    "# Reiterating import for visibility\n",
    "MAX_NUM_EPOCHS = 1\n",
    "NUM_RUNS = 1\n",
    "WEIGHT_DECAY = WEIGHT_DECAY\n",
    "PATIENCE = PATIENCE\n",
    "\n",
    "# assign model-specific variable\n",
    "MODEL_LEARNING_RATE = getattr(configs, f\"{model_name}_REAL_LEARNING_RATE\")\n",
    "MODEL_REAL_RESULTS_DIR = getattr(configs, f\"{model_name}_REAL_RESULTS_DIR\")\n",
    "import os\n",
    "os.makedirs(MODEL_REAL_RESULTS_DIR, exist_ok = True)\n",
    "\n",
    "# basics\n",
    "import pandas as pd\n",
    "import torch\n",
    "import gpytorch\n",
    "\n",
    "# universals \n",
    "from metrics import compute_divergence_field, quantile_coverage_error_2d\n",
    "from utils import set_seed, make_grid\n",
    "import gc\n",
    "import warnings\n",
    "set_seed(42)\n",
    "\n",
    "# setting device to GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# overwrite if needed: # device = 'cpu'\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "### START TIMING ###\n",
    "import time\n",
    "start_time = time.time()  # Start timing after imports\n",
    "\n",
    "### START TRACKING EXPERIMENT EMISSIONS ###\n",
    "if TRACK_EMISSIONS_BOOL:\n",
    "    from codecarbon import EmissionsTracker\n",
    "    tracker = EmissionsTracker(project_name = \"dfGP_real_experiments\", output_dir = MODEL_REAL_RESULTS_DIR)\n",
    "    tracker.start()\n",
    "\n",
    "#############################\n",
    "### LOOP 1 - over REGIONS ###\n",
    "#############################\n",
    "\n",
    "for region_name in [\"region_lower_byrd\", \"region_mid_byrd\", \"region_upper_byrd\"]:\n",
    "\n",
    "    SCALE_DOMAIN = SCALE_INPUT[region_name]\n",
    "\n",
    "    print(f\"\\nTraining for {region_name.upper()}...\")\n",
    "\n",
    "    # Store metrics for the current region (used for *metrics_summary* report and *metrics_per_run*)\n",
    "    region_results = []\n",
    "\n",
    "    ##########################################\n",
    "    ### x_train & y_train, x_test & x_test ###\n",
    "    ##########################################\n",
    "\n",
    "    # define paths based on region_name\n",
    "    path_to_training_tensor = \"data/real_data/\" + region_name + \"_train_tensor.pt\"\n",
    "    path_to_test_tensor = \"data/real_data/\" + region_name + \"_test_tensor.pt\"\n",
    "\n",
    "    # load and tranpose to have rows as points\n",
    "    train = torch.load(path_to_training_tensor, weights_only = False).T \n",
    "    test = torch.load(path_to_test_tensor, weights_only = False).T\n",
    "\n",
    "    # The train and test tensors have the following columns:\n",
    "    # [:, 0] = x\n",
    "    # [:, 1] = y\n",
    "    # [:, 2] = surface elevation (s)\n",
    "    # [:, 3] = ice flux in x direction (u)\n",
    "    # [:, 4] = ice flux in y direction (v)\n",
    "    # [:, 5] = ice flux error in x direction (u_err)\n",
    "    # [:, 6] = ice flux error in y direction (v_err)\n",
    "    # [:, 7] = source age\n",
    "\n",
    "    # train\n",
    "    x_train = train[:, [0, 1]].to(device)\n",
    "    y_train = train[:, [3, 4]].to(device)\n",
    "\n",
    "    # test\n",
    "    x_test = test[:, [0, 1]].to(device)\n",
    "    y_test = test[:, [3, 4]].to(device)\n",
    "\n",
    "    # HACK: Scaling helps with numerical stability\n",
    "    # Units are not in km \n",
    "    x_test = x_test * SCALE_DOMAIN\n",
    "    x_train = x_train * SCALE_DOMAIN\n",
    "\n",
    "    # NOTE: Here we estimate the noise variance \n",
    "\n",
    "    # Print train details\n",
    "    print(f\"=== {region_name.upper()} ===\")\n",
    "    print(f\"Training inputs shape: {x_train.shape}\")\n",
    "    print(f\"Training observations shape: {y_train.shape}\")\n",
    "    print(f\"Training inputs dtype: {x_train.dtype}\")\n",
    "    print()\n",
    "\n",
    "    # Print test details\n",
    "    print(f\"=== {region_name.upper()} ===\")\n",
    "    print(f\"Test inputs shape: {x_test.shape}\")\n",
    "    print(f\"Test observations shape: {y_test.shape}\")\n",
    "    print(f\"Test inputs dtype: {x_test.dtype}\")\n",
    "    print()\n",
    "\n",
    "    ##################################\n",
    "    ### LOOP 2 - over training run ###\n",
    "    ##################################\n",
    "\n",
    "    # NOTE: GPs don't train on batches, use full data\n",
    "\n",
    "    for run in range(NUM_RUNS):\n",
    "\n",
    "        print(f\"\\n--- Training Run {run + 1}/{NUM_RUNS} ---\")\n",
    "\n",
    "        # Initialise the likelihood for the GP model (estimates noise)\n",
    "        # NOTE: we use a multitask likelihood for the dfGP model but with a global noise term\n",
    "        likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(\n",
    "            num_tasks = 2,\n",
    "            has_global_noise = True, \n",
    "            has_task_noise = False, # HACK: This still needs to be manually turned off\n",
    "            ).to(device)\n",
    "\n",
    "        model = dfGP(\n",
    "            x_train,\n",
    "            y_train, \n",
    "            likelihood\n",
    "            ).to(device)\n",
    "        \n",
    "        # Overwrite default lengthscale hyperparameter initialisation because we have a different input scale.\n",
    "        model.base_kernel.lengthscale = torch.empty([1, 2], device = device).uniform_( * REAL_L_RANGE) * 0.5\n",
    "        # Overwrite default outputscale variance initialisation.\n",
    "        model.covar_module.outputscale = torch.empty(1, device = device).uniform_( * REAL_OUTPUTSCALE_VAR_RANGE)\n",
    "        # Overwrite default noise variance initialisation because this is real noisy data.\n",
    "        model.likelihood.noise = torch.empty(1, device = device).uniform_( * REAL_NOISE_VAR_RANGE)\n",
    "        \n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr = MODEL_LEARNING_RATE, weight_decay = WEIGHT_DECAY)\n",
    "        \n",
    "        # Use ExactMarginalLogLikelihood\n",
    "        mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "        model.train()\n",
    "        likelihood.train()\n",
    "        # _________________\n",
    "        # BEFORE EPOCH LOOP\n",
    "        \n",
    "        # Export the convergence just for first run only\n",
    "        if run == 0:\n",
    "            # initialise tensors to store losses over epochs (for convergence plot)\n",
    "            train_losses_NLML_over_epochs = torch.zeros(MAX_NUM_EPOCHS) # objective\n",
    "            train_losses_RMSE_over_epochs = torch.zeros(MAX_NUM_EPOCHS) # by-product\n",
    "            # monitor performance transfer to test (only RMSE easy to calc without covar)\n",
    "            test_losses_RMSE_over_epochs = torch.zeros(MAX_NUM_EPOCHS)\n",
    "\n",
    "            # NOTE: Here, we estimate the noise\n",
    "            l1_over_epochs = torch.zeros(MAX_NUM_EPOCHS)\n",
    "            l2_over_epochs = torch.zeros(MAX_NUM_EPOCHS)\n",
    "            outputscale_var_over_epochs = torch.zeros(MAX_NUM_EPOCHS)\n",
    "            noise_var_over_epochs = torch.zeros(MAX_NUM_EPOCHS)\n",
    "\n",
    "        # Early stopping variables\n",
    "        best_loss = float('inf')\n",
    "        # counter starts at 0\n",
    "        epochs_no_improve = 0\n",
    "\n",
    "        ############################\n",
    "        ### LOOP 3 - over EPOCHS ###\n",
    "        ############################\n",
    "        print(\"\\nStart Training\")\n",
    "\n",
    "        for epoch in range(MAX_NUM_EPOCHS):\n",
    "\n",
    "            # Set to train\n",
    "            model.train()\n",
    "            likelihood.train()\n",
    "\n",
    "            # Do a step\n",
    "            optimizer.zero_grad()\n",
    "            # model outputs a multivariate normal distribution\n",
    "            train_pred_dist = model(x_train.to(device))\n",
    "            # Train on noisy or targets\n",
    "            # NOTE: We only have observational y_train i.e. noisy data\n",
    "            loss = - mll(train_pred_dist, y_train.to(device))  # negative marginal log likelihood\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # For Run 1 we save a bunch of metrics and update, while for the rest we only update\n",
    "            if run == 0:\n",
    "\n",
    "                model.eval()\n",
    "                likelihood.eval()\n",
    "\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\", gpytorch.utils.warnings.GPInputWarning)\n",
    "                    train_pred_dist = model(x_train.to(device))\n",
    "                test_pred_dist = model(x_test.to(device))\n",
    "\n",
    "                # Compute RMSE for training and test predictions (given true data, not noisy)\n",
    "                train_RMSE = torch.sqrt(gpytorch.metrics.mean_squared_error(train_pred_dist, y_train.to(device)).mean())\n",
    "                test_RMSE = torch.sqrt(gpytorch.metrics.mean_squared_error(test_pred_dist, y_test.to(device)).mean())\n",
    "\n",
    "                # Save losses for convergence plot\n",
    "                train_losses_NLML_over_epochs[epoch] = loss.item()\n",
    "                train_losses_RMSE_over_epochs[epoch] = train_RMSE.item()\n",
    "                test_losses_RMSE_over_epochs[epoch] = test_RMSE.item()\n",
    "\n",
    "                # Save evolution of hypers for convergence plot\n",
    "                # NOTE: lengthscale is [1, 2] in shape\n",
    "                l1_over_epochs[epoch] = model.base_kernel.lengthscale[:, 0].item()\n",
    "                l2_over_epochs[epoch] = model.base_kernel.lengthscale[:, 1].item()\n",
    "                outputscale_var_over_epochs[epoch] = model.covar_module.outputscale.item()\n",
    "                noise_var_over_epochs[epoch] = model.likelihood.noise.item()\n",
    "\n",
    "                # Print a bit more information for the first run\n",
    "                if epoch % 20 == 0:\n",
    "                    print(f\"{region_name} {model_name} Run {run + 1}/{NUM_RUNS}, Epoch {epoch + 1}/{MAX_NUM_EPOCHS}, Training Loss (NLML): {loss:.4f}, Training RMSE: {train_RMSE:.4f}\")\n",
    "\n",
    "                # delete after printing and saving\n",
    "                # NOTE: keep loss for early stopping check\n",
    "                del train_pred_dist, test_pred_dist, train_RMSE, test_RMSE\n",
    "                \n",
    "                # Free up memory every 20 epochs\n",
    "                if epoch % 20 == 0:\n",
    "                    gc.collect() and torch.cuda.empty_cache()\n",
    "            \n",
    "            # For all runs after the first we run a minimal version using only lml_train\n",
    "            else:\n",
    "\n",
    "                if epoch % 20 == 0:\n",
    "                    # After run 1 we only print lml, nothing else\n",
    "                    print(f\"{region_name} {model_name} Run {run + 1}/{NUM_RUNS}, Epoch {epoch + 1}/{MAX_NUM_EPOCHS}, Training Loss (NLML): {loss:.4f}\")\n",
    "                \n",
    "            # EVERY EPOCH: Early stopping check\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                # reset counter if loss improves\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "\n",
    "            if epochs_no_improve >= PATIENCE:\n",
    "                print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
    "                # exit epoch loop\n",
    "                break\n",
    "\n",
    "        ##############################\n",
    "        ### END LOOP 3 over EPOCHS ###\n",
    "        ##############################\n",
    "\n",
    "        # for every run...\n",
    "        #######################################################\n",
    "        ### EVALUATE after all training for RUN is finished ###\n",
    "        #######################################################\n",
    "\n",
    "        model.eval()\n",
    "        likelihood.eval()\n",
    "\n",
    "        # Need gradients for autograd divergence: We clone and detach\n",
    "        x_test_grad = x_test.to(device).clone().requires_grad_(True)\n",
    "        x_train_grad = x_train.to(device).clone().requires_grad_(True)\n",
    "\n",
    "        # Underlying (latent) distribution and predictive distribution\n",
    "        dist_test = model(x_test_grad)\n",
    "        pred_dist_test = likelihood(dist_test)\n",
    "\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", gpytorch.utils.warnings.GPInputWarning)\n",
    "            dist_train = model(x_train_grad)\n",
    "            pred_dist_train = likelihood(dist_train)\n",
    "        \n",
    "        # Compute divergence field (from latent distribution)\n",
    "        test_div_field = compute_divergence_field(dist_test.mean, x_test_grad)\n",
    "        train_div_field = compute_divergence_field(dist_train.mean, x_train_grad)\n",
    "\n",
    "        # Only save mean_pred, covar_pred and divergence fields for the first run\n",
    "        if run == 0:\n",
    "\n",
    "            # (3) Since all epoch training is finished, we can save the losses over epochs\n",
    "            df_losses = pd.DataFrame({\n",
    "                'Epoch': list(range(train_losses_NLML_over_epochs.shape[0])), # pythonic indexing\n",
    "                'Train NLML': train_losses_NLML_over_epochs.tolist(),\n",
    "                'Train RMSE': train_losses_RMSE_over_epochs.tolist(),\n",
    "                'Test RMSE': test_losses_RMSE_over_epochs.tolist(),\n",
    "                # hyperparameters\n",
    "                'l1': l1_over_epochs.tolist(),\n",
    "                'l2': l2_over_epochs.tolist(),\n",
    "                'outputscale_var': outputscale_var_over_epochs.tolist(),\n",
    "                'noise_var': noise_var_over_epochs.tolist(),\n",
    "                })\n",
    "    \n",
    "\n",
    "        # Compute TRAIN metrics (convert tensors to float) for every run's tuned model\n",
    "        # NOTE: gpytorch outputs metrics per task\n",
    "        train_RMSE = torch.sqrt(gpytorch.metrics.mean_squared_error(\n",
    "            pred_dist_train, y_train.to(device)).mean()).item()\n",
    "        train_MAE = gpytorch.metrics.mean_absolute_error(\n",
    "            pred_dist_train, y_train.to(device)).mean().item()\n",
    "        train_NLL = gpytorch.metrics.negative_log_predictive_density(\n",
    "            pred_dist_train, y_train.to(device)).item()\n",
    "        train_QCE = quantile_coverage_error_2d(\n",
    "            pred_dist_train, y_train.to(device), quantile = 95.0).item()\n",
    "        ## NOTE: It is important to use the absolute value of the divergence field, since both positive and negative deviations are violations and shouldn't cancel each other out \n",
    "        train_MAD = train_div_field.abs().mean().item()\n",
    "\n",
    "        # Compute TEST metrics (convert tensors to float) for every run's tuned model\n",
    "        test_RMSE = torch.sqrt(gpytorch.metrics.mean_squared_error(\n",
    "            pred_dist_test, y_test.to(device)).mean()).item()\n",
    "        test_MAE = gpytorch.metrics.mean_absolute_error(\n",
    "            pred_dist_test, y_test.to(device)).mean().item()\n",
    "        test_NLL = gpytorch.metrics.negative_log_predictive_density(\n",
    "            pred_dist_test, y_test.to(device)).item()\n",
    "        test_QCE = quantile_coverage_error_2d(\n",
    "            pred_dist_test, y_test.to(device), quantile = 95.0).item()\n",
    "        ## NOTE: It is important to use the absolute value of the divergence field, since both positive and negative deviations are violations and shouldn't cancel each other out \n",
    "        test_MAD = test_div_field.abs().mean().item()\n",
    "\n",
    "        region_results.append([\n",
    "            run + 1,\n",
    "            train_RMSE, train_MAE, train_NLL, train_QCE, train_MAD,\n",
    "            test_RMSE, test_MAE, test_NLL, test_QCE, test_MAD\n",
    "        ])\n",
    "\n",
    "        # clean up\n",
    "        del dist_train, dist_test, pred_dist_train, pred_dist_test, test_div_field, train_div_field\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    ############################\n",
    "    ### END LOOP 2 over RUNS ###\n",
    "    ############################\n",
    "\n",
    "    print(f\"Test RMSE: {test_RMSE:.4f}, Test NLL: {test_NLL:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
