{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "import math\n",
    "\n",
    "torch.set_printoptions(sci_mode = False)\n",
    "np.set_printoptions(suppress = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid discretisation over domain [0, 1] for x1 and x2\n",
    "N_grid_x1 = 20 + 1 # 21 so steps are easy numbers\n",
    "N_grid_x2 = N_grid_x1\n",
    "\n",
    "N_grid = N_grid_x1 * N_grid_x2\n",
    "\n",
    "x1 = np.linspace(0, 1, N_grid_x1) # x\n",
    "x2 = np.linspace(0, 1, N_grid_x2) # y\n",
    "\n",
    "# define distances\n",
    "dx = 1 / (N_grid_x1 - 1)\n",
    "dy = 1 / (N_grid_x2 - 1)\n",
    "\n",
    "X1_test, X2_test = np.meshgrid(x1, x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do: normalise?\n",
    "\n",
    "This will of course impact the output scalar sigma_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_convergence(X1, X2):\n",
    "    U = X2\n",
    "    V = X1\n",
    "    return U, V\n",
    "\n",
    "def simulate_merge(X1, X2):\n",
    "    U = (X2 + 0.5)**2\n",
    "    V = np.sin(X1 * math.pi)\n",
    "    return U, V\n",
    "\n",
    "def simulate_branching(X1, X2):\n",
    "    U = X1 * X2\n",
    "    V = - 0.5 * X2**2 + (X1 - 0.8)\n",
    "    return U, V\n",
    "\n",
    "def simulate_deflection(X1, X2):\n",
    "    U = (X2 * 6 - 3)**2 + (X1 * 6 - 3)**2 + 3\n",
    "    V = -2 * (X2 * 6 - 3) * (X1 * 6 - 3)\n",
    "    return U, V\n",
    "\n",
    "def simulate_ridge(X1, X2):\n",
    "    U = X2 + 1\n",
    "    V = - np.cos(3 * X1**3 * math.pi)\n",
    "    return U, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flat [N, 1] arrays - unsqueeze for explicit last dim\n",
    "X1_train = torch.tensor(\n",
    "    [0.8750, 0.1500, 0.2500, 0.5000, 0.3853, 0.9991, 0.4333, 0.1494, 0.6196, 0.7808, 0.5609, 0.5895, 0.3395, 0.7232]).unsqueeze(-1)\n",
    "\n",
    "X2_train = torch.tensor(\n",
    "    [0.6750, 0.9750, 0.9000, 0.3500, 0.6010, 0.3451, 0.7451, 0.8499, 0.6240, 0.7218, 0.8389, 0.1664, 0.9771, 0.0598]).unsqueeze(-1)\n",
    "\n",
    "X1_test = torch.tensor(X1_test.flatten()).squeeze().unsqueeze(-1)\n",
    "X2_test = torch.tensor(X2_test.flatten()).squeeze().unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_n = torch.tensor([0.05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_train, V_train = simulate_convergence(X1_train, X2_train)\n",
    "\n",
    "U_train_noisy = U_train + sigma_n * torch.randn_like(U_train)\n",
    "V_train_noisy = V_train + sigma_n * torch.randn_like(V_train)\n",
    "\n",
    "# Concatenate along the first axis so that first half is Y1 and second half is Y2\n",
    "# torch.Size([n_train x 2, 1])\n",
    "Y_train_noisy = torch.cat((U_train_noisy, V_train_noisy), dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_rows = X1_train\n",
    "X2_rows = X2_train\n",
    "\n",
    "X1_columns = X1_test\n",
    "X2_columns = X2_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 441])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1_dist = (X1_rows.unsqueeze(1) - X1_columns.unsqueeze(0)).squeeze()\n",
    "X2_dist = (X2_rows.unsqueeze(1) - X2_columns.unsqueeze(0)).squeeze()\n",
    "\n",
    "X2_dist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengthscale = torch.tensor([0.3])\n",
    "output_scalar = torch.tensor([1.0])\n",
    "\n",
    "def divergence_free_se_kernel(X1_rows,\n",
    "                              X2_rows, \n",
    "                              X1_columns, \n",
    "                              X2_columns, \n",
    "                              hyperparameters):\n",
    "    \n",
    "    # Extract hyperparameters\n",
    "    sigma_f = hyperparameters[0]\n",
    "    l = hyperparameters[1]\n",
    "    \n",
    "    # Calculate the (not quite Euclidean) distance between all pairs of points\n",
    "    # This approach yields negative values as well\n",
    "    # X1_dist: torch.Size([n_rows, n_columns])\n",
    "    X1_dist = (X1_rows.unsqueeze(1) - X1_columns.unsqueeze(0)).squeeze()\n",
    "    # ALTERNATIVE (pos.) torch.cdist(X1_rows, X1_columns): this is just torch.abs(X1_dist)\n",
    "\n",
    "    # X2_dist: torch.Size([n_rows, n_columns])\n",
    "    X2_dist = (X2_rows.unsqueeze(1) - X2_columns.unsqueeze(0)).squeeze()\n",
    "\n",
    "    # torch.Size([n_rows, n_columns])\n",
    "    upper_left = (1 - X2_dist.square().div(l**2)).div(l**2)\n",
    "\n",
    "    # elementwise multiplication and division by scalar\n",
    "    # Matlab version has negative values here! \n",
    "    upper_right = torch.mul(X1_dist, X2_dist).div(l**4)\n",
    "    lower_left = upper_right\n",
    "    lower_right = (1 - X1_dist.square().div(l**2)).div(l**2)\n",
    "\n",
    "    # Concatenate upper and lower blocks column-wise, and then concatenate them row-wise\n",
    "    # torch.Size([2 * n_train, 2 * n_test])\n",
    "    block = torch.cat((torch.cat((upper_left, upper_right), 1), torch.cat((lower_left, lower_right), 1)), 0)\n",
    "\n",
    "    # torch.Size([2 * n_train, 2 * n_test])\n",
    "    # elementwise multiplication\n",
    "    K = sigma_f.square() * block.mul((X1_dist.square() + X2_dist.square()).div(-2 * l**2).exp().tile(2, 2))\n",
    "\n",
    "    return K\n",
    "\n",
    "def predict(X1_train,\n",
    "            X2_train,\n",
    "            Y_train_noisy,\n",
    "            X1_test, \n",
    "            X2_test, \n",
    "            hyperparameters,\n",
    "            divergence_free_bool = True):\n",
    "    \n",
    "    # Extract N_x\n",
    "    N_x_test = X1_test.shape[0]\n",
    "    \n",
    "    kernel_func = divergence_free_se_kernel if divergence_free_bool else block_diagonal_se_kernel\n",
    "\n",
    "    K_train_train = kernel_func(\n",
    "        X1_train, \n",
    "        X2_train, \n",
    "        X1_train, \n",
    "        X2_train,\n",
    "        hyperparameters)\n",
    "\n",
    "    # Add noise to the diagonal\n",
    "    K_train_train_noisy = K_train_train + torch.eye(K_train_train.shape[0]) * sigma_n**2\n",
    "\n",
    "    # torch.Size([2 * n_train, 2 * n_test])\n",
    "    # K_* in Rasmussen is (X_train, X_test)\n",
    "    K_train_test = kernel_func(\n",
    "        X1_train, \n",
    "        X2_train, \n",
    "        X1_test, \n",
    "        X2_test,\n",
    "        hyperparameters)\n",
    "\n",
    "    # torch.Size([2 * n_test, 2 * n_train])\n",
    "    K_test_train = K_train_test.mT\n",
    "\n",
    "    K_test_test = kernel_func(\n",
    "        X1_test, \n",
    "        X2_test, \n",
    "        X1_test, \n",
    "        X2_test,\n",
    "        hyperparameters)\n",
    "    \n",
    "    # Determine L - torch.Size([2 * n_train, 2 * n_train])\n",
    "    L = torch.linalg.cholesky(K_train_train_noisy, upper = False)\n",
    "    # L.T \\ (L \\ y) in one step - torch.Size([2 * n_train, 1])\n",
    "    alpha = torch.cholesky_solve(Y_train_noisy, L, upper = False)\n",
    "\n",
    "    # matrix multiplication\n",
    "    # torch.Size([2 * n_test, 2 * n_train]) * torch.Size([2 * n_train, 1])\n",
    "    # alpha needs to be changes to double because K is\n",
    "    predictive_mean = torch.matmul(K_test_train, alpha.double())\n",
    "    predictive_mean_y1 = predictive_mean[: len(predictive_mean) // 2].reshape(N_x_test, N_x_test)\n",
    "    predictive_mean_y2 = predictive_mean[len(predictive_mean) // 2 :].reshape(N_x_test, N_x_test)\n",
    "\n",
    "    # Step 3: Solve for V = L^-1 * K(X_*, X)\n",
    "    # K_* is K_train_test\n",
    "    # L is lower triangular\n",
    "    v = torch.linalg.solve_triangular(L, K_train_test, upper = False)\n",
    "    # same as\n",
    "    # v = torch.linalg.solve(L, K_train_test)\n",
    "    # torch.matmul(v, v.T) would give the wrong shape\n",
    "    predictive_covariance = K_test_test - torch.matmul(v.T, v)\n",
    "\n",
    "    # Extract variance from diagonal for either dim and reshape into square grid\n",
    "    predictive_variance_y1 = torch.diagonal(predictive_covariance)[:int(predictive_covariance.shape[0]/2)].reshape(N_x_test, N_x_test)\n",
    "    predictive_variance_y2 = torch.diagonal(predictive_covariance)[int(predictive_covariance.shape[0]/2):].reshape(N_x_test, N_x_test)\n",
    "\n",
    "    # matmul: torch.Size([1, 10]) * torch.Size([10, 1])\n",
    "    # 0.5 * y^T * alpha\n",
    "    # squeeze to remove redundant dimension\n",
    "    nlml_term1 = 0.5 * torch.matmul(Y_train_noisy.T, alpha).squeeze()\n",
    "\n",
    "    # sum(log(L_ii)))\n",
    "    nlml_term2 = torch.sum(torch.log(torch.diagonal(L)))\n",
    "\n",
    "    # Constant term - technically not optimised \n",
    "    # n/2 * log(2 * pi)\n",
    "    nlml_term3 = (Y_train_noisy.shape[0]/2) * torch.log(torch.tensor(2 * math.pi))\n",
    "\n",
    "    nlml = - nlml_term1 - nlml_term2 - nlml_term3\n",
    "\n",
    "    # return predictive_mean_y1, predictive_mean_y2, predictive_variance_y1, predictive_variance_y2, nlml\n",
    "    return predictive_mean_y1, predictive_mean_y2, predictive_variance_y1, predictive_variance_y2, nlml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 441, 1])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_diff[:, :, 0].shape\n",
    "X1_dist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In example, train are the rows\n",
    "\n",
    "X1_dist = (X1_rows.unsqueeze(1) - X1_columns.unsqueeze(0))\n",
    "\n",
    "rows = torch.cat((X1_rows, X2_rows), 1)[:, None, :]\n",
    "# [1, n_columns, 2]\n",
    "columns = torch.cat((X1_columns, X2_columns), 1)[None, :, :] \n",
    "\n",
    "scaled_diff = (rows - columns)\n",
    "\n",
    "(scaled_diff[:, :, 0] == X1_dist.squeeze()).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 441])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_diff.square().sum(dim = -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divergence_free_se_kernel(\n",
    "        row_tensor, # torch.Size([n_rows, 2])\n",
    "        column_tensor, # torch.Size([n_columns, 2])\n",
    "        hyperparameters):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculate the divergence-free SE kernel for two sets of points in 2D space.\n",
    "    R^2 -> R^2\n",
    "\n",
    "    Inputs:\n",
    "        row_tensor: torch.Size([n_rows, 2])\n",
    "        column_tensor: torch.Size([n_columns, 2])\n",
    "        hyperparameters: list of length 2 containing sigma_f and l\n",
    "\n",
    "    Returns:\n",
    "        K: torch.Size([n_rows * 2, n_columns * 2])\n",
    "    \"\"\"\n",
    "    \n",
    "    # We calculate the kernel for each pair of points\n",
    "    \n",
    "    # Extract hyperparameters\n",
    "    sigma_f = hyperparameters[0]\n",
    "    l = hyperparameters[1]\n",
    "\n",
    "    # Add dimension (broadcasting) for difference calculation\n",
    "    # torch.Size([n_rows, 1, 2]) - 1 is for n_columns\n",
    "    row_tensor_expanded = row_tensor[:, None, :]\n",
    "    # torch.Size([1, n_columns, 2]) - 1 is for n_rowns\n",
    "    column_tensor_expanded = column_tensor[None, :, :]\n",
    "\n",
    "    # [:, :, 0] are the x1 differences and [:, :, 1] are the x2 differences\n",
    "    # yields negative values as well\n",
    "    diff = row_tensor_expanded - column_tensor_expanded\n",
    "\n",
    "    ### 2x2 BLOCKS ###\n",
    "    # x2 diffs: torch.Size([n_rows, n_columns])\n",
    "    upper_left = (1 - diff[:, :, 1].square().div(l.square())).div(l.square())\n",
    "\n",
    "    # x1 diffs: torch.Size([n_rows, n_columns])\n",
    "    lower_right = (1 - diff[:, :, 0].square().div(l.square())).div(l.square())\n",
    "\n",
    "    # Elementwise multiplication of x1 and x2 diffs and division by scalar\n",
    "    # Matlab version has negative values here!\n",
    "    upper_right = torch.prod(diff, dim = -1).div(l**4)\n",
    "\n",
    "    # same as other off-diagonal block\n",
    "    lower_left = upper_right\n",
    "\n",
    "    # x1 diffs: torch.Size([n_rows, n_columns])\n",
    "    lower_right = (1 - diff[:, :, 0].square().div(l.square())).div(l.square())\n",
    "\n",
    "    # Concatenate upper and lower blocks column-wise, and then concatenate them row-wise\n",
    "    # torch.Size([2 * n_train, 2 * n_test])\n",
    "    blocks = torch.cat((\n",
    "        torch.cat((upper_left, upper_right), 1), \n",
    "        torch.cat((lower_left, lower_right), 1)\n",
    "        ), 0)\n",
    "\n",
    "    # torch.Size([2 * n_row, 2 * n_column])\n",
    "    # elementwise multiplication\n",
    "    # sum squared difference over x1 and x2, divide by -2 * l^2, and exponentiate. Tile for blocks\n",
    "    K = sigma_f.square() * blocks.mul(diff.square().sum(dim = -1).div(-2 * l.square()).exp().tile(2, 2))\n",
    "\n",
    "    return K"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "def divergence_free_se_kernel(X1_rows,\n",
    "                              X2_rows, \n",
    "                              X1_columns, \n",
    "                              X2_columns, \n",
    "                              hyperparameters):\n",
    "    \n",
    "    # Extract hyperparameters\n",
    "    sigma_f = hyperparameters[0]\n",
    "    l = hyperparameters[1]\n",
    "    \n",
    "    # Calculate the (not quite Euclidean) distance between all pairs of points\n",
    "    # This approach yields negative values as well\n",
    "    # X1_dist: torch.Size([n_rows, n_columns])\n",
    "    X1_dist = (X1_rows.unsqueeze(1) - X1_columns.unsqueeze(0)).squeeze()\n",
    "    # ALTERNATIVE (pos.) torch.cdist(X1_rows, X1_columns): this is just torch.abs(X1_dist)\n",
    "\n",
    "    # X2_dist: torch.Size([n_rows, n_columns])\n",
    "    X2_dist = (X2_rows.unsqueeze(1) - X2_columns.unsqueeze(0)).squeeze()\n",
    "\n",
    "    # torch.Size([n_rows, n_columns])\n",
    "    upper_left = (1 - X2_dist.square().div(l**2)).div(l**2)\n",
    "\n",
    "    # elementwise multiplication and division by scalar\n",
    "    # Matlab version has negative values here! \n",
    "    upper_right = torch.mul(X1_dist, X2_dist).div(l**4)\n",
    "    lower_left = upper_right\n",
    "    lower_right = (1 - X1_dist.square().div(l**2)).div(l**2)\n",
    "\n",
    "    # Concatenate upper and lower blocks column-wise, and then concatenate them row-wise\n",
    "    # torch.Size([2 * n_train, 2 * n_test])\n",
    "    block = torch.cat((torch.cat((upper_left, upper_right), 1), torch.cat((lower_left, lower_right), 1)), 0)\n",
    "\n",
    "    # torch.Size([2 * n_train, 2 * n_test])\n",
    "    # elementwise multiplication\n",
    "    K = sigma_f.square() * block.mul((X1_dist.square() + X2_dist.square()).div(-2 * l**2).exp().tile(2, 2))\n",
    "\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5430, 0.5666, 0.5897,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.6147, 0.6186, 0.6209,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.6465, 0.6538, 0.6595,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        ...,\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.6732, 0.6620, 0.6494],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.8544, 0.8298, 0.8038],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.6328, 0.6264, 0.6186]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1_rows\n",
    "\n",
    "# reshape to [n_rows, 2]\n",
    "rows = torch.cat((X1_rows, X2_rows), 1)[:, None, :]\n",
    "# [1, n_columns, 2]\n",
    "columns = torch.cat((X1_columns, X2_columns), 1)[None, :, :] \n",
    "\n",
    "scaled_diff = (rows - columns)\n",
    "sqdist = torch.sum(scaled_diff ** 2, dim = -1)\n",
    "\n",
    "K_SE = torch.exp(-0.5 * sqdist)\n",
    "\n",
    "B = torch.tensor([[1.0, 0.0], [0.0, 1.0]])\n",
    "\n",
    "torch.kron(B, K_SE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def block_diagonal_se_kernel(\n",
    "        X1_rows,\n",
    "        X2_rows, \n",
    "        X1_columns, \n",
    "        X2_columns, \n",
    "        hyperparameters):\n",
    "    \n",
    "    # block diagonal R^2 -> R^2\n",
    "    # correlations between outputs are not considered\n",
    "    \n",
    "    # Extract hyperparameters\n",
    "    sigma_f = hyperparameters[0]\n",
    "    l = hyperparameters[1]\n",
    "    B = hyperparameters[2]\n",
    "\n",
    "    # reshape to [n_rows, 2] and then add a dimension [n_rows, 1, 2]\n",
    "    rows = torch.cat((X1_rows, X2_rows), 1)[:, None, :]\n",
    "    # [1, n_columns, 2]\n",
    "    columns = torch.cat((X1_columns, X2_columns), 1)[None, :, :] \n",
    "\n",
    "    # difference scaled by lengthscales (2D)\n",
    "    # torch.Size([n_rows, n_columns, 2])\n",
    "    # Mahalanobis Distance (scaled)\n",
    "    scaled_diff = (rows - columns) / l**2\n",
    "\n",
    "    # square and reduce dimensions\n",
    "    # torch.Size([n_rows, n_columns])\n",
    "    sqdist = torch.sum(scaled_diff ** 2, dim = -1)\n",
    "\n",
    "    # only one signal variance\n",
    "    K_SE = sigma_f**2 * torch.exp(-0.5 * sqdist)\n",
    "    \n",
    "    # B is 2 x 2 and defines the cross correlation\n",
    "    # Kronecker produces block diagonal\n",
    "    K = torch.kron(B, K_SE)\n",
    "\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define initial hyperparameters\n",
    "hyperparameters_df = torch.tensor([\n",
    "    torch.tensor([0.05]), \n",
    "    torch.tensor([0.6])], \n",
    "    requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8750, 0.6750],\n",
       "        [0.1500, 0.9750],\n",
       "        [0.2500, 0.9000],\n",
       "        [0.5000, 0.3500],\n",
       "        [0.3853, 0.6010],\n",
       "        [0.9991, 0.3451],\n",
       "        [0.4333, 0.7451],\n",
       "        [0.1494, 0.8499],\n",
       "        [0.6196, 0.6240],\n",
       "        [0.7808, 0.7218],\n",
       "        [0.5609, 0.8389],\n",
       "        [0.5895, 0.1664],\n",
       "        [0.3395, 0.9771],\n",
       "        [0.7232, 0.0598]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((X1_train, X2_train), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_f = torch.tensor([0.05], requires_grad = True)\n",
    "l = torch.tensor([0.6], requires_grad = True)\n",
    "\n",
    "# list of tensors\n",
    "hyperparameters_df = [sigma_f, l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_f = torch.tensor([0.05], requires_grad = True)\n",
    "l = torch.tensor([0.6, 0.6], requires_grad = True)\n",
    "B = torch.tensor([[1.0, 0.0], [0.0, 1.0]], requires_grad = True)\n",
    "\n",
    "hyperparameters_bd = [sigma_f, l, B]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictive_mean_y1, predictive_mean_y2, predictive_variance_y1, predictive_variance_y2, nlml = predict(\n",
    "    X1_train,\n",
    "    X2_train,\n",
    "    Y_train_noisy,\n",
    "    X1_test, \n",
    "    X2_test, \n",
    "    hyperparameters_df,\n",
    "    divergence_free_bool = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[441, 441]' is invalid for input of size 441",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m predictive_mean_y1, predictive_mean_y2, predictive_variance_y1, predictive_variance_y2, nlml \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX1_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX2_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mY_train_noisy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX1_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX2_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhyperparameters_bd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdivergence_free_bool\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[74], line 93\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(X1_train, X2_train, Y_train_noisy, X1_test, X2_test, hyperparameters, divergence_free_bool)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# matrix multiplication\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# torch.Size([2 * n_test, 2 * n_train]) * torch.Size([2 * n_train, 1])\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# alpha needs to be changes to double because K is\u001b[39;00m\n\u001b[1;32m     92\u001b[0m predictive_mean \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(K_test_train, alpha\u001b[38;5;241m.\u001b[39mdouble())\n\u001b[0;32m---> 93\u001b[0m predictive_mean_y1 \u001b[38;5;241m=\u001b[39m \u001b[43mpredictive_mean\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpredictive_mean\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mN_x_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_x_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m predictive_mean_y2 \u001b[38;5;241m=\u001b[39m predictive_mean[\u001b[38;5;28mlen\u001b[39m(predictive_mean) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m :]\u001b[38;5;241m.\u001b[39mreshape(N_x_test, N_x_test)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Step 3: Solve for V = L^-1 * K(X_*, X)\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# K_* is K_train_test\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# L is lower triangular\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[441, 441]' is invalid for input of size 441"
     ]
    }
   ],
   "source": [
    "predictive_mean_y1, predictive_mean_y2, predictive_variance_y1, predictive_variance_y2, nlml = predict(\n",
    "    X1_train,\n",
    "    X2_train,\n",
    "    Y_train_noisy,\n",
    "    X1_test, \n",
    "    X2_test, \n",
    "    hyperparameters_bd,\n",
    "    divergence_free_bool = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0: NLML = -2186.07177734375, Hypers = (0.009983333, 1.303558)\n",
      "Iter 10: NLML = -2114.39599609375, Hypers = (0.0017196932, 1.3740392)\n",
      "Iter 20: NLML = -2151.230224609375, Hypers = (0.0021803058, 1.4258841)\n",
      "Iter 30: NLML = -2176.03955078125, Hypers = (0.0016361853, 1.4572905)\n",
      "Iter 40: NLML = -2183.72900390625, Hypers = (0.0009911356, 1.4742708)\n",
      "Iter 50: NLML = -2185.247802734375, Hypers = (0.00060369703, 1.4827492)\n",
      "Iter 60: NLML = -2185.60986328125, Hypers = (0.00036646012, 1.4867553)\n",
      "Iter 70: NLML = -2185.84765625, Hypers = (0.00016836595, 1.4885722)\n",
      "Iter 80: NLML = -2186.035888671875, Hypers = (2.7742935e-06, 1.4893696)\n",
      "Iter 90: NLML = -2186.07080078125, Hypers = (-7.0177135e-05, 1.4897105)\n",
      "Final Hypers: (-5.102157e-05, 1.4898444)\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam([sigma_f, l], lr = 0.01)\n",
    "\n",
    "# Optimization loop\n",
    "for i in range(100):  # Number of iterations\n",
    "    optimizer.zero_grad()  # Reset gradients\n",
    "\n",
    "    # Compute nlml\n",
    "    _, _, _, _, nlml = predict(\n",
    "        X1_train,\n",
    "        X2_train,\n",
    "        Y_train_noisy,\n",
    "        X1_test, \n",
    "        X2_test, \n",
    "        hyperparameters_df,\n",
    "        divergence_free_bool = True) # Compute NLML\n",
    "\n",
    "    # Assign as loss \n",
    "    loss = nlml\n",
    "    loss.backward()  # Compute gradients\n",
    "\n",
    "    optimizer.step()  # Update hypers\n",
    "\n",
    "    if i % 10 == 0:  # Print every 10 iterations\n",
    "        print(f\"Iter {i}: NLML = {loss.item()}, Hypers = {sigma_f.detach().numpy()[0], l.detach().numpy()[0]}\")\n",
    "\n",
    "print(f\"Final Hypers: {sigma_f.detach().numpy()[0], l.detach().numpy()[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00005945, 0.8900958 ], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparameters.detach().numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
