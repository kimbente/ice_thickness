{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.func import jacrev, jacfwd, vmap\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt # need 3.10 plus for \"berlin\" cmap\n",
    "import matplotlib.colors as mcolors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgAAAABACAYAAABsv8+/AAAAFXRFWHRUaXRsZQBiZXJsaW4gY29sb3JtYXAy4K9UAAAAG3RFWHREZXNjcmlwdGlvbgBiZXJsaW4gY29sb3JtYXB7d7ewAAAAMHRFWHRBdXRob3IATWF0cGxvdGxpYiB2My45LjIsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmdhmcVTAAAAMnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHYzLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZ08/WnQAAAItSURBVHic7dZBEuIgEAVQYu5/hbnCnFCYhUErJIg12//ehuqm6ejub3/+1lZKKbW0Ukopz+PsdW2Tfp9vk36p39+P/VaHerZnqI93n99Tz/vaZH42V5/n+zrMX+5fdXvPDfXivtVVfZyTd+XHPaV9nyuT71364/xk7+V3tbHf6+ewZ5hrs73n75fp/53Ul/f3/3u25zPfbucv9XjWNrkf+696u9zf93u99br82C9tcZZF/zi3c/8x6Y/nY7vvX/dM+os9Y3/c96m3xX2vt9v7R3/fv/fed+5/5ibn47G47/Vrbh/q9/2wZ+/1uz/U47vj3C9z/dwX+1bn/vXdNvvO5X8s7qfvz/19+b9+2/Nrf/q9af++3vf/3FMAgDgCAAAEEgAAIJAAAACBBAAACCQAAEAgAQAAAgkAABBIAACAQAIAAAQSAAAgkAAAAIEEAAAIJAAAQCABAAACCQAAEEgAAIBAAgAABBIAACCQAAAAgQQAAAgkAABAIAEAAAIJAAAQSAAAgEACAAAEEgAAIJAAAACBBAAACCQAAEAgAQAAAgkAABBIAACAQAIAAAQSAAAgkAAAAIEEAAAIJAAAQCABAAACCQAAEEgAAIBAAgAABBIAACCQAAAAgQQAAAgkAABAIAEAAAIJAAAQSAAAgEACAAAEEgAAIJAAAACBBAAACCQAAEAgAQAAAgkAABBIAACAQAIAAAQSAAAgkAAAAIEEAAAIJAAAQCABAAAC/QPNUU7m7EFKwQAAAABJRU5ErkJggg==",
      "text/html": [
       "<div style=\"vertical-align: middle;\"><strong>berlin</strong> </div><div class=\"cmap\"><img alt=\"berlin colormap\" title=\"berlin\" style=\"border: 1px solid #555;\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAgAAAABACAYAAABsv8+/AAAAFXRFWHRUaXRsZQBiZXJsaW4gY29sb3JtYXAy4K9UAAAAG3RFWHREZXNjcmlwdGlvbgBiZXJsaW4gY29sb3JtYXB7d7ewAAAAMHRFWHRBdXRob3IATWF0cGxvdGxpYiB2My45LjIsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmdhmcVTAAAAMnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHYzLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZ08/WnQAAAItSURBVHic7dZBEuIgEAVQYu5/hbnCnFCYhUErJIg12//ehuqm6ejub3/+1lZKKbW0Ukopz+PsdW2Tfp9vk36p39+P/VaHerZnqI93n99Tz/vaZH42V5/n+zrMX+5fdXvPDfXivtVVfZyTd+XHPaV9nyuT71364/xk7+V3tbHf6+ewZ5hrs73n75fp/53Ul/f3/3u25zPfbucv9XjWNrkf+696u9zf93u99br82C9tcZZF/zi3c/8x6Y/nY7vvX/dM+os9Y3/c96m3xX2vt9v7R3/fv/fed+5/5ibn47G47/Vrbh/q9/2wZ+/1uz/U47vj3C9z/dwX+1bn/vXdNvvO5X8s7qfvz/19+b9+2/Nrf/q9af++3vf/3FMAgDgCAAAEEgAAIJAAAACBBAAACCQAAEAgAQAAAgkAABBIAACAQAIAAAQSAAAgkAAAAIEEAAAIJAAAQCABAAACCQAAEEgAAIBAAgAABBIAACCQAAAAgQQAAAgkAABAIAEAAAIJAAAQSAAAgEACAAAEEgAAIJAAAACBBAAACCQAAEAgAQAAAgkAABBIAACAQAIAAAQSAAAgkAAAAIEEAAAIJAAAQCABAAACCQAAEEgAAIBAAgAABBIAACCQAAAAgQQAAAgkAABAIAEAAAIJAAAQSAAAgEACAAAEEgAAIJAAAACBBAAACCQAAEAgAQAAAgkAABBIAACAQAIAAAQSAAAgkAAAAIEEAAAIJAAAQCABAAAC/QPNUU7m7EFKwQAAAABJRU5ErkJggg==\"></div><div style=\"vertical-align: middle; max-width: 514px; display: flex; justify-content: space-between;\"><div style=\"float: left;\"><div title=\"#9eb0ffff\" style=\"display: inline-block; width: 1em; height: 1em; margin: 0; vertical-align: middle; border: 1px solid #555; background-color: #9eb0ffff;\"></div> under</div><div style=\"margin: 0 auto; display: inline-block;\">bad <div title=\"#00000000\" style=\"display: inline-block; width: 1em; height: 1em; margin: 0; vertical-align: middle; border: 1px solid #555; background-color: #00000000;\"></div></div><div style=\"float: right;\">over <div title=\"#ffadadff\" style=\"display: inline-block; width: 1em; height: 1em; margin: 0; vertical-align: middle; border: 1px solid #555; background-color: #ffadadff;\"></div></div></div>"
      ],
      "text/plain": [
       "<matplotlib.colors.ListedColormap at 0x7f859486a2e0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate Berlin colormap although it is only available in Matplotlib 3.10\n",
    "# print(matplotlib.__version__)\n",
    "\n",
    "_berlin_data = [\n",
    "    [0.62108, 0.69018, 0.99951],\n",
    "    [0.61216, 0.68923, 0.99537],\n",
    "    [0.6032, 0.68825, 0.99124],\n",
    "    [0.5942, 0.68726, 0.98709],\n",
    "    [0.58517, 0.68625, 0.98292],\n",
    "    [0.57609, 0.68522, 0.97873],\n",
    "    [0.56696, 0.68417, 0.97452],\n",
    "    [0.55779, 0.6831, 0.97029],\n",
    "    [0.54859, 0.68199, 0.96602],\n",
    "    [0.53933, 0.68086, 0.9617],\n",
    "    [0.53003, 0.67969, 0.95735],\n",
    "    [0.52069, 0.67848, 0.95294],\n",
    "    [0.51129, 0.67723, 0.94847],\n",
    "    [0.50186, 0.67591, 0.94392],\n",
    "    [0.49237, 0.67453, 0.9393],\n",
    "    [0.48283, 0.67308, 0.93457],\n",
    "    [0.47324, 0.67153, 0.92975],\n",
    "    [0.46361, 0.6699, 0.92481],\n",
    "    [0.45393, 0.66815, 0.91974],\n",
    "    [0.44421, 0.66628, 0.91452],\n",
    "    [0.43444, 0.66427, 0.90914],\n",
    "    [0.42465, 0.66212, 0.90359],\n",
    "    [0.41482, 0.65979, 0.89785],\n",
    "    [0.40498, 0.65729, 0.89191],\n",
    "    [0.39514, 0.65458, 0.88575],\n",
    "    [0.3853, 0.65167, 0.87937],\n",
    "    [0.37549, 0.64854, 0.87276],\n",
    "    [0.36574, 0.64516, 0.8659],\n",
    "    [0.35606, 0.64155, 0.8588],\n",
    "    [0.34645, 0.63769, 0.85145],\n",
    "    [0.33698, 0.63357, 0.84386],\n",
    "    [0.32764, 0.62919, 0.83602],\n",
    "    [0.31849, 0.62455, 0.82794],\n",
    "    [0.30954, 0.61966, 0.81963],\n",
    "    [0.30078, 0.6145, 0.81111],\n",
    "    [0.29231, 0.60911, 0.80238],\n",
    "    [0.2841, 0.60348, 0.79347],\n",
    "    [0.27621, 0.59763, 0.78439],\n",
    "    [0.26859, 0.59158, 0.77514],\n",
    "    [0.26131, 0.58534, 0.76578],\n",
    "    [0.25437, 0.57891, 0.7563],\n",
    "    [0.24775, 0.57233, 0.74672],\n",
    "    [0.24146, 0.5656, 0.73707],\n",
    "    [0.23552, 0.55875, 0.72735],\n",
    "    [0.22984, 0.5518, 0.7176],\n",
    "    [0.2245, 0.54475, 0.7078],\n",
    "    [0.21948, 0.53763, 0.698],\n",
    "    [0.21469, 0.53043, 0.68819],\n",
    "    [0.21017, 0.52319, 0.67838],\n",
    "    [0.20589, 0.5159, 0.66858],\n",
    "    [0.20177, 0.5086, 0.65879],\n",
    "    [0.19788, 0.50126, 0.64903],\n",
    "    [0.19417, 0.4939, 0.63929],\n",
    "    [0.19056, 0.48654, 0.62957],\n",
    "    [0.18711, 0.47918, 0.6199],\n",
    "    [0.18375, 0.47183, 0.61024],\n",
    "    [0.1805, 0.46447, 0.60062],\n",
    "    [0.17737, 0.45712, 0.59104],\n",
    "    [0.17426, 0.44979, 0.58148],\n",
    "    [0.17122, 0.44247, 0.57197],\n",
    "    [0.16824, 0.43517, 0.56249],\n",
    "    [0.16529, 0.42788, 0.55302],\n",
    "    [0.16244, 0.42061, 0.5436],\n",
    "    [0.15954, 0.41337, 0.53421],\n",
    "    [0.15674, 0.40615, 0.52486],\n",
    "    [0.15391, 0.39893, 0.51552],\n",
    "    [0.15112, 0.39176, 0.50623],\n",
    "    [0.14835, 0.38459, 0.49697],\n",
    "    [0.14564, 0.37746, 0.48775],\n",
    "    [0.14288, 0.37034, 0.47854],\n",
    "    [0.14014, 0.36326, 0.46939],\n",
    "    [0.13747, 0.3562, 0.46024],\n",
    "    [0.13478, 0.34916, 0.45115],\n",
    "    [0.13208, 0.34215, 0.44209],\n",
    "    [0.1294, 0.33517, 0.43304],\n",
    "    [0.12674, 0.3282, 0.42404],\n",
    "    [0.12409, 0.32126, 0.41507],\n",
    "    [0.12146, 0.31435, 0.40614],\n",
    "    [0.1189, 0.30746, 0.39723],\n",
    "    [0.11632, 0.30061, 0.38838],\n",
    "    [0.11373, 0.29378, 0.37955],\n",
    "    [0.11119, 0.28698, 0.37075],\n",
    "    [0.10861, 0.28022, 0.362],\n",
    "    [0.10616, 0.2735, 0.35328],\n",
    "    [0.10367, 0.26678, 0.34459],\n",
    "    [0.10118, 0.26011, 0.33595],\n",
    "    [0.098776, 0.25347, 0.32734],\n",
    "    [0.096347, 0.24685, 0.31878],\n",
    "    [0.094059, 0.24026, 0.31027],\n",
    "    [0.091788, 0.23373, 0.30176],\n",
    "    [0.089506, 0.22725, 0.29332],\n",
    "    [0.087341, 0.2208, 0.28491],\n",
    "    [0.085142, 0.21436, 0.27658],\n",
    "    [0.083069, 0.20798, 0.26825],\n",
    "    [0.081098, 0.20163, 0.25999],\n",
    "    [0.07913, 0.19536, 0.25178],\n",
    "    [0.077286, 0.18914, 0.24359],\n",
    "    [0.075571, 0.18294, 0.2355],\n",
    "    [0.073993, 0.17683, 0.22743],\n",
    "    [0.07241, 0.17079, 0.21943],\n",
    "    [0.071045, 0.1648, 0.2115],\n",
    "    [0.069767, 0.1589, 0.20363],\n",
    "    [0.068618, 0.15304, 0.19582],\n",
    "    [0.06756, 0.14732, 0.18812],\n",
    "    [0.066665, 0.14167, 0.18045],\n",
    "    [0.065923, 0.13608, 0.17292],\n",
    "    [0.065339, 0.1307, 0.16546],\n",
    "    [0.064911, 0.12535, 0.15817],\n",
    "    [0.064636, 0.12013, 0.15095],\n",
    "    [0.064517, 0.11507, 0.14389],\n",
    "    [0.064554, 0.11022, 0.13696],\n",
    "    [0.064749, 0.10543, 0.13023],\n",
    "    [0.0651, 0.10085, 0.12357],\n",
    "    [0.065383, 0.096469, 0.11717],\n",
    "    [0.065574, 0.092338, 0.11101],\n",
    "    [0.065892, 0.088201, 0.10498],\n",
    "    [0.066388, 0.084134, 0.099288],\n",
    "    [0.067108, 0.080051, 0.093829],\n",
    "    [0.068193, 0.076099, 0.08847],\n",
    "    [0.06972, 0.072283, 0.083025],\n",
    "    [0.071639, 0.068654, 0.077544],\n",
    "    [0.073978, 0.065058, 0.07211],\n",
    "    [0.076596, 0.061657, 0.066651],\n",
    "    [0.079637, 0.05855, 0.061133],\n",
    "    [0.082963, 0.055666, 0.055745],\n",
    "    [0.086537, 0.052997, 0.050336],\n",
    "    [0.090315, 0.050699, 0.04504],\n",
    "    [0.09426, 0.048753, 0.039773],\n",
    "    [0.098319, 0.047041, 0.034683],\n",
    "    [0.10246, 0.045624, 0.030074],\n",
    "    [0.10673, 0.044705, 0.026012],\n",
    "    [0.11099, 0.043972, 0.022379],\n",
    "    [0.11524, 0.043596, 0.01915],\n",
    "    [0.11955, 0.043567, 0.016299],\n",
    "    [0.12381, 0.043861, 0.013797],\n",
    "    [0.1281, 0.044459, 0.011588],\n",
    "    [0.13232, 0.045229, 0.0095315],\n",
    "    [0.13645, 0.046164, 0.0078947],\n",
    "    [0.14063, 0.047374, 0.006502],\n",
    "    [0.14488, 0.048634, 0.0053266],\n",
    "    [0.14923, 0.049836, 0.0043455],\n",
    "    [0.15369, 0.050997, 0.0035374],\n",
    "    [0.15831, 0.05213, 0.0028824],\n",
    "    [0.16301, 0.053218, 0.0023628],\n",
    "    [0.16781, 0.05424, 0.0019629],\n",
    "    [0.17274, 0.055172, 0.001669],\n",
    "    [0.1778, 0.056018, 0.0014692],\n",
    "    [0.18286, 0.05682, 0.0013401],\n",
    "    [0.18806, 0.057574, 0.0012617],\n",
    "    [0.19323, 0.058514, 0.0012261],\n",
    "    [0.19846, 0.05955, 0.0012271],\n",
    "    [0.20378, 0.060501, 0.0012601],\n",
    "    [0.20909, 0.061486, 0.0013221],\n",
    "    [0.21447, 0.06271, 0.0014116],\n",
    "    [0.2199, 0.063823, 0.0015287],\n",
    "    [0.22535, 0.065027, 0.0016748],\n",
    "    [0.23086, 0.066297, 0.0018529],\n",
    "    [0.23642, 0.067645, 0.0020675],\n",
    "    [0.24202, 0.069092, 0.0023247],\n",
    "    [0.24768, 0.070458, 0.0026319],\n",
    "    [0.25339, 0.071986, 0.0029984],\n",
    "    [0.25918, 0.07364, 0.003435],\n",
    "    [0.265, 0.075237, 0.0039545],\n",
    "    [0.27093, 0.076965, 0.004571],\n",
    "    [0.27693, 0.078822, 0.0053006],\n",
    "    [0.28302, 0.080819, 0.0061608],\n",
    "    [0.2892, 0.082879, 0.0071713],\n",
    "    [0.29547, 0.085075, 0.0083494],\n",
    "    [0.30186, 0.08746, 0.0097258],\n",
    "    [0.30839, 0.089912, 0.011455],\n",
    "    [0.31502, 0.09253, 0.013324],\n",
    "    [0.32181, 0.095392, 0.015413],\n",
    "    [0.32874, 0.098396, 0.01778],\n",
    "    [0.3358, 0.10158, 0.020449],\n",
    "    [0.34304, 0.10498, 0.02344],\n",
    "    [0.35041, 0.10864, 0.026771],\n",
    "    [0.35795, 0.11256, 0.030456],\n",
    "    [0.36563, 0.11666, 0.034571],\n",
    "    [0.37347, 0.12097, 0.039115],\n",
    "    [0.38146, 0.12561, 0.043693],\n",
    "    [0.38958, 0.13046, 0.048471],\n",
    "    [0.39785, 0.13547, 0.053136],\n",
    "    [0.40622, 0.1408, 0.057848],\n",
    "    [0.41469, 0.14627, 0.062715],\n",
    "    [0.42323, 0.15198, 0.067685],\n",
    "    [0.43184, 0.15791, 0.073044],\n",
    "    [0.44044, 0.16403, 0.07862],\n",
    "    [0.44909, 0.17027, 0.084644],\n",
    "    [0.4577, 0.17667, 0.090869],\n",
    "    [0.46631, 0.18321, 0.097335],\n",
    "    [0.4749, 0.18989, 0.10406],\n",
    "    [0.48342, 0.19668, 0.11104],\n",
    "    [0.49191, 0.20352, 0.11819],\n",
    "    [0.50032, 0.21043, 0.1255],\n",
    "    [0.50869, 0.21742, 0.13298],\n",
    "    [0.51698, 0.22443, 0.14062],\n",
    "    [0.5252, 0.23154, 0.14835],\n",
    "    [0.53335, 0.23862, 0.15626],\n",
    "    [0.54144, 0.24575, 0.16423],\n",
    "    [0.54948, 0.25292, 0.17226],\n",
    "    [0.55746, 0.26009, 0.1804],\n",
    "    [0.56538, 0.26726, 0.18864],\n",
    "    [0.57327, 0.27446, 0.19692],\n",
    "    [0.58111, 0.28167, 0.20524],\n",
    "    [0.58892, 0.28889, 0.21362],\n",
    "    [0.59672, 0.29611, 0.22205],\n",
    "    [0.60448, 0.30335, 0.23053],\n",
    "    [0.61223, 0.31062, 0.23905],\n",
    "    [0.61998, 0.31787, 0.24762],\n",
    "    [0.62771, 0.32513, 0.25619],\n",
    "    [0.63544, 0.33244, 0.26481],\n",
    "    [0.64317, 0.33975, 0.27349],\n",
    "    [0.65092, 0.34706, 0.28218],\n",
    "    [0.65866, 0.3544, 0.29089],\n",
    "    [0.66642, 0.36175, 0.29964],\n",
    "    [0.67419, 0.36912, 0.30842],\n",
    "    [0.68198, 0.37652, 0.31722],\n",
    "    [0.68978, 0.38392, 0.32604],\n",
    "    [0.6976, 0.39135, 0.33493],\n",
    "    [0.70543, 0.39879, 0.3438],\n",
    "    [0.71329, 0.40627, 0.35272],\n",
    "    [0.72116, 0.41376, 0.36166],\n",
    "    [0.72905, 0.42126, 0.37062],\n",
    "    [0.73697, 0.4288, 0.37962],\n",
    "    [0.7449, 0.43635, 0.38864],\n",
    "    [0.75285, 0.44392, 0.39768],\n",
    "    [0.76083, 0.45151, 0.40675],\n",
    "    [0.76882, 0.45912, 0.41584],\n",
    "    [0.77684, 0.46676, 0.42496],\n",
    "    [0.78488, 0.47441, 0.43409],\n",
    "    [0.79293, 0.48208, 0.44327],\n",
    "    [0.80101, 0.48976, 0.45246],\n",
    "    [0.80911, 0.49749, 0.46167],\n",
    "    [0.81722, 0.50521, 0.47091],\n",
    "    [0.82536, 0.51296, 0.48017],\n",
    "    [0.83352, 0.52073, 0.48945],\n",
    "    [0.84169, 0.52853, 0.49876],\n",
    "    [0.84988, 0.53634, 0.5081],\n",
    "    [0.85809, 0.54416, 0.51745],\n",
    "    [0.86632, 0.55201, 0.52683],\n",
    "    [0.87457, 0.55988, 0.53622],\n",
    "    [0.88283, 0.56776, 0.54564],\n",
    "    [0.89111, 0.57567, 0.55508],\n",
    "    [0.89941, 0.58358, 0.56455],\n",
    "    [0.90772, 0.59153, 0.57404],\n",
    "    [0.91603, 0.59949, 0.58355],\n",
    "    [0.92437, 0.60747, 0.59309],\n",
    "    [0.93271, 0.61546, 0.60265],\n",
    "    [0.94108, 0.62348, 0.61223],\n",
    "    [0.94945, 0.63151, 0.62183],\n",
    "    [0.95783, 0.63956, 0.63147],\n",
    "    [0.96622, 0.64763, 0.64111],\n",
    "    [0.97462, 0.65572, 0.65079],\n",
    "    [0.98303, 0.66382, 0.66049],\n",
    "    [0.99145, 0.67194, 0.67022],\n",
    "    [0.99987, 0.68007, 0.67995]]\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "cmaps = {\n",
    "    name: ListedColormap(data, name = name) for name, data in [\n",
    "        ('berlin', _berlin_data),\n",
    "    ]}\n",
    "\n",
    "cmaps['berlin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed = 42):\n",
    "    np.random.seed(seed)            # NumPy\n",
    "    torch.manual_seed(seed)         # PyTorch CPU\n",
    "    torch.cuda.manual_seed(seed)    # PyTorch GPU (single-GPU)\n",
    "\n",
    "# Set seed before initializing the model\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix field reconstruction\n",
    "\n",
    "The **matrix field reconstruction** has one less Jacobian step than the **vector field reconstruction** which is why it is computationally more efficient.\n",
    "\n",
    "We use the following deterministic transformations to get from the NN output to a divergence-free vector field v:\n",
    "1. Parameterise the Skew-Symmetric decomposition of A\n",
    "    - **U_topright = NN(x)** (non-zero values U_topright (N x 1) for the Upper Triangular U (N x dims x dims). For 2D, the dimensionality of U_topright is just 1, but generally it is (dims(dims - 1)/2)).\n",
    "2. Construct anti-symmetric matrix A\n",
    "    - **A = U - U.T**\n",
    "3. Attain divergence-free vector field v via row-wise Jacobians\n",
    "    - **v = (div(A[0,:]), div(A[1,:]))**, trace of the Jacobian\n",
    "\n",
    "To check that this is divergence free we need another Jacobian.\n",
    "\n",
    "see https://github.com/facebookresearch/neural-conservation-law/blob/main/pytorch/divfree.py\n",
    "\n",
    "Dimensionalities:\n",
    "- If our input in (4 x 4, 2) so flat that is (16, 2) coordinate pairs, U_topright should be (16, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "- Do we use a NN that processes a batch or points individually? (Batch)\n",
    "- What is the output shape of u_v?\n",
    "    - For the dim = 2 case, do it is always (dim * (dim - 1) / 2), which is 1 because we only estimate the upper right corner of every (N) 2 x 2 matrix\n",
    "    - Can we build this directly into the net?\n",
    "\n",
    "The model seems to only be implemented under [jax > models.py > Divfree()](https://github.com/facebookresearch/neural-conservation-law/blob/20a403d00affad905d1c47b041bc60d0ff0ea360/jax/models.py#L118). DivfreeSparse() and DivFreeImplicit() are not used anywhere.\n",
    "\n",
    "The model is used in [jax > hh_experiment_DivFree.py](https://github.com/facebookresearch/neural-conservation-law/blob/20a403d00affad905d1c47b041bc60d0ff0ea360/jax/hh_experiment_DivFree.py#L53). Hodge decomp.\n",
    "\n",
    "dim = 10.  \n",
    "mlp = MLP(depth = layers, width = width, act = act, out_dim = **dim * (dim-1) // 2**, std = 1, bias = True)\n",
    "\n",
    "For dim = 2, at each point, each matrix A (2 x 2) is antisymm. So we only have to estimate a scalar for each input point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "u_fn, params, _ = build_divfree_vector_field(self.module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim = 2, hidden_dim = 32):\n",
    "        super().__init__()\n",
    "        \n",
    "        output_dim = int((input_dim * (input_dim - 1)) / 2)\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # put deterministic transformations here with torch functional\n",
    "        return self.net(x)  # Output shape: (4,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_compute_A(nn.Module):\n",
    "    def __init__(self, input_dim = 2, hidden_dim = 32):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        # Output dim follows input dims\n",
    "        # for 2D input the NN output dim is 1\n",
    "        self.output_dim = int((input_dim * (input_dim - 1)) / 2)\n",
    "\n",
    "        # Replace with something more sophisticated\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, self.output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # put deterministic transformations here with torch functional\n",
    "\n",
    "        # RUN THROUGH NET\n",
    "        U_fill = self.net(x)\n",
    "\n",
    "        # Extract dims\n",
    "        N = int(x.shape[0])\n",
    "        \n",
    "        # This version works with vmap, diagonal shifts diagional one up\n",
    "        U_zero_one = torch.triu(torch.ones(N, self.input_dim, self.input_dim), diagonal = 1)\n",
    "\n",
    "        # U_zero_one is (N, 2, 2), U_fill is (N, 1), so we need to unsqueeze the first dim of U_fill\n",
    "        U = U_zero_one * U_fill.unsqueeze(1)\n",
    "\n",
    "        # U is (N, 2, 2), so we need to swap the last two dims and then subtract\n",
    "        A = U - U.transpose(1, 2)\n",
    "\n",
    "        return A  # Output shape: (N, 2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 16  # N should be a perfect square\n",
    "N_side = int(N ** 0.5)\n",
    "\n",
    "dims = 2\n",
    "\n",
    "# inputs = torch.randn(N, dims)  # Random (N, 2) inputs\n",
    "xy_side = torch.linspace(0, 1, N_side)\n",
    "# torch meshgrid, not default but we want to index by x, y\n",
    "X_mesh, Y_mesh = torch.meshgrid(xy_side, xy_side, indexing = \"xy\")\n",
    "# Construct flat pairs of points\n",
    "inputs = torch.cat([X_mesh.reshape(-1, 1), Y_mesh.reshape(-1, 1)], dim = 1)\n",
    "print(inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400, 2])\n"
     ]
    }
   ],
   "source": [
    "N = 20 * 20 # N should be a perfect square\n",
    "N_side = int(N ** 0.5)\n",
    "\n",
    "dims = 2\n",
    "\n",
    "# inputs = torch.randn(N, dims)  # Random (N, 2) inputs\n",
    "xy_side = torch.linspace(0, 3, N_side)\n",
    "# torch meshgrid, not default but we want to index by x, y\n",
    "X_mesh, Y_mesh = torch.meshgrid(xy_side, xy_side, indexing = \"xy\")\n",
    "# Construct flat pairs of points\n",
    "inputs_03 = torch.cat([X_mesh.reshape(-1, 1), Y_mesh.reshape(-1, 1)], dim = 1)\n",
    "print(inputs_03.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write this as a function because we need to input a function for the Jacobian calculation\n",
    "def compute_A(x):\n",
    "    # U_fill is (N, 1), which is the model output, fills upper right corner of all 2x2 matrices\n",
    "    U_fill = model_A(x)\n",
    "    N = int(x.shape[0])\n",
    "\n",
    "    # This version works with vmap, diagonal shifts diagional one up\n",
    "    U_zero_one = torch.triu(torch.ones(N, dims, dims), diagonal = 1)\n",
    "    # U_zero_one is (N, 2, 2), U_fill is (N, 1), so we need to unsqueeze the first dim of U_fill\n",
    "    U = U_zero_one * U_fill.unsqueeze(1)\n",
    "    # result is the same as U[:, 0, 1] = U_fill.squeeze()\n",
    "    # alternative might be torch.index_add\n",
    "    # U is (N, 2, 2), so we need to swap the last two dims and then subtract\n",
    "    A = U - U.transpose(1, 2)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_v(inputs):\n",
    "    # Compute the Jacobian of A \n",
    "    jacobian_A = vmap(jacfwd(model_A))(inputs)\n",
    "    # assert that the second dim (size N) is redundant\n",
    "    assert(jacobian_A[:, 0, : , :, :] == jacobian_A[:, -1, : , :, :]).any()\n",
    "    # remove this redundant dimension\n",
    "    jacobian_A_lean = jacobian_A[:, 0, : , :, :]\n",
    "    # print(jacobian_A_lean.shape): torch.Size([N, 2, 2, 2])\n",
    "    # dim1 is what is considered the rows, dim2 is what is considered the columns\n",
    "    # So we compute this over the last two dims because the Jacobian is N x two (2 x 2)\n",
    "    v = torch.diagonal(jacobian_A_lean, dim1 = -2, dim2 = -1).sum(dim = 1)\n",
    "    # diagonal returns torch.Size([16, 2, 2]) so now we sum over the middle dim\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def div_functional(x):\n",
    "    # print(jacobian_v.shape): torch.Size([16, 16, 2, 2])\n",
    "    jacobian_v = vmap(jacfwd(compute_v))(x)\n",
    "    print(jacobian_v.shape)\n",
    "    # assert that the second dim (size N) is redundant\n",
    "    assert(jacobian_v[:, 0, :, :] == jacobian_v[:, -1, :, :]).any()\n",
    "    # print(jacobian_v_lean.shape): torch.Size([16, 2, 2])\n",
    "    jacobian_v_lean = jacobian_v[:, 0, :, :]\n",
    "    # torch.Size([16, 2]) and then we sum over the last dim to combine div_x and div_y\n",
    "    # attain flat divergence field \n",
    "    div_flat = torch.diagonal(jacobian_v_lean, dim1 = -2, dim2 = -1).sum(dim = -1)\n",
    "    return div_flat\n",
    "\n",
    "# div_v = div_functional(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "both arguments to matmul need to be at least 1D, but they are 0D and 2D",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[150], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdiv_functional\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_03\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[149], line 3\u001b[0m, in \u001b[0;36mdiv_functional\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdiv_functional\u001b[39m(x):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# print(jacobian_v.shape): torch.Size([16, 16, 2, 2])\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     jacobian_v \u001b[38;5;241m=\u001b[39m \u001b[43mvmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjacfwd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompute_v\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(jacobian_v\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# assert that the second dim (size N) is redundant\u001b[39;00m\n",
      "File \u001b[0;32m~/ice_thickness/.conda/lib/python3.9/site-packages/torch/_functorch/apis.py:188\u001b[0m, in \u001b[0;36mvmap.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvmap_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ice_thickness/.conda/lib/python3.9/site-packages/torch/_functorch/vmap.py:281\u001b[0m, in \u001b[0;36mvmap_impl\u001b[0;34m(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _chunked_vmap(func, flat_in_dims, chunks_flat_args,\n\u001b[1;32m    278\u001b[0m                          args_spec, out_dims, randomness, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    280\u001b[0m \u001b[38;5;66;03m# If chunk_size is not specified.\u001b[39;00m\n\u001b[0;32m--> 281\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_flat_vmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_in_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs_spec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ice_thickness/.conda/lib/python3.9/site-packages/torch/_functorch/vmap.py:47\u001b[0m, in \u001b[0;36mdoesnt_support_saved_tensors_hooks.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mdisable_saved_tensors_hooks(message):\n\u001b[0;32m---> 47\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ice_thickness/.conda/lib/python3.9/site-packages/torch/_functorch/vmap.py:403\u001b[0m, in \u001b[0;36m_flat_vmap\u001b[0;34m(func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, **kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m vmap_increment_nesting(batch_size, randomness) \u001b[38;5;28;01mas\u001b[39;00m vmap_level:\n\u001b[1;32m    402\u001b[0m     batched_inputs \u001b[38;5;241m=\u001b[39m _create_batched_inputs(flat_in_dims, flat_args, vmap_level, args_spec)\n\u001b[0;32m--> 403\u001b[0m     batched_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatched_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)\n",
      "File \u001b[0;32m~/ice_thickness/.conda/lib/python3.9/site-packages/torch/_functorch/eager_transforms.py:1163\u001b[0m, in \u001b[0;36mjacfwd.<locals>.wrapper_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   1160\u001b[0m     _, jvp_out \u001b[38;5;241m=\u001b[39m output\n\u001b[1;32m   1161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jvp_out\n\u001b[0;32m-> 1163\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mvmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpush_jvp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandomness\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbasis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_aux:\n\u001b[1;32m   1165\u001b[0m     results, aux \u001b[38;5;241m=\u001b[39m results\n",
      "File \u001b[0;32m~/ice_thickness/.conda/lib/python3.9/site-packages/torch/_functorch/apis.py:188\u001b[0m, in \u001b[0;36mvmap.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvmap_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ice_thickness/.conda/lib/python3.9/site-packages/torch/_functorch/vmap.py:281\u001b[0m, in \u001b[0;36mvmap_impl\u001b[0;34m(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _chunked_vmap(func, flat_in_dims, chunks_flat_args,\n\u001b[1;32m    278\u001b[0m                          args_spec, out_dims, randomness, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    280\u001b[0m \u001b[38;5;66;03m# If chunk_size is not specified.\u001b[39;00m\n\u001b[0;32m--> 281\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_flat_vmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_in_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs_spec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ice_thickness/.conda/lib/python3.9/site-packages/torch/_functorch/vmap.py:47\u001b[0m, in \u001b[0;36mdoesnt_support_saved_tensors_hooks.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mdisable_saved_tensors_hooks(message):\n\u001b[0;32m---> 47\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ice_thickness/.conda/lib/python3.9/site-packages/torch/_functorch/vmap.py:403\u001b[0m, in \u001b[0;36m_flat_vmap\u001b[0;34m(func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, **kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m vmap_increment_nesting(batch_size, randomness) \u001b[38;5;28;01mas\u001b[39;00m vmap_level:\n\u001b[1;32m    402\u001b[0m     batched_inputs \u001b[38;5;241m=\u001b[39m _create_batched_inputs(flat_in_dims, flat_args, vmap_level, args_spec)\n\u001b[0;32m--> 403\u001b[0m     batched_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatched_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)\n",
      "File \u001b[0;32m~/ice_thickness/.conda/lib/python3.9/site-packages/torch/_functorch/eager_transforms.py:1154\u001b[0m, in \u001b[0;36mjacfwd.<locals>.wrapper_fn.<locals>.push_jvp\u001b[0;34m(basis)\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpush_jvp\u001b[39m(basis):\n\u001b[0;32m-> 1154\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43m_jvp_with_argnums\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbasis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margnums\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margnums\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_aux\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1155\u001b[0m     \u001b[38;5;66;03m# output[0] is the output of `func(*args)`\u001b[39;00m\n\u001b[1;32m   1156\u001b[0m     error_if_complex(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjacfwd\u001b[39m\u001b[38;5;124m\"\u001b[39m, output[\u001b[38;5;241m0\u001b[39m], is_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/ice_thickness/.conda/lib/python3.9/site-packages/torch/_functorch/vmap.py:47\u001b[0m, in \u001b[0;36mdoesnt_support_saved_tensors_hooks.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mdisable_saved_tensors_hooks(message):\n\u001b[0;32m---> 47\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ice_thickness/.conda/lib/python3.9/site-packages/torch/_functorch/eager_transforms.py:1000\u001b[0m, in \u001b[0;36m_jvp_with_argnums\u001b[0;34m(func, primals, tangents, argnums, strict, has_aux)\u001b[0m\n\u001b[1;32m    998\u001b[0m     primals \u001b[38;5;241m=\u001b[39m _wrap_all_tensors(primals, level)\n\u001b[1;32m    999\u001b[0m     duals \u001b[38;5;241m=\u001b[39m _replace_args(primals, duals, argnums)\n\u001b[0;32m-> 1000\u001b[0m result_duals \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mduals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_aux:\n\u001b[1;32m   1002\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(result_duals, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result_duals) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n",
      "Cell \u001b[0;32mIn[148], line 3\u001b[0m, in \u001b[0;36mcompute_v\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_v\u001b[39m(inputs):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Compute the Jacobian of A \u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     jacobian_A \u001b[38;5;241m=\u001b[39m \u001b[43mvmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjacfwd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_A\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# assert that the second dim (size N) is redundant\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m(jacobian_A[:, \u001b[38;5;241m0\u001b[39m, : , :, :] \u001b[38;5;241m==\u001b[39m jacobian_A[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, : , :, :])\u001b[38;5;241m.\u001b[39many()\n",
      "File \u001b[0;32m~/ice_thickness/.conda/lib/python3.9/site-packages/torch/_functorch/apis.py:188\u001b[0m, in \u001b[0;36mvmap.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvmap_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ice_thickness/.conda/lib/python3.9/site-packages/torch/_functorch/vmap.py:281\u001b[0m, in \u001b[0;36mvmap_impl\u001b[0;34m(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _chunked_vmap(func, flat_in_dims, chunks_flat_args,\n\u001b[1;32m    278\u001b[0m                          args_spec, out_dims, randomness, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    280\u001b[0m \u001b[38;5;66;03m# If chunk_size is not specified.\u001b[39;00m\n\u001b[0;32m--> 281\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_flat_vmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_in_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs_spec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ice_thickness/.conda/lib/python3.9/site-packages/torch/_functorch/vmap.py:47\u001b[0m, in \u001b[0;36mdoesnt_support_saved_tensors_hooks.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mdisable_saved_tensors_hooks(message):\n\u001b[0;32m---> 47\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ice_thickness/.conda/lib/python3.9/site-packages/torch/_functorch/vmap.py:403\u001b[0m, in \u001b[0;36m_flat_vmap\u001b[0;34m(func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, **kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m vmap_increment_nesting(batch_size, randomness) \u001b[38;5;28;01mas\u001b[39;00m vmap_level:\n\u001b[1;32m    402\u001b[0m     batched_inputs \u001b[38;5;241m=\u001b[39m _create_batched_inputs(flat_in_dims, flat_args, vmap_level, args_spec)\n\u001b[0;32m--> 403\u001b[0m     batched_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatched_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)\n",
      "File \u001b[0;32m~/ice_thickness/.conda/lib/python3.9/site-packages/torch/_functorch/eager_transforms.py:1163\u001b[0m, in \u001b[0;36mjacfwd.<locals>.wrapper_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   1160\u001b[0m     _, jvp_out \u001b[38;5;241m=\u001b[39m output\n\u001b[1;32m   1161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jvp_out\n\u001b[0;32m-> 1163\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mvmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpush_jvp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandomness\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbasis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_aux:\n\u001b[1;32m   1165\u001b[0m     results, aux \u001b[38;5;241m=\u001b[39m results\n",
      "File \u001b[0;32m~/ice_thickness/.conda/lib/python3.9/site-packages/torch/_functorch/apis.py:188\u001b[0m, in \u001b[0;36mvmap.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvmap_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ice_thickness/.conda/lib/python3.9/site-packages/torch/_functorch/vmap.py:281\u001b[0m, in \u001b[0;36mvmap_impl\u001b[0;34m(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _chunked_vmap(func, flat_in_dims, chunks_flat_args,\n\u001b[1;32m    278\u001b[0m                          args_spec, out_dims, randomness, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    280\u001b[0m \u001b[38;5;66;03m# If chunk_size is not specified.\u001b[39;00m\n\u001b[0;32m--> 281\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_flat_vmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_in_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs_spec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ice_thickness/.conda/lib/python3.9/site-packages/torch/_functorch/vmap.py:47\u001b[0m, in \u001b[0;36mdoesnt_support_saved_tensors_hooks.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mdisable_saved_tensors_hooks(message):\n\u001b[0;32m---> 47\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ice_thickness/.conda/lib/python3.9/site-packages/torch/_functorch/vmap.py:403\u001b[0m, in \u001b[0;36m_flat_vmap\u001b[0;34m(func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, **kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m vmap_increment_nesting(batch_size, randomness) \u001b[38;5;28;01mas\u001b[39;00m vmap_level:\n\u001b[1;32m    402\u001b[0m     batched_inputs \u001b[38;5;241m=\u001b[39m _create_batched_inputs(flat_in_dims, flat_args, vmap_level, args_spec)\n\u001b[0;32m--> 403\u001b[0m     batched_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatched_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)\n",
      "File \u001b[0;32m~/ice_thickness/.conda/lib/python3.9/site-packages/torch/_functorch/eager_transforms.py:1154\u001b[0m, in \u001b[0;36mjacfwd.<locals>.wrapper_fn.<locals>.push_jvp\u001b[0;34m(basis)\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpush_jvp\u001b[39m(basis):\n\u001b[0;32m-> 1154\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43m_jvp_with_argnums\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbasis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margnums\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margnums\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_aux\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1155\u001b[0m     \u001b[38;5;66;03m# output[0] is the output of `func(*args)`\u001b[39;00m\n\u001b[1;32m   1156\u001b[0m     error_if_complex(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjacfwd\u001b[39m\u001b[38;5;124m\"\u001b[39m, output[\u001b[38;5;241m0\u001b[39m], is_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/ice_thickness/.conda/lib/python3.9/site-packages/torch/_functorch/vmap.py:47\u001b[0m, in \u001b[0;36mdoesnt_support_saved_tensors_hooks.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mdisable_saved_tensors_hooks(message):\n\u001b[0;32m---> 47\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ice_thickness/.conda/lib/python3.9/site-packages/torch/_functorch/eager_transforms.py:1000\u001b[0m, in \u001b[0;36m_jvp_with_argnums\u001b[0;34m(func, primals, tangents, argnums, strict, has_aux)\u001b[0m\n\u001b[1;32m    998\u001b[0m     primals \u001b[38;5;241m=\u001b[39m _wrap_all_tensors(primals, level)\n\u001b[1;32m    999\u001b[0m     duals \u001b[38;5;241m=\u001b[39m _replace_args(primals, duals, argnums)\n\u001b[0;32m-> 1000\u001b[0m result_duals \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mduals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_aux:\n\u001b[1;32m   1002\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(result_duals, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result_duals) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n",
      "File \u001b[0;32m~/ice_thickness/.conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ice_thickness/.conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 24\u001b[0m, in \u001b[0;36mMLP_compute_A.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# put deterministic transformations here with torch functional\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# RUN THROUGH NET\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     U_fill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# Extract dims\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     N \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/ice_thickness/.conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ice_thickness/.conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/ice_thickness/.conda/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/ice_thickness/.conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ice_thickness/.conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/ice_thickness/.conda/lib/python3.9/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: both arguments to matmul need to be at least 1D, but they are 0D and 2D"
     ]
    }
   ],
   "source": [
    "div_functional(inputs_03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_v_stream(v, div_v, x, title_string = \"v(x)\", color_abs_max = 0.5, lw_scalar = 2):\n",
    "    \"\"\"Plots a vector field v(x) and its divergence div_v(x) as a streamplot on a square grid.\n",
    "    The linewidth corresponds to the magnitude/speed of the vector field.\n",
    "    The color corresponds to the divergence of the vector field. We use the dark doiverging colormap \"berlin\" so that zero divergence is visible as black. \n",
    "\n",
    "    Args:\n",
    "        v (torch.Size([N_long, 2])): flattened square vector field, where the first column is the u component and the second column is the v component\n",
    "        div_v (torch.Size([N_long])): flat divergence of the vector field\n",
    "        x (torch.Size([N_long, 2])): flattend meshgrids, where the first column is the x component and the second column is the y component\n",
    "        title_string (str, optional): Title for plot. Defaults to \"v(x)\".\n",
    "        color_abs_max (float, optional): Maximum absolute value for color normalization. Defaults to 0.5.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract N_long and calculate sqrt of N_long, N_side\n",
    "    N_long = torch.tensor(v.shape[0])\n",
    "    N_side = int(torch.sqrt(N_long))\n",
    "\n",
    "    # Extract both columns/components from v and make square\n",
    "    U = v[:, 0].reshape(N_side, N_side)\n",
    "    V = v[:, 1].reshape(N_side, N_side)\n",
    "\n",
    "    # Make coordinates square again\n",
    "    X = x[:, 0].reshape(N_side, N_side)\n",
    "    Y = x[:, 1].reshape(N_side, N_side)\n",
    "\n",
    "    div_v_square = div_v.reshape(N_side, N_side)\n",
    "    # Define symmetric normalization with zero centered\n",
    "    norm = mcolors.TwoSlopeNorm(vmin = - color_abs_max, vcenter = 0, vmax = color_abs_max)\n",
    "\n",
    "    # Magnitude i.e. speed: square each element to remove negative direction, then square root\n",
    "    mag = torch.sqrt(torch.square(U) + torch.square(V))\n",
    "    lw = mag * lw_scalar / torch.max(mag) # normalise mag\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize = (4, 4))\n",
    "\n",
    "    ax.streamplot(X.numpy(), Y.numpy(), U.numpy(), V.numpy(), \n",
    "                  linewidth = lw.numpy(),\n",
    "                  color = div_v_square.numpy(), \n",
    "                  cmap = cmaps['berlin'],\n",
    "                  norm = norm)\n",
    "    \n",
    "    # coolwarm is diverging but has grey in middle\n",
    "    # add norm\n",
    "    ax.set_aspect(1)\n",
    "    ax.set_title(title_string)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_v_quiver(v, div_v, x, title_string = \"v(x)\", color_abs_max = 0.5, lw_scalar = 2):\n",
    "    \"\"\"Plots a vector field v(x) and its divergence div_v(x) as a quiverplot on a square grid.\n",
    "    The quiverlength automatically corresponds to the magnitude/speed of the vector field.\n",
    "    The color corresponds to the divergence of the vector field. We use the dark diverging colormap \"berlin\" so that zero divergence is visible as black. \n",
    "\n",
    "    Args:\n",
    "        v (torch.Size([N_long, 2])): flattened square vector field, where the first column is the u component and the second column is the v component\n",
    "        div_v (torch.Size([N_long])): flat divergence of the vector field\n",
    "        x (torch.Size([N_long, 2])): flattend meshgrids, where the first column is the x component and the second column is the y component\n",
    "        title_string (str, optional): Title for plot. Defaults to \"v(x)\".\n",
    "        color_abs_max (float, optional): Maximum absolute value for color normalization. Defaults to 0.5.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract N_long and calculate sqrt of N_long, N_side\n",
    "    N_long = torch.tensor(v.shape[0])\n",
    "    N_side = int(torch.sqrt(N_long))\n",
    "\n",
    "    # Extract both columns/components from v and make square\n",
    "    U = v[:, 0].reshape(N_side, N_side)\n",
    "    V = v[:, 1].reshape(N_side, N_side)\n",
    "\n",
    "    # Make coordinates square again\n",
    "    X = x[:, 0].reshape(N_side, N_side)\n",
    "    Y = x[:, 1].reshape(N_side, N_side)\n",
    "\n",
    "    div_v_square = div_v.reshape(N_side, N_side)\n",
    "    # Define symmetric normalization with zero centered\n",
    "    norm = mcolors.TwoSlopeNorm(vmin = - color_abs_max, vcenter = 0, vmax = color_abs_max)\n",
    "\n",
    "    # Magnitude i.e. speed: square each element to remove negative direction, then square root\n",
    "    # mag = torch.sqrt(torch.square(U) + torch.square(V))\n",
    "    # lw = mag * lw_scalar / torch.max(mag) # normalise mag\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize = (4, 4))\n",
    "\n",
    "    ax.quiver(X.numpy(), Y.numpy(), U.numpy(), V.numpy(),\n",
    "              div_v_square.numpy(), # color is passed directly\n",
    "              cmap = cmaps['berlin'],\n",
    "              norm = norm)\n",
    "\n",
    "    # ax.quiver(x[:, 0], x[:, 1], v[:, 0], v[:, 1])\n",
    "    \n",
    "    # coolwarm is diverging but has grey in middle\n",
    "    # add norm\n",
    "    ax.set_aspect(1)\n",
    "    ax.set_title(title_string)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = compute_v(inputs)\n",
    "div_v = div_functional(inputs)\n",
    "\n",
    "visualise_v_stream(v.detach(), div_v.detach(), inputs, title_string = \"v(x)\", color_abs_max = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def div_discrete(v_flat_grid, dx = 1, dy = 1, return_components_bool = False):\n",
    "    \"\"\"Compute\n",
    "    div(v) = dU/dx + dV/dy\n",
    "    where v = (U, V) is a vector field\n",
    "    edges can be funky\n",
    "\n",
    "    Args:\n",
    "        v_flat_grid (torch.Size([N, 2])): flattened vector field\n",
    "        dx (int, optional): unit spacing between x columns. Defaults to 1.\n",
    "        dy (int, optional): unit spacing between y rows. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        div_v (torch.Size([N, 1])): Same extent as v input, but only 1 dim, torch tensor for consistency\n",
    "    OR Returns:\n",
    "        div_v (torch.Size([N, 1])): Same extent as v input, but only 1 dim\n",
    "        dU_dx (torch.Size([N, 1])): x derivative of U component\n",
    "        dV_dy (torch.Size([N, 1])): y derivative of V component\n",
    "    \"\"\"\n",
    "    N_side = int(np.sqrt(v_flat_grid.shape[0]))\n",
    "    v_square = v_flat_grid.reshape(N_side, N_side, 2)\n",
    "\n",
    "    U = v_square[:, :, 0]\n",
    "    V = v_square[:, :, 1]\n",
    "\n",
    "    # Compute the x and y derivatives\n",
    "    dU_dx = np.gradient(U, dx, axis = 1) # axis 1 is x\n",
    "    dV_dy = np.gradient(V, dy, axis = 0) # axis 0 is y\n",
    "\n",
    "    # Compute the divergence\n",
    "    div_v = dU_dx + dV_dy\n",
    "\n",
    "    if return_components_bool == True:\n",
    "        return torch.tensor(div_v).reshape(-1, 1), torch.tensor(dU_dx).reshape(-1, 1), torch.tensor(dV_dy).reshape(-1, 1)\n",
    "    else:\n",
    "        return torch.tensor(div_v).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = simulate_convergence(inputs_03)\n",
    "div_v = div_discrete(v)\n",
    "\n",
    "visualise_v_quiver(v, div_v, inputs_03, title_string = \"v(x)\", color_abs_max = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000, -0.1466],\n",
       "         [ 0.1466,  0.0000]],\n",
       "\n",
       "        [[ 0.0000, -0.2350],\n",
       "         [ 0.2350,  0.0000]],\n",
       "\n",
       "        [[ 0.0000, -0.4260],\n",
       "         [ 0.4260,  0.0000]],\n",
       "\n",
       "        [[ 0.0000, -0.6009],\n",
       "         [ 0.6009,  0.0000]]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_A = MLP_compute_A()\n",
    "model_A.train()\n",
    "\n",
    "# fixed_value = 0.5  # Change this to your desired fixed value\n",
    "\n",
    "# with torch.no_grad():  # Ensure we don't track gradients\n",
    "#    for param in model_A.parameters():\n",
    "#        param.fill_(fixed_value)  # Set all elements to fixed_value\n",
    "\n",
    "\n",
    "x = torch.tensor([[0.0, 0.5], [1.0, 0.8], [2.0, 2.0], [3.0, 3.0]]).requires_grad_()\n",
    "A = model_A(x)\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.autograd.functional.jacobian: \n",
    "\n",
    "For the N's it seems we have to use pairwise indices: 0, 0, 1, 1, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2, 2, 4, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0110, -0.0779],\n",
       "        [ 0.1238, -0.0442],\n",
       "        [ 0.1249, -0.0522],\n",
       "        [ 0.1226, -0.0511]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jacobian_A_autograd = torch.autograd.functional.jacobian(model_A, x)\n",
    "\n",
    "print(jacobian_A_autograd.shape)\n",
    "n_i = 2\n",
    "print(jacobian_A_autograd[n_i, :, :, n_i, :])\n",
    "\n",
    "jacobian_A_autograd_lean = torch.einsum('nabnc -> nabc', jacobian_A_autograd)\n",
    "print(jacobian_A_autograd_lean.shape)\n",
    "\n",
    "jacobian_A_autograd_lean.diagonal(dim1 = -2, dim2 = -1).sum(dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2, 2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0642,  0.1081],\n",
       "        [ 0.0055,  0.2341],\n",
       "        [ 0.0249,  0.2973],\n",
       "        [ 0.0266,  0.3012]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jacobian_A_vmap_jacrev = vmap(jacrev(model_A))(x)\n",
    "# assert that the second dim (size 2) is redundant\n",
    "assert(jacobian_A_vmap_jacrev[:, 0, :, :, :] == jacobian_A_vmap_jacrev[:, 1, :, :, :]).any()\n",
    "jacobian_A_vmap_jacrev_lean = torch.einsum('n r a b c -> n a b c', jacobian_A_vmap_jacrev)\n",
    "\n",
    "print(jacobian_A_vmap_jacrev_lean.shape)\n",
    "\n",
    "jacobian_A_vmap_jacrev_lean.diagonal(dim1 = -2, dim2 = -1).sum(dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0000,  0.0000],\n",
       "          [ 0.0321,  0.0540]],\n",
       "\n",
       "         [[-0.0321, -0.0540],\n",
       "          [ 0.0000,  0.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000,  0.0000],\n",
       "          [ 0.0321,  0.0540]],\n",
       "\n",
       "         [[-0.0321, -0.0540],\n",
       "          [ 0.0000,  0.0000]]]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jacobian_A_vmap_jacrev[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[164], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m jacobian_A_vmap_jacrev[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, :, :, :]\n\u001b[1;32m      2\u001b[0m jacobian_A_vmap_jacrev[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, :, :, :]\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m(jacobian_A_vmap_jacrev[:, \u001b[38;5;241m1\u001b[39m, :, :, :] \u001b[38;5;241m==\u001b[39m jacobian_A_vmap_jacrev[:, \u001b[38;5;241m0\u001b[39m, :, :, :])\u001b[38;5;241m.\u001b[39mall()\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "jacobian_A_vmap_jacrev[0, 0, :, :, :]\n",
    "jacobian_A_vmap_jacrev[1, 0, :, :, :]\n",
    "\n",
    "tolerance = 1e-6  # Adjust based on precision needs\n",
    "assert torch.allclose(jacobian_A_vmap_jacrev[:, 1, :, :, :], jacobian_A_vmap_jacrev[:, 0, :, :, :], atol = tolerance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simulate import simulate_convergence, simulate_merge, simulate_branching, simulate_deflection, simulate_ridge\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the training data is torch.Size([400, 2]).\n",
      "The dtype of the input data is torch.float32.\n",
      "\n",
      "Start Training\n",
      "Epoch 1/3000, Training Loss (MSE): 2.7404\n",
      "Epoch 2/3000, Training Loss (MSE): 2.9020\n",
      "Epoch 3/3000, Training Loss (MSE): 3.4113\n",
      "Epoch 4/3000, Training Loss (MSE): 3.4349\n",
      "Epoch 5/3000, Training Loss (MSE): 2.4699\n",
      "Epoch 6/3000, Training Loss (MSE): 2.9376\n",
      "Epoch 7/3000, Training Loss (MSE): 3.5780\n",
      "Epoch 8/3000, Training Loss (MSE): 3.0373\n",
      "Epoch 9/3000, Training Loss (MSE): 2.6617\n",
      "Epoch 10/3000, Training Loss (MSE): 2.7791\n",
      "Epoch 11/3000, Training Loss (MSE): 3.2579\n",
      "Epoch 12/3000, Training Loss (MSE): 3.0620\n",
      "Epoch 13/3000, Training Loss (MSE): 2.3281\n",
      "Epoch 14/3000, Training Loss (MSE): 3.2117\n",
      "Epoch 15/3000, Training Loss (MSE): 2.5134\n",
      "Epoch 16/3000, Training Loss (MSE): 2.6285\n",
      "Epoch 17/3000, Training Loss (MSE): 2.3388\n",
      "Epoch 18/3000, Training Loss (MSE): 2.6393\n",
      "Epoch 19/3000, Training Loss (MSE): 1.8373\n",
      "Epoch 20/3000, Training Loss (MSE): 2.4156\n",
      "Epoch 21/3000, Training Loss (MSE): 2.1667\n",
      "Epoch 22/3000, Training Loss (MSE): 1.8900\n",
      "Epoch 23/3000, Training Loss (MSE): 2.8682\n",
      "Epoch 24/3000, Training Loss (MSE): 2.5801\n",
      "Epoch 25/3000, Training Loss (MSE): 2.2168\n",
      "Epoch 26/3000, Training Loss (MSE): 2.7293\n",
      "Epoch 27/3000, Training Loss (MSE): 2.5737\n",
      "Epoch 28/3000, Training Loss (MSE): 2.4986\n",
      "Epoch 29/3000, Training Loss (MSE): 2.5036\n",
      "Epoch 30/3000, Training Loss (MSE): 1.8184\n",
      "Epoch 31/3000, Training Loss (MSE): 1.9047\n",
      "Epoch 32/3000, Training Loss (MSE): 2.0933\n",
      "Epoch 33/3000, Training Loss (MSE): 2.3835\n",
      "Epoch 34/3000, Training Loss (MSE): 1.8606\n",
      "Epoch 35/3000, Training Loss (MSE): 2.3193\n",
      "Epoch 36/3000, Training Loss (MSE): 1.6749\n",
      "Epoch 37/3000, Training Loss (MSE): 2.4161\n",
      "Epoch 38/3000, Training Loss (MSE): 2.1353\n",
      "Epoch 39/3000, Training Loss (MSE): 1.9479\n",
      "Epoch 40/3000, Training Loss (MSE): 1.8868\n",
      "Epoch 41/3000, Training Loss (MSE): 1.7018\n",
      "Epoch 42/3000, Training Loss (MSE): 1.8226\n",
      "Epoch 43/3000, Training Loss (MSE): 1.4453\n",
      "Epoch 44/3000, Training Loss (MSE): 1.9115\n",
      "Epoch 45/3000, Training Loss (MSE): 1.4197\n",
      "Epoch 46/3000, Training Loss (MSE): 1.5660\n",
      "Epoch 47/3000, Training Loss (MSE): 1.6374\n",
      "Epoch 48/3000, Training Loss (MSE): 1.4312\n",
      "Epoch 49/3000, Training Loss (MSE): 1.2712\n",
      "Epoch 50/3000, Training Loss (MSE): 1.3190\n",
      "Epoch 51/3000, Training Loss (MSE): 1.8509\n",
      "Epoch 52/3000, Training Loss (MSE): 1.1475\n",
      "Epoch 53/3000, Training Loss (MSE): 1.4929\n",
      "Epoch 54/3000, Training Loss (MSE): 1.2581\n",
      "Epoch 55/3000, Training Loss (MSE): 1.4484\n",
      "Epoch 56/3000, Training Loss (MSE): 0.8880\n",
      "Epoch 57/3000, Training Loss (MSE): 1.3528\n",
      "Epoch 58/3000, Training Loss (MSE): 1.2651\n",
      "Epoch 59/3000, Training Loss (MSE): 1.4406\n",
      "Epoch 60/3000, Training Loss (MSE): 1.0902\n",
      "Epoch 61/3000, Training Loss (MSE): 1.7243\n",
      "Epoch 62/3000, Training Loss (MSE): 1.6024\n",
      "Epoch 63/3000, Training Loss (MSE): 1.2037\n",
      "Epoch 64/3000, Training Loss (MSE): 1.3058\n",
      "Epoch 65/3000, Training Loss (MSE): 1.5076\n",
      "Epoch 66/3000, Training Loss (MSE): 0.7763\n",
      "Epoch 67/3000, Training Loss (MSE): 1.2755\n",
      "Epoch 68/3000, Training Loss (MSE): 1.1216\n",
      "Epoch 69/3000, Training Loss (MSE): 1.1295\n",
      "Epoch 70/3000, Training Loss (MSE): 1.1700\n",
      "Epoch 71/3000, Training Loss (MSE): 0.8773\n",
      "Epoch 72/3000, Training Loss (MSE): 1.3610\n",
      "Epoch 73/3000, Training Loss (MSE): 1.1364\n",
      "Epoch 74/3000, Training Loss (MSE): 0.8330\n",
      "Epoch 75/3000, Training Loss (MSE): 1.1289\n",
      "Epoch 76/3000, Training Loss (MSE): 1.2967\n",
      "Epoch 77/3000, Training Loss (MSE): 1.3018\n",
      "Epoch 78/3000, Training Loss (MSE): 1.5094\n",
      "Epoch 79/3000, Training Loss (MSE): 1.0931\n",
      "Epoch 80/3000, Training Loss (MSE): 1.0358\n",
      "Epoch 81/3000, Training Loss (MSE): 0.9876\n",
      "Epoch 82/3000, Training Loss (MSE): 0.9797\n",
      "Epoch 83/3000, Training Loss (MSE): 1.2880\n",
      "Epoch 84/3000, Training Loss (MSE): 1.2994\n",
      "Epoch 85/3000, Training Loss (MSE): 0.9015\n",
      "Epoch 86/3000, Training Loss (MSE): 1.0986\n",
      "Epoch 87/3000, Training Loss (MSE): 1.3530\n",
      "Epoch 88/3000, Training Loss (MSE): 0.8345\n",
      "Epoch 89/3000, Training Loss (MSE): 0.8608\n",
      "Epoch 90/3000, Training Loss (MSE): 0.9222\n",
      "Epoch 91/3000, Training Loss (MSE): 1.1589\n",
      "Epoch 92/3000, Training Loss (MSE): 0.6359\n",
      "Epoch 93/3000, Training Loss (MSE): 1.2996\n",
      "Epoch 94/3000, Training Loss (MSE): 1.1443\n",
      "Epoch 95/3000, Training Loss (MSE): 1.3543\n",
      "Epoch 96/3000, Training Loss (MSE): 0.8291\n",
      "Epoch 97/3000, Training Loss (MSE): 1.0646\n",
      "Epoch 98/3000, Training Loss (MSE): 1.2653\n",
      "Epoch 99/3000, Training Loss (MSE): 1.0958\n",
      "Epoch 100/3000, Training Loss (MSE): 1.3204\n",
      "Epoch 101/3000, Training Loss (MSE): 1.0159\n",
      "Epoch 102/3000, Training Loss (MSE): 1.0971\n",
      "Epoch 103/3000, Training Loss (MSE): 0.9923\n",
      "Epoch 104/3000, Training Loss (MSE): 0.9799\n",
      "Epoch 105/3000, Training Loss (MSE): 0.8309\n",
      "Epoch 106/3000, Training Loss (MSE): 0.9903\n",
      "Epoch 107/3000, Training Loss (MSE): 0.8122\n",
      "Epoch 108/3000, Training Loss (MSE): 0.9749\n",
      "Epoch 109/3000, Training Loss (MSE): 1.0261\n",
      "Epoch 110/3000, Training Loss (MSE): 1.4803\n",
      "Epoch 111/3000, Training Loss (MSE): 1.1141\n",
      "Epoch 112/3000, Training Loss (MSE): 1.0073\n",
      "Epoch 113/3000, Training Loss (MSE): 0.8750\n",
      "Epoch 114/3000, Training Loss (MSE): 1.1461\n",
      "Epoch 115/3000, Training Loss (MSE): 0.6818\n",
      "Epoch 116/3000, Training Loss (MSE): 1.1112\n",
      "Epoch 117/3000, Training Loss (MSE): 1.1232\n",
      "Epoch 118/3000, Training Loss (MSE): 0.6791\n",
      "Epoch 119/3000, Training Loss (MSE): 1.0949\n",
      "Epoch 120/3000, Training Loss (MSE): 1.0136\n",
      "Epoch 121/3000, Training Loss (MSE): 1.0854\n",
      "Epoch 122/3000, Training Loss (MSE): 0.9196\n",
      "Epoch 123/3000, Training Loss (MSE): 1.1070\n",
      "Epoch 124/3000, Training Loss (MSE): 1.0672\n",
      "Epoch 125/3000, Training Loss (MSE): 1.2648\n",
      "Epoch 126/3000, Training Loss (MSE): 1.0960\n",
      "Epoch 127/3000, Training Loss (MSE): 1.2287\n",
      "Epoch 128/3000, Training Loss (MSE): 1.1135\n",
      "Epoch 129/3000, Training Loss (MSE): 1.5920\n",
      "Epoch 130/3000, Training Loss (MSE): 1.3893\n",
      "Epoch 131/3000, Training Loss (MSE): 1.5160\n",
      "Epoch 132/3000, Training Loss (MSE): 1.4138\n",
      "Epoch 133/3000, Training Loss (MSE): 1.2346\n",
      "Epoch 134/3000, Training Loss (MSE): 1.3627\n",
      "Epoch 135/3000, Training Loss (MSE): 0.9864\n",
      "Epoch 136/3000, Training Loss (MSE): 0.9085\n",
      "Epoch 137/3000, Training Loss (MSE): 1.2800\n",
      "Epoch 138/3000, Training Loss (MSE): 1.1743\n",
      "Epoch 139/3000, Training Loss (MSE): 1.0585\n",
      "Epoch 140/3000, Training Loss (MSE): 1.6703\n",
      "Epoch 141/3000, Training Loss (MSE): 0.9668\n",
      "Epoch 142/3000, Training Loss (MSE): 0.7956\n",
      "Epoch 143/3000, Training Loss (MSE): 1.0174\n",
      "Epoch 144/3000, Training Loss (MSE): 1.1711\n",
      "Epoch 145/3000, Training Loss (MSE): 0.7147\n",
      "Epoch 146/3000, Training Loss (MSE): 1.0807\n",
      "Epoch 147/3000, Training Loss (MSE): 1.2326\n",
      "Epoch 148/3000, Training Loss (MSE): 1.6239\n",
      "Epoch 149/3000, Training Loss (MSE): 1.5246\n",
      "Epoch 150/3000, Training Loss (MSE): 1.3333\n",
      "Epoch 151/3000, Training Loss (MSE): 1.0748\n",
      "Epoch 152/3000, Training Loss (MSE): 1.8836\n",
      "Epoch 153/3000, Training Loss (MSE): 1.3291\n",
      "Epoch 154/3000, Training Loss (MSE): 1.0220\n",
      "Epoch 155/3000, Training Loss (MSE): 1.3232\n",
      "Epoch 156/3000, Training Loss (MSE): 1.1539\n",
      "Epoch 157/3000, Training Loss (MSE): 2.0066\n",
      "Epoch 158/3000, Training Loss (MSE): 1.0939\n",
      "Epoch 159/3000, Training Loss (MSE): 1.3949\n",
      "Epoch 160/3000, Training Loss (MSE): 1.5294\n",
      "Epoch 161/3000, Training Loss (MSE): 0.7663\n",
      "Epoch 162/3000, Training Loss (MSE): 1.7039\n",
      "Epoch 163/3000, Training Loss (MSE): 1.3477\n",
      "Epoch 164/3000, Training Loss (MSE): 1.4811\n",
      "Epoch 165/3000, Training Loss (MSE): 1.2867\n",
      "Epoch 166/3000, Training Loss (MSE): 1.3734\n",
      "Epoch 167/3000, Training Loss (MSE): 1.2897\n",
      "Epoch 168/3000, Training Loss (MSE): 1.5565\n",
      "Epoch 169/3000, Training Loss (MSE): 1.3784\n",
      "Epoch 170/3000, Training Loss (MSE): 1.0092\n",
      "Epoch 171/3000, Training Loss (MSE): 1.2703\n",
      "Epoch 172/3000, Training Loss (MSE): 1.5298\n",
      "Epoch 173/3000, Training Loss (MSE): 1.3243\n",
      "Epoch 174/3000, Training Loss (MSE): 1.9139\n",
      "Epoch 175/3000, Training Loss (MSE): 1.4646\n",
      "Epoch 176/3000, Training Loss (MSE): 1.3474\n",
      "Epoch 177/3000, Training Loss (MSE): 2.6787\n",
      "Epoch 178/3000, Training Loss (MSE): 1.7650\n",
      "Epoch 179/3000, Training Loss (MSE): 1.9155\n",
      "Epoch 180/3000, Training Loss (MSE): 1.1727\n",
      "Epoch 181/3000, Training Loss (MSE): 0.6548\n",
      "Epoch 182/3000, Training Loss (MSE): 1.1563\n",
      "Epoch 183/3000, Training Loss (MSE): 1.3766\n",
      "Epoch 184/3000, Training Loss (MSE): 1.4136\n",
      "Epoch 185/3000, Training Loss (MSE): 2.0026\n",
      "Epoch 186/3000, Training Loss (MSE): 1.3438\n",
      "Epoch 187/3000, Training Loss (MSE): 1.4825\n",
      "Epoch 188/3000, Training Loss (MSE): 0.9747\n",
      "Epoch 189/3000, Training Loss (MSE): 2.5478\n",
      "Epoch 190/3000, Training Loss (MSE): 1.6091\n",
      "Epoch 191/3000, Training Loss (MSE): 1.8845\n",
      "Epoch 192/3000, Training Loss (MSE): 2.7411\n",
      "Epoch 193/3000, Training Loss (MSE): 0.9259\n",
      "Epoch 194/3000, Training Loss (MSE): 2.1563\n",
      "Epoch 195/3000, Training Loss (MSE): 2.5474\n",
      "Epoch 196/3000, Training Loss (MSE): 1.4806\n",
      "Epoch 197/3000, Training Loss (MSE): 1.1532\n",
      "Epoch 198/3000, Training Loss (MSE): 1.7421\n",
      "Epoch 199/3000, Training Loss (MSE): 2.1196\n",
      "Epoch 200/3000, Training Loss (MSE): 1.6407\n",
      "Epoch 201/3000, Training Loss (MSE): 1.7384\n",
      "Epoch 202/3000, Training Loss (MSE): 1.2643\n",
      "Epoch 203/3000, Training Loss (MSE): 2.3624\n",
      "Epoch 204/3000, Training Loss (MSE): 1.0782\n",
      "Epoch 205/3000, Training Loss (MSE): 1.5691\n",
      "Epoch 206/3000, Training Loss (MSE): 1.4817\n",
      "Epoch 207/3000, Training Loss (MSE): 1.6965\n",
      "Epoch 208/3000, Training Loss (MSE): 1.5694\n",
      "Epoch 209/3000, Training Loss (MSE): 1.8790\n",
      "Epoch 210/3000, Training Loss (MSE): 1.1847\n",
      "Epoch 211/3000, Training Loss (MSE): 1.6825\n",
      "Epoch 212/3000, Training Loss (MSE): 1.6462\n",
      "Epoch 213/3000, Training Loss (MSE): 0.7941\n",
      "Epoch 214/3000, Training Loss (MSE): 1.5320\n",
      "Epoch 215/3000, Training Loss (MSE): 2.0398\n",
      "Epoch 216/3000, Training Loss (MSE): 2.4422\n",
      "Epoch 217/3000, Training Loss (MSE): 0.8832\n",
      "Epoch 218/3000, Training Loss (MSE): 1.4440\n",
      "Epoch 219/3000, Training Loss (MSE): 2.1104\n",
      "Epoch 220/3000, Training Loss (MSE): 1.2101\n",
      "Epoch 221/3000, Training Loss (MSE): 2.7214\n",
      "Epoch 222/3000, Training Loss (MSE): 1.6055\n",
      "Epoch 223/3000, Training Loss (MSE): 2.2188\n",
      "Epoch 224/3000, Training Loss (MSE): 1.7495\n",
      "Epoch 225/3000, Training Loss (MSE): 2.1227\n",
      "Epoch 226/3000, Training Loss (MSE): 2.0779\n",
      "Epoch 227/3000, Training Loss (MSE): 1.9403\n",
      "Epoch 228/3000, Training Loss (MSE): 2.5520\n",
      "Epoch 229/3000, Training Loss (MSE): 1.8084\n",
      "Epoch 230/3000, Training Loss (MSE): 2.1514\n",
      "Epoch 231/3000, Training Loss (MSE): 3.3675\n",
      "Epoch 232/3000, Training Loss (MSE): 2.2737\n",
      "Epoch 233/3000, Training Loss (MSE): 1.6139\n",
      "Epoch 234/3000, Training Loss (MSE): 2.0849\n",
      "Epoch 235/3000, Training Loss (MSE): 2.4959\n",
      "Epoch 236/3000, Training Loss (MSE): 2.5256\n",
      "Epoch 237/3000, Training Loss (MSE): 1.3421\n",
      "Epoch 238/3000, Training Loss (MSE): 1.7404\n",
      "Epoch 239/3000, Training Loss (MSE): 2.5045\n",
      "Epoch 240/3000, Training Loss (MSE): 2.2805\n",
      "Epoch 241/3000, Training Loss (MSE): 1.5810\n",
      "Epoch 242/3000, Training Loss (MSE): 2.3489\n",
      "Epoch 243/3000, Training Loss (MSE): 0.9947\n",
      "Epoch 244/3000, Training Loss (MSE): 2.6449\n",
      "Epoch 245/3000, Training Loss (MSE): 3.3048\n",
      "Epoch 246/3000, Training Loss (MSE): 1.5082\n",
      "Epoch 247/3000, Training Loss (MSE): 2.9779\n",
      "Epoch 248/3000, Training Loss (MSE): 2.3795\n",
      "Epoch 249/3000, Training Loss (MSE): 1.6024\n",
      "Epoch 250/3000, Training Loss (MSE): 1.5271\n",
      "Epoch 251/3000, Training Loss (MSE): 2.2049\n",
      "Epoch 252/3000, Training Loss (MSE): 2.6768\n",
      "Epoch 253/3000, Training Loss (MSE): 1.5867\n",
      "Epoch 254/3000, Training Loss (MSE): 2.2043\n",
      "Epoch 255/3000, Training Loss (MSE): 2.0690\n",
      "Epoch 256/3000, Training Loss (MSE): 1.9494\n",
      "Epoch 257/3000, Training Loss (MSE): 1.8067\n",
      "Epoch 258/3000, Training Loss (MSE): 1.4321\n",
      "Epoch 259/3000, Training Loss (MSE): 1.9817\n",
      "Epoch 260/3000, Training Loss (MSE): 1.9043\n",
      "Epoch 261/3000, Training Loss (MSE): 2.5394\n",
      "Epoch 262/3000, Training Loss (MSE): 1.6497\n",
      "Epoch 263/3000, Training Loss (MSE): 1.9823\n",
      "Epoch 264/3000, Training Loss (MSE): 2.2318\n",
      "Epoch 265/3000, Training Loss (MSE): 3.5948\n",
      "Epoch 266/3000, Training Loss (MSE): 1.7113\n",
      "Epoch 267/3000, Training Loss (MSE): 2.8496\n",
      "Epoch 268/3000, Training Loss (MSE): 2.4644\n",
      "Epoch 269/3000, Training Loss (MSE): 2.2124\n",
      "Epoch 270/3000, Training Loss (MSE): 2.2064\n",
      "Epoch 271/3000, Training Loss (MSE): 2.1450\n",
      "Epoch 272/3000, Training Loss (MSE): 3.7886\n",
      "Epoch 273/3000, Training Loss (MSE): 2.7350\n",
      "Epoch 274/3000, Training Loss (MSE): 2.2051\n",
      "Epoch 275/3000, Training Loss (MSE): 2.9158\n",
      "Epoch 276/3000, Training Loss (MSE): 3.1844\n",
      "Epoch 277/3000, Training Loss (MSE): 1.7070\n",
      "Epoch 278/3000, Training Loss (MSE): 2.8870\n",
      "Epoch 279/3000, Training Loss (MSE): 2.5310\n",
      "Epoch 280/3000, Training Loss (MSE): 1.5895\n",
      "Epoch 281/3000, Training Loss (MSE): 2.0541\n",
      "Epoch 282/3000, Training Loss (MSE): 2.1653\n",
      "Epoch 283/3000, Training Loss (MSE): 0.8239\n",
      "Epoch 284/3000, Training Loss (MSE): 2.3681\n",
      "Epoch 285/3000, Training Loss (MSE): 2.3117\n",
      "Epoch 286/3000, Training Loss (MSE): 2.2586\n",
      "Epoch 287/3000, Training Loss (MSE): 1.7554\n",
      "Epoch 288/3000, Training Loss (MSE): 2.3818\n",
      "Epoch 289/3000, Training Loss (MSE): 2.7474\n",
      "Epoch 290/3000, Training Loss (MSE): 1.3612\n",
      "Epoch 291/3000, Training Loss (MSE): 2.6217\n",
      "Epoch 292/3000, Training Loss (MSE): 3.3781\n",
      "Epoch 293/3000, Training Loss (MSE): 2.4599\n",
      "Epoch 294/3000, Training Loss (MSE): 2.2354\n",
      "Epoch 295/3000, Training Loss (MSE): 2.6979\n",
      "Epoch 296/3000, Training Loss (MSE): 3.0082\n",
      "Epoch 297/3000, Training Loss (MSE): 3.1766\n",
      "Epoch 298/3000, Training Loss (MSE): 2.5888\n",
      "Epoch 299/3000, Training Loss (MSE): 1.8839\n",
      "Epoch 300/3000, Training Loss (MSE): 2.8536\n",
      "Epoch 301/3000, Training Loss (MSE): 3.3876\n",
      "Epoch 302/3000, Training Loss (MSE): 2.4395\n",
      "Epoch 303/3000, Training Loss (MSE): 2.6603\n",
      "Epoch 304/3000, Training Loss (MSE): 3.3777\n",
      "Epoch 305/3000, Training Loss (MSE): 3.9379\n",
      "Epoch 306/3000, Training Loss (MSE): 2.1874\n",
      "Epoch 307/3000, Training Loss (MSE): 3.1227\n",
      "Epoch 308/3000, Training Loss (MSE): 2.5999\n",
      "Epoch 309/3000, Training Loss (MSE): 1.0331\n",
      "Epoch 310/3000, Training Loss (MSE): 1.4375\n",
      "Epoch 311/3000, Training Loss (MSE): 2.7484\n",
      "Epoch 312/3000, Training Loss (MSE): 2.3128\n",
      "Epoch 313/3000, Training Loss (MSE): 2.5035\n",
      "Epoch 314/3000, Training Loss (MSE): 3.1596\n",
      "Epoch 315/3000, Training Loss (MSE): 2.5417\n",
      "Epoch 316/3000, Training Loss (MSE): 3.4097\n",
      "Epoch 317/3000, Training Loss (MSE): 2.8746\n",
      "Epoch 318/3000, Training Loss (MSE): 2.6650\n",
      "Epoch 319/3000, Training Loss (MSE): 2.4350\n",
      "Epoch 320/3000, Training Loss (MSE): 2.5847\n",
      "Epoch 321/3000, Training Loss (MSE): 2.3374\n",
      "Epoch 322/3000, Training Loss (MSE): 2.2253\n",
      "Epoch 323/3000, Training Loss (MSE): 2.7233\n",
      "Epoch 324/3000, Training Loss (MSE): 2.3122\n",
      "Epoch 325/3000, Training Loss (MSE): 1.9488\n",
      "Epoch 326/3000, Training Loss (MSE): 2.6450\n",
      "Epoch 327/3000, Training Loss (MSE): 2.4491\n",
      "Epoch 328/3000, Training Loss (MSE): 1.3745\n",
      "Epoch 329/3000, Training Loss (MSE): 3.0216\n",
      "Epoch 330/3000, Training Loss (MSE): 1.8280\n",
      "Epoch 331/3000, Training Loss (MSE): 2.7944\n",
      "Epoch 332/3000, Training Loss (MSE): 3.1829\n",
      "Epoch 333/3000, Training Loss (MSE): 2.4270\n",
      "Epoch 334/3000, Training Loss (MSE): 1.9043\n",
      "Epoch 335/3000, Training Loss (MSE): 2.8980\n",
      "Epoch 336/3000, Training Loss (MSE): 3.2883\n",
      "Epoch 337/3000, Training Loss (MSE): 3.0582\n",
      "Epoch 338/3000, Training Loss (MSE): 2.4638\n",
      "Epoch 339/3000, Training Loss (MSE): 2.6037\n",
      "Epoch 340/3000, Training Loss (MSE): 1.4046\n",
      "Epoch 341/3000, Training Loss (MSE): 3.0433\n",
      "Epoch 342/3000, Training Loss (MSE): 2.4327\n",
      "Epoch 343/3000, Training Loss (MSE): 1.8001\n",
      "Epoch 344/3000, Training Loss (MSE): 2.4099\n",
      "Epoch 345/3000, Training Loss (MSE): 2.6058\n",
      "Epoch 346/3000, Training Loss (MSE): 3.0136\n",
      "Epoch 347/3000, Training Loss (MSE): 2.7994\n",
      "Epoch 348/3000, Training Loss (MSE): 2.4731\n",
      "Epoch 349/3000, Training Loss (MSE): 3.0455\n",
      "Epoch 350/3000, Training Loss (MSE): 2.3535\n",
      "Epoch 351/3000, Training Loss (MSE): 2.5174\n",
      "Epoch 352/3000, Training Loss (MSE): 2.6473\n",
      "Epoch 353/3000, Training Loss (MSE): 2.2436\n",
      "Epoch 354/3000, Training Loss (MSE): 2.7233\n",
      "Epoch 355/3000, Training Loss (MSE): 1.7664\n",
      "Epoch 356/3000, Training Loss (MSE): 2.0556\n",
      "Epoch 357/3000, Training Loss (MSE): 2.7310\n",
      "Epoch 358/3000, Training Loss (MSE): 1.8638\n",
      "Epoch 359/3000, Training Loss (MSE): 2.3736\n",
      "Epoch 360/3000, Training Loss (MSE): 1.7905\n",
      "Epoch 361/3000, Training Loss (MSE): 2.3686\n",
      "Epoch 362/3000, Training Loss (MSE): 2.6910\n",
      "Epoch 363/3000, Training Loss (MSE): 2.3386\n",
      "Epoch 364/3000, Training Loss (MSE): 3.0237\n",
      "Epoch 365/3000, Training Loss (MSE): 3.0166\n",
      "Epoch 366/3000, Training Loss (MSE): 1.9106\n",
      "Epoch 367/3000, Training Loss (MSE): 3.4974\n",
      "Epoch 368/3000, Training Loss (MSE): 3.8769\n",
      "Epoch 369/3000, Training Loss (MSE): 2.4306\n",
      "Epoch 370/3000, Training Loss (MSE): 2.1744\n",
      "Epoch 371/3000, Training Loss (MSE): 2.7684\n",
      "Epoch 372/3000, Training Loss (MSE): 2.0246\n",
      "Epoch 373/3000, Training Loss (MSE): 2.2805\n",
      "Epoch 374/3000, Training Loss (MSE): 2.8044\n",
      "Epoch 375/3000, Training Loss (MSE): 1.7966\n",
      "Epoch 376/3000, Training Loss (MSE): 2.8969\n",
      "Epoch 377/3000, Training Loss (MSE): 2.3454\n",
      "Epoch 378/3000, Training Loss (MSE): 1.9768\n",
      "Epoch 379/3000, Training Loss (MSE): 2.3044\n",
      "Epoch 380/3000, Training Loss (MSE): 3.0560\n",
      "Epoch 381/3000, Training Loss (MSE): 2.5181\n",
      "Epoch 382/3000, Training Loss (MSE): 2.0435\n",
      "Epoch 383/3000, Training Loss (MSE): 3.4816\n",
      "Epoch 384/3000, Training Loss (MSE): 2.2998\n",
      "Epoch 385/3000, Training Loss (MSE): 3.2431\n",
      "Epoch 386/3000, Training Loss (MSE): 2.2975\n",
      "Epoch 387/3000, Training Loss (MSE): 2.5434\n",
      "Epoch 388/3000, Training Loss (MSE): 2.5085\n",
      "Epoch 389/3000, Training Loss (MSE): 2.7884\n",
      "Epoch 390/3000, Training Loss (MSE): 2.5853\n",
      "Epoch 391/3000, Training Loss (MSE): 2.4462\n",
      "Epoch 392/3000, Training Loss (MSE): 4.0889\n",
      "Epoch 393/3000, Training Loss (MSE): 1.5427\n",
      "Epoch 394/3000, Training Loss (MSE): 2.8717\n",
      "Epoch 395/3000, Training Loss (MSE): 3.6131\n",
      "Epoch 396/3000, Training Loss (MSE): 2.5061\n",
      "Epoch 397/3000, Training Loss (MSE): 2.5966\n",
      "Epoch 398/3000, Training Loss (MSE): 2.5067\n",
      "Epoch 399/3000, Training Loss (MSE): 2.4592\n",
      "Epoch 400/3000, Training Loss (MSE): 2.8623\n",
      "Epoch 401/3000, Training Loss (MSE): 3.5565\n",
      "Epoch 402/3000, Training Loss (MSE): 3.2886\n",
      "Epoch 403/3000, Training Loss (MSE): 2.9609\n",
      "Epoch 404/3000, Training Loss (MSE): 3.3182\n",
      "Epoch 405/3000, Training Loss (MSE): 2.7263\n",
      "Epoch 406/3000, Training Loss (MSE): 3.5125\n",
      "Epoch 407/3000, Training Loss (MSE): 3.0786\n",
      "Epoch 408/3000, Training Loss (MSE): 2.1161\n",
      "Epoch 409/3000, Training Loss (MSE): 2.4167\n",
      "Epoch 410/3000, Training Loss (MSE): 3.0271\n",
      "Epoch 411/3000, Training Loss (MSE): 2.4483\n",
      "Epoch 412/3000, Training Loss (MSE): 2.3126\n",
      "Epoch 413/3000, Training Loss (MSE): 3.9558\n",
      "Epoch 414/3000, Training Loss (MSE): 2.8310\n",
      "Epoch 415/3000, Training Loss (MSE): 2.4036\n",
      "Epoch 416/3000, Training Loss (MSE): 3.2125\n",
      "Epoch 417/3000, Training Loss (MSE): 2.2828\n",
      "Epoch 418/3000, Training Loss (MSE): 3.3068\n",
      "Epoch 419/3000, Training Loss (MSE): 3.5748\n",
      "Epoch 420/3000, Training Loss (MSE): 2.6604\n",
      "Epoch 421/3000, Training Loss (MSE): 3.0111\n",
      "Epoch 422/3000, Training Loss (MSE): 2.9062\n",
      "Epoch 423/3000, Training Loss (MSE): 2.9614\n",
      "Epoch 424/3000, Training Loss (MSE): 1.8472\n",
      "Epoch 425/3000, Training Loss (MSE): 3.1546\n",
      "Epoch 426/3000, Training Loss (MSE): 2.8760\n",
      "Epoch 427/3000, Training Loss (MSE): 2.6650\n",
      "Epoch 428/3000, Training Loss (MSE): 2.7498\n",
      "Epoch 429/3000, Training Loss (MSE): 2.9383\n",
      "Epoch 430/3000, Training Loss (MSE): 3.0130\n",
      "Epoch 431/3000, Training Loss (MSE): 2.5003\n",
      "Epoch 432/3000, Training Loss (MSE): 3.7797\n",
      "Epoch 433/3000, Training Loss (MSE): 3.0448\n",
      "Epoch 434/3000, Training Loss (MSE): 3.0124\n",
      "Epoch 435/3000, Training Loss (MSE): 3.2226\n",
      "Epoch 436/3000, Training Loss (MSE): 3.3038\n",
      "Epoch 437/3000, Training Loss (MSE): 1.6934\n",
      "Epoch 438/3000, Training Loss (MSE): 2.3999\n",
      "Epoch 439/3000, Training Loss (MSE): 3.4638\n",
      "Epoch 440/3000, Training Loss (MSE): 2.6516\n",
      "Epoch 441/3000, Training Loss (MSE): 1.9709\n",
      "Epoch 442/3000, Training Loss (MSE): 3.1532\n",
      "Epoch 443/3000, Training Loss (MSE): 1.9754\n",
      "Epoch 444/3000, Training Loss (MSE): 3.1475\n",
      "Epoch 445/3000, Training Loss (MSE): 2.2704\n",
      "Epoch 446/3000, Training Loss (MSE): 1.7395\n",
      "Epoch 447/3000, Training Loss (MSE): 2.8217\n",
      "Epoch 448/3000, Training Loss (MSE): 2.6622\n",
      "Epoch 449/3000, Training Loss (MSE): 2.8920\n",
      "Epoch 450/3000, Training Loss (MSE): 2.1198\n",
      "Epoch 451/3000, Training Loss (MSE): 2.7286\n",
      "Epoch 452/3000, Training Loss (MSE): 1.9414\n",
      "Epoch 453/3000, Training Loss (MSE): 3.4731\n",
      "Epoch 454/3000, Training Loss (MSE): 3.9344\n",
      "Epoch 455/3000, Training Loss (MSE): 2.8341\n",
      "Epoch 456/3000, Training Loss (MSE): 3.6738\n",
      "Epoch 457/3000, Training Loss (MSE): 3.1754\n",
      "Epoch 458/3000, Training Loss (MSE): 2.9220\n",
      "Epoch 459/3000, Training Loss (MSE): 3.2868\n",
      "Epoch 460/3000, Training Loss (MSE): 4.3262\n",
      "Epoch 461/3000, Training Loss (MSE): 2.2452\n",
      "Epoch 462/3000, Training Loss (MSE): 2.4439\n",
      "Epoch 463/3000, Training Loss (MSE): 2.7747\n",
      "Epoch 464/3000, Training Loss (MSE): 4.2413\n",
      "Epoch 465/3000, Training Loss (MSE): 2.6501\n",
      "Epoch 466/3000, Training Loss (MSE): 3.5023\n",
      "Epoch 467/3000, Training Loss (MSE): 3.6271\n",
      "Epoch 468/3000, Training Loss (MSE): 3.4432\n",
      "Epoch 469/3000, Training Loss (MSE): 2.9232\n",
      "Epoch 470/3000, Training Loss (MSE): 2.8706\n",
      "Epoch 471/3000, Training Loss (MSE): 2.4694\n",
      "Epoch 472/3000, Training Loss (MSE): 1.8877\n",
      "Epoch 473/3000, Training Loss (MSE): 2.7250\n",
      "Epoch 474/3000, Training Loss (MSE): 3.6689\n",
      "Epoch 475/3000, Training Loss (MSE): 2.3677\n",
      "Epoch 476/3000, Training Loss (MSE): 2.1330\n",
      "Epoch 477/3000, Training Loss (MSE): 3.3201\n",
      "Epoch 478/3000, Training Loss (MSE): 2.9132\n",
      "Epoch 479/3000, Training Loss (MSE): 2.9590\n",
      "Epoch 480/3000, Training Loss (MSE): 2.5820\n",
      "Epoch 481/3000, Training Loss (MSE): 2.1264\n",
      "Epoch 482/3000, Training Loss (MSE): 3.6928\n",
      "Epoch 483/3000, Training Loss (MSE): 3.7931\n",
      "Epoch 484/3000, Training Loss (MSE): 2.3865\n",
      "Epoch 485/3000, Training Loss (MSE): 3.1931\n",
      "Epoch 486/3000, Training Loss (MSE): 2.0319\n",
      "Epoch 487/3000, Training Loss (MSE): 3.3648\n",
      "Epoch 488/3000, Training Loss (MSE): 2.8342\n",
      "Epoch 489/3000, Training Loss (MSE): 3.2217\n",
      "Epoch 490/3000, Training Loss (MSE): 2.7979\n",
      "Epoch 491/3000, Training Loss (MSE): 2.9132\n",
      "Epoch 492/3000, Training Loss (MSE): 2.8316\n",
      "Epoch 493/3000, Training Loss (MSE): 3.4726\n",
      "Epoch 494/3000, Training Loss (MSE): 3.2514\n",
      "Epoch 495/3000, Training Loss (MSE): 2.4349\n",
      "Epoch 496/3000, Training Loss (MSE): 2.4429\n",
      "Epoch 497/3000, Training Loss (MSE): 3.1497\n",
      "Epoch 498/3000, Training Loss (MSE): 2.2134\n",
      "Epoch 499/3000, Training Loss (MSE): 2.6456\n",
      "Epoch 500/3000, Training Loss (MSE): 2.9717\n",
      "Epoch 501/3000, Training Loss (MSE): 2.7588\n",
      "Epoch 502/3000, Training Loss (MSE): 2.2415\n",
      "Epoch 503/3000, Training Loss (MSE): 3.3569\n",
      "Epoch 504/3000, Training Loss (MSE): 2.6039\n",
      "Epoch 505/3000, Training Loss (MSE): 2.1288\n",
      "Epoch 506/3000, Training Loss (MSE): 3.9269\n",
      "Epoch 507/3000, Training Loss (MSE): 2.3845\n",
      "Epoch 508/3000, Training Loss (MSE): 3.0243\n",
      "Epoch 509/3000, Training Loss (MSE): 2.9691\n",
      "Epoch 510/3000, Training Loss (MSE): 2.7222\n",
      "Epoch 511/3000, Training Loss (MSE): 2.0546\n",
      "Epoch 512/3000, Training Loss (MSE): 2.8337\n",
      "Epoch 513/3000, Training Loss (MSE): 2.5241\n",
      "Epoch 514/3000, Training Loss (MSE): 2.9991\n",
      "Epoch 515/3000, Training Loss (MSE): 3.1241\n",
      "Epoch 516/3000, Training Loss (MSE): 3.1177\n",
      "Epoch 517/3000, Training Loss (MSE): 3.1570\n",
      "Epoch 518/3000, Training Loss (MSE): 3.7616\n",
      "Epoch 519/3000, Training Loss (MSE): 2.3486\n",
      "Epoch 520/3000, Training Loss (MSE): 2.8145\n",
      "Epoch 521/3000, Training Loss (MSE): 2.8913\n",
      "Epoch 522/3000, Training Loss (MSE): 2.6130\n",
      "Epoch 523/3000, Training Loss (MSE): 2.6452\n",
      "Epoch 524/3000, Training Loss (MSE): 3.6489\n",
      "Epoch 525/3000, Training Loss (MSE): 2.5675\n",
      "Epoch 526/3000, Training Loss (MSE): 2.5736\n",
      "Epoch 527/3000, Training Loss (MSE): 2.4096\n",
      "Epoch 528/3000, Training Loss (MSE): 2.5389\n",
      "Epoch 529/3000, Training Loss (MSE): 3.2970\n",
      "Epoch 530/3000, Training Loss (MSE): 2.9562\n",
      "Epoch 531/3000, Training Loss (MSE): 3.7467\n",
      "Epoch 532/3000, Training Loss (MSE): 2.9848\n",
      "Epoch 533/3000, Training Loss (MSE): 2.8008\n",
      "Epoch 534/3000, Training Loss (MSE): 2.4129\n",
      "Epoch 535/3000, Training Loss (MSE): 3.4522\n",
      "Epoch 536/3000, Training Loss (MSE): 2.6050\n",
      "Epoch 537/3000, Training Loss (MSE): 2.3772\n",
      "Epoch 538/3000, Training Loss (MSE): 2.4166\n",
      "Epoch 539/3000, Training Loss (MSE): 2.9381\n",
      "Epoch 540/3000, Training Loss (MSE): 3.3485\n",
      "Epoch 541/3000, Training Loss (MSE): 2.4596\n",
      "Epoch 542/3000, Training Loss (MSE): 2.5725\n",
      "Epoch 543/3000, Training Loss (MSE): 3.3673\n",
      "Epoch 544/3000, Training Loss (MSE): 2.8131\n",
      "Epoch 545/3000, Training Loss (MSE): 3.5968\n",
      "Epoch 546/3000, Training Loss (MSE): 2.5714\n",
      "Epoch 547/3000, Training Loss (MSE): 2.8218\n",
      "Epoch 548/3000, Training Loss (MSE): 3.6093\n",
      "Epoch 549/3000, Training Loss (MSE): 3.3214\n",
      "Epoch 550/3000, Training Loss (MSE): 2.6113\n",
      "Epoch 551/3000, Training Loss (MSE): 2.7230\n",
      "Epoch 552/3000, Training Loss (MSE): 3.6539\n",
      "Epoch 553/3000, Training Loss (MSE): 2.0355\n",
      "Epoch 554/3000, Training Loss (MSE): 2.9213\n",
      "Epoch 555/3000, Training Loss (MSE): 2.8517\n",
      "Epoch 556/3000, Training Loss (MSE): 2.4368\n",
      "Epoch 557/3000, Training Loss (MSE): 2.0307\n",
      "Epoch 558/3000, Training Loss (MSE): 3.6091\n",
      "Epoch 559/3000, Training Loss (MSE): 2.7210\n",
      "Epoch 560/3000, Training Loss (MSE): 3.1480\n",
      "Epoch 561/3000, Training Loss (MSE): 3.0131\n",
      "Epoch 562/3000, Training Loss (MSE): 2.7360\n",
      "Epoch 563/3000, Training Loss (MSE): 2.9130\n",
      "Epoch 564/3000, Training Loss (MSE): 3.7794\n",
      "Epoch 565/3000, Training Loss (MSE): 2.7976\n",
      "Epoch 566/3000, Training Loss (MSE): 3.5846\n",
      "Epoch 567/3000, Training Loss (MSE): 2.7698\n",
      "Epoch 568/3000, Training Loss (MSE): 3.0479\n",
      "Epoch 569/3000, Training Loss (MSE): 2.9757\n",
      "Epoch 570/3000, Training Loss (MSE): 2.3891\n",
      "Epoch 571/3000, Training Loss (MSE): 3.4154\n",
      "Epoch 572/3000, Training Loss (MSE): 3.0540\n",
      "Epoch 573/3000, Training Loss (MSE): 2.7245\n",
      "Epoch 574/3000, Training Loss (MSE): 2.9438\n",
      "Epoch 575/3000, Training Loss (MSE): 3.1656\n",
      "Epoch 576/3000, Training Loss (MSE): 3.6399\n",
      "Epoch 577/3000, Training Loss (MSE): 3.0691\n",
      "Epoch 578/3000, Training Loss (MSE): 1.9641\n",
      "Epoch 579/3000, Training Loss (MSE): 2.6406\n",
      "Epoch 580/3000, Training Loss (MSE): 4.3496\n",
      "Epoch 581/3000, Training Loss (MSE): 2.3335\n",
      "Epoch 582/3000, Training Loss (MSE): 3.9128\n",
      "Epoch 583/3000, Training Loss (MSE): 3.1098\n",
      "Epoch 584/3000, Training Loss (MSE): 3.1904\n",
      "Epoch 585/3000, Training Loss (MSE): 2.7195\n",
      "Epoch 586/3000, Training Loss (MSE): 3.0520\n",
      "Epoch 587/3000, Training Loss (MSE): 2.6244\n",
      "Epoch 588/3000, Training Loss (MSE): 2.5819\n",
      "Epoch 589/3000, Training Loss (MSE): 2.9449\n",
      "Epoch 590/3000, Training Loss (MSE): 2.4169\n",
      "Epoch 591/3000, Training Loss (MSE): 2.8550\n",
      "Epoch 592/3000, Training Loss (MSE): 2.7883\n",
      "Epoch 593/3000, Training Loss (MSE): 2.1796\n",
      "Epoch 594/3000, Training Loss (MSE): 3.0684\n",
      "Epoch 595/3000, Training Loss (MSE): 3.2083\n",
      "Epoch 596/3000, Training Loss (MSE): 2.5175\n",
      "Epoch 597/3000, Training Loss (MSE): 3.6712\n",
      "Epoch 598/3000, Training Loss (MSE): 2.6504\n",
      "Epoch 599/3000, Training Loss (MSE): 3.4835\n",
      "Epoch 600/3000, Training Loss (MSE): 3.4311\n",
      "Epoch 601/3000, Training Loss (MSE): 3.3052\n",
      "Epoch 602/3000, Training Loss (MSE): 3.1745\n",
      "Epoch 603/3000, Training Loss (MSE): 3.3123\n",
      "Epoch 604/3000, Training Loss (MSE): 1.8651\n",
      "Epoch 605/3000, Training Loss (MSE): 2.5072\n",
      "Epoch 606/3000, Training Loss (MSE): 3.7982\n",
      "Epoch 607/3000, Training Loss (MSE): 3.2290\n",
      "Epoch 608/3000, Training Loss (MSE): 2.5236\n",
      "Epoch 609/3000, Training Loss (MSE): 3.4902\n",
      "Epoch 610/3000, Training Loss (MSE): 2.4318\n",
      "Epoch 611/3000, Training Loss (MSE): 3.2166\n",
      "Epoch 612/3000, Training Loss (MSE): 3.1570\n",
      "Epoch 613/3000, Training Loss (MSE): 2.7052\n",
      "Epoch 614/3000, Training Loss (MSE): 2.7053\n",
      "Epoch 615/3000, Training Loss (MSE): 3.4070\n",
      "Epoch 616/3000, Training Loss (MSE): 3.0825\n",
      "Epoch 617/3000, Training Loss (MSE): 2.9959\n",
      "Epoch 618/3000, Training Loss (MSE): 3.1388\n",
      "Epoch 619/3000, Training Loss (MSE): 2.3638\n",
      "Epoch 620/3000, Training Loss (MSE): 3.5435\n",
      "Epoch 621/3000, Training Loss (MSE): 2.3569\n",
      "Epoch 622/3000, Training Loss (MSE): 3.0000\n",
      "Epoch 623/3000, Training Loss (MSE): 3.0648\n",
      "Epoch 624/3000, Training Loss (MSE): 3.4718\n",
      "Epoch 625/3000, Training Loss (MSE): 3.1366\n",
      "Epoch 626/3000, Training Loss (MSE): 2.6539\n",
      "Epoch 627/3000, Training Loss (MSE): 3.6134\n",
      "Epoch 628/3000, Training Loss (MSE): 2.1855\n",
      "Epoch 629/3000, Training Loss (MSE): 3.1989\n",
      "Epoch 630/3000, Training Loss (MSE): 3.3141\n",
      "Epoch 631/3000, Training Loss (MSE): 3.3809\n",
      "Epoch 632/3000, Training Loss (MSE): 1.9319\n",
      "Epoch 633/3000, Training Loss (MSE): 2.8509\n",
      "Epoch 634/3000, Training Loss (MSE): 2.6571\n",
      "Epoch 635/3000, Training Loss (MSE): 2.5845\n",
      "Epoch 636/3000, Training Loss (MSE): 2.3452\n",
      "Epoch 637/3000, Training Loss (MSE): 3.1081\n",
      "Epoch 638/3000, Training Loss (MSE): 2.2744\n",
      "Epoch 639/3000, Training Loss (MSE): 2.9650\n",
      "Epoch 640/3000, Training Loss (MSE): 3.4422\n",
      "Epoch 641/3000, Training Loss (MSE): 2.0010\n",
      "Epoch 642/3000, Training Loss (MSE): 2.2364\n",
      "Epoch 643/3000, Training Loss (MSE): 3.2737\n",
      "Epoch 644/3000, Training Loss (MSE): 3.0151\n",
      "Epoch 645/3000, Training Loss (MSE): 3.4529\n",
      "Epoch 646/3000, Training Loss (MSE): 2.7961\n",
      "Epoch 647/3000, Training Loss (MSE): 2.5680\n",
      "Epoch 648/3000, Training Loss (MSE): 3.3099\n",
      "Epoch 649/3000, Training Loss (MSE): 3.3214\n",
      "Epoch 650/3000, Training Loss (MSE): 3.5956\n",
      "Epoch 651/3000, Training Loss (MSE): 3.0293\n",
      "Epoch 652/3000, Training Loss (MSE): 2.7116\n",
      "Epoch 653/3000, Training Loss (MSE): 2.8602\n",
      "Epoch 654/3000, Training Loss (MSE): 3.8646\n",
      "Epoch 655/3000, Training Loss (MSE): 3.0151\n",
      "Epoch 656/3000, Training Loss (MSE): 3.1459\n",
      "Epoch 657/3000, Training Loss (MSE): 2.8673\n",
      "Epoch 658/3000, Training Loss (MSE): 2.6710\n",
      "Epoch 659/3000, Training Loss (MSE): 2.2383\n",
      "Epoch 660/3000, Training Loss (MSE): 4.0567\n",
      "Epoch 661/3000, Training Loss (MSE): 1.7634\n",
      "Epoch 662/3000, Training Loss (MSE): 3.6109\n",
      "Epoch 663/3000, Training Loss (MSE): 2.9447\n",
      "Epoch 664/3000, Training Loss (MSE): 2.6518\n",
      "Epoch 665/3000, Training Loss (MSE): 2.6818\n",
      "Epoch 666/3000, Training Loss (MSE): 3.1201\n",
      "Epoch 667/3000, Training Loss (MSE): 2.5198\n",
      "Epoch 668/3000, Training Loss (MSE): 3.3337\n",
      "Epoch 669/3000, Training Loss (MSE): 3.5659\n",
      "Epoch 670/3000, Training Loss (MSE): 3.4114\n",
      "Epoch 671/3000, Training Loss (MSE): 1.8990\n",
      "Epoch 672/3000, Training Loss (MSE): 3.0920\n",
      "Epoch 673/3000, Training Loss (MSE): 3.3595\n",
      "Epoch 674/3000, Training Loss (MSE): 2.9290\n",
      "Epoch 675/3000, Training Loss (MSE): 3.0544\n",
      "Epoch 676/3000, Training Loss (MSE): 2.6910\n",
      "Epoch 677/3000, Training Loss (MSE): 2.9559\n",
      "Epoch 678/3000, Training Loss (MSE): 3.5117\n",
      "Epoch 679/3000, Training Loss (MSE): 3.3907\n",
      "Epoch 680/3000, Training Loss (MSE): 2.5372\n",
      "Epoch 681/3000, Training Loss (MSE): 3.8418\n",
      "Epoch 682/3000, Training Loss (MSE): 3.6980\n",
      "Epoch 683/3000, Training Loss (MSE): 2.7862\n",
      "Epoch 684/3000, Training Loss (MSE): 3.1997\n",
      "Epoch 685/3000, Training Loss (MSE): 2.2238\n",
      "Epoch 686/3000, Training Loss (MSE): 2.8287\n",
      "Epoch 687/3000, Training Loss (MSE): 2.7520\n",
      "Epoch 688/3000, Training Loss (MSE): 3.7138\n",
      "Epoch 689/3000, Training Loss (MSE): 3.6422\n",
      "Epoch 690/3000, Training Loss (MSE): 3.2045\n",
      "Epoch 691/3000, Training Loss (MSE): 2.9047\n",
      "Epoch 692/3000, Training Loss (MSE): 3.1446\n",
      "Epoch 693/3000, Training Loss (MSE): 3.2579\n",
      "Epoch 694/3000, Training Loss (MSE): 2.8593\n",
      "Epoch 695/3000, Training Loss (MSE): 2.9533\n",
      "Epoch 696/3000, Training Loss (MSE): 2.1071\n",
      "Epoch 697/3000, Training Loss (MSE): 2.1073\n",
      "Epoch 698/3000, Training Loss (MSE): 2.9717\n",
      "Epoch 699/3000, Training Loss (MSE): 3.7997\n",
      "Epoch 700/3000, Training Loss (MSE): 2.2987\n",
      "Epoch 701/3000, Training Loss (MSE): 3.8880\n",
      "Epoch 702/3000, Training Loss (MSE): 2.4642\n",
      "Epoch 703/3000, Training Loss (MSE): 3.6041\n",
      "Epoch 704/3000, Training Loss (MSE): 2.3361\n",
      "Epoch 705/3000, Training Loss (MSE): 3.2747\n",
      "Epoch 706/3000, Training Loss (MSE): 3.2378\n",
      "Epoch 707/3000, Training Loss (MSE): 3.2531\n",
      "Epoch 708/3000, Training Loss (MSE): 4.1888\n",
      "Epoch 709/3000, Training Loss (MSE): 3.5924\n",
      "Epoch 710/3000, Training Loss (MSE): 3.2199\n",
      "Epoch 711/3000, Training Loss (MSE): 3.8506\n",
      "Epoch 712/3000, Training Loss (MSE): 3.0401\n",
      "Epoch 713/3000, Training Loss (MSE): 2.4076\n",
      "Epoch 714/3000, Training Loss (MSE): 1.6322\n",
      "Epoch 715/3000, Training Loss (MSE): 2.9683\n",
      "Epoch 716/3000, Training Loss (MSE): 2.5942\n",
      "Epoch 717/3000, Training Loss (MSE): 2.8074\n",
      "Epoch 718/3000, Training Loss (MSE): 3.7536\n",
      "Epoch 719/3000, Training Loss (MSE): 3.6015\n",
      "Epoch 720/3000, Training Loss (MSE): 3.3574\n",
      "Epoch 721/3000, Training Loss (MSE): 2.2100\n",
      "Epoch 722/3000, Training Loss (MSE): 2.8717\n",
      "Epoch 723/3000, Training Loss (MSE): 3.4088\n",
      "Epoch 724/3000, Training Loss (MSE): 2.9128\n",
      "Epoch 725/3000, Training Loss (MSE): 2.9753\n",
      "Epoch 726/3000, Training Loss (MSE): 3.0375\n",
      "Epoch 727/3000, Training Loss (MSE): 2.6219\n",
      "Epoch 728/3000, Training Loss (MSE): 2.5139\n",
      "Epoch 729/3000, Training Loss (MSE): 2.4647\n",
      "Epoch 730/3000, Training Loss (MSE): 2.6674\n",
      "Epoch 731/3000, Training Loss (MSE): 2.8397\n",
      "Epoch 732/3000, Training Loss (MSE): 3.1323\n",
      "Epoch 733/3000, Training Loss (MSE): 1.6540\n",
      "Epoch 734/3000, Training Loss (MSE): 3.0161\n",
      "Epoch 735/3000, Training Loss (MSE): 2.8431\n",
      "Epoch 736/3000, Training Loss (MSE): 2.6918\n",
      "Epoch 737/3000, Training Loss (MSE): 3.1139\n",
      "Epoch 738/3000, Training Loss (MSE): 2.3564\n",
      "Epoch 739/3000, Training Loss (MSE): 2.5564\n",
      "Epoch 740/3000, Training Loss (MSE): 3.3209\n",
      "Epoch 741/3000, Training Loss (MSE): 2.8703\n",
      "Epoch 742/3000, Training Loss (MSE): 3.7829\n",
      "Epoch 743/3000, Training Loss (MSE): 2.9612\n",
      "Epoch 744/3000, Training Loss (MSE): 3.2083\n",
      "Epoch 745/3000, Training Loss (MSE): 1.6649\n",
      "Epoch 746/3000, Training Loss (MSE): 3.3347\n",
      "Epoch 747/3000, Training Loss (MSE): 2.7572\n",
      "Epoch 748/3000, Training Loss (MSE): 3.1374\n",
      "Epoch 749/3000, Training Loss (MSE): 3.4669\n",
      "Epoch 750/3000, Training Loss (MSE): 1.8982\n",
      "Epoch 751/3000, Training Loss (MSE): 2.8020\n",
      "Epoch 752/3000, Training Loss (MSE): 1.9743\n",
      "Epoch 753/3000, Training Loss (MSE): 3.1152\n",
      "Epoch 754/3000, Training Loss (MSE): 2.0251\n",
      "Epoch 755/3000, Training Loss (MSE): 3.4727\n",
      "Epoch 756/3000, Training Loss (MSE): 2.1874\n",
      "Epoch 757/3000, Training Loss (MSE): 3.5226\n",
      "Epoch 758/3000, Training Loss (MSE): 3.7811\n",
      "Epoch 759/3000, Training Loss (MSE): 2.2482\n",
      "Epoch 760/3000, Training Loss (MSE): 3.5614\n",
      "Epoch 761/3000, Training Loss (MSE): 2.4252\n",
      "Epoch 762/3000, Training Loss (MSE): 2.6239\n",
      "Epoch 763/3000, Training Loss (MSE): 2.7977\n",
      "Epoch 764/3000, Training Loss (MSE): 2.7059\n",
      "Epoch 765/3000, Training Loss (MSE): 2.7074\n",
      "Epoch 766/3000, Training Loss (MSE): 3.4390\n",
      "Epoch 767/3000, Training Loss (MSE): 2.1044\n",
      "Epoch 768/3000, Training Loss (MSE): 3.3461\n",
      "Epoch 769/3000, Training Loss (MSE): 2.5168\n",
      "Epoch 770/3000, Training Loss (MSE): 3.1006\n",
      "Epoch 771/3000, Training Loss (MSE): 3.7446\n",
      "Epoch 772/3000, Training Loss (MSE): 3.2195\n",
      "Epoch 773/3000, Training Loss (MSE): 2.6567\n",
      "Epoch 774/3000, Training Loss (MSE): 3.3560\n",
      "Epoch 775/3000, Training Loss (MSE): 3.2834\n",
      "Epoch 776/3000, Training Loss (MSE): 3.1966\n",
      "Epoch 777/3000, Training Loss (MSE): 3.3659\n",
      "Epoch 778/3000, Training Loss (MSE): 2.0730\n",
      "Epoch 779/3000, Training Loss (MSE): 3.2471\n",
      "Epoch 780/3000, Training Loss (MSE): 1.7653\n",
      "Epoch 781/3000, Training Loss (MSE): 2.7475\n",
      "Epoch 782/3000, Training Loss (MSE): 2.8066\n",
      "Epoch 783/3000, Training Loss (MSE): 3.6258\n",
      "Epoch 784/3000, Training Loss (MSE): 2.2096\n",
      "Epoch 785/3000, Training Loss (MSE): 4.2452\n",
      "Epoch 786/3000, Training Loss (MSE): 3.1212\n",
      "Epoch 787/3000, Training Loss (MSE): 1.5932\n",
      "Epoch 788/3000, Training Loss (MSE): 2.0705\n",
      "Epoch 789/3000, Training Loss (MSE): 2.6505\n",
      "Epoch 790/3000, Training Loss (MSE): 2.8261\n",
      "Epoch 791/3000, Training Loss (MSE): 1.7614\n",
      "Epoch 792/3000, Training Loss (MSE): 2.0597\n",
      "Epoch 793/3000, Training Loss (MSE): 4.3429\n",
      "Epoch 794/3000, Training Loss (MSE): 3.3967\n",
      "Epoch 795/3000, Training Loss (MSE): 3.0815\n",
      "Epoch 796/3000, Training Loss (MSE): 2.3276\n",
      "Epoch 797/3000, Training Loss (MSE): 2.9284\n",
      "Epoch 798/3000, Training Loss (MSE): 3.0086\n",
      "Epoch 799/3000, Training Loss (MSE): 1.9385\n",
      "Epoch 800/3000, Training Loss (MSE): 2.9891\n",
      "Epoch 801/3000, Training Loss (MSE): 2.3474\n",
      "Epoch 802/3000, Training Loss (MSE): 1.9229\n",
      "Epoch 803/3000, Training Loss (MSE): 3.2367\n",
      "Epoch 804/3000, Training Loss (MSE): 2.7650\n",
      "Epoch 805/3000, Training Loss (MSE): 2.7226\n",
      "Epoch 806/3000, Training Loss (MSE): 2.7274\n",
      "Epoch 807/3000, Training Loss (MSE): 3.2287\n",
      "Epoch 808/3000, Training Loss (MSE): 3.0467\n",
      "Epoch 809/3000, Training Loss (MSE): 2.7019\n",
      "Epoch 810/3000, Training Loss (MSE): 3.7525\n",
      "Epoch 811/3000, Training Loss (MSE): 3.5085\n",
      "Epoch 812/3000, Training Loss (MSE): 1.5054\n",
      "Epoch 813/3000, Training Loss (MSE): 2.8090\n",
      "Epoch 814/3000, Training Loss (MSE): 2.9908\n",
      "Epoch 815/3000, Training Loss (MSE): 3.3435\n",
      "Epoch 816/3000, Training Loss (MSE): 2.9025\n",
      "Epoch 817/3000, Training Loss (MSE): 2.3699\n",
      "Epoch 818/3000, Training Loss (MSE): 2.9095\n",
      "Epoch 819/3000, Training Loss (MSE): 2.7118\n",
      "Epoch 820/3000, Training Loss (MSE): 3.9668\n",
      "Epoch 821/3000, Training Loss (MSE): 3.5980\n",
      "Epoch 822/3000, Training Loss (MSE): 4.2335\n",
      "Epoch 823/3000, Training Loss (MSE): 2.9785\n",
      "Epoch 824/3000, Training Loss (MSE): 3.7048\n",
      "Epoch 825/3000, Training Loss (MSE): 3.3264\n",
      "Epoch 826/3000, Training Loss (MSE): 2.7783\n",
      "Epoch 827/3000, Training Loss (MSE): 3.6626\n",
      "Epoch 828/3000, Training Loss (MSE): 2.2304\n",
      "Epoch 829/3000, Training Loss (MSE): 3.1289\n",
      "Epoch 830/3000, Training Loss (MSE): 2.6663\n",
      "Epoch 831/3000, Training Loss (MSE): 2.5998\n",
      "Epoch 832/3000, Training Loss (MSE): 3.2176\n",
      "Epoch 833/3000, Training Loss (MSE): 2.5485\n",
      "Epoch 834/3000, Training Loss (MSE): 2.8638\n",
      "Epoch 835/3000, Training Loss (MSE): 2.7451\n",
      "Epoch 836/3000, Training Loss (MSE): 2.9573\n",
      "Epoch 837/3000, Training Loss (MSE): 3.9572\n",
      "Epoch 838/3000, Training Loss (MSE): 3.4607\n",
      "Epoch 839/3000, Training Loss (MSE): 1.6157\n",
      "Epoch 840/3000, Training Loss (MSE): 3.2359\n",
      "Epoch 841/3000, Training Loss (MSE): 2.3983\n",
      "Epoch 842/3000, Training Loss (MSE): 2.0784\n",
      "Epoch 843/3000, Training Loss (MSE): 2.8074\n",
      "Epoch 844/3000, Training Loss (MSE): 2.4698\n",
      "Epoch 845/3000, Training Loss (MSE): 2.7551\n",
      "Epoch 846/3000, Training Loss (MSE): 2.8733\n",
      "Epoch 847/3000, Training Loss (MSE): 3.4366\n",
      "Epoch 848/3000, Training Loss (MSE): 3.0547\n",
      "Epoch 849/3000, Training Loss (MSE): 3.4369\n",
      "Epoch 850/3000, Training Loss (MSE): 2.1569\n",
      "Epoch 851/3000, Training Loss (MSE): 3.4308\n",
      "Epoch 852/3000, Training Loss (MSE): 2.6536\n",
      "Epoch 853/3000, Training Loss (MSE): 2.6694\n",
      "Epoch 854/3000, Training Loss (MSE): 2.0179\n",
      "Epoch 855/3000, Training Loss (MSE): 2.4424\n",
      "Epoch 856/3000, Training Loss (MSE): 3.1288\n",
      "Epoch 857/3000, Training Loss (MSE): 3.8149\n",
      "Epoch 858/3000, Training Loss (MSE): 2.6123\n",
      "Epoch 859/3000, Training Loss (MSE): 3.6292\n",
      "Epoch 860/3000, Training Loss (MSE): 2.9326\n",
      "Epoch 861/3000, Training Loss (MSE): 2.3265\n",
      "Epoch 862/3000, Training Loss (MSE): 3.5390\n",
      "Epoch 863/3000, Training Loss (MSE): 3.7883\n",
      "Epoch 864/3000, Training Loss (MSE): 3.3837\n",
      "Epoch 865/3000, Training Loss (MSE): 3.0151\n",
      "Epoch 866/3000, Training Loss (MSE): 2.7165\n",
      "Epoch 867/3000, Training Loss (MSE): 2.4762\n",
      "Epoch 868/3000, Training Loss (MSE): 3.2145\n",
      "Epoch 869/3000, Training Loss (MSE): 3.2020\n",
      "Epoch 870/3000, Training Loss (MSE): 2.2740\n",
      "Epoch 871/3000, Training Loss (MSE): 2.5361\n",
      "Epoch 872/3000, Training Loss (MSE): 2.9645\n",
      "Epoch 873/3000, Training Loss (MSE): 3.5233\n",
      "Epoch 874/3000, Training Loss (MSE): 1.8474\n",
      "Epoch 875/3000, Training Loss (MSE): 2.8798\n",
      "Epoch 876/3000, Training Loss (MSE): 1.9569\n",
      "Epoch 877/3000, Training Loss (MSE): 2.2978\n",
      "Epoch 878/3000, Training Loss (MSE): 3.6594\n",
      "Epoch 879/3000, Training Loss (MSE): 2.6416\n",
      "Epoch 880/3000, Training Loss (MSE): 2.9971\n",
      "Epoch 881/3000, Training Loss (MSE): 3.2624\n",
      "Epoch 882/3000, Training Loss (MSE): 3.0448\n",
      "Epoch 883/3000, Training Loss (MSE): 3.1951\n",
      "Epoch 884/3000, Training Loss (MSE): 3.0786\n",
      "Epoch 885/3000, Training Loss (MSE): 3.3859\n",
      "Epoch 886/3000, Training Loss (MSE): 2.7617\n",
      "Epoch 887/3000, Training Loss (MSE): 1.6874\n",
      "Epoch 888/3000, Training Loss (MSE): 2.7048\n",
      "Epoch 889/3000, Training Loss (MSE): 2.4393\n",
      "Epoch 890/3000, Training Loss (MSE): 3.2690\n",
      "Epoch 891/3000, Training Loss (MSE): 1.9075\n",
      "Epoch 892/3000, Training Loss (MSE): 2.4748\n",
      "Epoch 893/3000, Training Loss (MSE): 2.8985\n",
      "Epoch 894/3000, Training Loss (MSE): 3.3087\n",
      "Epoch 895/3000, Training Loss (MSE): 3.0170\n",
      "Epoch 896/3000, Training Loss (MSE): 3.0153\n",
      "Epoch 897/3000, Training Loss (MSE): 3.3280\n",
      "Epoch 898/3000, Training Loss (MSE): 2.3156\n",
      "Epoch 899/3000, Training Loss (MSE): 2.9747\n",
      "Epoch 900/3000, Training Loss (MSE): 3.0945\n",
      "Epoch 901/3000, Training Loss (MSE): 3.0694\n",
      "Epoch 902/3000, Training Loss (MSE): 1.9071\n",
      "Epoch 903/3000, Training Loss (MSE): 2.4233\n",
      "Epoch 904/3000, Training Loss (MSE): 1.8932\n",
      "Epoch 905/3000, Training Loss (MSE): 2.6614\n",
      "Epoch 906/3000, Training Loss (MSE): 2.3151\n",
      "Epoch 907/3000, Training Loss (MSE): 2.8133\n",
      "Epoch 908/3000, Training Loss (MSE): 2.6058\n",
      "Epoch 909/3000, Training Loss (MSE): 3.1697\n",
      "Epoch 910/3000, Training Loss (MSE): 2.0150\n",
      "Epoch 911/3000, Training Loss (MSE): 2.5997\n",
      "Epoch 912/3000, Training Loss (MSE): 3.9679\n",
      "Epoch 913/3000, Training Loss (MSE): 2.8825\n",
      "Epoch 914/3000, Training Loss (MSE): 2.9187\n",
      "Epoch 915/3000, Training Loss (MSE): 3.4080\n",
      "Epoch 916/3000, Training Loss (MSE): 2.0079\n",
      "Epoch 917/3000, Training Loss (MSE): 3.2597\n",
      "Epoch 918/3000, Training Loss (MSE): 3.5355\n",
      "Epoch 919/3000, Training Loss (MSE): 2.5028\n",
      "Epoch 920/3000, Training Loss (MSE): 2.6188\n",
      "Epoch 921/3000, Training Loss (MSE): 3.1839\n",
      "Epoch 922/3000, Training Loss (MSE): 2.9331\n",
      "Epoch 923/3000, Training Loss (MSE): 1.3704\n",
      "Epoch 924/3000, Training Loss (MSE): 3.0214\n",
      "Epoch 925/3000, Training Loss (MSE): 2.9376\n",
      "Epoch 926/3000, Training Loss (MSE): 2.9267\n",
      "Epoch 927/3000, Training Loss (MSE): 2.6728\n",
      "Epoch 928/3000, Training Loss (MSE): 3.3647\n",
      "Epoch 929/3000, Training Loss (MSE): 2.8694\n",
      "Epoch 930/3000, Training Loss (MSE): 2.2467\n",
      "Epoch 931/3000, Training Loss (MSE): 3.9165\n",
      "Epoch 932/3000, Training Loss (MSE): 3.5072\n",
      "Epoch 933/3000, Training Loss (MSE): 3.3399\n",
      "Epoch 934/3000, Training Loss (MSE): 3.4935\n",
      "Epoch 935/3000, Training Loss (MSE): 3.2327\n",
      "Epoch 936/3000, Training Loss (MSE): 3.0979\n",
      "Epoch 937/3000, Training Loss (MSE): 2.7592\n",
      "Epoch 938/3000, Training Loss (MSE): 2.8171\n",
      "Epoch 939/3000, Training Loss (MSE): 2.6083\n",
      "Epoch 940/3000, Training Loss (MSE): 2.4704\n",
      "Epoch 941/3000, Training Loss (MSE): 3.2918\n",
      "Epoch 942/3000, Training Loss (MSE): 2.3419\n",
      "Epoch 943/3000, Training Loss (MSE): 1.7418\n",
      "Epoch 944/3000, Training Loss (MSE): 2.6779\n",
      "Epoch 945/3000, Training Loss (MSE): 2.9288\n",
      "Epoch 946/3000, Training Loss (MSE): 2.0033\n",
      "Epoch 947/3000, Training Loss (MSE): 2.4571\n",
      "Epoch 948/3000, Training Loss (MSE): 2.0392\n",
      "Epoch 949/3000, Training Loss (MSE): 2.9495\n",
      "Epoch 950/3000, Training Loss (MSE): 2.7807\n",
      "Epoch 951/3000, Training Loss (MSE): 2.5107\n",
      "Epoch 952/3000, Training Loss (MSE): 3.0111\n",
      "Epoch 953/3000, Training Loss (MSE): 2.5137\n",
      "Epoch 954/3000, Training Loss (MSE): 2.3934\n",
      "Epoch 955/3000, Training Loss (MSE): 2.1599\n",
      "Epoch 956/3000, Training Loss (MSE): 2.5660\n",
      "Epoch 957/3000, Training Loss (MSE): 2.9246\n",
      "Epoch 958/3000, Training Loss (MSE): 2.7750\n",
      "Epoch 959/3000, Training Loss (MSE): 2.6235\n",
      "Epoch 960/3000, Training Loss (MSE): 2.2568\n",
      "Epoch 961/3000, Training Loss (MSE): 2.6292\n",
      "Epoch 962/3000, Training Loss (MSE): 1.8612\n",
      "Epoch 963/3000, Training Loss (MSE): 2.7263\n",
      "Epoch 964/3000, Training Loss (MSE): 2.1451\n",
      "Epoch 965/3000, Training Loss (MSE): 2.9850\n",
      "Epoch 966/3000, Training Loss (MSE): 3.4724\n",
      "Epoch 967/3000, Training Loss (MSE): 2.2028\n",
      "Epoch 968/3000, Training Loss (MSE): 2.5231\n",
      "Epoch 969/3000, Training Loss (MSE): 2.5115\n",
      "Epoch 970/3000, Training Loss (MSE): 2.2653\n",
      "Epoch 971/3000, Training Loss (MSE): 1.8199\n",
      "Epoch 972/3000, Training Loss (MSE): 2.7819\n",
      "Epoch 973/3000, Training Loss (MSE): 3.1055\n",
      "Epoch 974/3000, Training Loss (MSE): 2.9817\n",
      "Epoch 975/3000, Training Loss (MSE): 1.7023\n",
      "Epoch 976/3000, Training Loss (MSE): 2.4321\n",
      "Epoch 977/3000, Training Loss (MSE): 2.6242\n",
      "Epoch 978/3000, Training Loss (MSE): 2.5174\n",
      "Epoch 979/3000, Training Loss (MSE): 2.3468\n",
      "Epoch 980/3000, Training Loss (MSE): 2.7225\n",
      "Epoch 981/3000, Training Loss (MSE): 2.7012\n",
      "Epoch 982/3000, Training Loss (MSE): 2.0180\n",
      "Epoch 983/3000, Training Loss (MSE): 2.5326\n",
      "Epoch 984/3000, Training Loss (MSE): 3.4329\n",
      "Epoch 985/3000, Training Loss (MSE): 3.4544\n",
      "Epoch 986/3000, Training Loss (MSE): 2.7753\n",
      "Epoch 987/3000, Training Loss (MSE): 2.1639\n",
      "Epoch 988/3000, Training Loss (MSE): 2.9672\n",
      "Epoch 989/3000, Training Loss (MSE): 3.5108\n",
      "Epoch 990/3000, Training Loss (MSE): 2.8396\n",
      "Epoch 991/3000, Training Loss (MSE): 2.3682\n",
      "Epoch 992/3000, Training Loss (MSE): 3.6222\n",
      "Epoch 993/3000, Training Loss (MSE): 2.4645\n",
      "Epoch 994/3000, Training Loss (MSE): 2.5438\n",
      "Epoch 995/3000, Training Loss (MSE): 3.3696\n",
      "Epoch 996/3000, Training Loss (MSE): 3.1188\n",
      "Epoch 997/3000, Training Loss (MSE): 3.4278\n",
      "Epoch 998/3000, Training Loss (MSE): 3.4578\n",
      "Epoch 999/3000, Training Loss (MSE): 3.0271\n",
      "Epoch 1000/3000, Training Loss (MSE): 2.6650\n",
      "Epoch 1001/3000, Training Loss (MSE): 2.9702\n",
      "Epoch 1002/3000, Training Loss (MSE): 2.8794\n",
      "Epoch 1003/3000, Training Loss (MSE): 2.4948\n",
      "Epoch 1004/3000, Training Loss (MSE): 2.4879\n",
      "Epoch 1005/3000, Training Loss (MSE): 3.9108\n",
      "Epoch 1006/3000, Training Loss (MSE): 2.6016\n",
      "Epoch 1007/3000, Training Loss (MSE): 3.5245\n",
      "Epoch 1008/3000, Training Loss (MSE): 2.1423\n",
      "Epoch 1009/3000, Training Loss (MSE): 2.4227\n",
      "Epoch 1010/3000, Training Loss (MSE): 1.9301\n",
      "Epoch 1011/3000, Training Loss (MSE): 3.7348\n",
      "Epoch 1012/3000, Training Loss (MSE): 3.7413\n",
      "Epoch 1013/3000, Training Loss (MSE): 2.3274\n",
      "Epoch 1014/3000, Training Loss (MSE): 1.6143\n",
      "Epoch 1015/3000, Training Loss (MSE): 2.7973\n",
      "Epoch 1016/3000, Training Loss (MSE): 2.3224\n",
      "Epoch 1017/3000, Training Loss (MSE): 3.3122\n",
      "Epoch 1018/3000, Training Loss (MSE): 2.7536\n",
      "Epoch 1019/3000, Training Loss (MSE): 1.9425\n",
      "Epoch 1020/3000, Training Loss (MSE): 3.2013\n",
      "Epoch 1021/3000, Training Loss (MSE): 2.6624\n",
      "Epoch 1022/3000, Training Loss (MSE): 3.1906\n",
      "Epoch 1023/3000, Training Loss (MSE): 2.5892\n",
      "Epoch 1024/3000, Training Loss (MSE): 2.1615\n",
      "Epoch 1025/3000, Training Loss (MSE): 2.7282\n",
      "Epoch 1026/3000, Training Loss (MSE): 3.1018\n",
      "Epoch 1027/3000, Training Loss (MSE): 3.3683\n",
      "Epoch 1028/3000, Training Loss (MSE): 3.7739\n",
      "Epoch 1029/3000, Training Loss (MSE): 2.7068\n",
      "Epoch 1030/3000, Training Loss (MSE): 3.8749\n",
      "Epoch 1031/3000, Training Loss (MSE): 2.6218\n",
      "Epoch 1032/3000, Training Loss (MSE): 2.9506\n",
      "Epoch 1033/3000, Training Loss (MSE): 2.6784\n",
      "Epoch 1034/3000, Training Loss (MSE): 3.0611\n",
      "Epoch 1035/3000, Training Loss (MSE): 2.9636\n",
      "Epoch 1036/3000, Training Loss (MSE): 2.0509\n",
      "Epoch 1037/3000, Training Loss (MSE): 3.1945\n",
      "Epoch 1038/3000, Training Loss (MSE): 2.9081\n",
      "Epoch 1039/3000, Training Loss (MSE): 2.5363\n",
      "Epoch 1040/3000, Training Loss (MSE): 2.5227\n",
      "Epoch 1041/3000, Training Loss (MSE): 3.2280\n",
      "Epoch 1042/3000, Training Loss (MSE): 3.1260\n",
      "Epoch 1043/3000, Training Loss (MSE): 2.4437\n",
      "Epoch 1044/3000, Training Loss (MSE): 2.0577\n",
      "Epoch 1045/3000, Training Loss (MSE): 3.3068\n",
      "Epoch 1046/3000, Training Loss (MSE): 2.6539\n",
      "Epoch 1047/3000, Training Loss (MSE): 2.2580\n",
      "Epoch 1048/3000, Training Loss (MSE): 4.0777\n",
      "Epoch 1049/3000, Training Loss (MSE): 3.1348\n",
      "Epoch 1050/3000, Training Loss (MSE): 2.5323\n",
      "Epoch 1051/3000, Training Loss (MSE): 2.7482\n",
      "Epoch 1052/3000, Training Loss (MSE): 1.4151\n",
      "Epoch 1053/3000, Training Loss (MSE): 2.8037\n",
      "Epoch 1054/3000, Training Loss (MSE): 1.9168\n",
      "Epoch 1055/3000, Training Loss (MSE): 3.1821\n",
      "Epoch 1056/3000, Training Loss (MSE): 3.2895\n",
      "Epoch 1057/3000, Training Loss (MSE): 3.2633\n",
      "Epoch 1058/3000, Training Loss (MSE): 3.6368\n",
      "Epoch 1059/3000, Training Loss (MSE): 3.7510\n",
      "Epoch 1060/3000, Training Loss (MSE): 3.2589\n",
      "Epoch 1061/3000, Training Loss (MSE): 3.2492\n",
      "Epoch 1062/3000, Training Loss (MSE): 2.6778\n",
      "Epoch 1063/3000, Training Loss (MSE): 3.2948\n",
      "Epoch 1064/3000, Training Loss (MSE): 2.7784\n",
      "Epoch 1065/3000, Training Loss (MSE): 2.9187\n",
      "Epoch 1066/3000, Training Loss (MSE): 3.0581\n",
      "Epoch 1067/3000, Training Loss (MSE): 2.6718\n",
      "Epoch 1068/3000, Training Loss (MSE): 2.2291\n",
      "Epoch 1069/3000, Training Loss (MSE): 3.6002\n",
      "Epoch 1070/3000, Training Loss (MSE): 3.6066\n",
      "Epoch 1071/3000, Training Loss (MSE): 4.1853\n",
      "Epoch 1072/3000, Training Loss (MSE): 3.2239\n",
      "Epoch 1073/3000, Training Loss (MSE): 3.2699\n",
      "Epoch 1074/3000, Training Loss (MSE): 2.2302\n",
      "Epoch 1075/3000, Training Loss (MSE): 2.9038\n",
      "Epoch 1076/3000, Training Loss (MSE): 1.8509\n",
      "Epoch 1077/3000, Training Loss (MSE): 3.4322\n",
      "Epoch 1078/3000, Training Loss (MSE): 2.4774\n",
      "Epoch 1079/3000, Training Loss (MSE): 3.1701\n",
      "Epoch 1080/3000, Training Loss (MSE): 2.9482\n",
      "Epoch 1081/3000, Training Loss (MSE): 1.5967\n",
      "Epoch 1082/3000, Training Loss (MSE): 2.5870\n",
      "Epoch 1083/3000, Training Loss (MSE): 2.7822\n",
      "Epoch 1084/3000, Training Loss (MSE): 2.6447\n",
      "Epoch 1085/3000, Training Loss (MSE): 2.6825\n",
      "Epoch 1086/3000, Training Loss (MSE): 4.1750\n",
      "Epoch 1087/3000, Training Loss (MSE): 2.6186\n",
      "Epoch 1088/3000, Training Loss (MSE): 2.8653\n",
      "Epoch 1089/3000, Training Loss (MSE): 3.1242\n",
      "Epoch 1090/3000, Training Loss (MSE): 2.7672\n",
      "Epoch 1091/3000, Training Loss (MSE): 2.0649\n",
      "Epoch 1092/3000, Training Loss (MSE): 3.5003\n",
      "Epoch 1093/3000, Training Loss (MSE): 3.4462\n",
      "Epoch 1094/3000, Training Loss (MSE): 2.7329\n",
      "Epoch 1095/3000, Training Loss (MSE): 3.7347\n",
      "Epoch 1096/3000, Training Loss (MSE): 2.3281\n",
      "Epoch 1097/3000, Training Loss (MSE): 2.5002\n",
      "Epoch 1098/3000, Training Loss (MSE): 2.4617\n",
      "Epoch 1099/3000, Training Loss (MSE): 1.7247\n",
      "Epoch 1100/3000, Training Loss (MSE): 3.8427\n",
      "Epoch 1101/3000, Training Loss (MSE): 2.9330\n",
      "Epoch 1102/3000, Training Loss (MSE): 3.3764\n",
      "Epoch 1103/3000, Training Loss (MSE): 3.4798\n",
      "Epoch 1104/3000, Training Loss (MSE): 3.0411\n",
      "Epoch 1105/3000, Training Loss (MSE): 3.5140\n",
      "Epoch 1106/3000, Training Loss (MSE): 3.2534\n",
      "Epoch 1107/3000, Training Loss (MSE): 2.8035\n",
      "Epoch 1108/3000, Training Loss (MSE): 3.3808\n",
      "Epoch 1109/3000, Training Loss (MSE): 3.8628\n",
      "Epoch 1110/3000, Training Loss (MSE): 2.9984\n",
      "Epoch 1111/3000, Training Loss (MSE): 2.9843\n",
      "Epoch 1112/3000, Training Loss (MSE): 3.4024\n",
      "Epoch 1113/3000, Training Loss (MSE): 3.1998\n",
      "Epoch 1114/3000, Training Loss (MSE): 3.1397\n",
      "Epoch 1115/3000, Training Loss (MSE): 2.4422\n",
      "Epoch 1116/3000, Training Loss (MSE): 3.8490\n",
      "Epoch 1117/3000, Training Loss (MSE): 2.3769\n",
      "Epoch 1118/3000, Training Loss (MSE): 3.2928\n",
      "Epoch 1119/3000, Training Loss (MSE): 3.6044\n",
      "Epoch 1120/3000, Training Loss (MSE): 2.0812\n",
      "Epoch 1121/3000, Training Loss (MSE): 2.4913\n",
      "Epoch 1122/3000, Training Loss (MSE): 3.5320\n",
      "Epoch 1123/3000, Training Loss (MSE): 2.9983\n",
      "Epoch 1124/3000, Training Loss (MSE): 2.4449\n",
      "Epoch 1125/3000, Training Loss (MSE): 2.7489\n",
      "Epoch 1126/3000, Training Loss (MSE): 3.9433\n",
      "Epoch 1127/3000, Training Loss (MSE): 3.7104\n",
      "Epoch 1128/3000, Training Loss (MSE): 2.6629\n",
      "Epoch 1129/3000, Training Loss (MSE): 2.5321\n",
      "Epoch 1130/3000, Training Loss (MSE): 2.7174\n",
      "Epoch 1131/3000, Training Loss (MSE): 3.3143\n",
      "Epoch 1132/3000, Training Loss (MSE): 2.8002\n",
      "Epoch 1133/3000, Training Loss (MSE): 2.1142\n",
      "Epoch 1134/3000, Training Loss (MSE): 2.5187\n",
      "Epoch 1135/3000, Training Loss (MSE): 3.8090\n",
      "Epoch 1136/3000, Training Loss (MSE): 2.9403\n",
      "Epoch 1137/3000, Training Loss (MSE): 2.8211\n",
      "Epoch 1138/3000, Training Loss (MSE): 2.8971\n",
      "Epoch 1139/3000, Training Loss (MSE): 3.0845\n",
      "Epoch 1140/3000, Training Loss (MSE): 1.6582\n",
      "Epoch 1141/3000, Training Loss (MSE): 2.6191\n",
      "Epoch 1142/3000, Training Loss (MSE): 2.8045\n",
      "Epoch 1143/3000, Training Loss (MSE): 3.4328\n",
      "Epoch 1144/3000, Training Loss (MSE): 3.7536\n",
      "Epoch 1145/3000, Training Loss (MSE): 3.0081\n",
      "Epoch 1146/3000, Training Loss (MSE): 2.9455\n",
      "Epoch 1147/3000, Training Loss (MSE): 3.6251\n",
      "Epoch 1148/3000, Training Loss (MSE): 2.7312\n",
      "Epoch 1149/3000, Training Loss (MSE): 2.6140\n",
      "Epoch 1150/3000, Training Loss (MSE): 3.2664\n",
      "Epoch 1151/3000, Training Loss (MSE): 2.6349\n",
      "Epoch 1152/3000, Training Loss (MSE): 2.9143\n",
      "Epoch 1153/3000, Training Loss (MSE): 2.5325\n",
      "Epoch 1154/3000, Training Loss (MSE): 2.4896\n",
      "Epoch 1155/3000, Training Loss (MSE): 2.8158\n",
      "Epoch 1156/3000, Training Loss (MSE): 3.0622\n",
      "Epoch 1157/3000, Training Loss (MSE): 2.3002\n",
      "Epoch 1158/3000, Training Loss (MSE): 3.5019\n",
      "Epoch 1159/3000, Training Loss (MSE): 2.7699\n",
      "Epoch 1160/3000, Training Loss (MSE): 2.6921\n",
      "Epoch 1161/3000, Training Loss (MSE): 2.7720\n",
      "Epoch 1162/3000, Training Loss (MSE): 3.3325\n",
      "Epoch 1163/3000, Training Loss (MSE): 2.3246\n",
      "Epoch 1164/3000, Training Loss (MSE): 3.4351\n",
      "Epoch 1165/3000, Training Loss (MSE): 3.0919\n",
      "Epoch 1166/3000, Training Loss (MSE): 2.9352\n",
      "Epoch 1167/3000, Training Loss (MSE): 2.0417\n",
      "Epoch 1168/3000, Training Loss (MSE): 2.3805\n",
      "Epoch 1169/3000, Training Loss (MSE): 2.8560\n",
      "Epoch 1170/3000, Training Loss (MSE): 2.5814\n",
      "Epoch 1171/3000, Training Loss (MSE): 2.4402\n",
      "Epoch 1172/3000, Training Loss (MSE): 2.8660\n",
      "Epoch 1173/3000, Training Loss (MSE): 2.0725\n",
      "Epoch 1174/3000, Training Loss (MSE): 2.8270\n",
      "Epoch 1175/3000, Training Loss (MSE): 2.3251\n",
      "Epoch 1176/3000, Training Loss (MSE): 2.3744\n",
      "Epoch 1177/3000, Training Loss (MSE): 2.6922\n",
      "Epoch 1178/3000, Training Loss (MSE): 3.7053\n",
      "Epoch 1179/3000, Training Loss (MSE): 2.6605\n",
      "Epoch 1180/3000, Training Loss (MSE): 3.5097\n",
      "Epoch 1181/3000, Training Loss (MSE): 2.4829\n",
      "Epoch 1182/3000, Training Loss (MSE): 2.5237\n",
      "Epoch 1183/3000, Training Loss (MSE): 2.7901\n",
      "Epoch 1184/3000, Training Loss (MSE): 1.9323\n",
      "Epoch 1185/3000, Training Loss (MSE): 3.4267\n",
      "Epoch 1186/3000, Training Loss (MSE): 3.0474\n",
      "Epoch 1187/3000, Training Loss (MSE): 3.1201\n",
      "Epoch 1188/3000, Training Loss (MSE): 3.0981\n",
      "Epoch 1189/3000, Training Loss (MSE): 2.3319\n",
      "Epoch 1190/3000, Training Loss (MSE): 2.4783\n",
      "Epoch 1191/3000, Training Loss (MSE): 2.0987\n",
      "Epoch 1192/3000, Training Loss (MSE): 2.8596\n",
      "Epoch 1193/3000, Training Loss (MSE): 2.4812\n",
      "Epoch 1194/3000, Training Loss (MSE): 2.4259\n",
      "Epoch 1195/3000, Training Loss (MSE): 3.1292\n",
      "Epoch 1196/3000, Training Loss (MSE): 2.6924\n",
      "Epoch 1197/3000, Training Loss (MSE): 2.4397\n",
      "Epoch 1198/3000, Training Loss (MSE): 2.9110\n",
      "Epoch 1199/3000, Training Loss (MSE): 3.0085\n",
      "Epoch 1200/3000, Training Loss (MSE): 3.2081\n",
      "Epoch 1201/3000, Training Loss (MSE): 3.4778\n",
      "Epoch 1202/3000, Training Loss (MSE): 2.0851\n",
      "Epoch 1203/3000, Training Loss (MSE): 3.4615\n",
      "Epoch 1204/3000, Training Loss (MSE): 2.2582\n",
      "Epoch 1205/3000, Training Loss (MSE): 3.3125\n",
      "Epoch 1206/3000, Training Loss (MSE): 3.2592\n",
      "Epoch 1207/3000, Training Loss (MSE): 3.5816\n",
      "Epoch 1208/3000, Training Loss (MSE): 2.4197\n",
      "Epoch 1209/3000, Training Loss (MSE): 2.5439\n",
      "Epoch 1210/3000, Training Loss (MSE): 2.5399\n",
      "Epoch 1211/3000, Training Loss (MSE): 2.2951\n",
      "Epoch 1212/3000, Training Loss (MSE): 2.8197\n",
      "Epoch 1213/3000, Training Loss (MSE): 2.6432\n",
      "Epoch 1214/3000, Training Loss (MSE): 3.4027\n",
      "Epoch 1215/3000, Training Loss (MSE): 2.5213\n",
      "Epoch 1216/3000, Training Loss (MSE): 2.8603\n",
      "Epoch 1217/3000, Training Loss (MSE): 2.0808\n",
      "Epoch 1218/3000, Training Loss (MSE): 3.2849\n",
      "Epoch 1219/3000, Training Loss (MSE): 3.1352\n",
      "Epoch 1220/3000, Training Loss (MSE): 2.7117\n",
      "Epoch 1221/3000, Training Loss (MSE): 1.5659\n",
      "Epoch 1222/3000, Training Loss (MSE): 3.1908\n",
      "Epoch 1223/3000, Training Loss (MSE): 2.6788\n",
      "Epoch 1224/3000, Training Loss (MSE): 3.3174\n",
      "Epoch 1225/3000, Training Loss (MSE): 1.6957\n",
      "Epoch 1226/3000, Training Loss (MSE): 2.9701\n",
      "Epoch 1227/3000, Training Loss (MSE): 2.5696\n",
      "Epoch 1228/3000, Training Loss (MSE): 2.7761\n",
      "Epoch 1229/3000, Training Loss (MSE): 2.6910\n",
      "Epoch 1230/3000, Training Loss (MSE): 2.5039\n",
      "Epoch 1231/3000, Training Loss (MSE): 3.1240\n",
      "Epoch 1232/3000, Training Loss (MSE): 3.1506\n",
      "Epoch 1233/3000, Training Loss (MSE): 2.2233\n",
      "Epoch 1234/3000, Training Loss (MSE): 2.8253\n",
      "Epoch 1235/3000, Training Loss (MSE): 3.3023\n",
      "Epoch 1236/3000, Training Loss (MSE): 3.3244\n",
      "Epoch 1237/3000, Training Loss (MSE): 3.0503\n",
      "Epoch 1238/3000, Training Loss (MSE): 2.5391\n",
      "Epoch 1239/3000, Training Loss (MSE): 3.2730\n",
      "Epoch 1240/3000, Training Loss (MSE): 2.5803\n",
      "Epoch 1241/3000, Training Loss (MSE): 2.8573\n",
      "Epoch 1242/3000, Training Loss (MSE): 2.8584\n",
      "Epoch 1243/3000, Training Loss (MSE): 1.7244\n",
      "Epoch 1244/3000, Training Loss (MSE): 3.1906\n",
      "Epoch 1245/3000, Training Loss (MSE): 3.1220\n",
      "Epoch 1246/3000, Training Loss (MSE): 3.6101\n",
      "Epoch 1247/3000, Training Loss (MSE): 2.4806\n",
      "Epoch 1248/3000, Training Loss (MSE): 2.3214\n",
      "Epoch 1249/3000, Training Loss (MSE): 2.8956\n",
      "Epoch 1250/3000, Training Loss (MSE): 2.3048\n",
      "Epoch 1251/3000, Training Loss (MSE): 2.6795\n",
      "Epoch 1252/3000, Training Loss (MSE): 3.1743\n",
      "Epoch 1253/3000, Training Loss (MSE): 3.1063\n",
      "Epoch 1254/3000, Training Loss (MSE): 3.5827\n",
      "Epoch 1255/3000, Training Loss (MSE): 2.8656\n",
      "Epoch 1256/3000, Training Loss (MSE): 2.5282\n",
      "Epoch 1257/3000, Training Loss (MSE): 3.4038\n",
      "Epoch 1258/3000, Training Loss (MSE): 2.8837\n",
      "Epoch 1259/3000, Training Loss (MSE): 2.4135\n",
      "Epoch 1260/3000, Training Loss (MSE): 4.0025\n",
      "Epoch 1261/3000, Training Loss (MSE): 2.6672\n",
      "Epoch 1262/3000, Training Loss (MSE): 2.9802\n",
      "Epoch 1263/3000, Training Loss (MSE): 3.1804\n",
      "Epoch 1264/3000, Training Loss (MSE): 2.8131\n",
      "Epoch 1265/3000, Training Loss (MSE): 2.5631\n",
      "Epoch 1266/3000, Training Loss (MSE): 2.2716\n",
      "Epoch 1267/3000, Training Loss (MSE): 2.7257\n",
      "Epoch 1268/3000, Training Loss (MSE): 3.1334\n",
      "Epoch 1269/3000, Training Loss (MSE): 2.5614\n",
      "Epoch 1270/3000, Training Loss (MSE): 2.9954\n",
      "Epoch 1271/3000, Training Loss (MSE): 2.8121\n",
      "Epoch 1272/3000, Training Loss (MSE): 3.2167\n",
      "Epoch 1273/3000, Training Loss (MSE): 3.3430\n",
      "Epoch 1274/3000, Training Loss (MSE): 2.8035\n",
      "Epoch 1275/3000, Training Loss (MSE): 3.0548\n",
      "Epoch 1276/3000, Training Loss (MSE): 2.4618\n",
      "Epoch 1277/3000, Training Loss (MSE): 2.8392\n",
      "Epoch 1278/3000, Training Loss (MSE): 2.4458\n",
      "Epoch 1279/3000, Training Loss (MSE): 2.8474\n",
      "Epoch 1280/3000, Training Loss (MSE): 3.0639\n",
      "Epoch 1281/3000, Training Loss (MSE): 2.4121\n",
      "Epoch 1282/3000, Training Loss (MSE): 3.0487\n",
      "Epoch 1283/3000, Training Loss (MSE): 2.8762\n",
      "Epoch 1284/3000, Training Loss (MSE): 2.9807\n",
      "Epoch 1285/3000, Training Loss (MSE): 2.8344\n",
      "Epoch 1286/3000, Training Loss (MSE): 2.8972\n",
      "Epoch 1287/3000, Training Loss (MSE): 3.4544\n",
      "Epoch 1288/3000, Training Loss (MSE): 2.0608\n",
      "Epoch 1289/3000, Training Loss (MSE): 3.0910\n",
      "Epoch 1290/3000, Training Loss (MSE): 3.1125\n",
      "Epoch 1291/3000, Training Loss (MSE): 2.2802\n",
      "Epoch 1292/3000, Training Loss (MSE): 3.3954\n",
      "Epoch 1293/3000, Training Loss (MSE): 2.7403\n",
      "Epoch 1294/3000, Training Loss (MSE): 3.6699\n",
      "Epoch 1295/3000, Training Loss (MSE): 2.8943\n",
      "Epoch 1296/3000, Training Loss (MSE): 2.2497\n",
      "Epoch 1297/3000, Training Loss (MSE): 2.8380\n",
      "Epoch 1298/3000, Training Loss (MSE): 2.6831\n",
      "Epoch 1299/3000, Training Loss (MSE): 2.6216\n",
      "Epoch 1300/3000, Training Loss (MSE): 3.1611\n",
      "Epoch 1301/3000, Training Loss (MSE): 3.0384\n",
      "Epoch 1302/3000, Training Loss (MSE): 3.1552\n",
      "Epoch 1303/3000, Training Loss (MSE): 3.3746\n",
      "Epoch 1304/3000, Training Loss (MSE): 3.1828\n",
      "Epoch 1305/3000, Training Loss (MSE): 3.1691\n",
      "Epoch 1306/3000, Training Loss (MSE): 3.0091\n",
      "Epoch 1307/3000, Training Loss (MSE): 3.1923\n",
      "Epoch 1308/3000, Training Loss (MSE): 2.6807\n",
      "Epoch 1309/3000, Training Loss (MSE): 3.3606\n",
      "Epoch 1310/3000, Training Loss (MSE): 2.6828\n",
      "Epoch 1311/3000, Training Loss (MSE): 2.3080\n",
      "Epoch 1312/3000, Training Loss (MSE): 3.8734\n",
      "Epoch 1313/3000, Training Loss (MSE): 3.6006\n",
      "Epoch 1314/3000, Training Loss (MSE): 1.8591\n",
      "Epoch 1315/3000, Training Loss (MSE): 3.9336\n",
      "Epoch 1316/3000, Training Loss (MSE): 2.8627\n",
      "Epoch 1317/3000, Training Loss (MSE): 2.4713\n",
      "Epoch 1318/3000, Training Loss (MSE): 2.3545\n",
      "Epoch 1319/3000, Training Loss (MSE): 2.0513\n",
      "Epoch 1320/3000, Training Loss (MSE): 3.1051\n",
      "Epoch 1321/3000, Training Loss (MSE): 3.5089\n",
      "Epoch 1322/3000, Training Loss (MSE): 3.0546\n",
      "Epoch 1323/3000, Training Loss (MSE): 3.3794\n",
      "Epoch 1324/3000, Training Loss (MSE): 1.6123\n",
      "Epoch 1325/3000, Training Loss (MSE): 2.8519\n",
      "Epoch 1326/3000, Training Loss (MSE): 2.0363\n",
      "Epoch 1327/3000, Training Loss (MSE): 3.2140\n",
      "Epoch 1328/3000, Training Loss (MSE): 3.1910\n",
      "Epoch 1329/3000, Training Loss (MSE): 3.5600\n",
      "Epoch 1330/3000, Training Loss (MSE): 2.4990\n",
      "Epoch 1331/3000, Training Loss (MSE): 2.7898\n",
      "Epoch 1332/3000, Training Loss (MSE): 3.4943\n",
      "Epoch 1333/3000, Training Loss (MSE): 2.7423\n",
      "Epoch 1334/3000, Training Loss (MSE): 3.0805\n",
      "Epoch 1335/3000, Training Loss (MSE): 3.7198\n",
      "Epoch 1336/3000, Training Loss (MSE): 3.1832\n",
      "Epoch 1337/3000, Training Loss (MSE): 3.2090\n",
      "Epoch 1338/3000, Training Loss (MSE): 2.8821\n",
      "Epoch 1339/3000, Training Loss (MSE): 2.6300\n",
      "Epoch 1340/3000, Training Loss (MSE): 3.9554\n",
      "Epoch 1341/3000, Training Loss (MSE): 2.8000\n",
      "Epoch 1342/3000, Training Loss (MSE): 2.3733\n",
      "Epoch 1343/3000, Training Loss (MSE): 1.8971\n",
      "Epoch 1344/3000, Training Loss (MSE): 2.5361\n",
      "Epoch 1345/3000, Training Loss (MSE): 4.0777\n",
      "Epoch 1346/3000, Training Loss (MSE): 2.9403\n",
      "Epoch 1347/3000, Training Loss (MSE): 2.9712\n",
      "Epoch 1348/3000, Training Loss (MSE): 2.4880\n",
      "Epoch 1349/3000, Training Loss (MSE): 3.3297\n",
      "Epoch 1350/3000, Training Loss (MSE): 2.8681\n",
      "Epoch 1351/3000, Training Loss (MSE): 2.8251\n",
      "Epoch 1352/3000, Training Loss (MSE): 3.3744\n",
      "Epoch 1353/3000, Training Loss (MSE): 2.0568\n",
      "Epoch 1354/3000, Training Loss (MSE): 2.7976\n",
      "Epoch 1355/3000, Training Loss (MSE): 3.0516\n",
      "Epoch 1356/3000, Training Loss (MSE): 2.8001\n",
      "Epoch 1357/3000, Training Loss (MSE): 2.6302\n",
      "Epoch 1358/3000, Training Loss (MSE): 3.6971\n",
      "Epoch 1359/3000, Training Loss (MSE): 3.0560\n",
      "Epoch 1360/3000, Training Loss (MSE): 2.3291\n",
      "Epoch 1361/3000, Training Loss (MSE): 2.3851\n",
      "Epoch 1362/3000, Training Loss (MSE): 2.4643\n",
      "Epoch 1363/3000, Training Loss (MSE): 2.9789\n",
      "Epoch 1364/3000, Training Loss (MSE): 2.9038\n",
      "Epoch 1365/3000, Training Loss (MSE): 1.7829\n",
      "Epoch 1366/3000, Training Loss (MSE): 2.5324\n",
      "Epoch 1367/3000, Training Loss (MSE): 2.0904\n",
      "Epoch 1368/3000, Training Loss (MSE): 2.4220\n",
      "Epoch 1369/3000, Training Loss (MSE): 3.0910\n",
      "Epoch 1370/3000, Training Loss (MSE): 2.2480\n",
      "Epoch 1371/3000, Training Loss (MSE): 1.3894\n",
      "Epoch 1372/3000, Training Loss (MSE): 2.9466\n",
      "Epoch 1373/3000, Training Loss (MSE): 2.7024\n",
      "Epoch 1374/3000, Training Loss (MSE): 3.2683\n",
      "Epoch 1375/3000, Training Loss (MSE): 2.9519\n",
      "Epoch 1376/3000, Training Loss (MSE): 2.5947\n",
      "Epoch 1377/3000, Training Loss (MSE): 2.8615\n",
      "Epoch 1378/3000, Training Loss (MSE): 2.7776\n",
      "Epoch 1379/3000, Training Loss (MSE): 2.3799\n",
      "Epoch 1380/3000, Training Loss (MSE): 2.1030\n",
      "Epoch 1381/3000, Training Loss (MSE): 2.7166\n",
      "Epoch 1382/3000, Training Loss (MSE): 3.1263\n",
      "Epoch 1383/3000, Training Loss (MSE): 3.1630\n",
      "Epoch 1384/3000, Training Loss (MSE): 2.5830\n",
      "Epoch 1385/3000, Training Loss (MSE): 2.3454\n",
      "Epoch 1386/3000, Training Loss (MSE): 2.8953\n",
      "Epoch 1387/3000, Training Loss (MSE): 1.8169\n",
      "Epoch 1388/3000, Training Loss (MSE): 3.1683\n",
      "Epoch 1389/3000, Training Loss (MSE): 3.0350\n",
      "Epoch 1390/3000, Training Loss (MSE): 2.4379\n",
      "Epoch 1391/3000, Training Loss (MSE): 3.2725\n",
      "Epoch 1392/3000, Training Loss (MSE): 2.9218\n",
      "Epoch 1393/3000, Training Loss (MSE): 3.6759\n",
      "Epoch 1394/3000, Training Loss (MSE): 3.7110\n",
      "Epoch 1395/3000, Training Loss (MSE): 3.4927\n",
      "Epoch 1396/3000, Training Loss (MSE): 2.8613\n",
      "Epoch 1397/3000, Training Loss (MSE): 2.6617\n",
      "Epoch 1398/3000, Training Loss (MSE): 2.2254\n",
      "Epoch 1399/3000, Training Loss (MSE): 3.3835\n",
      "Epoch 1400/3000, Training Loss (MSE): 2.9844\n",
      "Epoch 1401/3000, Training Loss (MSE): 2.2941\n",
      "Epoch 1402/3000, Training Loss (MSE): 2.9183\n",
      "Epoch 1403/3000, Training Loss (MSE): 3.5919\n",
      "Epoch 1404/3000, Training Loss (MSE): 2.8501\n",
      "Epoch 1405/3000, Training Loss (MSE): 2.7717\n",
      "Epoch 1406/3000, Training Loss (MSE): 2.4703\n",
      "Epoch 1407/3000, Training Loss (MSE): 2.9609\n",
      "Epoch 1408/3000, Training Loss (MSE): 3.5242\n",
      "Epoch 1409/3000, Training Loss (MSE): 2.4471\n",
      "Epoch 1410/3000, Training Loss (MSE): 2.9996\n",
      "Epoch 1411/3000, Training Loss (MSE): 3.0017\n",
      "Epoch 1412/3000, Training Loss (MSE): 2.3919\n",
      "Epoch 1413/3000, Training Loss (MSE): 3.1050\n",
      "Epoch 1414/3000, Training Loss (MSE): 2.8850\n",
      "Epoch 1415/3000, Training Loss (MSE): 2.0326\n",
      "Epoch 1416/3000, Training Loss (MSE): 2.9708\n",
      "Epoch 1417/3000, Training Loss (MSE): 3.5628\n",
      "Epoch 1418/3000, Training Loss (MSE): 2.7939\n",
      "Epoch 1419/3000, Training Loss (MSE): 3.8732\n",
      "Epoch 1420/3000, Training Loss (MSE): 2.5134\n",
      "Epoch 1421/3000, Training Loss (MSE): 2.7997\n",
      "Epoch 1422/3000, Training Loss (MSE): 2.5789\n",
      "Epoch 1423/3000, Training Loss (MSE): 3.2791\n",
      "Epoch 1424/3000, Training Loss (MSE): 2.2595\n",
      "Epoch 1425/3000, Training Loss (MSE): 3.5220\n",
      "Epoch 1426/3000, Training Loss (MSE): 2.4202\n",
      "Epoch 1427/3000, Training Loss (MSE): 3.0362\n",
      "Epoch 1428/3000, Training Loss (MSE): 2.7216\n",
      "Epoch 1429/3000, Training Loss (MSE): 3.9953\n",
      "Epoch 1430/3000, Training Loss (MSE): 2.8233\n",
      "Epoch 1431/3000, Training Loss (MSE): 2.0960\n",
      "Epoch 1432/3000, Training Loss (MSE): 2.6465\n",
      "Epoch 1433/3000, Training Loss (MSE): 2.8367\n",
      "Epoch 1434/3000, Training Loss (MSE): 3.1359\n",
      "Epoch 1435/3000, Training Loss (MSE): 2.5533\n",
      "Epoch 1436/3000, Training Loss (MSE): 1.9304\n",
      "Epoch 1437/3000, Training Loss (MSE): 3.4201\n",
      "Epoch 1438/3000, Training Loss (MSE): 3.2882\n",
      "Epoch 1439/3000, Training Loss (MSE): 2.8641\n",
      "Epoch 1440/3000, Training Loss (MSE): 2.6829\n",
      "Epoch 1441/3000, Training Loss (MSE): 2.6068\n",
      "Epoch 1442/3000, Training Loss (MSE): 2.8722\n",
      "Epoch 1443/3000, Training Loss (MSE): 3.0781\n",
      "Epoch 1444/3000, Training Loss (MSE): 3.0991\n",
      "Epoch 1445/3000, Training Loss (MSE): 2.3739\n",
      "Epoch 1446/3000, Training Loss (MSE): 3.4479\n",
      "Epoch 1447/3000, Training Loss (MSE): 2.4875\n",
      "Epoch 1448/3000, Training Loss (MSE): 2.8233\n",
      "Epoch 1449/3000, Training Loss (MSE): 2.2645\n",
      "Epoch 1450/3000, Training Loss (MSE): 3.5782\n",
      "Epoch 1451/3000, Training Loss (MSE): 3.1238\n",
      "Epoch 1452/3000, Training Loss (MSE): 2.6044\n",
      "Epoch 1453/3000, Training Loss (MSE): 1.9293\n",
      "Epoch 1454/3000, Training Loss (MSE): 2.5711\n",
      "Epoch 1455/3000, Training Loss (MSE): 3.4025\n",
      "Epoch 1456/3000, Training Loss (MSE): 3.0845\n",
      "Epoch 1457/3000, Training Loss (MSE): 3.0210\n",
      "Epoch 1458/3000, Training Loss (MSE): 3.4677\n",
      "Epoch 1459/3000, Training Loss (MSE): 3.1008\n",
      "Epoch 1460/3000, Training Loss (MSE): 2.7739\n",
      "Epoch 1461/3000, Training Loss (MSE): 3.1251\n",
      "Epoch 1462/3000, Training Loss (MSE): 2.8734\n",
      "Epoch 1463/3000, Training Loss (MSE): 1.9512\n",
      "Epoch 1464/3000, Training Loss (MSE): 2.8072\n",
      "Epoch 1465/3000, Training Loss (MSE): 3.3533\n",
      "Epoch 1466/3000, Training Loss (MSE): 2.6923\n",
      "Epoch 1467/3000, Training Loss (MSE): 1.7536\n",
      "Epoch 1468/3000, Training Loss (MSE): 2.7707\n",
      "Epoch 1469/3000, Training Loss (MSE): 4.2213\n",
      "Epoch 1470/3000, Training Loss (MSE): 2.0057\n",
      "Epoch 1471/3000, Training Loss (MSE): 3.5876\n",
      "Epoch 1472/3000, Training Loss (MSE): 2.2114\n",
      "Epoch 1473/3000, Training Loss (MSE): 2.0400\n",
      "Epoch 1474/3000, Training Loss (MSE): 2.7023\n",
      "Epoch 1475/3000, Training Loss (MSE): 2.5176\n",
      "Epoch 1476/3000, Training Loss (MSE): 2.6597\n",
      "Epoch 1477/3000, Training Loss (MSE): 1.9504\n",
      "Epoch 1478/3000, Training Loss (MSE): 2.4120\n",
      "Epoch 1479/3000, Training Loss (MSE): 2.6533\n",
      "Epoch 1480/3000, Training Loss (MSE): 2.9883\n",
      "Epoch 1481/3000, Training Loss (MSE): 2.9216\n",
      "Epoch 1482/3000, Training Loss (MSE): 2.6491\n",
      "Epoch 1483/3000, Training Loss (MSE): 2.9480\n",
      "Epoch 1484/3000, Training Loss (MSE): 2.1366\n",
      "Epoch 1485/3000, Training Loss (MSE): 2.6379\n",
      "Epoch 1486/3000, Training Loss (MSE): 3.2253\n",
      "Epoch 1487/3000, Training Loss (MSE): 2.6030\n",
      "Epoch 1488/3000, Training Loss (MSE): 3.3434\n",
      "Epoch 1489/3000, Training Loss (MSE): 2.2685\n",
      "Epoch 1490/3000, Training Loss (MSE): 2.6089\n",
      "Epoch 1491/3000, Training Loss (MSE): 2.6904\n",
      "Epoch 1492/3000, Training Loss (MSE): 2.4181\n",
      "Epoch 1493/3000, Training Loss (MSE): 3.3764\n",
      "Epoch 1494/3000, Training Loss (MSE): 2.5044\n",
      "Epoch 1495/3000, Training Loss (MSE): 2.3220\n",
      "Epoch 1496/3000, Training Loss (MSE): 3.4837\n",
      "Epoch 1497/3000, Training Loss (MSE): 3.0760\n",
      "Epoch 1498/3000, Training Loss (MSE): 2.7784\n",
      "Epoch 1499/3000, Training Loss (MSE): 3.2902\n",
      "Epoch 1500/3000, Training Loss (MSE): 3.2935\n",
      "Epoch 1501/3000, Training Loss (MSE): 2.3330\n",
      "Epoch 1502/3000, Training Loss (MSE): 2.3630\n",
      "Epoch 1503/3000, Training Loss (MSE): 3.3551\n",
      "Epoch 1504/3000, Training Loss (MSE): 1.9412\n",
      "Epoch 1505/3000, Training Loss (MSE): 3.5398\n",
      "Epoch 1506/3000, Training Loss (MSE): 3.2429\n",
      "Epoch 1507/3000, Training Loss (MSE): 3.1003\n",
      "Epoch 1508/3000, Training Loss (MSE): 2.4309\n",
      "Epoch 1509/3000, Training Loss (MSE): 2.7267\n",
      "Epoch 1510/3000, Training Loss (MSE): 2.7387\n",
      "Epoch 1511/3000, Training Loss (MSE): 2.7896\n",
      "Epoch 1512/3000, Training Loss (MSE): 2.7117\n",
      "Epoch 1513/3000, Training Loss (MSE): 3.4858\n",
      "Epoch 1514/3000, Training Loss (MSE): 2.7763\n",
      "Epoch 1515/3000, Training Loss (MSE): 2.0764\n",
      "Epoch 1516/3000, Training Loss (MSE): 2.6483\n",
      "Epoch 1517/3000, Training Loss (MSE): 3.0299\n",
      "Epoch 1518/3000, Training Loss (MSE): 2.6272\n",
      "Epoch 1519/3000, Training Loss (MSE): 2.4602\n",
      "Epoch 1520/3000, Training Loss (MSE): 3.7694\n",
      "Epoch 1521/3000, Training Loss (MSE): 1.9193\n",
      "Epoch 1522/3000, Training Loss (MSE): 2.4356\n",
      "Epoch 1523/3000, Training Loss (MSE): 3.6356\n",
      "Epoch 1524/3000, Training Loss (MSE): 2.8020\n",
      "Epoch 1525/3000, Training Loss (MSE): 3.5368\n",
      "Epoch 1526/3000, Training Loss (MSE): 4.0233\n",
      "Epoch 1527/3000, Training Loss (MSE): 2.7288\n",
      "Epoch 1528/3000, Training Loss (MSE): 2.6768\n",
      "Epoch 1529/3000, Training Loss (MSE): 2.9313\n",
      "Epoch 1530/3000, Training Loss (MSE): 2.2768\n",
      "Epoch 1531/3000, Training Loss (MSE): 2.8142\n",
      "Epoch 1532/3000, Training Loss (MSE): 3.0144\n",
      "Epoch 1533/3000, Training Loss (MSE): 3.2739\n",
      "Epoch 1534/3000, Training Loss (MSE): 2.8210\n",
      "Epoch 1535/3000, Training Loss (MSE): 2.8183\n",
      "Epoch 1536/3000, Training Loss (MSE): 1.8530\n",
      "Epoch 1537/3000, Training Loss (MSE): 3.6485\n",
      "Epoch 1538/3000, Training Loss (MSE): 2.6397\n",
      "Epoch 1539/3000, Training Loss (MSE): 2.9573\n",
      "Epoch 1540/3000, Training Loss (MSE): 3.8230\n",
      "Epoch 1541/3000, Training Loss (MSE): 2.8837\n",
      "Epoch 1542/3000, Training Loss (MSE): 2.6156\n",
      "Epoch 1543/3000, Training Loss (MSE): 2.1720\n",
      "Epoch 1544/3000, Training Loss (MSE): 2.4491\n",
      "Epoch 1545/3000, Training Loss (MSE): 2.9712\n",
      "Epoch 1546/3000, Training Loss (MSE): 2.7702\n",
      "Epoch 1547/3000, Training Loss (MSE): 2.2973\n",
      "Epoch 1548/3000, Training Loss (MSE): 3.1821\n",
      "Epoch 1549/3000, Training Loss (MSE): 2.9890\n",
      "Epoch 1550/3000, Training Loss (MSE): 2.5978\n",
      "Epoch 1551/3000, Training Loss (MSE): 3.5290\n",
      "Epoch 1552/3000, Training Loss (MSE): 2.5578\n",
      "Epoch 1553/3000, Training Loss (MSE): 2.3544\n",
      "Epoch 1554/3000, Training Loss (MSE): 2.0405\n",
      "Epoch 1555/3000, Training Loss (MSE): 2.4634\n",
      "Epoch 1556/3000, Training Loss (MSE): 3.1770\n",
      "Epoch 1557/3000, Training Loss (MSE): 3.2414\n",
      "Epoch 1558/3000, Training Loss (MSE): 3.4131\n",
      "Epoch 1559/3000, Training Loss (MSE): 3.1031\n",
      "Epoch 1560/3000, Training Loss (MSE): 1.5832\n",
      "Epoch 1561/3000, Training Loss (MSE): 2.1181\n",
      "Epoch 1562/3000, Training Loss (MSE): 3.3454\n",
      "Epoch 1563/3000, Training Loss (MSE): 2.4573\n",
      "Epoch 1564/3000, Training Loss (MSE): 2.3816\n",
      "Epoch 1565/3000, Training Loss (MSE): 2.7854\n",
      "Epoch 1566/3000, Training Loss (MSE): 2.0340\n",
      "Epoch 1567/3000, Training Loss (MSE): 2.7314\n",
      "Epoch 1568/3000, Training Loss (MSE): 2.1469\n",
      "Epoch 1569/3000, Training Loss (MSE): 3.6980\n",
      "Epoch 1570/3000, Training Loss (MSE): 2.9922\n",
      "Epoch 1571/3000, Training Loss (MSE): 3.2168\n",
      "Epoch 1572/3000, Training Loss (MSE): 3.3632\n",
      "Epoch 1573/3000, Training Loss (MSE): 2.6659\n",
      "Epoch 1574/3000, Training Loss (MSE): 3.2729\n",
      "Epoch 1575/3000, Training Loss (MSE): 3.3978\n",
      "Epoch 1576/3000, Training Loss (MSE): 2.9906\n",
      "Epoch 1577/3000, Training Loss (MSE): 4.0987\n",
      "Epoch 1578/3000, Training Loss (MSE): 2.8451\n",
      "Epoch 1579/3000, Training Loss (MSE): 2.3380\n",
      "Epoch 1580/3000, Training Loss (MSE): 1.9149\n",
      "Epoch 1581/3000, Training Loss (MSE): 2.8149\n",
      "Epoch 1582/3000, Training Loss (MSE): 2.7598\n",
      "Epoch 1583/3000, Training Loss (MSE): 2.0036\n",
      "Epoch 1584/3000, Training Loss (MSE): 1.6154\n",
      "Epoch 1585/3000, Training Loss (MSE): 3.2156\n",
      "Epoch 1586/3000, Training Loss (MSE): 2.0171\n",
      "Epoch 1587/3000, Training Loss (MSE): 2.6466\n",
      "Epoch 1588/3000, Training Loss (MSE): 3.2596\n",
      "Epoch 1589/3000, Training Loss (MSE): 2.3473\n",
      "Epoch 1590/3000, Training Loss (MSE): 2.4627\n",
      "Epoch 1591/3000, Training Loss (MSE): 3.6055\n",
      "Epoch 1592/3000, Training Loss (MSE): 2.8938\n",
      "Epoch 1593/3000, Training Loss (MSE): 3.2018\n",
      "Epoch 1594/3000, Training Loss (MSE): 3.4110\n",
      "Epoch 1595/3000, Training Loss (MSE): 3.6068\n",
      "Epoch 1596/3000, Training Loss (MSE): 2.7379\n",
      "Epoch 1597/3000, Training Loss (MSE): 3.4952\n",
      "Epoch 1598/3000, Training Loss (MSE): 2.3815\n",
      "Epoch 1599/3000, Training Loss (MSE): 1.9383\n",
      "Epoch 1600/3000, Training Loss (MSE): 2.4421\n",
      "Epoch 1601/3000, Training Loss (MSE): 3.1839\n",
      "Epoch 1602/3000, Training Loss (MSE): 3.0539\n",
      "Epoch 1603/3000, Training Loss (MSE): 2.4169\n",
      "Epoch 1604/3000, Training Loss (MSE): 3.2443\n",
      "Epoch 1605/3000, Training Loss (MSE): 3.2083\n",
      "Epoch 1606/3000, Training Loss (MSE): 2.1964\n",
      "Epoch 1607/3000, Training Loss (MSE): 2.6678\n",
      "Epoch 1608/3000, Training Loss (MSE): 2.7272\n",
      "Epoch 1609/3000, Training Loss (MSE): 3.1213\n",
      "Epoch 1610/3000, Training Loss (MSE): 3.2042\n",
      "Epoch 1611/3000, Training Loss (MSE): 2.5609\n",
      "Epoch 1612/3000, Training Loss (MSE): 3.7955\n",
      "Epoch 1613/3000, Training Loss (MSE): 2.9104\n",
      "Epoch 1614/3000, Training Loss (MSE): 2.0203\n",
      "Epoch 1615/3000, Training Loss (MSE): 2.2104\n",
      "Epoch 1616/3000, Training Loss (MSE): 2.9613\n",
      "Epoch 1617/3000, Training Loss (MSE): 2.2648\n",
      "Epoch 1618/3000, Training Loss (MSE): 2.3135\n",
      "Epoch 1619/3000, Training Loss (MSE): 3.0567\n",
      "Epoch 1620/3000, Training Loss (MSE): 1.8366\n",
      "Epoch 1621/3000, Training Loss (MSE): 2.7368\n",
      "Epoch 1622/3000, Training Loss (MSE): 2.3595\n",
      "Epoch 1623/3000, Training Loss (MSE): 2.6697\n",
      "Epoch 1624/3000, Training Loss (MSE): 2.9044\n",
      "Epoch 1625/3000, Training Loss (MSE): 2.0197\n",
      "Epoch 1626/3000, Training Loss (MSE): 3.1375\n",
      "Epoch 1627/3000, Training Loss (MSE): 3.9591\n",
      "Epoch 1628/3000, Training Loss (MSE): 1.3741\n",
      "Epoch 1629/3000, Training Loss (MSE): 2.5448\n",
      "Epoch 1630/3000, Training Loss (MSE): 2.2459\n",
      "Epoch 1631/3000, Training Loss (MSE): 3.3951\n",
      "Epoch 1632/3000, Training Loss (MSE): 2.8721\n",
      "Epoch 1633/3000, Training Loss (MSE): 2.2996\n",
      "Epoch 1634/3000, Training Loss (MSE): 2.4659\n",
      "Epoch 1635/3000, Training Loss (MSE): 1.9421\n",
      "Epoch 1636/3000, Training Loss (MSE): 2.3404\n",
      "Epoch 1637/3000, Training Loss (MSE): 3.0850\n",
      "Epoch 1638/3000, Training Loss (MSE): 2.8369\n",
      "Epoch 1639/3000, Training Loss (MSE): 2.4384\n",
      "Epoch 1640/3000, Training Loss (MSE): 2.1331\n",
      "Epoch 1641/3000, Training Loss (MSE): 2.2283\n",
      "Epoch 1642/3000, Training Loss (MSE): 2.4841\n",
      "Epoch 1643/3000, Training Loss (MSE): 2.9988\n",
      "Epoch 1644/3000, Training Loss (MSE): 4.0548\n",
      "Epoch 1645/3000, Training Loss (MSE): 3.3209\n",
      "Epoch 1646/3000, Training Loss (MSE): 2.3979\n",
      "Epoch 1647/3000, Training Loss (MSE): 2.2838\n",
      "Epoch 1648/3000, Training Loss (MSE): 2.3456\n",
      "Epoch 1649/3000, Training Loss (MSE): 2.1556\n",
      "Epoch 1650/3000, Training Loss (MSE): 3.2092\n",
      "Epoch 1651/3000, Training Loss (MSE): 2.8643\n",
      "Epoch 1652/3000, Training Loss (MSE): 2.6344\n",
      "Epoch 1653/3000, Training Loss (MSE): 2.1980\n",
      "Epoch 1654/3000, Training Loss (MSE): 3.0828\n",
      "Epoch 1655/3000, Training Loss (MSE): 2.7316\n",
      "Epoch 1656/3000, Training Loss (MSE): 2.8455\n",
      "Epoch 1657/3000, Training Loss (MSE): 3.3199\n",
      "Epoch 1658/3000, Training Loss (MSE): 2.9642\n",
      "Epoch 1659/3000, Training Loss (MSE): 1.6298\n",
      "Epoch 1660/3000, Training Loss (MSE): 3.2374\n",
      "Epoch 1661/3000, Training Loss (MSE): 2.1639\n",
      "Epoch 1662/3000, Training Loss (MSE): 3.8585\n",
      "Epoch 1663/3000, Training Loss (MSE): 3.0835\n",
      "Epoch 1664/3000, Training Loss (MSE): 2.3839\n",
      "Epoch 1665/3000, Training Loss (MSE): 3.7947\n",
      "Epoch 1666/3000, Training Loss (MSE): 3.0049\n",
      "Epoch 1667/3000, Training Loss (MSE): 2.0988\n",
      "Epoch 1668/3000, Training Loss (MSE): 2.7002\n",
      "Epoch 1669/3000, Training Loss (MSE): 2.3367\n",
      "Epoch 1670/3000, Training Loss (MSE): 2.9590\n",
      "Epoch 1671/3000, Training Loss (MSE): 3.0736\n",
      "Epoch 1672/3000, Training Loss (MSE): 3.0777\n",
      "Epoch 1673/3000, Training Loss (MSE): 2.1377\n",
      "Epoch 1674/3000, Training Loss (MSE): 2.8658\n",
      "Epoch 1675/3000, Training Loss (MSE): 1.8462\n",
      "Epoch 1676/3000, Training Loss (MSE): 2.8680\n",
      "Epoch 1677/3000, Training Loss (MSE): 2.1693\n",
      "Epoch 1678/3000, Training Loss (MSE): 2.9527\n",
      "Epoch 1679/3000, Training Loss (MSE): 3.1612\n",
      "Epoch 1680/3000, Training Loss (MSE): 2.9904\n",
      "Epoch 1681/3000, Training Loss (MSE): 3.6204\n",
      "Epoch 1682/3000, Training Loss (MSE): 3.2132\n",
      "Epoch 1683/3000, Training Loss (MSE): 3.8758\n",
      "Epoch 1684/3000, Training Loss (MSE): 3.2919\n",
      "Epoch 1685/3000, Training Loss (MSE): 2.6711\n",
      "Epoch 1686/3000, Training Loss (MSE): 2.9346\n",
      "Epoch 1687/3000, Training Loss (MSE): 2.0855\n",
      "Epoch 1688/3000, Training Loss (MSE): 2.4952\n",
      "Epoch 1689/3000, Training Loss (MSE): 2.4030\n",
      "Epoch 1690/3000, Training Loss (MSE): 2.1624\n",
      "Epoch 1691/3000, Training Loss (MSE): 2.8167\n",
      "Epoch 1692/3000, Training Loss (MSE): 2.3251\n",
      "Epoch 1693/3000, Training Loss (MSE): 2.4009\n",
      "Epoch 1694/3000, Training Loss (MSE): 2.5960\n",
      "Epoch 1695/3000, Training Loss (MSE): 2.5116\n",
      "Epoch 1696/3000, Training Loss (MSE): 2.4398\n",
      "Epoch 1697/3000, Training Loss (MSE): 3.2246\n",
      "Epoch 1698/3000, Training Loss (MSE): 1.9657\n",
      "Epoch 1699/3000, Training Loss (MSE): 2.6709\n",
      "Epoch 1700/3000, Training Loss (MSE): 3.4356\n",
      "Epoch 1701/3000, Training Loss (MSE): 3.0392\n",
      "Epoch 1702/3000, Training Loss (MSE): 3.6284\n",
      "Epoch 1703/3000, Training Loss (MSE): 1.9853\n",
      "Epoch 1704/3000, Training Loss (MSE): 2.0092\n",
      "Epoch 1705/3000, Training Loss (MSE): 3.2183\n",
      "Epoch 1706/3000, Training Loss (MSE): 3.0190\n",
      "Epoch 1707/3000, Training Loss (MSE): 3.3559\n",
      "Epoch 1708/3000, Training Loss (MSE): 2.7056\n",
      "Epoch 1709/3000, Training Loss (MSE): 2.8925\n",
      "Epoch 1710/3000, Training Loss (MSE): 2.8022\n",
      "Epoch 1711/3000, Training Loss (MSE): 2.6768\n",
      "Epoch 1712/3000, Training Loss (MSE): 3.2319\n",
      "Epoch 1713/3000, Training Loss (MSE): 3.6980\n",
      "Epoch 1714/3000, Training Loss (MSE): 2.7957\n",
      "Epoch 1715/3000, Training Loss (MSE): 3.5060\n",
      "Epoch 1716/3000, Training Loss (MSE): 2.2476\n",
      "Epoch 1717/3000, Training Loss (MSE): 1.9599\n",
      "Epoch 1718/3000, Training Loss (MSE): 2.0613\n",
      "Epoch 1719/3000, Training Loss (MSE): 2.2799\n",
      "Epoch 1720/3000, Training Loss (MSE): 2.7567\n",
      "Epoch 1721/3000, Training Loss (MSE): 2.9532\n",
      "Epoch 1722/3000, Training Loss (MSE): 4.1108\n",
      "Epoch 1723/3000, Training Loss (MSE): 3.3025\n",
      "Epoch 1724/3000, Training Loss (MSE): 2.4350\n",
      "Epoch 1725/3000, Training Loss (MSE): 2.7004\n",
      "Epoch 1726/3000, Training Loss (MSE): 3.1469\n",
      "Epoch 1727/3000, Training Loss (MSE): 2.2343\n",
      "Epoch 1728/3000, Training Loss (MSE): 2.6179\n",
      "Epoch 1729/3000, Training Loss (MSE): 2.5311\n",
      "Epoch 1730/3000, Training Loss (MSE): 2.3751\n",
      "Epoch 1731/3000, Training Loss (MSE): 3.2194\n",
      "Epoch 1732/3000, Training Loss (MSE): 1.5496\n",
      "Epoch 1733/3000, Training Loss (MSE): 3.3038\n",
      "Epoch 1734/3000, Training Loss (MSE): 3.0759\n",
      "Epoch 1735/3000, Training Loss (MSE): 1.9966\n",
      "Epoch 1736/3000, Training Loss (MSE): 2.4845\n",
      "Epoch 1737/3000, Training Loss (MSE): 2.9464\n",
      "Epoch 1738/3000, Training Loss (MSE): 3.1243\n",
      "Epoch 1739/3000, Training Loss (MSE): 3.1966\n",
      "Epoch 1740/3000, Training Loss (MSE): 3.3216\n",
      "Epoch 1741/3000, Training Loss (MSE): 3.7148\n",
      "Epoch 1742/3000, Training Loss (MSE): 3.2244\n",
      "Epoch 1743/3000, Training Loss (MSE): 3.4240\n",
      "Epoch 1744/3000, Training Loss (MSE): 2.8947\n",
      "Epoch 1745/3000, Training Loss (MSE): 1.9176\n",
      "Epoch 1746/3000, Training Loss (MSE): 2.4173\n",
      "Epoch 1747/3000, Training Loss (MSE): 2.0694\n",
      "Epoch 1748/3000, Training Loss (MSE): 2.7803\n",
      "Epoch 1749/3000, Training Loss (MSE): 2.2413\n",
      "Epoch 1750/3000, Training Loss (MSE): 2.8720\n",
      "Epoch 1751/3000, Training Loss (MSE): 3.2331\n",
      "Epoch 1752/3000, Training Loss (MSE): 2.1902\n",
      "Epoch 1753/3000, Training Loss (MSE): 2.3120\n",
      "Epoch 1754/3000, Training Loss (MSE): 3.2163\n",
      "Epoch 1755/3000, Training Loss (MSE): 2.1170\n",
      "Epoch 1756/3000, Training Loss (MSE): 2.2430\n",
      "Epoch 1757/3000, Training Loss (MSE): 1.9730\n",
      "Epoch 1758/3000, Training Loss (MSE): 1.8000\n",
      "Epoch 1759/3000, Training Loss (MSE): 2.0169\n",
      "Epoch 1760/3000, Training Loss (MSE): 2.9277\n",
      "Epoch 1761/3000, Training Loss (MSE): 2.0165\n",
      "Epoch 1762/3000, Training Loss (MSE): 3.0276\n",
      "Epoch 1763/3000, Training Loss (MSE): 1.8328\n",
      "Epoch 1764/3000, Training Loss (MSE): 2.9029\n",
      "Epoch 1765/3000, Training Loss (MSE): 3.1175\n",
      "Epoch 1766/3000, Training Loss (MSE): 2.0569\n",
      "Epoch 1767/3000, Training Loss (MSE): 4.0646\n",
      "Epoch 1768/3000, Training Loss (MSE): 2.8187\n",
      "Epoch 1769/3000, Training Loss (MSE): 2.6629\n",
      "Epoch 1770/3000, Training Loss (MSE): 3.2027\n",
      "Epoch 1771/3000, Training Loss (MSE): 3.2154\n",
      "Epoch 1772/3000, Training Loss (MSE): 2.6012\n",
      "Epoch 1773/3000, Training Loss (MSE): 3.3086\n",
      "Epoch 1774/3000, Training Loss (MSE): 2.1731\n",
      "Epoch 1775/3000, Training Loss (MSE): 3.5555\n",
      "Epoch 1776/3000, Training Loss (MSE): 2.8841\n",
      "Epoch 1777/3000, Training Loss (MSE): 2.6825\n",
      "Epoch 1778/3000, Training Loss (MSE): 2.6647\n",
      "Epoch 1779/3000, Training Loss (MSE): 2.6289\n",
      "Epoch 1780/3000, Training Loss (MSE): 2.3591\n",
      "Epoch 1781/3000, Training Loss (MSE): 2.5900\n",
      "Epoch 1782/3000, Training Loss (MSE): 1.6945\n",
      "Epoch 1783/3000, Training Loss (MSE): 3.8874\n",
      "Epoch 1784/3000, Training Loss (MSE): 3.6274\n",
      "Epoch 1785/3000, Training Loss (MSE): 1.9424\n",
      "Epoch 1786/3000, Training Loss (MSE): 2.9855\n",
      "Epoch 1787/3000, Training Loss (MSE): 2.4917\n",
      "Epoch 1788/3000, Training Loss (MSE): 2.5259\n",
      "Epoch 1789/3000, Training Loss (MSE): 2.2420\n",
      "Epoch 1790/3000, Training Loss (MSE): 3.1652\n",
      "Epoch 1791/3000, Training Loss (MSE): 3.2604\n",
      "Epoch 1792/3000, Training Loss (MSE): 2.1692\n",
      "Epoch 1793/3000, Training Loss (MSE): 1.9446\n",
      "Epoch 1794/3000, Training Loss (MSE): 2.8050\n",
      "Epoch 1795/3000, Training Loss (MSE): 3.6943\n",
      "Epoch 1796/3000, Training Loss (MSE): 2.9979\n",
      "Epoch 1797/3000, Training Loss (MSE): 3.1112\n",
      "Epoch 1798/3000, Training Loss (MSE): 2.5842\n",
      "Epoch 1799/3000, Training Loss (MSE): 2.0032\n",
      "Epoch 1800/3000, Training Loss (MSE): 3.3842\n",
      "Epoch 1801/3000, Training Loss (MSE): 2.6561\n",
      "Epoch 1802/3000, Training Loss (MSE): 3.8133\n",
      "Epoch 1803/3000, Training Loss (MSE): 1.9913\n",
      "Epoch 1804/3000, Training Loss (MSE): 2.8329\n",
      "Epoch 1805/3000, Training Loss (MSE): 2.7226\n",
      "Epoch 1806/3000, Training Loss (MSE): 2.1598\n",
      "Epoch 1807/3000, Training Loss (MSE): 2.6240\n",
      "Epoch 1808/3000, Training Loss (MSE): 3.1748\n",
      "Epoch 1809/3000, Training Loss (MSE): 2.6964\n",
      "Epoch 1810/3000, Training Loss (MSE): 3.1024\n",
      "Epoch 1811/3000, Training Loss (MSE): 2.0574\n",
      "Epoch 1812/3000, Training Loss (MSE): 3.3976\n",
      "Epoch 1813/3000, Training Loss (MSE): 3.3378\n",
      "Epoch 1814/3000, Training Loss (MSE): 3.1544\n",
      "Epoch 1815/3000, Training Loss (MSE): 3.0500\n",
      "Epoch 1816/3000, Training Loss (MSE): 3.6542\n",
      "Epoch 1817/3000, Training Loss (MSE): 2.5906\n",
      "Epoch 1818/3000, Training Loss (MSE): 2.9470\n",
      "Epoch 1819/3000, Training Loss (MSE): 2.6212\n",
      "Epoch 1820/3000, Training Loss (MSE): 3.0156\n",
      "Epoch 1821/3000, Training Loss (MSE): 2.8041\n",
      "Epoch 1822/3000, Training Loss (MSE): 3.2694\n",
      "Epoch 1823/3000, Training Loss (MSE): 2.0672\n",
      "Epoch 1824/3000, Training Loss (MSE): 2.8548\n",
      "Epoch 1825/3000, Training Loss (MSE): 2.9696\n",
      "Epoch 1826/3000, Training Loss (MSE): 2.3839\n",
      "Epoch 1827/3000, Training Loss (MSE): 2.7426\n",
      "Epoch 1828/3000, Training Loss (MSE): 1.9480\n",
      "Epoch 1829/3000, Training Loss (MSE): 2.3746\n",
      "Epoch 1830/3000, Training Loss (MSE): 2.2179\n",
      "Epoch 1831/3000, Training Loss (MSE): 3.5129\n",
      "Epoch 1832/3000, Training Loss (MSE): 1.4376\n",
      "Epoch 1833/3000, Training Loss (MSE): 3.2077\n",
      "Epoch 1834/3000, Training Loss (MSE): 2.9190\n",
      "Epoch 1835/3000, Training Loss (MSE): 2.1715\n",
      "Epoch 1836/3000, Training Loss (MSE): 2.9669\n",
      "Epoch 1837/3000, Training Loss (MSE): 2.2939\n",
      "Epoch 1838/3000, Training Loss (MSE): 2.6443\n",
      "Epoch 1839/3000, Training Loss (MSE): 3.5103\n",
      "Epoch 1840/3000, Training Loss (MSE): 2.5248\n",
      "Epoch 1841/3000, Training Loss (MSE): 3.1012\n",
      "Epoch 1842/3000, Training Loss (MSE): 2.7273\n",
      "Epoch 1843/3000, Training Loss (MSE): 3.5606\n",
      "Epoch 1844/3000, Training Loss (MSE): 1.8260\n",
      "Epoch 1845/3000, Training Loss (MSE): 3.3653\n",
      "Epoch 1846/3000, Training Loss (MSE): 2.7872\n",
      "Epoch 1847/3000, Training Loss (MSE): 2.1054\n",
      "Epoch 1848/3000, Training Loss (MSE): 2.2795\n",
      "Epoch 1849/3000, Training Loss (MSE): 2.8095\n",
      "Epoch 1850/3000, Training Loss (MSE): 3.1730\n",
      "Epoch 1851/3000, Training Loss (MSE): 4.5339\n",
      "Epoch 1852/3000, Training Loss (MSE): 3.3858\n",
      "Epoch 1853/3000, Training Loss (MSE): 2.3214\n",
      "Epoch 1854/3000, Training Loss (MSE): 3.1324\n",
      "Epoch 1855/3000, Training Loss (MSE): 3.1891\n",
      "Epoch 1856/3000, Training Loss (MSE): 2.7110\n",
      "Epoch 1857/3000, Training Loss (MSE): 3.3386\n",
      "Epoch 1858/3000, Training Loss (MSE): 3.8659\n",
      "Epoch 1859/3000, Training Loss (MSE): 3.8481\n",
      "Epoch 1860/3000, Training Loss (MSE): 3.2839\n",
      "Epoch 1861/3000, Training Loss (MSE): 2.6673\n",
      "Epoch 1862/3000, Training Loss (MSE): 2.6565\n",
      "Epoch 1863/3000, Training Loss (MSE): 2.3267\n",
      "Epoch 1864/3000, Training Loss (MSE): 2.8399\n",
      "Epoch 1865/3000, Training Loss (MSE): 2.9788\n",
      "Epoch 1866/3000, Training Loss (MSE): 2.8057\n",
      "Epoch 1867/3000, Training Loss (MSE): 2.1718\n",
      "Epoch 1868/3000, Training Loss (MSE): 1.9967\n",
      "Epoch 1869/3000, Training Loss (MSE): 3.2802\n",
      "Epoch 1870/3000, Training Loss (MSE): 3.0341\n",
      "Epoch 1871/3000, Training Loss (MSE): 3.3594\n",
      "Epoch 1872/3000, Training Loss (MSE): 3.1661\n",
      "Epoch 1873/3000, Training Loss (MSE): 2.6780\n",
      "Epoch 1874/3000, Training Loss (MSE): 2.7872\n",
      "Epoch 1875/3000, Training Loss (MSE): 2.9888\n",
      "Epoch 1876/3000, Training Loss (MSE): 2.5120\n",
      "Epoch 1877/3000, Training Loss (MSE): 2.2586\n",
      "Epoch 1878/3000, Training Loss (MSE): 2.8942\n",
      "Epoch 1879/3000, Training Loss (MSE): 2.5899\n",
      "Epoch 1880/3000, Training Loss (MSE): 2.7210\n",
      "Epoch 1881/3000, Training Loss (MSE): 3.1162\n",
      "Epoch 1882/3000, Training Loss (MSE): 2.8595\n",
      "Epoch 1883/3000, Training Loss (MSE): 2.1107\n",
      "Epoch 1884/3000, Training Loss (MSE): 3.8923\n",
      "Epoch 1885/3000, Training Loss (MSE): 3.7266\n",
      "Epoch 1886/3000, Training Loss (MSE): 2.9169\n",
      "Epoch 1887/3000, Training Loss (MSE): 2.1355\n",
      "Epoch 1888/3000, Training Loss (MSE): 2.6189\n",
      "Epoch 1889/3000, Training Loss (MSE): 2.7855\n",
      "Epoch 1890/3000, Training Loss (MSE): 2.6284\n",
      "Epoch 1891/3000, Training Loss (MSE): 2.6689\n",
      "Epoch 1892/3000, Training Loss (MSE): 3.1446\n",
      "Epoch 1893/3000, Training Loss (MSE): 3.0527\n",
      "Epoch 1894/3000, Training Loss (MSE): 2.3275\n",
      "Epoch 1895/3000, Training Loss (MSE): 3.1394\n",
      "Epoch 1896/3000, Training Loss (MSE): 2.4613\n",
      "Epoch 1897/3000, Training Loss (MSE): 1.8085\n",
      "Epoch 1898/3000, Training Loss (MSE): 2.7255\n",
      "Epoch 1899/3000, Training Loss (MSE): 2.3742\n",
      "Epoch 1900/3000, Training Loss (MSE): 2.1920\n",
      "Epoch 1901/3000, Training Loss (MSE): 2.5956\n",
      "Epoch 1902/3000, Training Loss (MSE): 2.1621\n",
      "Epoch 1903/3000, Training Loss (MSE): 2.4923\n",
      "Epoch 1904/3000, Training Loss (MSE): 2.2665\n",
      "Epoch 1905/3000, Training Loss (MSE): 3.1213\n",
      "Epoch 1906/3000, Training Loss (MSE): 2.2462\n",
      "Epoch 1907/3000, Training Loss (MSE): 3.5944\n",
      "Epoch 1908/3000, Training Loss (MSE): 2.2708\n",
      "Epoch 1909/3000, Training Loss (MSE): 2.8735\n",
      "Epoch 1910/3000, Training Loss (MSE): 2.2583\n",
      "Epoch 1911/3000, Training Loss (MSE): 3.6828\n",
      "Epoch 1912/3000, Training Loss (MSE): 2.4098\n",
      "Epoch 1913/3000, Training Loss (MSE): 1.8420\n",
      "Epoch 1914/3000, Training Loss (MSE): 2.1397\n",
      "Epoch 1915/3000, Training Loss (MSE): 2.8576\n",
      "Epoch 1916/3000, Training Loss (MSE): 3.0121\n",
      "Epoch 1917/3000, Training Loss (MSE): 3.1609\n",
      "Epoch 1918/3000, Training Loss (MSE): 2.1826\n",
      "Epoch 1919/3000, Training Loss (MSE): 2.8415\n",
      "Epoch 1920/3000, Training Loss (MSE): 3.3244\n",
      "Epoch 1921/3000, Training Loss (MSE): 2.5686\n",
      "Epoch 1922/3000, Training Loss (MSE): 3.1381\n",
      "Epoch 1923/3000, Training Loss (MSE): 3.1235\n",
      "Epoch 1924/3000, Training Loss (MSE): 2.8601\n",
      "Epoch 1925/3000, Training Loss (MSE): 2.3611\n",
      "Epoch 1926/3000, Training Loss (MSE): 2.2597\n",
      "Epoch 1927/3000, Training Loss (MSE): 2.5340\n",
      "Epoch 1928/3000, Training Loss (MSE): 3.3320\n",
      "Epoch 1929/3000, Training Loss (MSE): 3.0458\n",
      "Epoch 1930/3000, Training Loss (MSE): 3.0040\n",
      "Epoch 1931/3000, Training Loss (MSE): 3.4436\n",
      "Epoch 1932/3000, Training Loss (MSE): 3.1049\n",
      "Epoch 1933/3000, Training Loss (MSE): 2.4886\n",
      "Epoch 1934/3000, Training Loss (MSE): 2.8683\n",
      "Epoch 1935/3000, Training Loss (MSE): 3.9454\n",
      "Epoch 1936/3000, Training Loss (MSE): 3.1331\n",
      "Epoch 1937/3000, Training Loss (MSE): 2.7244\n",
      "Epoch 1938/3000, Training Loss (MSE): 2.4741\n",
      "Epoch 1939/3000, Training Loss (MSE): 2.7421\n",
      "Epoch 1940/3000, Training Loss (MSE): 2.3068\n",
      "Epoch 1941/3000, Training Loss (MSE): 2.9835\n",
      "Epoch 1942/3000, Training Loss (MSE): 2.0891\n",
      "Epoch 1943/3000, Training Loss (MSE): 2.0934\n",
      "Epoch 1944/3000, Training Loss (MSE): 2.8218\n",
      "Epoch 1945/3000, Training Loss (MSE): 2.8898\n",
      "Epoch 1946/3000, Training Loss (MSE): 3.3797\n",
      "Epoch 1947/3000, Training Loss (MSE): 2.8266\n",
      "Epoch 1948/3000, Training Loss (MSE): 3.1974\n",
      "Epoch 1949/3000, Training Loss (MSE): 2.6632\n",
      "Epoch 1950/3000, Training Loss (MSE): 2.8581\n",
      "Epoch 1951/3000, Training Loss (MSE): 2.7427\n",
      "Epoch 1952/3000, Training Loss (MSE): 2.6135\n",
      "Epoch 1953/3000, Training Loss (MSE): 2.9557\n",
      "Epoch 1954/3000, Training Loss (MSE): 2.9418\n",
      "Epoch 1955/3000, Training Loss (MSE): 3.0686\n",
      "Epoch 1956/3000, Training Loss (MSE): 2.6031\n",
      "Epoch 1957/3000, Training Loss (MSE): 3.4623\n",
      "Epoch 1958/3000, Training Loss (MSE): 4.2997\n",
      "Epoch 1959/3000, Training Loss (MSE): 2.8137\n",
      "Epoch 1960/3000, Training Loss (MSE): 3.1218\n",
      "Epoch 1961/3000, Training Loss (MSE): 3.3127\n",
      "Epoch 1962/3000, Training Loss (MSE): 2.7132\n",
      "Epoch 1963/3000, Training Loss (MSE): 2.7917\n",
      "Epoch 1964/3000, Training Loss (MSE): 2.6225\n",
      "Epoch 1965/3000, Training Loss (MSE): 3.1473\n",
      "Epoch 1966/3000, Training Loss (MSE): 2.9382\n",
      "Epoch 1967/3000, Training Loss (MSE): 3.4716\n",
      "Epoch 1968/3000, Training Loss (MSE): 1.9895\n",
      "Epoch 1969/3000, Training Loss (MSE): 2.7610\n",
      "Epoch 1970/3000, Training Loss (MSE): 2.5161\n",
      "Epoch 1971/3000, Training Loss (MSE): 3.8657\n",
      "Epoch 1972/3000, Training Loss (MSE): 2.4910\n",
      "Epoch 1973/3000, Training Loss (MSE): 2.7794\n",
      "Epoch 1974/3000, Training Loss (MSE): 2.6064\n",
      "Epoch 1975/3000, Training Loss (MSE): 2.0435\n",
      "Epoch 1976/3000, Training Loss (MSE): 2.5522\n",
      "Epoch 1977/3000, Training Loss (MSE): 4.1552\n",
      "Epoch 1978/3000, Training Loss (MSE): 2.8175\n",
      "Epoch 1979/3000, Training Loss (MSE): 2.9004\n",
      "Epoch 1980/3000, Training Loss (MSE): 2.9034\n",
      "Epoch 1981/3000, Training Loss (MSE): 2.7131\n",
      "Epoch 1982/3000, Training Loss (MSE): 2.1953\n",
      "Epoch 1983/3000, Training Loss (MSE): 3.0930\n",
      "Epoch 1984/3000, Training Loss (MSE): 3.3496\n",
      "Epoch 1985/3000, Training Loss (MSE): 2.5825\n",
      "Epoch 1986/3000, Training Loss (MSE): 2.7305\n",
      "Epoch 1987/3000, Training Loss (MSE): 2.4909\n",
      "Epoch 1988/3000, Training Loss (MSE): 3.1583\n",
      "Epoch 1989/3000, Training Loss (MSE): 1.9207\n",
      "Epoch 1990/3000, Training Loss (MSE): 3.4612\n",
      "Epoch 1991/3000, Training Loss (MSE): 2.3787\n",
      "Epoch 1992/3000, Training Loss (MSE): 2.2133\n",
      "Epoch 1993/3000, Training Loss (MSE): 2.0147\n",
      "Epoch 1994/3000, Training Loss (MSE): 2.3111\n",
      "Epoch 1995/3000, Training Loss (MSE): 2.3470\n",
      "Epoch 1996/3000, Training Loss (MSE): 3.8028\n",
      "Epoch 1997/3000, Training Loss (MSE): 3.4295\n",
      "Epoch 1998/3000, Training Loss (MSE): 2.5655\n",
      "Epoch 1999/3000, Training Loss (MSE): 2.2303\n",
      "Epoch 2000/3000, Training Loss (MSE): 2.3265\n",
      "Epoch 2001/3000, Training Loss (MSE): 3.4538\n",
      "Epoch 2002/3000, Training Loss (MSE): 3.0943\n",
      "Epoch 2003/3000, Training Loss (MSE): 3.5438\n",
      "Epoch 2004/3000, Training Loss (MSE): 3.4734\n",
      "Epoch 2005/3000, Training Loss (MSE): 2.7421\n",
      "Epoch 2006/3000, Training Loss (MSE): 3.3692\n",
      "Epoch 2007/3000, Training Loss (MSE): 3.3119\n",
      "Epoch 2008/3000, Training Loss (MSE): 2.5360\n",
      "Epoch 2009/3000, Training Loss (MSE): 2.3196\n",
      "Epoch 2010/3000, Training Loss (MSE): 4.0129\n",
      "Epoch 2011/3000, Training Loss (MSE): 3.8504\n",
      "Epoch 2012/3000, Training Loss (MSE): 3.3527\n",
      "Epoch 2013/3000, Training Loss (MSE): 2.6836\n",
      "Epoch 2014/3000, Training Loss (MSE): 1.5299\n",
      "Epoch 2015/3000, Training Loss (MSE): 2.1670\n",
      "Epoch 2016/3000, Training Loss (MSE): 2.8165\n",
      "Epoch 2017/3000, Training Loss (MSE): 3.3013\n",
      "Epoch 2018/3000, Training Loss (MSE): 3.1934\n",
      "Epoch 2019/3000, Training Loss (MSE): 1.9398\n",
      "Epoch 2020/3000, Training Loss (MSE): 2.1111\n",
      "Epoch 2021/3000, Training Loss (MSE): 2.6546\n",
      "Epoch 2022/3000, Training Loss (MSE): 2.4416\n",
      "Epoch 2023/3000, Training Loss (MSE): 3.6835\n",
      "Epoch 2024/3000, Training Loss (MSE): 3.0641\n",
      "Epoch 2025/3000, Training Loss (MSE): 2.7553\n",
      "Epoch 2026/3000, Training Loss (MSE): 2.5360\n",
      "Epoch 2027/3000, Training Loss (MSE): 3.4176\n",
      "Epoch 2028/3000, Training Loss (MSE): 2.5649\n",
      "Epoch 2029/3000, Training Loss (MSE): 3.2283\n",
      "Epoch 2030/3000, Training Loss (MSE): 2.1770\n",
      "Epoch 2031/3000, Training Loss (MSE): 3.7065\n",
      "Epoch 2032/3000, Training Loss (MSE): 2.9500\n",
      "Epoch 2033/3000, Training Loss (MSE): 2.6008\n",
      "Epoch 2034/3000, Training Loss (MSE): 2.6957\n",
      "Epoch 2035/3000, Training Loss (MSE): 3.6331\n",
      "Epoch 2036/3000, Training Loss (MSE): 3.3465\n",
      "Epoch 2037/3000, Training Loss (MSE): 2.8751\n",
      "Epoch 2038/3000, Training Loss (MSE): 2.7181\n",
      "Epoch 2039/3000, Training Loss (MSE): 2.5506\n",
      "Epoch 2040/3000, Training Loss (MSE): 3.2647\n",
      "Epoch 2041/3000, Training Loss (MSE): 3.2984\n",
      "Epoch 2042/3000, Training Loss (MSE): 1.5154\n",
      "Epoch 2043/3000, Training Loss (MSE): 2.6337\n",
      "Epoch 2044/3000, Training Loss (MSE): 3.6619\n",
      "Epoch 2045/3000, Training Loss (MSE): 2.6747\n",
      "Epoch 2046/3000, Training Loss (MSE): 2.4132\n",
      "Epoch 2047/3000, Training Loss (MSE): 2.6889\n",
      "Epoch 2048/3000, Training Loss (MSE): 2.5170\n",
      "Epoch 2049/3000, Training Loss (MSE): 3.1989\n",
      "Epoch 2050/3000, Training Loss (MSE): 2.6687\n",
      "Epoch 2051/3000, Training Loss (MSE): 3.3021\n",
      "Epoch 2052/3000, Training Loss (MSE): 3.6145\n",
      "Epoch 2053/3000, Training Loss (MSE): 1.6902\n",
      "Epoch 2054/3000, Training Loss (MSE): 3.0142\n",
      "Epoch 2055/3000, Training Loss (MSE): 2.4274\n",
      "Epoch 2056/3000, Training Loss (MSE): 3.2211\n",
      "Epoch 2057/3000, Training Loss (MSE): 3.4240\n",
      "Epoch 2058/3000, Training Loss (MSE): 2.9227\n",
      "Epoch 2059/3000, Training Loss (MSE): 2.8988\n",
      "Epoch 2060/3000, Training Loss (MSE): 3.6843\n",
      "Epoch 2061/3000, Training Loss (MSE): 2.2291\n",
      "Epoch 2062/3000, Training Loss (MSE): 2.4520\n",
      "Epoch 2063/3000, Training Loss (MSE): 2.4981\n",
      "Epoch 2064/3000, Training Loss (MSE): 2.1222\n",
      "Epoch 2065/3000, Training Loss (MSE): 2.4244\n",
      "Epoch 2066/3000, Training Loss (MSE): 1.7250\n",
      "Epoch 2067/3000, Training Loss (MSE): 3.4628\n",
      "Epoch 2068/3000, Training Loss (MSE): 2.3123\n",
      "Epoch 2069/3000, Training Loss (MSE): 2.5499\n",
      "Epoch 2070/3000, Training Loss (MSE): 4.2868\n",
      "Epoch 2071/3000, Training Loss (MSE): 2.3516\n",
      "Epoch 2072/3000, Training Loss (MSE): 2.0862\n",
      "Epoch 2073/3000, Training Loss (MSE): 2.9710\n",
      "Epoch 2074/3000, Training Loss (MSE): 3.4903\n",
      "Epoch 2075/3000, Training Loss (MSE): 2.9039\n",
      "Epoch 2076/3000, Training Loss (MSE): 2.3812\n",
      "Epoch 2077/3000, Training Loss (MSE): 3.2602\n",
      "Epoch 2078/3000, Training Loss (MSE): 2.2324\n",
      "Epoch 2079/3000, Training Loss (MSE): 3.4881\n",
      "Epoch 2080/3000, Training Loss (MSE): 2.6692\n",
      "Epoch 2081/3000, Training Loss (MSE): 2.2525\n",
      "Epoch 2082/3000, Training Loss (MSE): 2.1936\n",
      "Epoch 2083/3000, Training Loss (MSE): 1.4767\n",
      "Epoch 2084/3000, Training Loss (MSE): 2.3901\n",
      "Epoch 2085/3000, Training Loss (MSE): 2.7911\n",
      "Epoch 2086/3000, Training Loss (MSE): 2.6351\n",
      "Epoch 2087/3000, Training Loss (MSE): 4.1452\n",
      "Epoch 2088/3000, Training Loss (MSE): 3.2693\n",
      "Epoch 2089/3000, Training Loss (MSE): 2.5590\n",
      "Epoch 2090/3000, Training Loss (MSE): 3.5576\n",
      "Epoch 2091/3000, Training Loss (MSE): 2.8888\n",
      "Epoch 2092/3000, Training Loss (MSE): 3.2549\n",
      "Epoch 2093/3000, Training Loss (MSE): 2.7369\n",
      "Epoch 2094/3000, Training Loss (MSE): 3.0729\n",
      "Epoch 2095/3000, Training Loss (MSE): 2.3828\n",
      "Epoch 2096/3000, Training Loss (MSE): 2.0447\n",
      "Epoch 2097/3000, Training Loss (MSE): 3.0778\n",
      "Epoch 2098/3000, Training Loss (MSE): 3.4156\n",
      "Epoch 2099/3000, Training Loss (MSE): 3.3107\n",
      "Epoch 2100/3000, Training Loss (MSE): 2.6328\n",
      "Epoch 2101/3000, Training Loss (MSE): 2.9483\n",
      "Epoch 2102/3000, Training Loss (MSE): 3.2000\n",
      "Epoch 2103/3000, Training Loss (MSE): 2.2468\n",
      "Epoch 2104/3000, Training Loss (MSE): 2.2012\n",
      "Epoch 2105/3000, Training Loss (MSE): 2.1597\n",
      "Epoch 2106/3000, Training Loss (MSE): 2.2819\n",
      "Epoch 2107/3000, Training Loss (MSE): 2.0144\n",
      "Epoch 2108/3000, Training Loss (MSE): 2.0714\n",
      "Epoch 2109/3000, Training Loss (MSE): 2.2030\n",
      "Epoch 2110/3000, Training Loss (MSE): 1.3073\n",
      "Epoch 2111/3000, Training Loss (MSE): 2.3199\n",
      "Epoch 2112/3000, Training Loss (MSE): 2.6907\n",
      "Epoch 2113/3000, Training Loss (MSE): 3.1361\n",
      "Epoch 2114/3000, Training Loss (MSE): 1.9045\n",
      "Epoch 2115/3000, Training Loss (MSE): 2.3053\n",
      "Epoch 2116/3000, Training Loss (MSE): 2.8673\n",
      "Epoch 2117/3000, Training Loss (MSE): 2.9855\n",
      "Epoch 2118/3000, Training Loss (MSE): 3.8411\n",
      "Epoch 2119/3000, Training Loss (MSE): 2.6740\n",
      "Epoch 2120/3000, Training Loss (MSE): 2.5811\n",
      "Epoch 2121/3000, Training Loss (MSE): 2.3512\n",
      "Epoch 2122/3000, Training Loss (MSE): 3.0603\n",
      "Epoch 2123/3000, Training Loss (MSE): 2.7106\n",
      "Epoch 2124/3000, Training Loss (MSE): 3.0092\n",
      "Epoch 2125/3000, Training Loss (MSE): 1.8906\n",
      "Epoch 2126/3000, Training Loss (MSE): 2.3554\n",
      "Epoch 2127/3000, Training Loss (MSE): 2.2556\n",
      "Epoch 2128/3000, Training Loss (MSE): 2.7843\n",
      "Epoch 2129/3000, Training Loss (MSE): 2.4936\n",
      "Epoch 2130/3000, Training Loss (MSE): 1.6132\n",
      "Epoch 2131/3000, Training Loss (MSE): 4.0134\n",
      "Epoch 2132/3000, Training Loss (MSE): 1.9905\n",
      "Epoch 2133/3000, Training Loss (MSE): 3.1959\n",
      "Epoch 2134/3000, Training Loss (MSE): 2.9271\n",
      "Epoch 2135/3000, Training Loss (MSE): 2.1103\n",
      "Epoch 2136/3000, Training Loss (MSE): 2.9445\n",
      "Epoch 2137/3000, Training Loss (MSE): 2.7699\n",
      "Epoch 2138/3000, Training Loss (MSE): 3.0067\n",
      "Epoch 2139/3000, Training Loss (MSE): 3.0158\n",
      "Epoch 2140/3000, Training Loss (MSE): 3.1236\n",
      "Epoch 2141/3000, Training Loss (MSE): 2.6302\n",
      "Epoch 2142/3000, Training Loss (MSE): 2.8375\n",
      "Epoch 2143/3000, Training Loss (MSE): 2.1804\n",
      "Epoch 2144/3000, Training Loss (MSE): 2.3657\n",
      "Epoch 2145/3000, Training Loss (MSE): 2.8084\n",
      "Epoch 2146/3000, Training Loss (MSE): 3.0132\n",
      "Epoch 2147/3000, Training Loss (MSE): 1.6125\n",
      "Epoch 2148/3000, Training Loss (MSE): 2.6492\n",
      "Epoch 2149/3000, Training Loss (MSE): 2.9150\n",
      "Epoch 2150/3000, Training Loss (MSE): 3.0082\n",
      "Epoch 2151/3000, Training Loss (MSE): 3.5837\n",
      "Epoch 2152/3000, Training Loss (MSE): 2.5052\n",
      "Epoch 2153/3000, Training Loss (MSE): 2.9957\n",
      "Epoch 2154/3000, Training Loss (MSE): 2.4913\n",
      "Epoch 2155/3000, Training Loss (MSE): 2.1046\n",
      "Epoch 2156/3000, Training Loss (MSE): 2.4298\n",
      "Epoch 2157/3000, Training Loss (MSE): 2.1993\n",
      "Epoch 2158/3000, Training Loss (MSE): 2.6997\n",
      "Epoch 2159/3000, Training Loss (MSE): 3.5183\n",
      "Epoch 2160/3000, Training Loss (MSE): 2.6633\n",
      "Epoch 2161/3000, Training Loss (MSE): 3.2375\n",
      "Epoch 2162/3000, Training Loss (MSE): 2.2234\n",
      "Epoch 2163/3000, Training Loss (MSE): 2.9428\n",
      "Epoch 2164/3000, Training Loss (MSE): 2.5260\n",
      "Epoch 2165/3000, Training Loss (MSE): 3.3332\n",
      "Epoch 2166/3000, Training Loss (MSE): 2.9727\n",
      "Epoch 2167/3000, Training Loss (MSE): 3.1010\n",
      "Epoch 2168/3000, Training Loss (MSE): 2.4553\n",
      "Epoch 2169/3000, Training Loss (MSE): 2.6222\n",
      "Epoch 2170/3000, Training Loss (MSE): 3.0402\n",
      "Epoch 2171/3000, Training Loss (MSE): 2.6755\n",
      "Epoch 2172/3000, Training Loss (MSE): 3.5364\n",
      "Epoch 2173/3000, Training Loss (MSE): 2.2528\n",
      "Epoch 2174/3000, Training Loss (MSE): 2.8325\n",
      "Epoch 2175/3000, Training Loss (MSE): 2.3023\n",
      "Epoch 2176/3000, Training Loss (MSE): 2.7260\n",
      "Epoch 2177/3000, Training Loss (MSE): 2.4408\n",
      "Epoch 2178/3000, Training Loss (MSE): 3.2621\n",
      "Epoch 2179/3000, Training Loss (MSE): 2.4672\n",
      "Epoch 2180/3000, Training Loss (MSE): 2.6032\n",
      "Epoch 2181/3000, Training Loss (MSE): 1.9208\n",
      "Epoch 2182/3000, Training Loss (MSE): 2.6331\n",
      "Epoch 2183/3000, Training Loss (MSE): 1.8142\n",
      "Epoch 2184/3000, Training Loss (MSE): 3.1486\n",
      "Epoch 2185/3000, Training Loss (MSE): 2.1512\n",
      "Epoch 2186/3000, Training Loss (MSE): 2.7760\n",
      "Epoch 2187/3000, Training Loss (MSE): 2.9441\n",
      "Epoch 2188/3000, Training Loss (MSE): 3.1286\n",
      "Epoch 2189/3000, Training Loss (MSE): 2.0252\n",
      "Epoch 2190/3000, Training Loss (MSE): 2.2978\n",
      "Epoch 2191/3000, Training Loss (MSE): 2.8189\n",
      "Epoch 2192/3000, Training Loss (MSE): 1.7606\n",
      "Epoch 2193/3000, Training Loss (MSE): 2.1487\n",
      "Epoch 2194/3000, Training Loss (MSE): 2.6349\n",
      "Epoch 2195/3000, Training Loss (MSE): 2.7542\n",
      "Epoch 2196/3000, Training Loss (MSE): 3.6877\n",
      "Epoch 2197/3000, Training Loss (MSE): 2.3809\n",
      "Epoch 2198/3000, Training Loss (MSE): 2.6575\n",
      "Epoch 2199/3000, Training Loss (MSE): 2.1972\n",
      "Epoch 2200/3000, Training Loss (MSE): 2.8611\n",
      "Epoch 2201/3000, Training Loss (MSE): 3.4428\n",
      "Epoch 2202/3000, Training Loss (MSE): 3.5114\n",
      "Epoch 2203/3000, Training Loss (MSE): 3.4327\n",
      "Epoch 2204/3000, Training Loss (MSE): 2.2331\n",
      "Epoch 2205/3000, Training Loss (MSE): 2.9753\n",
      "Epoch 2206/3000, Training Loss (MSE): 2.7562\n",
      "Epoch 2207/3000, Training Loss (MSE): 3.2927\n",
      "Epoch 2208/3000, Training Loss (MSE): 3.3085\n",
      "Epoch 2209/3000, Training Loss (MSE): 2.2908\n",
      "Epoch 2210/3000, Training Loss (MSE): 2.9768\n",
      "Epoch 2211/3000, Training Loss (MSE): 2.9722\n",
      "Epoch 2212/3000, Training Loss (MSE): 2.3396\n",
      "Epoch 2213/3000, Training Loss (MSE): 2.6695\n",
      "Epoch 2214/3000, Training Loss (MSE): 2.9452\n",
      "Epoch 2215/3000, Training Loss (MSE): 3.1517\n",
      "Epoch 2216/3000, Training Loss (MSE): 3.0735\n",
      "Epoch 2217/3000, Training Loss (MSE): 2.0964\n",
      "Epoch 2218/3000, Training Loss (MSE): 3.4639\n",
      "Epoch 2219/3000, Training Loss (MSE): 2.9792\n",
      "Epoch 2220/3000, Training Loss (MSE): 2.1611\n",
      "Epoch 2221/3000, Training Loss (MSE): 2.9670\n",
      "Epoch 2222/3000, Training Loss (MSE): 3.8879\n",
      "Epoch 2223/3000, Training Loss (MSE): 2.8014\n",
      "Epoch 2224/3000, Training Loss (MSE): 3.0051\n",
      "Epoch 2225/3000, Training Loss (MSE): 2.8758\n",
      "Epoch 2226/3000, Training Loss (MSE): 1.7439\n",
      "Epoch 2227/3000, Training Loss (MSE): 3.3240\n",
      "Epoch 2228/3000, Training Loss (MSE): 3.2415\n",
      "Epoch 2229/3000, Training Loss (MSE): 2.8182\n",
      "Epoch 2230/3000, Training Loss (MSE): 3.3745\n",
      "Epoch 2231/3000, Training Loss (MSE): 2.5510\n",
      "Epoch 2232/3000, Training Loss (MSE): 2.6305\n",
      "Epoch 2233/3000, Training Loss (MSE): 1.6770\n",
      "Epoch 2234/3000, Training Loss (MSE): 2.8984\n",
      "Epoch 2235/3000, Training Loss (MSE): 3.0207\n",
      "Epoch 2236/3000, Training Loss (MSE): 2.6947\n",
      "Epoch 2237/3000, Training Loss (MSE): 2.1134\n",
      "Epoch 2238/3000, Training Loss (MSE): 2.9873\n",
      "Epoch 2239/3000, Training Loss (MSE): 2.0667\n",
      "Epoch 2240/3000, Training Loss (MSE): 2.0094\n",
      "Epoch 2241/3000, Training Loss (MSE): 3.5096\n",
      "Epoch 2242/3000, Training Loss (MSE): 2.3789\n",
      "Epoch 2243/3000, Training Loss (MSE): 3.3248\n",
      "Epoch 2244/3000, Training Loss (MSE): 2.5173\n",
      "Epoch 2245/3000, Training Loss (MSE): 2.2429\n",
      "Epoch 2246/3000, Training Loss (MSE): 2.9237\n",
      "Epoch 2247/3000, Training Loss (MSE): 3.4704\n",
      "Epoch 2248/3000, Training Loss (MSE): 3.4414\n",
      "Epoch 2249/3000, Training Loss (MSE): 3.0765\n",
      "Epoch 2250/3000, Training Loss (MSE): 2.7417\n",
      "Epoch 2251/3000, Training Loss (MSE): 2.3974\n",
      "Epoch 2252/3000, Training Loss (MSE): 2.9920\n",
      "Epoch 2253/3000, Training Loss (MSE): 2.5688\n",
      "Epoch 2254/3000, Training Loss (MSE): 3.0190\n",
      "Epoch 2255/3000, Training Loss (MSE): 1.9358\n",
      "Epoch 2256/3000, Training Loss (MSE): 2.7314\n",
      "Epoch 2257/3000, Training Loss (MSE): 3.7856\n",
      "Epoch 2258/3000, Training Loss (MSE): 2.8776\n",
      "Epoch 2259/3000, Training Loss (MSE): 1.9910\n",
      "Epoch 2260/3000, Training Loss (MSE): 1.5175\n",
      "Epoch 2261/3000, Training Loss (MSE): 2.9157\n",
      "Epoch 2262/3000, Training Loss (MSE): 3.0423\n",
      "Epoch 2263/3000, Training Loss (MSE): 2.8838\n",
      "Epoch 2264/3000, Training Loss (MSE): 2.8809\n",
      "Epoch 2265/3000, Training Loss (MSE): 3.0759\n",
      "Epoch 2266/3000, Training Loss (MSE): 2.1924\n",
      "Epoch 2267/3000, Training Loss (MSE): 2.5374\n",
      "Epoch 2268/3000, Training Loss (MSE): 2.4156\n",
      "Epoch 2269/3000, Training Loss (MSE): 2.0791\n",
      "Epoch 2270/3000, Training Loss (MSE): 2.3031\n",
      "Epoch 2271/3000, Training Loss (MSE): 3.3582\n",
      "Epoch 2272/3000, Training Loss (MSE): 3.2198\n",
      "Epoch 2273/3000, Training Loss (MSE): 3.1601\n",
      "Epoch 2274/3000, Training Loss (MSE): 3.1007\n",
      "Epoch 2275/3000, Training Loss (MSE): 3.1815\n",
      "Epoch 2276/3000, Training Loss (MSE): 2.0927\n",
      "Epoch 2277/3000, Training Loss (MSE): 2.4759\n",
      "Epoch 2278/3000, Training Loss (MSE): 1.9749\n",
      "Epoch 2279/3000, Training Loss (MSE): 3.1420\n",
      "Epoch 2280/3000, Training Loss (MSE): 2.5422\n",
      "Epoch 2281/3000, Training Loss (MSE): 2.7549\n",
      "Epoch 2282/3000, Training Loss (MSE): 3.1191\n",
      "Epoch 2283/3000, Training Loss (MSE): 2.3458\n",
      "Epoch 2284/3000, Training Loss (MSE): 2.5481\n",
      "Epoch 2285/3000, Training Loss (MSE): 2.4061\n",
      "Epoch 2286/3000, Training Loss (MSE): 3.0134\n",
      "Epoch 2287/3000, Training Loss (MSE): 1.8912\n",
      "Epoch 2288/3000, Training Loss (MSE): 2.6239\n",
      "Epoch 2289/3000, Training Loss (MSE): 3.6997\n",
      "Epoch 2290/3000, Training Loss (MSE): 3.1337\n",
      "Epoch 2291/3000, Training Loss (MSE): 2.4103\n",
      "Epoch 2292/3000, Training Loss (MSE): 3.2738\n",
      "Epoch 2293/3000, Training Loss (MSE): 3.2213\n",
      "Epoch 2294/3000, Training Loss (MSE): 2.8387\n",
      "Epoch 2295/3000, Training Loss (MSE): 2.6616\n",
      "Epoch 2296/3000, Training Loss (MSE): 2.5886\n",
      "Epoch 2297/3000, Training Loss (MSE): 3.4104\n",
      "Epoch 2298/3000, Training Loss (MSE): 2.6380\n",
      "Epoch 2299/3000, Training Loss (MSE): 2.5554\n",
      "Epoch 2300/3000, Training Loss (MSE): 2.6133\n",
      "Epoch 2301/3000, Training Loss (MSE): 1.8928\n",
      "Epoch 2302/3000, Training Loss (MSE): 3.0101\n",
      "Epoch 2303/3000, Training Loss (MSE): 3.2454\n",
      "Epoch 2304/3000, Training Loss (MSE): 2.4656\n",
      "Epoch 2305/3000, Training Loss (MSE): 2.4600\n",
      "Epoch 2306/3000, Training Loss (MSE): 3.0431\n",
      "Epoch 2307/3000, Training Loss (MSE): 3.3635\n",
      "Epoch 2308/3000, Training Loss (MSE): 2.6400\n",
      "Epoch 2309/3000, Training Loss (MSE): 2.1392\n",
      "Epoch 2310/3000, Training Loss (MSE): 2.7774\n",
      "Epoch 2311/3000, Training Loss (MSE): 2.1543\n",
      "Epoch 2312/3000, Training Loss (MSE): 2.5028\n",
      "Epoch 2313/3000, Training Loss (MSE): 3.1180\n",
      "Epoch 2314/3000, Training Loss (MSE): 3.4023\n",
      "Epoch 2315/3000, Training Loss (MSE): 2.2532\n",
      "Epoch 2316/3000, Training Loss (MSE): 3.1161\n",
      "Epoch 2317/3000, Training Loss (MSE): 2.7695\n",
      "Epoch 2318/3000, Training Loss (MSE): 2.2297\n",
      "Epoch 2319/3000, Training Loss (MSE): 2.6657\n",
      "Epoch 2320/3000, Training Loss (MSE): 3.5006\n",
      "Epoch 2321/3000, Training Loss (MSE): 3.2483\n",
      "Epoch 2322/3000, Training Loss (MSE): 2.4400\n",
      "Epoch 2323/3000, Training Loss (MSE): 2.1066\n",
      "Epoch 2324/3000, Training Loss (MSE): 3.1041\n",
      "Epoch 2325/3000, Training Loss (MSE): 2.8423\n",
      "Epoch 2326/3000, Training Loss (MSE): 2.4597\n",
      "Epoch 2327/3000, Training Loss (MSE): 2.1723\n",
      "Epoch 2328/3000, Training Loss (MSE): 3.8568\n",
      "Epoch 2329/3000, Training Loss (MSE): 2.6185\n",
      "Epoch 2330/3000, Training Loss (MSE): 2.8699\n",
      "Epoch 2331/3000, Training Loss (MSE): 3.2199\n",
      "Epoch 2332/3000, Training Loss (MSE): 2.8386\n",
      "Epoch 2333/3000, Training Loss (MSE): 2.7072\n",
      "Epoch 2334/3000, Training Loss (MSE): 2.6875\n",
      "Epoch 2335/3000, Training Loss (MSE): 1.7648\n",
      "Epoch 2336/3000, Training Loss (MSE): 2.0732\n",
      "Epoch 2337/3000, Training Loss (MSE): 4.3868\n",
      "Epoch 2338/3000, Training Loss (MSE): 2.5226\n",
      "Epoch 2339/3000, Training Loss (MSE): 2.9983\n",
      "Epoch 2340/3000, Training Loss (MSE): 3.5504\n",
      "Epoch 2341/3000, Training Loss (MSE): 2.9070\n",
      "Epoch 2342/3000, Training Loss (MSE): 2.5126\n",
      "Epoch 2343/3000, Training Loss (MSE): 2.7150\n",
      "Epoch 2344/3000, Training Loss (MSE): 1.4919\n",
      "Epoch 2345/3000, Training Loss (MSE): 2.9469\n",
      "Epoch 2346/3000, Training Loss (MSE): 2.7809\n",
      "Epoch 2347/3000, Training Loss (MSE): 2.9917\n",
      "Epoch 2348/3000, Training Loss (MSE): 2.6361\n",
      "Epoch 2349/3000, Training Loss (MSE): 2.9788\n",
      "Epoch 2350/3000, Training Loss (MSE): 3.6573\n",
      "Epoch 2351/3000, Training Loss (MSE): 2.4808\n",
      "Epoch 2352/3000, Training Loss (MSE): 3.5049\n",
      "Epoch 2353/3000, Training Loss (MSE): 1.9943\n",
      "Epoch 2354/3000, Training Loss (MSE): 1.8043\n",
      "Epoch 2355/3000, Training Loss (MSE): 2.7043\n",
      "Epoch 2356/3000, Training Loss (MSE): 2.5259\n",
      "Epoch 2357/3000, Training Loss (MSE): 1.9696\n",
      "Epoch 2358/3000, Training Loss (MSE): 1.6854\n",
      "Epoch 2359/3000, Training Loss (MSE): 2.2730\n",
      "Epoch 2360/3000, Training Loss (MSE): 1.5851\n",
      "Epoch 2361/3000, Training Loss (MSE): 1.6827\n",
      "Epoch 2362/3000, Training Loss (MSE): 2.5286\n",
      "Epoch 2363/3000, Training Loss (MSE): 2.4476\n",
      "Epoch 2364/3000, Training Loss (MSE): 1.9324\n",
      "Epoch 2365/3000, Training Loss (MSE): 2.5092\n",
      "Epoch 2366/3000, Training Loss (MSE): 2.0263\n",
      "Epoch 2367/3000, Training Loss (MSE): 2.5797\n",
      "Epoch 2368/3000, Training Loss (MSE): 2.6279\n",
      "Epoch 2369/3000, Training Loss (MSE): 2.7643\n",
      "Epoch 2370/3000, Training Loss (MSE): 0.8628\n",
      "Epoch 2371/3000, Training Loss (MSE): 2.9805\n",
      "Epoch 2372/3000, Training Loss (MSE): 2.3261\n",
      "Epoch 2373/3000, Training Loss (MSE): 1.6693\n",
      "Epoch 2374/3000, Training Loss (MSE): 2.1324\n",
      "Epoch 2375/3000, Training Loss (MSE): 1.9957\n",
      "Epoch 2376/3000, Training Loss (MSE): 2.2089\n",
      "Epoch 2377/3000, Training Loss (MSE): 2.9472\n",
      "Epoch 2378/3000, Training Loss (MSE): 1.9156\n",
      "Epoch 2379/3000, Training Loss (MSE): 2.3328\n",
      "Epoch 2380/3000, Training Loss (MSE): 2.7302\n",
      "Epoch 2381/3000, Training Loss (MSE): 3.0565\n",
      "Epoch 2382/3000, Training Loss (MSE): 2.5260\n",
      "Epoch 2383/3000, Training Loss (MSE): 2.1077\n",
      "Epoch 2384/3000, Training Loss (MSE): 2.9951\n",
      "Epoch 2385/3000, Training Loss (MSE): 1.3127\n",
      "Epoch 2386/3000, Training Loss (MSE): 2.8241\n",
      "Epoch 2387/3000, Training Loss (MSE): 1.9890\n",
      "Epoch 2388/3000, Training Loss (MSE): 1.2641\n",
      "Epoch 2389/3000, Training Loss (MSE): 2.1877\n",
      "Epoch 2390/3000, Training Loss (MSE): 1.6326\n",
      "Epoch 2391/3000, Training Loss (MSE): 2.9194\n",
      "Epoch 2392/3000, Training Loss (MSE): 2.6031\n",
      "Epoch 2393/3000, Training Loss (MSE): 2.4808\n",
      "Epoch 2394/3000, Training Loss (MSE): 1.9872\n",
      "Epoch 2395/3000, Training Loss (MSE): 2.4937\n",
      "Epoch 2396/3000, Training Loss (MSE): 2.2700\n",
      "Epoch 2397/3000, Training Loss (MSE): 2.7373\n",
      "Epoch 2398/3000, Training Loss (MSE): 2.1253\n",
      "Epoch 2399/3000, Training Loss (MSE): 2.6967\n",
      "Epoch 2400/3000, Training Loss (MSE): 2.0614\n",
      "Epoch 2401/3000, Training Loss (MSE): 2.2911\n",
      "Epoch 2402/3000, Training Loss (MSE): 2.4667\n",
      "Epoch 2403/3000, Training Loss (MSE): 1.9866\n",
      "Epoch 2404/3000, Training Loss (MSE): 1.7000\n",
      "Epoch 2405/3000, Training Loss (MSE): 1.9787\n",
      "Epoch 2406/3000, Training Loss (MSE): 1.4983\n",
      "Epoch 2407/3000, Training Loss (MSE): 2.9803\n",
      "Epoch 2408/3000, Training Loss (MSE): 1.7744\n",
      "Epoch 2409/3000, Training Loss (MSE): 1.4225\n",
      "Epoch 2410/3000, Training Loss (MSE): 3.2847\n",
      "Epoch 2411/3000, Training Loss (MSE): 3.1243\n",
      "Epoch 2412/3000, Training Loss (MSE): 1.7172\n",
      "Epoch 2413/3000, Training Loss (MSE): 2.0625\n",
      "Epoch 2414/3000, Training Loss (MSE): 2.1068\n",
      "Epoch 2415/3000, Training Loss (MSE): 2.7346\n",
      "Epoch 2416/3000, Training Loss (MSE): 1.4046\n",
      "Epoch 2417/3000, Training Loss (MSE): 2.9733\n",
      "Epoch 2418/3000, Training Loss (MSE): 1.9270\n",
      "Epoch 2419/3000, Training Loss (MSE): 1.8511\n",
      "Epoch 2420/3000, Training Loss (MSE): 2.1854\n",
      "Epoch 2421/3000, Training Loss (MSE): 3.1540\n",
      "Epoch 2422/3000, Training Loss (MSE): 1.6644\n",
      "Epoch 2423/3000, Training Loss (MSE): 2.2324\n",
      "Epoch 2424/3000, Training Loss (MSE): 2.6705\n",
      "Epoch 2425/3000, Training Loss (MSE): 1.7126\n",
      "Epoch 2426/3000, Training Loss (MSE): 2.0642\n",
      "Epoch 2427/3000, Training Loss (MSE): 3.2185\n",
      "Epoch 2428/3000, Training Loss (MSE): 2.7089\n",
      "Epoch 2429/3000, Training Loss (MSE): 2.5679\n",
      "Epoch 2430/3000, Training Loss (MSE): 2.0344\n",
      "Epoch 2431/3000, Training Loss (MSE): 1.9586\n",
      "Epoch 2432/3000, Training Loss (MSE): 2.1603\n",
      "Epoch 2433/3000, Training Loss (MSE): 3.0674\n",
      "Epoch 2434/3000, Training Loss (MSE): 2.0741\n",
      "Epoch 2435/3000, Training Loss (MSE): 2.1246\n",
      "Epoch 2436/3000, Training Loss (MSE): 1.0022\n",
      "Epoch 2437/3000, Training Loss (MSE): 2.0557\n",
      "Epoch 2438/3000, Training Loss (MSE): 2.5958\n",
      "Epoch 2439/3000, Training Loss (MSE): 3.0139\n",
      "Epoch 2440/3000, Training Loss (MSE): 1.8535\n",
      "Epoch 2441/3000, Training Loss (MSE): 3.3155\n",
      "Epoch 2442/3000, Training Loss (MSE): 3.7851\n",
      "Epoch 2443/3000, Training Loss (MSE): 2.4063\n",
      "Epoch 2444/3000, Training Loss (MSE): 3.1185\n",
      "Epoch 2445/3000, Training Loss (MSE): 2.2413\n",
      "Epoch 2446/3000, Training Loss (MSE): 1.6819\n",
      "Epoch 2447/3000, Training Loss (MSE): 1.6706\n",
      "Epoch 2448/3000, Training Loss (MSE): 2.3006\n",
      "Epoch 2449/3000, Training Loss (MSE): 1.6865\n",
      "Epoch 2450/3000, Training Loss (MSE): 1.8759\n",
      "Epoch 2451/3000, Training Loss (MSE): 1.4512\n",
      "Epoch 2452/3000, Training Loss (MSE): 2.2201\n",
      "Epoch 2453/3000, Training Loss (MSE): 2.0722\n",
      "Epoch 2454/3000, Training Loss (MSE): 2.4133\n",
      "Epoch 2455/3000, Training Loss (MSE): 1.3526\n",
      "Epoch 2456/3000, Training Loss (MSE): 2.7823\n",
      "Epoch 2457/3000, Training Loss (MSE): 2.8323\n",
      "Epoch 2458/3000, Training Loss (MSE): 2.2696\n",
      "Epoch 2459/3000, Training Loss (MSE): 2.3026\n",
      "Epoch 2460/3000, Training Loss (MSE): 2.4868\n",
      "Epoch 2461/3000, Training Loss (MSE): 2.8977\n",
      "Epoch 2462/3000, Training Loss (MSE): 1.9578\n",
      "Epoch 2463/3000, Training Loss (MSE): 3.5206\n",
      "Epoch 2464/3000, Training Loss (MSE): 1.4469\n",
      "Epoch 2465/3000, Training Loss (MSE): 2.4974\n",
      "Epoch 2466/3000, Training Loss (MSE): 2.3604\n",
      "Epoch 2467/3000, Training Loss (MSE): 3.3879\n",
      "Epoch 2468/3000, Training Loss (MSE): 1.7253\n",
      "Epoch 2469/3000, Training Loss (MSE): 2.8673\n",
      "Epoch 2470/3000, Training Loss (MSE): 2.1767\n",
      "Epoch 2471/3000, Training Loss (MSE): 2.5324\n",
      "Epoch 2472/3000, Training Loss (MSE): 2.0817\n",
      "Epoch 2473/3000, Training Loss (MSE): 1.9916\n",
      "Epoch 2474/3000, Training Loss (MSE): 2.6287\n",
      "Epoch 2475/3000, Training Loss (MSE): 2.3232\n",
      "Epoch 2476/3000, Training Loss (MSE): 1.6564\n",
      "Epoch 2477/3000, Training Loss (MSE): 1.5964\n",
      "Epoch 2478/3000, Training Loss (MSE): 1.9319\n",
      "Epoch 2479/3000, Training Loss (MSE): 2.3566\n",
      "Epoch 2480/3000, Training Loss (MSE): 2.2415\n",
      "Epoch 2481/3000, Training Loss (MSE): 2.3788\n",
      "Epoch 2482/3000, Training Loss (MSE): 3.3934\n",
      "Epoch 2483/3000, Training Loss (MSE): 1.6333\n",
      "Epoch 2484/3000, Training Loss (MSE): 2.4003\n",
      "Epoch 2485/3000, Training Loss (MSE): 2.0852\n",
      "Epoch 2486/3000, Training Loss (MSE): 2.2948\n",
      "Epoch 2487/3000, Training Loss (MSE): 2.5779\n",
      "Epoch 2488/3000, Training Loss (MSE): 2.4681\n",
      "Epoch 2489/3000, Training Loss (MSE): 2.5167\n",
      "Epoch 2490/3000, Training Loss (MSE): 2.1305\n",
      "Epoch 2491/3000, Training Loss (MSE): 1.9453\n",
      "Epoch 2492/3000, Training Loss (MSE): 3.5094\n",
      "Epoch 2493/3000, Training Loss (MSE): 2.2308\n",
      "Epoch 2494/3000, Training Loss (MSE): 2.6317\n",
      "Epoch 2495/3000, Training Loss (MSE): 1.6655\n",
      "Epoch 2496/3000, Training Loss (MSE): 1.9421\n",
      "Epoch 2497/3000, Training Loss (MSE): 2.7192\n",
      "Epoch 2498/3000, Training Loss (MSE): 2.8575\n",
      "Epoch 2499/3000, Training Loss (MSE): 2.9225\n",
      "Epoch 2500/3000, Training Loss (MSE): 2.3412\n",
      "Epoch 2501/3000, Training Loss (MSE): 2.0752\n",
      "Epoch 2502/3000, Training Loss (MSE): 2.2902\n",
      "Epoch 2503/3000, Training Loss (MSE): 2.3137\n",
      "Epoch 2504/3000, Training Loss (MSE): 1.9135\n",
      "Epoch 2505/3000, Training Loss (MSE): 2.1869\n",
      "Epoch 2506/3000, Training Loss (MSE): 1.9309\n",
      "Epoch 2507/3000, Training Loss (MSE): 3.0024\n",
      "Epoch 2508/3000, Training Loss (MSE): 2.9808\n",
      "Epoch 2509/3000, Training Loss (MSE): 2.1146\n",
      "Epoch 2510/3000, Training Loss (MSE): 2.5633\n",
      "Epoch 2511/3000, Training Loss (MSE): 1.9834\n",
      "Epoch 2512/3000, Training Loss (MSE): 2.7786\n",
      "Epoch 2513/3000, Training Loss (MSE): 2.0100\n",
      "Epoch 2514/3000, Training Loss (MSE): 1.5435\n",
      "Epoch 2515/3000, Training Loss (MSE): 1.5746\n",
      "Epoch 2516/3000, Training Loss (MSE): 2.5596\n",
      "Epoch 2517/3000, Training Loss (MSE): 1.8315\n",
      "Epoch 2518/3000, Training Loss (MSE): 2.9596\n",
      "Epoch 2519/3000, Training Loss (MSE): 3.2198\n",
      "Epoch 2520/3000, Training Loss (MSE): 4.0559\n",
      "Epoch 2521/3000, Training Loss (MSE): 2.0540\n",
      "Epoch 2522/3000, Training Loss (MSE): 2.6711\n",
      "Epoch 2523/3000, Training Loss (MSE): 2.5494\n",
      "Epoch 2524/3000, Training Loss (MSE): 0.7082\n",
      "Epoch 2525/3000, Training Loss (MSE): 2.7729\n",
      "Epoch 2526/3000, Training Loss (MSE): 1.2370\n",
      "Epoch 2527/3000, Training Loss (MSE): 2.6329\n",
      "Epoch 2528/3000, Training Loss (MSE): 2.1159\n",
      "Epoch 2529/3000, Training Loss (MSE): 1.7547\n",
      "Epoch 2530/3000, Training Loss (MSE): 2.4319\n",
      "Epoch 2531/3000, Training Loss (MSE): 2.9591\n",
      "Epoch 2532/3000, Training Loss (MSE): 2.0230\n",
      "Epoch 2533/3000, Training Loss (MSE): 1.4868\n",
      "Epoch 2534/3000, Training Loss (MSE): 2.9797\n",
      "Epoch 2535/3000, Training Loss (MSE): 2.6144\n",
      "Epoch 2536/3000, Training Loss (MSE): 3.8393\n",
      "Epoch 2537/3000, Training Loss (MSE): 3.1452\n",
      "Epoch 2538/3000, Training Loss (MSE): 1.7945\n",
      "Epoch 2539/3000, Training Loss (MSE): 2.1121\n",
      "Epoch 2540/3000, Training Loss (MSE): 2.3834\n",
      "Epoch 2541/3000, Training Loss (MSE): 2.8996\n",
      "Epoch 2542/3000, Training Loss (MSE): 2.1727\n",
      "Epoch 2543/3000, Training Loss (MSE): 2.7509\n",
      "Epoch 2544/3000, Training Loss (MSE): 2.6468\n",
      "Epoch 2545/3000, Training Loss (MSE): 2.4632\n",
      "Epoch 2546/3000, Training Loss (MSE): 2.6609\n",
      "Epoch 2547/3000, Training Loss (MSE): 2.1070\n",
      "Epoch 2548/3000, Training Loss (MSE): 1.8040\n",
      "Epoch 2549/3000, Training Loss (MSE): 2.5185\n",
      "Epoch 2550/3000, Training Loss (MSE): 1.7392\n",
      "Epoch 2551/3000, Training Loss (MSE): 2.9411\n",
      "Epoch 2552/3000, Training Loss (MSE): 2.0365\n",
      "Epoch 2553/3000, Training Loss (MSE): 2.3999\n",
      "Epoch 2554/3000, Training Loss (MSE): 2.4395\n",
      "Epoch 2555/3000, Training Loss (MSE): 2.4774\n",
      "Epoch 2556/3000, Training Loss (MSE): 2.8364\n",
      "Epoch 2557/3000, Training Loss (MSE): 1.9207\n",
      "Epoch 2558/3000, Training Loss (MSE): 1.7744\n",
      "Epoch 2559/3000, Training Loss (MSE): 1.9686\n",
      "Epoch 2560/3000, Training Loss (MSE): 2.4385\n",
      "Epoch 2561/3000, Training Loss (MSE): 2.9273\n",
      "Epoch 2562/3000, Training Loss (MSE): 1.6045\n",
      "Epoch 2563/3000, Training Loss (MSE): 2.8335\n",
      "Epoch 2564/3000, Training Loss (MSE): 1.9724\n",
      "Epoch 2565/3000, Training Loss (MSE): 2.2340\n",
      "Epoch 2566/3000, Training Loss (MSE): 1.7766\n",
      "Epoch 2567/3000, Training Loss (MSE): 2.6535\n",
      "Epoch 2568/3000, Training Loss (MSE): 2.8246\n",
      "Epoch 2569/3000, Training Loss (MSE): 1.7173\n",
      "Epoch 2570/3000, Training Loss (MSE): 2.3503\n",
      "Epoch 2571/3000, Training Loss (MSE): 2.4678\n",
      "Epoch 2572/3000, Training Loss (MSE): 2.6111\n",
      "Epoch 2573/3000, Training Loss (MSE): 2.7336\n",
      "Epoch 2574/3000, Training Loss (MSE): 3.3408\n",
      "Epoch 2575/3000, Training Loss (MSE): 2.3716\n",
      "Epoch 2576/3000, Training Loss (MSE): 2.8423\n",
      "Epoch 2577/3000, Training Loss (MSE): 2.0534\n",
      "Epoch 2578/3000, Training Loss (MSE): 2.3519\n",
      "Epoch 2579/3000, Training Loss (MSE): 1.6593\n",
      "Epoch 2580/3000, Training Loss (MSE): 2.4578\n",
      "Epoch 2581/3000, Training Loss (MSE): 2.8372\n",
      "Epoch 2582/3000, Training Loss (MSE): 2.7129\n",
      "Epoch 2583/3000, Training Loss (MSE): 2.4796\n",
      "Epoch 2584/3000, Training Loss (MSE): 2.1268\n",
      "Epoch 2585/3000, Training Loss (MSE): 1.9292\n",
      "Epoch 2586/3000, Training Loss (MSE): 2.1693\n",
      "Epoch 2587/3000, Training Loss (MSE): 1.4164\n",
      "Epoch 2588/3000, Training Loss (MSE): 1.4763\n",
      "Epoch 2589/3000, Training Loss (MSE): 3.5244\n",
      "Epoch 2590/3000, Training Loss (MSE): 2.1329\n",
      "Epoch 2591/3000, Training Loss (MSE): 2.1640\n",
      "Epoch 2592/3000, Training Loss (MSE): 1.6073\n",
      "Epoch 2593/3000, Training Loss (MSE): 1.7383\n",
      "Epoch 2594/3000, Training Loss (MSE): 1.8415\n",
      "Epoch 2595/3000, Training Loss (MSE): 3.0877\n",
      "Epoch 2596/3000, Training Loss (MSE): 2.9662\n",
      "Epoch 2597/3000, Training Loss (MSE): 2.0005\n",
      "Epoch 2598/3000, Training Loss (MSE): 1.9699\n",
      "Epoch 2599/3000, Training Loss (MSE): 2.8091\n",
      "Epoch 2600/3000, Training Loss (MSE): 1.1868\n",
      "Epoch 2601/3000, Training Loss (MSE): 1.6606\n",
      "Epoch 2602/3000, Training Loss (MSE): 1.6329\n",
      "Epoch 2603/3000, Training Loss (MSE): 2.2004\n",
      "Epoch 2604/3000, Training Loss (MSE): 1.9297\n",
      "Epoch 2605/3000, Training Loss (MSE): 3.4146\n",
      "Epoch 2606/3000, Training Loss (MSE): 3.5555\n",
      "Epoch 2607/3000, Training Loss (MSE): 2.2219\n",
      "Epoch 2608/3000, Training Loss (MSE): 2.2606\n",
      "Epoch 2609/3000, Training Loss (MSE): 1.5799\n",
      "Epoch 2610/3000, Training Loss (MSE): 2.6849\n",
      "Epoch 2611/3000, Training Loss (MSE): 2.9062\n",
      "Epoch 2612/3000, Training Loss (MSE): 2.7316\n",
      "Epoch 2613/3000, Training Loss (MSE): 2.6896\n",
      "Epoch 2614/3000, Training Loss (MSE): 2.6071\n",
      "Epoch 2615/3000, Training Loss (MSE): 2.5760\n",
      "Epoch 2616/3000, Training Loss (MSE): 2.0502\n",
      "Epoch 2617/3000, Training Loss (MSE): 2.0856\n",
      "Epoch 2618/3000, Training Loss (MSE): 1.3524\n",
      "Epoch 2619/3000, Training Loss (MSE): 3.3197\n",
      "Epoch 2620/3000, Training Loss (MSE): 2.4159\n",
      "Epoch 2621/3000, Training Loss (MSE): 1.9212\n",
      "Epoch 2622/3000, Training Loss (MSE): 2.8503\n",
      "Epoch 2623/3000, Training Loss (MSE): 2.6521\n",
      "Epoch 2624/3000, Training Loss (MSE): 2.7075\n",
      "Epoch 2625/3000, Training Loss (MSE): 3.2836\n",
      "Epoch 2626/3000, Training Loss (MSE): 1.7279\n",
      "Epoch 2627/3000, Training Loss (MSE): 3.6216\n",
      "Epoch 2628/3000, Training Loss (MSE): 2.9150\n",
      "Epoch 2629/3000, Training Loss (MSE): 2.5109\n",
      "Epoch 2630/3000, Training Loss (MSE): 2.5416\n",
      "Epoch 2631/3000, Training Loss (MSE): 1.9475\n",
      "Epoch 2632/3000, Training Loss (MSE): 2.3164\n",
      "Epoch 2633/3000, Training Loss (MSE): 2.7712\n",
      "Epoch 2634/3000, Training Loss (MSE): 2.4741\n",
      "Epoch 2635/3000, Training Loss (MSE): 2.0653\n",
      "Epoch 2636/3000, Training Loss (MSE): 3.4020\n",
      "Epoch 2637/3000, Training Loss (MSE): 2.3176\n",
      "Epoch 2638/3000, Training Loss (MSE): 2.5681\n",
      "Epoch 2639/3000, Training Loss (MSE): 2.2354\n",
      "Epoch 2640/3000, Training Loss (MSE): 2.7839\n",
      "Epoch 2641/3000, Training Loss (MSE): 2.7644\n",
      "Epoch 2642/3000, Training Loss (MSE): 2.2435\n",
      "Epoch 2643/3000, Training Loss (MSE): 2.8654\n",
      "Epoch 2644/3000, Training Loss (MSE): 2.2799\n",
      "Epoch 2645/3000, Training Loss (MSE): 2.5024\n",
      "Epoch 2646/3000, Training Loss (MSE): 3.2589\n",
      "Epoch 2647/3000, Training Loss (MSE): 1.9653\n",
      "Epoch 2648/3000, Training Loss (MSE): 2.2430\n",
      "Epoch 2649/3000, Training Loss (MSE): 3.4869\n",
      "Epoch 2650/3000, Training Loss (MSE): 2.9164\n",
      "Epoch 2651/3000, Training Loss (MSE): 3.2161\n",
      "Epoch 2652/3000, Training Loss (MSE): 1.6280\n",
      "Epoch 2653/3000, Training Loss (MSE): 1.8398\n",
      "Epoch 2654/3000, Training Loss (MSE): 3.2259\n",
      "Epoch 2655/3000, Training Loss (MSE): 3.4869\n",
      "Epoch 2656/3000, Training Loss (MSE): 3.2894\n",
      "Epoch 2657/3000, Training Loss (MSE): 3.1038\n",
      "Epoch 2658/3000, Training Loss (MSE): 2.7428\n",
      "Epoch 2659/3000, Training Loss (MSE): 2.7924\n",
      "Epoch 2660/3000, Training Loss (MSE): 3.7538\n",
      "Epoch 2661/3000, Training Loss (MSE): 2.1864\n",
      "Epoch 2662/3000, Training Loss (MSE): 3.1165\n",
      "Epoch 2663/3000, Training Loss (MSE): 2.2269\n",
      "Epoch 2664/3000, Training Loss (MSE): 2.0030\n",
      "Epoch 2665/3000, Training Loss (MSE): 2.1026\n",
      "Epoch 2666/3000, Training Loss (MSE): 2.5826\n",
      "Epoch 2667/3000, Training Loss (MSE): 2.2689\n",
      "Epoch 2668/3000, Training Loss (MSE): 2.0932\n",
      "Epoch 2669/3000, Training Loss (MSE): 2.0304\n",
      "Epoch 2670/3000, Training Loss (MSE): 2.6323\n",
      "Epoch 2671/3000, Training Loss (MSE): 2.9580\n",
      "Epoch 2672/3000, Training Loss (MSE): 3.1664\n",
      "Epoch 2673/3000, Training Loss (MSE): 1.8196\n",
      "Epoch 2674/3000, Training Loss (MSE): 3.5170\n",
      "Epoch 2675/3000, Training Loss (MSE): 2.1850\n",
      "Epoch 2676/3000, Training Loss (MSE): 2.9858\n",
      "Epoch 2677/3000, Training Loss (MSE): 1.8071\n",
      "Epoch 2678/3000, Training Loss (MSE): 2.3791\n",
      "Epoch 2679/3000, Training Loss (MSE): 3.2203\n",
      "Epoch 2680/3000, Training Loss (MSE): 2.1362\n",
      "Epoch 2681/3000, Training Loss (MSE): 2.3479\n",
      "Epoch 2682/3000, Training Loss (MSE): 1.6824\n",
      "Epoch 2683/3000, Training Loss (MSE): 3.1227\n",
      "Epoch 2684/3000, Training Loss (MSE): 1.7953\n",
      "Epoch 2685/3000, Training Loss (MSE): 1.9496\n",
      "Epoch 2686/3000, Training Loss (MSE): 2.7312\n",
      "Epoch 2687/3000, Training Loss (MSE): 2.8270\n",
      "Epoch 2688/3000, Training Loss (MSE): 3.0384\n",
      "Epoch 2689/3000, Training Loss (MSE): 2.1926\n",
      "Epoch 2690/3000, Training Loss (MSE): 2.6531\n",
      "Epoch 2691/3000, Training Loss (MSE): 2.2308\n",
      "Epoch 2692/3000, Training Loss (MSE): 2.1828\n",
      "Epoch 2693/3000, Training Loss (MSE): 2.3814\n",
      "Epoch 2694/3000, Training Loss (MSE): 2.7061\n",
      "Epoch 2695/3000, Training Loss (MSE): 2.6745\n",
      "Epoch 2696/3000, Training Loss (MSE): 3.5596\n",
      "Epoch 2697/3000, Training Loss (MSE): 2.9294\n",
      "Epoch 2698/3000, Training Loss (MSE): 2.9889\n",
      "Epoch 2699/3000, Training Loss (MSE): 3.2890\n",
      "Epoch 2700/3000, Training Loss (MSE): 2.3589\n",
      "Epoch 2701/3000, Training Loss (MSE): 2.6681\n",
      "Epoch 2702/3000, Training Loss (MSE): 2.9999\n",
      "Epoch 2703/3000, Training Loss (MSE): 3.1969\n",
      "Epoch 2704/3000, Training Loss (MSE): 2.4412\n",
      "Epoch 2705/3000, Training Loss (MSE): 1.5737\n",
      "Epoch 2706/3000, Training Loss (MSE): 3.0632\n",
      "Epoch 2707/3000, Training Loss (MSE): 3.0088\n",
      "Epoch 2708/3000, Training Loss (MSE): 2.6191\n",
      "Epoch 2709/3000, Training Loss (MSE): 2.0648\n",
      "Epoch 2710/3000, Training Loss (MSE): 1.5592\n",
      "Epoch 2711/3000, Training Loss (MSE): 2.6271\n",
      "Epoch 2712/3000, Training Loss (MSE): 2.1526\n",
      "Epoch 2713/3000, Training Loss (MSE): 3.4193\n",
      "Epoch 2714/3000, Training Loss (MSE): 2.0682\n",
      "Epoch 2715/3000, Training Loss (MSE): 2.3299\n",
      "Epoch 2716/3000, Training Loss (MSE): 2.2770\n",
      "Epoch 2717/3000, Training Loss (MSE): 2.6768\n",
      "Epoch 2718/3000, Training Loss (MSE): 2.2299\n",
      "Epoch 2719/3000, Training Loss (MSE): 3.6099\n",
      "Epoch 2720/3000, Training Loss (MSE): 2.1530\n",
      "Epoch 2721/3000, Training Loss (MSE): 2.5218\n",
      "Epoch 2722/3000, Training Loss (MSE): 2.3793\n",
      "Epoch 2723/3000, Training Loss (MSE): 2.7643\n",
      "Epoch 2724/3000, Training Loss (MSE): 2.9276\n",
      "Epoch 2725/3000, Training Loss (MSE): 2.2889\n",
      "Epoch 2726/3000, Training Loss (MSE): 2.6782\n",
      "Epoch 2727/3000, Training Loss (MSE): 3.6754\n",
      "Epoch 2728/3000, Training Loss (MSE): 2.7446\n",
      "Epoch 2729/3000, Training Loss (MSE): 1.8307\n",
      "Epoch 2730/3000, Training Loss (MSE): 3.3001\n",
      "Epoch 2731/3000, Training Loss (MSE): 2.8290\n",
      "Epoch 2732/3000, Training Loss (MSE): 1.9753\n",
      "Epoch 2733/3000, Training Loss (MSE): 2.4807\n",
      "Epoch 2734/3000, Training Loss (MSE): 2.2878\n",
      "Epoch 2735/3000, Training Loss (MSE): 2.8411\n",
      "Epoch 2736/3000, Training Loss (MSE): 3.0013\n",
      "Epoch 2737/3000, Training Loss (MSE): 2.6925\n",
      "Epoch 2738/3000, Training Loss (MSE): 2.4880\n",
      "Epoch 2739/3000, Training Loss (MSE): 1.6631\n",
      "Epoch 2740/3000, Training Loss (MSE): 2.5013\n",
      "Epoch 2741/3000, Training Loss (MSE): 3.1080\n",
      "Epoch 2742/3000, Training Loss (MSE): 2.2400\n",
      "Epoch 2743/3000, Training Loss (MSE): 1.4584\n",
      "Epoch 2744/3000, Training Loss (MSE): 1.5808\n",
      "Epoch 2745/3000, Training Loss (MSE): 2.5833\n",
      "Epoch 2746/3000, Training Loss (MSE): 2.4483\n",
      "Epoch 2747/3000, Training Loss (MSE): 2.9327\n",
      "Epoch 2748/3000, Training Loss (MSE): 2.1853\n",
      "Epoch 2749/3000, Training Loss (MSE): 3.1857\n",
      "Epoch 2750/3000, Training Loss (MSE): 1.8908\n",
      "Epoch 2751/3000, Training Loss (MSE): 2.8455\n",
      "Epoch 2752/3000, Training Loss (MSE): 2.7128\n",
      "Epoch 2753/3000, Training Loss (MSE): 2.8006\n",
      "Epoch 2754/3000, Training Loss (MSE): 1.7901\n",
      "Epoch 2755/3000, Training Loss (MSE): 2.2111\n",
      "Epoch 2756/3000, Training Loss (MSE): 2.7041\n",
      "Epoch 2757/3000, Training Loss (MSE): 3.3596\n",
      "Epoch 2758/3000, Training Loss (MSE): 2.0123\n",
      "Epoch 2759/3000, Training Loss (MSE): 4.1954\n",
      "Epoch 2760/3000, Training Loss (MSE): 2.1828\n",
      "Epoch 2761/3000, Training Loss (MSE): 2.3982\n",
      "Epoch 2762/3000, Training Loss (MSE): 2.0801\n",
      "Epoch 2763/3000, Training Loss (MSE): 1.7224\n",
      "Epoch 2764/3000, Training Loss (MSE): 1.6386\n",
      "Epoch 2765/3000, Training Loss (MSE): 2.1732\n",
      "Epoch 2766/3000, Training Loss (MSE): 2.9253\n",
      "Epoch 2767/3000, Training Loss (MSE): 2.7958\n",
      "Epoch 2768/3000, Training Loss (MSE): 1.9819\n",
      "Epoch 2769/3000, Training Loss (MSE): 2.6280\n",
      "Epoch 2770/3000, Training Loss (MSE): 2.0447\n",
      "Epoch 2771/3000, Training Loss (MSE): 3.4560\n",
      "Epoch 2772/3000, Training Loss (MSE): 2.7898\n",
      "Epoch 2773/3000, Training Loss (MSE): 1.7192\n",
      "Epoch 2774/3000, Training Loss (MSE): 2.9394\n",
      "Epoch 2775/3000, Training Loss (MSE): 1.6580\n",
      "Epoch 2776/3000, Training Loss (MSE): 1.3387\n",
      "Epoch 2777/3000, Training Loss (MSE): 1.6631\n",
      "Epoch 2778/3000, Training Loss (MSE): 1.6437\n",
      "Epoch 2779/3000, Training Loss (MSE): 1.6275\n",
      "Epoch 2780/3000, Training Loss (MSE): 3.2910\n",
      "Epoch 2781/3000, Training Loss (MSE): 2.6905\n",
      "Epoch 2782/3000, Training Loss (MSE): 1.9677\n",
      "Epoch 2783/3000, Training Loss (MSE): 3.1984\n",
      "Epoch 2784/3000, Training Loss (MSE): 1.7983\n",
      "Epoch 2785/3000, Training Loss (MSE): 2.2251\n",
      "Epoch 2786/3000, Training Loss (MSE): 2.0985\n",
      "Epoch 2787/3000, Training Loss (MSE): 1.5091\n",
      "Epoch 2788/3000, Training Loss (MSE): 1.9922\n",
      "Epoch 2789/3000, Training Loss (MSE): 2.6719\n",
      "Epoch 2790/3000, Training Loss (MSE): 2.1265\n",
      "Epoch 2791/3000, Training Loss (MSE): 3.1333\n",
      "Epoch 2792/3000, Training Loss (MSE): 1.5428\n",
      "Epoch 2793/3000, Training Loss (MSE): 2.2101\n",
      "Epoch 2794/3000, Training Loss (MSE): 2.4998\n",
      "Epoch 2795/3000, Training Loss (MSE): 2.3816\n",
      "Epoch 2796/3000, Training Loss (MSE): 2.6043\n",
      "Epoch 2797/3000, Training Loss (MSE): 1.2968\n",
      "Epoch 2798/3000, Training Loss (MSE): 1.4022\n",
      "Epoch 2799/3000, Training Loss (MSE): 1.9139\n",
      "Epoch 2800/3000, Training Loss (MSE): 3.0254\n",
      "Epoch 2801/3000, Training Loss (MSE): 2.0063\n",
      "Epoch 2802/3000, Training Loss (MSE): 2.2192\n",
      "Epoch 2803/3000, Training Loss (MSE): 3.1013\n",
      "Epoch 2804/3000, Training Loss (MSE): 1.6855\n",
      "Epoch 2805/3000, Training Loss (MSE): 1.7071\n",
      "Epoch 2806/3000, Training Loss (MSE): 2.8297\n",
      "Epoch 2807/3000, Training Loss (MSE): 2.4900\n",
      "Epoch 2808/3000, Training Loss (MSE): 2.3715\n",
      "Epoch 2809/3000, Training Loss (MSE): 2.7181\n",
      "Epoch 2810/3000, Training Loss (MSE): 3.3397\n",
      "Epoch 2811/3000, Training Loss (MSE): 2.2049\n",
      "Epoch 2812/3000, Training Loss (MSE): 2.2369\n",
      "Epoch 2813/3000, Training Loss (MSE): 2.8646\n",
      "Epoch 2814/3000, Training Loss (MSE): 1.7449\n",
      "Epoch 2815/3000, Training Loss (MSE): 2.1406\n",
      "Epoch 2816/3000, Training Loss (MSE): 1.8400\n",
      "Epoch 2817/3000, Training Loss (MSE): 2.2487\n",
      "Epoch 2818/3000, Training Loss (MSE): 2.5870\n",
      "Epoch 2819/3000, Training Loss (MSE): 2.8711\n",
      "Epoch 2820/3000, Training Loss (MSE): 2.0980\n",
      "Epoch 2821/3000, Training Loss (MSE): 2.6062\n",
      "Epoch 2822/3000, Training Loss (MSE): 3.4011\n",
      "Epoch 2823/3000, Training Loss (MSE): 2.6252\n",
      "Epoch 2824/3000, Training Loss (MSE): 1.8183\n",
      "Epoch 2825/3000, Training Loss (MSE): 1.9452\n",
      "Epoch 2826/3000, Training Loss (MSE): 2.7761\n",
      "Epoch 2827/3000, Training Loss (MSE): 2.4988\n",
      "Epoch 2828/3000, Training Loss (MSE): 1.8299\n",
      "Epoch 2829/3000, Training Loss (MSE): 3.2326\n",
      "Epoch 2830/3000, Training Loss (MSE): 1.9309\n",
      "Epoch 2831/3000, Training Loss (MSE): 1.4503\n",
      "Epoch 2832/3000, Training Loss (MSE): 3.0260\n",
      "Epoch 2833/3000, Training Loss (MSE): 2.0441\n",
      "Epoch 2834/3000, Training Loss (MSE): 2.0620\n",
      "Epoch 2835/3000, Training Loss (MSE): 3.3018\n",
      "Epoch 2836/3000, Training Loss (MSE): 2.7474\n",
      "Epoch 2837/3000, Training Loss (MSE): 3.1384\n",
      "Epoch 2838/3000, Training Loss (MSE): 2.7087\n",
      "Epoch 2839/3000, Training Loss (MSE): 2.8362\n",
      "Epoch 2840/3000, Training Loss (MSE): 1.7697\n",
      "Epoch 2841/3000, Training Loss (MSE): 2.5221\n",
      "Epoch 2842/3000, Training Loss (MSE): 2.9751\n",
      "Epoch 2843/3000, Training Loss (MSE): 2.8618\n",
      "Epoch 2844/3000, Training Loss (MSE): 2.2473\n",
      "Epoch 2845/3000, Training Loss (MSE): 1.7495\n",
      "Epoch 2846/3000, Training Loss (MSE): 2.7339\n",
      "Epoch 2847/3000, Training Loss (MSE): 3.3100\n",
      "Epoch 2848/3000, Training Loss (MSE): 2.1099\n",
      "Epoch 2849/3000, Training Loss (MSE): 2.0322\n",
      "Epoch 2850/3000, Training Loss (MSE): 1.0141\n",
      "Epoch 2851/3000, Training Loss (MSE): 2.0616\n",
      "Epoch 2852/3000, Training Loss (MSE): 3.3434\n",
      "Epoch 2853/3000, Training Loss (MSE): 3.3702\n",
      "Epoch 2854/3000, Training Loss (MSE): 1.2708\n",
      "Epoch 2855/3000, Training Loss (MSE): 3.0577\n",
      "Epoch 2856/3000, Training Loss (MSE): 2.3800\n",
      "Epoch 2857/3000, Training Loss (MSE): 3.3729\n",
      "Epoch 2858/3000, Training Loss (MSE): 3.3384\n",
      "Epoch 2859/3000, Training Loss (MSE): 2.6141\n",
      "Epoch 2860/3000, Training Loss (MSE): 3.2626\n",
      "Epoch 2861/3000, Training Loss (MSE): 2.9241\n",
      "Epoch 2862/3000, Training Loss (MSE): 1.5318\n",
      "Epoch 2863/3000, Training Loss (MSE): 3.5024\n",
      "Epoch 2864/3000, Training Loss (MSE): 1.9946\n",
      "Epoch 2865/3000, Training Loss (MSE): 2.2038\n",
      "Epoch 2866/3000, Training Loss (MSE): 1.8204\n",
      "Epoch 2867/3000, Training Loss (MSE): 2.1372\n",
      "Epoch 2868/3000, Training Loss (MSE): 3.3535\n",
      "Epoch 2869/3000, Training Loss (MSE): 2.7903\n",
      "Epoch 2870/3000, Training Loss (MSE): 2.4985\n",
      "Epoch 2871/3000, Training Loss (MSE): 1.8750\n",
      "Epoch 2872/3000, Training Loss (MSE): 2.7860\n",
      "Epoch 2873/3000, Training Loss (MSE): 2.5867\n",
      "Epoch 2874/3000, Training Loss (MSE): 1.7564\n",
      "Epoch 2875/3000, Training Loss (MSE): 1.3022\n",
      "Epoch 2876/3000, Training Loss (MSE): 1.9536\n",
      "Epoch 2877/3000, Training Loss (MSE): 2.0484\n",
      "Epoch 2878/3000, Training Loss (MSE): 2.0349\n",
      "Epoch 2879/3000, Training Loss (MSE): 2.2609\n",
      "Epoch 2880/3000, Training Loss (MSE): 2.3255\n",
      "Epoch 2881/3000, Training Loss (MSE): 2.6335\n",
      "Epoch 2882/3000, Training Loss (MSE): 2.6545\n",
      "Epoch 2883/3000, Training Loss (MSE): 2.3215\n",
      "Epoch 2884/3000, Training Loss (MSE): 2.5318\n",
      "Epoch 2885/3000, Training Loss (MSE): 2.3921\n",
      "Epoch 2886/3000, Training Loss (MSE): 2.1085\n",
      "Epoch 2887/3000, Training Loss (MSE): 2.3080\n",
      "Epoch 2888/3000, Training Loss (MSE): 2.4410\n",
      "Epoch 2889/3000, Training Loss (MSE): 1.8766\n",
      "Epoch 2890/3000, Training Loss (MSE): 2.5550\n",
      "Epoch 2891/3000, Training Loss (MSE): 2.7313\n",
      "Epoch 2892/3000, Training Loss (MSE): 2.4991\n",
      "Epoch 2893/3000, Training Loss (MSE): 2.7584\n",
      "Epoch 2894/3000, Training Loss (MSE): 1.4755\n",
      "Epoch 2895/3000, Training Loss (MSE): 2.1508\n",
      "Epoch 2896/3000, Training Loss (MSE): 1.6941\n",
      "Epoch 2897/3000, Training Loss (MSE): 3.0960\n",
      "Epoch 2898/3000, Training Loss (MSE): 2.7303\n",
      "Epoch 2899/3000, Training Loss (MSE): 2.2296\n",
      "Epoch 2900/3000, Training Loss (MSE): 1.2672\n",
      "Epoch 2901/3000, Training Loss (MSE): 3.0774\n",
      "Epoch 2902/3000, Training Loss (MSE): 2.1140\n",
      "Epoch 2903/3000, Training Loss (MSE): 2.5706\n",
      "Epoch 2904/3000, Training Loss (MSE): 2.5834\n",
      "Epoch 2905/3000, Training Loss (MSE): 1.6110\n",
      "Epoch 2906/3000, Training Loss (MSE): 1.4387\n",
      "Epoch 2907/3000, Training Loss (MSE): 2.7604\n",
      "Epoch 2908/3000, Training Loss (MSE): 2.4775\n",
      "Epoch 2909/3000, Training Loss (MSE): 2.7995\n",
      "Epoch 2910/3000, Training Loss (MSE): 1.9321\n",
      "Epoch 2911/3000, Training Loss (MSE): 2.3062\n",
      "Epoch 2912/3000, Training Loss (MSE): 1.9860\n",
      "Epoch 2913/3000, Training Loss (MSE): 2.3062\n",
      "Epoch 2914/3000, Training Loss (MSE): 2.2574\n",
      "Epoch 2915/3000, Training Loss (MSE): 1.7915\n",
      "Epoch 2916/3000, Training Loss (MSE): 2.4868\n",
      "Epoch 2917/3000, Training Loss (MSE): 2.3176\n",
      "Epoch 2918/3000, Training Loss (MSE): 2.2482\n",
      "Epoch 2919/3000, Training Loss (MSE): 1.5153\n",
      "Epoch 2920/3000, Training Loss (MSE): 2.3472\n",
      "Epoch 2921/3000, Training Loss (MSE): 2.4610\n",
      "Epoch 2922/3000, Training Loss (MSE): 3.1340\n",
      "Epoch 2923/3000, Training Loss (MSE): 3.7342\n",
      "Epoch 2924/3000, Training Loss (MSE): 3.9207\n",
      "Epoch 2925/3000, Training Loss (MSE): 2.3339\n",
      "Epoch 2926/3000, Training Loss (MSE): 3.0182\n",
      "Epoch 2927/3000, Training Loss (MSE): 2.9779\n",
      "Epoch 2928/3000, Training Loss (MSE): 2.3658\n",
      "Epoch 2929/3000, Training Loss (MSE): 1.7744\n",
      "Epoch 2930/3000, Training Loss (MSE): 2.6307\n",
      "Epoch 2931/3000, Training Loss (MSE): 2.1136\n",
      "Epoch 2932/3000, Training Loss (MSE): 2.0023\n",
      "Epoch 2933/3000, Training Loss (MSE): 1.6903\n",
      "Epoch 2934/3000, Training Loss (MSE): 3.6167\n",
      "Epoch 2935/3000, Training Loss (MSE): 2.2956\n",
      "Epoch 2936/3000, Training Loss (MSE): 2.3805\n",
      "Epoch 2937/3000, Training Loss (MSE): 1.7937\n",
      "Epoch 2938/3000, Training Loss (MSE): 2.4502\n",
      "Epoch 2939/3000, Training Loss (MSE): 2.6884\n",
      "Epoch 2940/3000, Training Loss (MSE): 2.1718\n",
      "Epoch 2941/3000, Training Loss (MSE): 2.1536\n",
      "Epoch 2942/3000, Training Loss (MSE): 1.9210\n",
      "Epoch 2943/3000, Training Loss (MSE): 2.2059\n",
      "Epoch 2944/3000, Training Loss (MSE): 2.6677\n",
      "Epoch 2945/3000, Training Loss (MSE): 2.2560\n",
      "Epoch 2946/3000, Training Loss (MSE): 1.8705\n",
      "Epoch 2947/3000, Training Loss (MSE): 2.1320\n",
      "Epoch 2948/3000, Training Loss (MSE): 2.7956\n",
      "Epoch 2949/3000, Training Loss (MSE): 2.1506\n",
      "Epoch 2950/3000, Training Loss (MSE): 2.4479\n",
      "Epoch 2951/3000, Training Loss (MSE): 2.8664\n",
      "Epoch 2952/3000, Training Loss (MSE): 1.8182\n",
      "Epoch 2953/3000, Training Loss (MSE): 1.7401\n",
      "Epoch 2954/3000, Training Loss (MSE): 2.9118\n",
      "Epoch 2955/3000, Training Loss (MSE): 3.4273\n",
      "Epoch 2956/3000, Training Loss (MSE): 2.5406\n",
      "Epoch 2957/3000, Training Loss (MSE): 2.7747\n",
      "Epoch 2958/3000, Training Loss (MSE): 2.8152\n",
      "Epoch 2959/3000, Training Loss (MSE): 3.0169\n",
      "Epoch 2960/3000, Training Loss (MSE): 2.5639\n",
      "Epoch 2961/3000, Training Loss (MSE): 2.1426\n",
      "Epoch 2962/3000, Training Loss (MSE): 2.0723\n",
      "Epoch 2963/3000, Training Loss (MSE): 1.9263\n",
      "Epoch 2964/3000, Training Loss (MSE): 2.7111\n",
      "Epoch 2965/3000, Training Loss (MSE): 3.4746\n",
      "Epoch 2966/3000, Training Loss (MSE): 2.4388\n",
      "Epoch 2967/3000, Training Loss (MSE): 2.8990\n",
      "Epoch 2968/3000, Training Loss (MSE): 2.5990\n",
      "Epoch 2969/3000, Training Loss (MSE): 2.4402\n",
      "Epoch 2970/3000, Training Loss (MSE): 2.3575\n",
      "Epoch 2971/3000, Training Loss (MSE): 2.6690\n",
      "Epoch 2972/3000, Training Loss (MSE): 3.7629\n",
      "Epoch 2973/3000, Training Loss (MSE): 2.1066\n",
      "Epoch 2974/3000, Training Loss (MSE): 2.6543\n",
      "Epoch 2975/3000, Training Loss (MSE): 2.2128\n",
      "Epoch 2976/3000, Training Loss (MSE): 2.4797\n",
      "Epoch 2977/3000, Training Loss (MSE): 2.3930\n",
      "Epoch 2978/3000, Training Loss (MSE): 2.0703\n",
      "Epoch 2979/3000, Training Loss (MSE): 2.8181\n",
      "Epoch 2980/3000, Training Loss (MSE): 1.8008\n",
      "Epoch 2981/3000, Training Loss (MSE): 2.5452\n",
      "Epoch 2982/3000, Training Loss (MSE): 3.1588\n",
      "Epoch 2983/3000, Training Loss (MSE): 3.2046\n",
      "Epoch 2984/3000, Training Loss (MSE): 2.8681\n",
      "Epoch 2985/3000, Training Loss (MSE): 1.9880\n",
      "Epoch 2986/3000, Training Loss (MSE): 1.9234\n",
      "Epoch 2987/3000, Training Loss (MSE): 1.9261\n",
      "Epoch 2988/3000, Training Loss (MSE): 3.2839\n",
      "Epoch 2989/3000, Training Loss (MSE): 1.5085\n",
      "Epoch 2990/3000, Training Loss (MSE): 2.3987\n",
      "Epoch 2991/3000, Training Loss (MSE): 1.9851\n",
      "Epoch 2992/3000, Training Loss (MSE): 2.1304\n",
      "Epoch 2993/3000, Training Loss (MSE): 2.1330\n",
      "Epoch 2994/3000, Training Loss (MSE): 1.7966\n",
      "Epoch 2995/3000, Training Loss (MSE): 2.6371\n",
      "Epoch 2996/3000, Training Loss (MSE): 2.6536\n",
      "Epoch 2997/3000, Training Loss (MSE): 2.7350\n",
      "Epoch 2998/3000, Training Loss (MSE): 1.9283\n",
      "Epoch 2999/3000, Training Loss (MSE): 2.0055\n",
      "Epoch 3000/3000, Training Loss (MSE): 2.9814\n"
     ]
    }
   ],
   "source": [
    "# float32 is default for\n",
    "# x_train = torch.load(\"data/sim_data/x_train_lines_discretised.pt\").float().requires_grad_()\n",
    "x_train = inputs_03\n",
    "y_train = simulate_convergence(x_train).float()\n",
    "\n",
    "print(f\"The shape of the training data is {y_train.shape}.\")\n",
    "print(f\"The dtype of the input data is {x_train.dtype}.\")\n",
    "\n",
    "# Convert to DataLoader for batching\n",
    "dataset = TensorDataset(x_train, y_train)\n",
    "dataloader = DataLoader(dataset, batch_size = 32, shuffle = True)\n",
    "\n",
    "# Initialise model\n",
    "model_A = MLP_compute_A()\n",
    "model_A.train()\n",
    "\n",
    "# Define loss function (e.g., MSE for regression)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define optimizer (e.g., Adam)\n",
    "optimizer = optim.AdamW(model_A.parameters(), lr = 0.0001, weight_decay = 1e-4)\n",
    "num_epochs = 3000\n",
    "\n",
    "# Initialise tensor to store losses\n",
    "epoch_losses = torch.zeros(num_epochs)\n",
    "\n",
    "print()\n",
    "print(\"Start Training\")\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    epoch_loss = 0.0  # Accumulate batch losses\n",
    "\n",
    "    for batch in dataloader:\n",
    "        x_batch, y_batch = batch\n",
    "        x_batch.requires_grad_()\n",
    "        \n",
    "        ### VMAP JACREF/JACFWRD Workflow ###\n",
    "        # 1 min on CPU\n",
    "        # torch.Size([N, 2, 2, 2, 2])\n",
    "        jacobian_A_vmap_jacrev = vmap(jacrev(model_A))(x_batch)\n",
    "        # assert that the second dim (size 2) is redundant (\"r)\")\n",
    "        # assert(jacobian_A_vmap_jacrev[:, 0, :, :, :] == jacobian_A_vmap_jacrev[:, 1, :, :, :]).all()\n",
    "        jacobian_A_vmap_jacrev_lean = torch.einsum('n r a b c -> n a b c', jacobian_A_vmap_jacrev)\n",
    "        y_pred = jacobian_A_vmap_jacrev_lean.diagonal(dim1 = -2, dim2 = -1).sum(dim = 1)\n",
    "\n",
    "        ### AUTOGRAD Workflow ###\n",
    "        # 7 minutes on CPU\n",
    "        # torch.Size([N, 2, 2, N, 2])\n",
    "        # jacobian_A_autograd = torch.autograd.functional.jacobian(model_A, x_batch)\n",
    "        # select pairs of n_i and n_i (the rest is empty)\n",
    "        # jacobian_A_autograd_lean = torch.einsum('nabnc -> nabc', jacobian_A_autograd)\n",
    "        # The last two dims, and then we sum\n",
    "        # y_pred = torch.diagonal(jacobian_A_autograd_lean, dim1 = -2, dim2 = -1).sum(dim = 1)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Store the average loss for the epoch\n",
    "    epoch_losses[epoch] = epoch_loss / len(dataloader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss (MSE): {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert to NumPy for plotting\n",
    "plt.figure(figsize = (6, 4))\n",
    "plt.plot(epoch_losses.numpy(), label = \"Training Loss\", color = \"b\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss (MSE)\")\n",
    "plt.title(\"Training Loss Over Epochs\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try this with another Jacobian function\n",
    "\n",
    "torch.autograd.functional.jacobian()\n",
    "Need to \"turn on\" requires_grad for x because we are taking gradients with respect to it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Eval mode\n",
    "model_A.eval()\n",
    "\n",
    "x = inputs_03\n",
    "\n",
    "### AUTOGRAD Workflow ###\n",
    "# jacobian_A_autograd = torch.autograd.functional.jacobian(model_A, x)\n",
    "# jacobian_A_autograd_lean = torch.einsum('nabnc -> nabc', jacobian_A_autograd)\n",
    "# y_pred = torch.diagonal(jacobian_A_autograd_lean , dim1 = -2, dim2 = -1).sum(dim = 1).detach()\n",
    "\n",
    "### VMAP JACREF/JACFWRD Workflow ###\n",
    "jacobian_A_vmap_jacrev = vmap(jacrev(model_A))(x)\n",
    "# assert that the second dim (size 2) is redundant (\"r)\")\n",
    "assert(jacobian_A_vmap_jacrev[:, 0, :, :, :] == jacobian_A_vmap_jacrev[:, 1, :, :, :]).all()\n",
    "jacobian_A_vmap_jacrev_lean = torch.einsum('n r a b c -> n a b c', jacobian_A_vmap_jacrev)\n",
    "y_pred = jacobian_A_vmap_jacrev_lean.diagonal(dim1 = -2, dim2 = -1).sum(dim = 1)\n",
    "\n",
    "# div_jacrav = div_functional(x)\n",
    "div_jacrav = div_discrete(y_pred.detach())\n",
    "div_jacrav = torch.zeros_like(div_jacrav)\n",
    "\n",
    "visualise_v_quiver(y_pred.detach(), div_jacrav, x.detach(), title_string = \"Prediction over field\", color_abs_max = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_v_quiver(simulate_convergence(inputs_03), div_functional(inputs_03), inputs_03, title_string = \"Prediction over field\", color_abs_max = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.detach().mean(dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulate_convergence(inputs_03).detach().mean(dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.mean(dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_v_quiver(y_pred.detach(), div_functional(inputs_03), inputs_03, title_string = \"Prediction over field\", color_abs_max = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jacobian w.r.t. v throughs issues if x is a variable in compute_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jacobian headache\n",
    "\n",
    "- Jacobian is probably among the most expensive functions\n",
    "- [torch.func.jacrev](https://pytorch.org/docs/stable/generated/torch.func.jacrev.html#torch.func.jacrev)\n",
    "    - The implementation goes forward\n",
    "    - torch.func.jacobian chooses based on efficiency\n",
    "- batched Jacobians via vmap\n",
    "- consider # _func_sum\n",
    "- torch.autograd.functional.jacobian(f, x)\n",
    "    - not as fast as func\n",
    "- Shape torch.Size([16, 2, 2, 2])\n",
    "    - for every i in N we have 2 (two) 2 x 2 matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(5, 2, 2)\n",
    "jacobian_vmap = vmap(jacrev(torch.sin))(x)\n",
    "print(jacobian_vmap.shape)\n",
    "\n",
    "jacobian = jacrev(torch.sin)(x)\n",
    "print(jacobian.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Issue with how we construct U \n",
    "# Jacobian: torch.Size([16, 16, 2, 2, 2])\n",
    "# without vmap it was also torch.Size([16, 2, 2, 16, 2])\n",
    "jacobian = vmap(jacfwd(compute_A))(inputs)\n",
    "print(jacobian.shape)\n",
    "\n",
    "# Remove redundant dim\n",
    "# (jacobian[:, 0, : , :, :] == jacobian[:, 15, : , :, :]).any()\n",
    "assert(jacobian[:, 0, : , :, :] == jacobian[:, -1, : , :, :]).any()\n",
    "#jacobian_sq = jacobian[:, 0, : , :, :]\n",
    "#jacobian_sq.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToDo\n",
    "\n",
    "Use apply function to pass arguments into function\n",
    "jacobian_A_vmap_jacrev = vmap(lambda x: jacrev(model_A)(x, param))(x_batch)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
