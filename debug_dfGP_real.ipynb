{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66bf128c",
   "metadata": {},
   "source": [
    "# Start with dfGP for real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69774e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "\n",
      "Training for REGION_LOWER_BYRD...\n",
      "Mean noise std per x dimension: 0.0132\n",
      "Mean noise std per y dimension: 0.0169\n",
      "\n",
      "--- Training Run 1/1 ---\n",
      "\n",
      "Start Training\n",
      "region_lower_byrd dfGP Run 1/1, Epoch 1/1, Training Loss (NLML): 9.3248, Training RMSE: 0.2575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kim/anaconda3/envs/gpytorch-env/lib/python3.10/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 52.37175750732422 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/home/kim/anaconda3/envs/gpytorch-env/lib/python3.10/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-06 to the diagonal\n",
      "  warnings.warn(\n",
      "/home/kim/anaconda3/envs/gpytorch-env/lib/python3.10/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-05 to the diagonal\n",
      "  warnings.warn(\n",
      "/home/kim/anaconda3/envs/gpytorch-env/lib/python3.10/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-04 to the diagonal\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NotPSDError",
     "evalue": "Matrix not positive definite after repeatedly adding jitter up to 1.0e-04.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotPSDError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 336\u001b[0m\n\u001b[1;32m    332\u001b[0m test_RMSE \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqrt(gpytorch\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mmean_squared_error(\n\u001b[1;32m    333\u001b[0m     pred_dist_test, y_test\u001b[38;5;241m.\u001b[39mto(device))\u001b[38;5;241m.\u001b[39mmean())\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    334\u001b[0m test_MAE \u001b[38;5;241m=\u001b[39m gpytorch\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mmean_absolute_error(\n\u001b[1;32m    335\u001b[0m     pred_dist_test, y_test\u001b[38;5;241m.\u001b[39mto(device))\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m--> 336\u001b[0m test_NLL \u001b[38;5;241m=\u001b[39m \u001b[43mgpytorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnegative_log_predictive_density\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpred_dist_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    338\u001b[0m test_QCE \u001b[38;5;241m=\u001b[39m quantile_coverage_error_2d(\n\u001b[1;32m    339\u001b[0m     pred_dist_test, y_test\u001b[38;5;241m.\u001b[39mto(device), quantile \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m95.0\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    340\u001b[0m \u001b[38;5;66;03m## NOTE: It is important to use the absolute value of the divergence field, since both positive and negative deviations are violations and shouldn't cancel each other out \u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/gpytorch-env/lib/python3.10/site-packages/gpytorch/metrics/metrics.py:57\u001b[0m, in \u001b[0;36mnegative_log_predictive_density\u001b[0;34m(pred_dist, test_y)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Negative log predictive density.\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \n\u001b[1;32m     54\u001b[0m \u001b[38;5;124;03mComputes the negative predictive log density normalized by the size of the test data.\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     56\u001b[0m combine_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pred_dist, MultitaskMultivariateNormal) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[43mpred_dist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_y\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m test_y\u001b[38;5;241m.\u001b[39mshape[combine_dim]\n",
      "File \u001b[0;32m~/anaconda3/envs/gpytorch-env/lib/python3.10/site-packages/gpytorch/distributions/multitask_multivariate_normal.py:218\u001b[0m, in \u001b[0;36mMultitaskMultivariateNormal.log_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    216\u001b[0m     new_shape \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m+\u001b[39m value\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    217\u001b[0m     value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mview(new_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m--> 218\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/gpytorch-env/lib/python3.10/site-packages/gpytorch/distributions/multivariate_normal.py:250\u001b[0m, in \u001b[0;36mMultivariateNormal.log_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# Get log determininant and first part of quadratic form\u001b[39;00m\n\u001b[1;32m    249\u001b[0m covar \u001b[38;5;241m=\u001b[39m covar\u001b[38;5;241m.\u001b[39mevaluate_kernel()\n\u001b[0;32m--> 250\u001b[0m inv_quad, logdet \u001b[38;5;241m=\u001b[39m \u001b[43mcovar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv_quad_logdet\u001b[49m\u001b[43m(\u001b[49m\u001b[43minv_quad_rhs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiff\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogdet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28msum\u001b[39m([inv_quad, logdet, diff\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mpi)])\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/anaconda3/envs/gpytorch-env/lib/python3.10/site-packages/linear_operator/operators/_linear_operator.py:1709\u001b[0m, in \u001b[0;36mLinearOperator.inv_quad_logdet\u001b[0;34m(self, inv_quad_rhs, logdet, reduce_inv_quad)\u001b[0m\n\u001b[1;32m   1707\u001b[0m             will_need_cholesky \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1708\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m will_need_cholesky:\n\u001b[0;32m-> 1709\u001b[0m         cholesky \u001b[38;5;241m=\u001b[39m CholLinearOperator(TriangularLinearOperator(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m   1710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cholesky\u001b[38;5;241m.\u001b[39minv_quad_logdet(\n\u001b[1;32m   1711\u001b[0m         inv_quad_rhs\u001b[38;5;241m=\u001b[39minv_quad_rhs,\n\u001b[1;32m   1712\u001b[0m         logdet\u001b[38;5;241m=\u001b[39mlogdet,\n\u001b[1;32m   1713\u001b[0m         reduce_inv_quad\u001b[38;5;241m=\u001b[39mreduce_inv_quad,\n\u001b[1;32m   1714\u001b[0m     )\n\u001b[1;32m   1716\u001b[0m \u001b[38;5;66;03m# Short circuit to inv_quad function if we're not computing logdet\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/gpytorch-env/lib/python3.10/site-packages/linear_operator/operators/_linear_operator.py:1311\u001b[0m, in \u001b[0;36mLinearOperator.cholesky\u001b[0;34m(self, upper)\u001b[0m\n\u001b[1;32m   1301\u001b[0m \u001b[38;5;129m@_implements\u001b[39m(torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mcholesky)\n\u001b[1;32m   1302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcholesky\u001b[39m(\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;28mself\u001b[39m: Float[LinearOperator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch N N\u001b[39m\u001b[38;5;124m\"\u001b[39m], upper: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1304\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Float[LinearOperator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch N N\u001b[39m\u001b[38;5;124m\"\u001b[39m]:  \u001b[38;5;66;03m# returns TriangularLinearOperator\u001b[39;00m\n\u001b[1;32m   1305\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1306\u001b[0m \u001b[38;5;124;03m    Cholesky-factorizes the LinearOperator.\u001b[39;00m\n\u001b[1;32m   1307\u001b[0m \n\u001b[1;32m   1308\u001b[0m \u001b[38;5;124;03m    :param upper: Upper triangular or lower triangular factor (default: False).\u001b[39;00m\n\u001b[1;32m   1309\u001b[0m \u001b[38;5;124;03m    :return: Cholesky factor (lower or upper triangular)\u001b[39;00m\n\u001b[1;32m   1310\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1311\u001b[0m     chol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1312\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m upper:\n\u001b[1;32m   1313\u001b[0m         chol \u001b[38;5;241m=\u001b[39m chol\u001b[38;5;241m.\u001b[39m_transpose_nonbatch()\n",
      "File \u001b[0;32m~/anaconda3/envs/gpytorch-env/lib/python3.10/site-packages/linear_operator/utils/memoize.py:59\u001b[0m, in \u001b[0;36m_cached.<locals>.g\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m kwargs_pkl \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mdumps(kwargs)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_in_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl):\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _add_to_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_from_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n",
      "File \u001b[0;32m~/anaconda3/envs/gpytorch-env/lib/python3.10/site-packages/linear_operator/operators/_linear_operator.py:528\u001b[0m, in \u001b[0;36mLinearOperator._cholesky\u001b[0;34m(self, upper)\u001b[0m\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TriangularLinearOperator(evaluated_mat\u001b[38;5;241m.\u001b[39mclamp_min(\u001b[38;5;241m0.0\u001b[39m)\u001b[38;5;241m.\u001b[39msqrt())\n\u001b[1;32m    527\u001b[0m \u001b[38;5;66;03m# contiguous call is necessary here\u001b[39;00m\n\u001b[0;32m--> 528\u001b[0m cholesky \u001b[38;5;241m=\u001b[39m \u001b[43mpsd_safe_cholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluated_mat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupper\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    529\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m TriangularLinearOperator(cholesky, upper\u001b[38;5;241m=\u001b[39mupper)\n",
      "File \u001b[0;32m~/anaconda3/envs/gpytorch-env/lib/python3.10/site-packages/linear_operator/utils/cholesky.py:65\u001b[0m, in \u001b[0;36mpsd_safe_cholesky\u001b[0;34m(A, upper, out, jitter, max_tries)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpsd_safe_cholesky\u001b[39m(A, upper\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, jitter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, max_tries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     51\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the Cholesky decomposition of A. If A is only p.s.d, add a small jitter to the diagonal.\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03m        :attr:`A` (Tensor):\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m            Number of attempts (with successively increasing jitter) to make before raising an error.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m     L \u001b[38;5;241m=\u001b[39m \u001b[43m_psd_safe_cholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjitter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjitter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tries\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m upper:\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/gpytorch-env/lib/python3.10/site-packages/linear_operator/utils/cholesky.py:47\u001b[0m, in \u001b[0;36m_psd_safe_cholesky\u001b[0;34m(A, out, jitter, max_tries)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39many(info):\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m L\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m NotPSDError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMatrix not positive definite after repeatedly adding jitter up to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjitter_new\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNotPSDError\u001b[0m: Matrix not positive definite after repeatedly adding jitter up to 1.0e-04."
     ]
    }
   ],
   "source": [
    "# REAL DATA EXPERIMENTS\n",
    "# RUN WITH python run_real_experiments_dfGP.py\n",
    "#               _                 _   _      \n",
    "#              | |               | | (_)     \n",
    "#    __ _ _ __ | |_ __ _ _ __ ___| |_ _  ___ \n",
    "#   / _` | '_ \\| __/ _` | '__/ __| __| |/ __|\n",
    "#  | (_| | | | | || (_| | | | (__| |_| | (__ \n",
    "#   \\__,_|_| |_|\\__\\__,_|_|  \\___|\\__|_|\\___|\n",
    "# \n",
    "model_name = \"dfGP\"\n",
    "from gpytorch_models import dfGP\n",
    "\n",
    "# import configs to we can access the hypers with getattr\n",
    "import configs\n",
    "from configs import PATIENCE, MAX_NUM_EPOCHS, NUM_RUNS, WEIGHT_DECAY\n",
    "from configs import TRACK_EMISSIONS_BOOL\n",
    "from configs import REAL_L_RANGE, REAL_OUTPUTSCALE_VAR_RANGE, REAL_NOISE_VAR_RANGE\n",
    "from configs import SCALE_INPUT_region_lower_byrd, SCALE_INPUT_region_mid_byrd, SCALE_INPUT_region_upper_byrd\n",
    "from configs import REAL_L_RANGE, REAL_NOISE_VAR_RANGE, REAL_OUTPUTSCALE_VAR_RANGE\n",
    "\n",
    "SCALE_INPUT = {\n",
    "    \"region_lower_byrd\": SCALE_INPUT_region_lower_byrd,\n",
    "    \"region_mid_byrd\": SCALE_INPUT_region_mid_byrd,\n",
    "    \"region_upper_byrd\": SCALE_INPUT_region_upper_byrd,\n",
    "}\n",
    "\n",
    "# Reiterating import for visibility\n",
    "MAX_NUM_EPOCHS = 1\n",
    "NUM_RUNS = 1\n",
    "WEIGHT_DECAY = WEIGHT_DECAY\n",
    "PATIENCE = PATIENCE\n",
    "\n",
    "# assign model-specific variable\n",
    "MODEL_LEARNING_RATE = getattr(configs, f\"{model_name}_REAL_LEARNING_RATE\")\n",
    "MODEL_REAL_RESULTS_DIR = getattr(configs, f\"{model_name}_REAL_RESULTS_DIR\")\n",
    "import os\n",
    "os.makedirs(MODEL_REAL_RESULTS_DIR, exist_ok = True)\n",
    "\n",
    "# basics\n",
    "import pandas as pd\n",
    "import torch\n",
    "import gpytorch\n",
    "\n",
    "# universals \n",
    "from metrics import compute_divergence_field, quantile_coverage_error_2d\n",
    "from utils import set_seed, make_grid\n",
    "import gc\n",
    "import warnings\n",
    "set_seed(42)\n",
    "\n",
    "# setting device to GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# overwrite if needed: # device = 'cpu'\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "### START TIMING ###\n",
    "import time\n",
    "start_time = time.time()  # Start timing after imports\n",
    "\n",
    "### START TRACKING EXPERIMENT EMISSIONS ###\n",
    "if TRACK_EMISSIONS_BOOL:\n",
    "    from codecarbon import EmissionsTracker\n",
    "    tracker = EmissionsTracker(project_name = \"dfGP_real_experiments\", output_dir = MODEL_REAL_RESULTS_DIR)\n",
    "    tracker.start()\n",
    "\n",
    "#############################\n",
    "### LOOP 1 - over REGIONS ###\n",
    "#############################\n",
    "\n",
    "for region_name in [\"region_lower_byrd\", \"region_mid_byrd\", \"region_upper_byrd\"]:\n",
    "\n",
    "    SCALE_DOMAIN = SCALE_INPUT[region_name]\n",
    "\n",
    "    print(f\"\\nTraining for {region_name.upper()}...\")\n",
    "\n",
    "    # Store metrics for the current region (used for *metrics_summary* report and *metrics_per_run*)\n",
    "    region_results = []\n",
    "\n",
    "    ##########################################\n",
    "    ### x_train & y_train, x_test & x_test ###\n",
    "    ##########################################\n",
    "\n",
    "    # define paths based on region_name\n",
    "    path_to_training_tensor = \"data/real_data/\" + region_name + \"_train_tensor.pt\"\n",
    "    path_to_test_tensor = \"data/real_data/\" + region_name + \"_test_tensor.pt\"\n",
    "\n",
    "    # load and tranpose to have rows as points\n",
    "    train = torch.load(path_to_training_tensor, weights_only = False).T \n",
    "    test = torch.load(path_to_test_tensor, weights_only = False).T\n",
    "\n",
    "    # The train and test tensors have the following columns:\n",
    "    # [:, 0] = x\n",
    "    # [:, 1] = y\n",
    "    # [:, 2] = surface elevation (s)\n",
    "    # [:, 3] = ice flux in x direction (u)\n",
    "    # [:, 4] = ice flux in y direction (v)\n",
    "    # [:, 5] = ice velocity error in x direction (u_err)\n",
    "    # [:, 6] = ice velocity error in y direction (v_err)\n",
    "    # [:, 7] = ice velocity in x direction (u)\n",
    "    # [:, 8] = ice velocity in y direction (v)\n",
    "    # [:, 9] = thickness\n",
    "    # [:, 10] = source age\n",
    "    # [:, 11] = sqrt flux scale (used for scaling the fluxes)\n",
    "\n",
    "    # train\n",
    "    x_train = train[:, [0, 1]].to(device)\n",
    "    y_train = train[:, [3, 4]].to(device)\n",
    "\n",
    "    # test\n",
    "    x_test = test[:, [0, 1]].to(device)\n",
    "    y_test = test[:, [3, 4]].to(device)\n",
    "\n",
    "    # HACK: Coordinate scaling helps with numerical stability\n",
    "    # Units are now in km \n",
    "    x_test = x_test * SCALE_DOMAIN\n",
    "    x_train = x_train * SCALE_DOMAIN\n",
    "\n",
    "    ### NOISE MODEL ###\n",
    "    # TRAIN\n",
    "    # noise variance (h * sigma_u)^2 and (h * sigma_v)^2 (tensor contains [h sig_u, h sig_v] stds)\n",
    "    noise_var_h_times_uv_train = torch.concat((train[:, 5], train[:, 6]), dim = 0)**2\n",
    "    # assume age dependent noise sigma_h on ice thickness measurements: ~10 - 20 m std (1000 scaling)\n",
    "    sigma_h = 0.01 * torch.log(train[:, 7] + 3)\n",
    "    # calculate noise variance (u * sigma_h)^2 and (v * sigma_h)^2\n",
    "    noise_var_uv_times_h_train = (torch.concat((train[:, 3], train[:, 4]), dim = 0) * torch.cat([sigma_h, sigma_h]))**2\n",
    "    # combine both noise variances into the std for each dimension\n",
    "    train_noise_diag = torch.sqrt(noise_var_h_times_uv_train + noise_var_uv_times_h_train).to(device)\n",
    "\n",
    "    # Compute midpoint\n",
    "    midpoint = train_noise_diag.shape[0] // 2\n",
    "\n",
    "    # Print noise levels for train, formatted to 4 decimal places\n",
    "    print(f\"Mean noise std per x dimension: {train_noise_diag[:midpoint].mean(dim = 0).item():.4f}\")\n",
    "    print(f\"Mean noise std per y dimension: {train_noise_diag[midpoint:].mean(dim = 0).item():.4f}\")\n",
    "\n",
    "    ##################################\n",
    "    ### LOOP 2 - over training run ###\n",
    "    ##################################\n",
    "\n",
    "    # NOTE: GPs don't train on batches, use full data\n",
    "\n",
    "    for run in range(NUM_RUNS):\n",
    "\n",
    "        print(f\"\\n--- Training Run {run + 1}/{NUM_RUNS} ---\")\n",
    "\n",
    "        # Initialise the likelihood for the GP model (estimates noise)\n",
    "        # NOTE: we use a multitask likelihood for the dfGP model but with a global noise term\n",
    "        likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(\n",
    "            num_tasks = 2,\n",
    "            has_global_noise = True, \n",
    "            has_task_noise = False, # HACK: This still needs to be manually turned off\n",
    "            ).to(device)\n",
    "\n",
    "        model = dfGP(\n",
    "            x_train,\n",
    "            y_train, \n",
    "            likelihood\n",
    "            ).to(device)\n",
    "        \n",
    "        model.base_kernel.lengthscale = torch.tensor([[5.0, 8.0]]).to(device)\n",
    "        \n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr = MODEL_LEARNING_RATE, weight_decay = WEIGHT_DECAY)\n",
    "        \n",
    "        # Use ExactMarginalLogLikelihood\n",
    "        mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "        model.train()\n",
    "        likelihood.train()\n",
    "        # _________________\n",
    "        # BEFORE EPOCH LOOP\n",
    "        \n",
    "        # Export the convergence just for first run only\n",
    "        if run == 0:\n",
    "            # initialise tensors to store losses over epochs (for convergence plot)\n",
    "            train_losses_NLML_over_epochs = torch.zeros(MAX_NUM_EPOCHS) # objective\n",
    "            train_losses_RMSE_over_epochs = torch.zeros(MAX_NUM_EPOCHS) # by-product\n",
    "            # monitor performance transfer to test (only RMSE easy to calc without covar)\n",
    "            test_losses_RMSE_over_epochs = torch.zeros(MAX_NUM_EPOCHS)\n",
    "\n",
    "            # NOTE: Here, we estimate the noise\n",
    "            l1_over_epochs = torch.zeros(MAX_NUM_EPOCHS)\n",
    "            l2_over_epochs = torch.zeros(MAX_NUM_EPOCHS)\n",
    "            outputscale_var_over_epochs = torch.zeros(MAX_NUM_EPOCHS)\n",
    "            noise_var_over_epochs = torch.zeros(MAX_NUM_EPOCHS)\n",
    "\n",
    "        # Early stopping variables\n",
    "        best_loss = float('inf')\n",
    "        # counter starts at 0\n",
    "        epochs_no_improve = 0\n",
    "\n",
    "        ############################\n",
    "        ### LOOP 3 - over EPOCHS ###\n",
    "        ############################\n",
    "        print(\"\\nStart Training\")\n",
    "\n",
    "        for epoch in range(MAX_NUM_EPOCHS):\n",
    "\n",
    "            # Set to train\n",
    "            model.train()\n",
    "            likelihood.train()\n",
    "\n",
    "            # Do a step\n",
    "            optimizer.zero_grad()\n",
    "            # model outputs a multivariate normal distribution\n",
    "            train_pred_dist = model(x_train.to(device))\n",
    "            # Train on noisy or targets\n",
    "            # NOTE: We only have observational y_train i.e. noisy data\n",
    "            loss = - mll(train_pred_dist, y_train.to(device))  # negative marginal log likelihood\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # For Run 1 we save a bunch of metrics and update, while for the rest we only update\n",
    "            if run == 0:\n",
    "\n",
    "                model.eval()\n",
    "                likelihood.eval()\n",
    "\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\", gpytorch.utils.warnings.GPInputWarning)\n",
    "                    train_pred_dist = model(x_train.to(device))\n",
    "                test_pred_dist = model(x_test.to(device))\n",
    "\n",
    "                # Compute RMSE for training and test predictions (given true data, not noisy)\n",
    "                train_RMSE = torch.sqrt(gpytorch.metrics.mean_squared_error(train_pred_dist, y_train.to(device)).mean())\n",
    "                test_RMSE = torch.sqrt(gpytorch.metrics.mean_squared_error(test_pred_dist, y_test.to(device)).mean())\n",
    "\n",
    "                # Save losses for convergence plot\n",
    "                train_losses_NLML_over_epochs[epoch] = loss.item()\n",
    "                train_losses_RMSE_over_epochs[epoch] = train_RMSE.item()\n",
    "                test_losses_RMSE_over_epochs[epoch] = test_RMSE.item()\n",
    "\n",
    "                # Save evolution of hypers for convergence plot\n",
    "                # NOTE: lengthscale is [1, 2] in shape\n",
    "                l1_over_epochs[epoch] = model.base_kernel.lengthscale[:, 0].item()\n",
    "                l2_over_epochs[epoch] = model.base_kernel.lengthscale[:, 1].item()\n",
    "                outputscale_var_over_epochs[epoch] = model.covar_module.outputscale.item()\n",
    "                noise_var_over_epochs[epoch] = model.likelihood.noise.item()\n",
    "\n",
    "                # Print a bit more information for the first run\n",
    "                if epoch % 20 == 0:\n",
    "                    print(f\"{region_name} {model_name} Run {run + 1}/{NUM_RUNS}, Epoch {epoch + 1}/{MAX_NUM_EPOCHS}, Training Loss (NLML): {loss:.4f}, Training RMSE: {train_RMSE:.4f}\")\n",
    "\n",
    "                # delete after printing and saving\n",
    "                # NOTE: keep loss for early stopping check\n",
    "                del train_pred_dist, test_pred_dist, train_RMSE, test_RMSE\n",
    "                \n",
    "                # Free up memory every 20 epochs\n",
    "                if epoch % 20 == 0:\n",
    "                    gc.collect() and torch.cuda.empty_cache()\n",
    "            \n",
    "            # For all runs after the first we run a minimal version using only lml_train\n",
    "            else:\n",
    "\n",
    "                if epoch % 20 == 0:\n",
    "                    # After run 1 we only print lml, nothing else\n",
    "                    print(f\"{region_name} {model_name} Run {run + 1}/{NUM_RUNS}, Epoch {epoch + 1}/{MAX_NUM_EPOCHS}, Training Loss (NLML): {loss:.4f}\")\n",
    "                \n",
    "            # EVERY EPOCH: Early stopping check\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                # reset counter if loss improves\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "\n",
    "            if epochs_no_improve >= PATIENCE:\n",
    "                print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
    "                # exit epoch loop\n",
    "                break\n",
    "\n",
    "        ##############################\n",
    "        ### END LOOP 3 over EPOCHS ###\n",
    "        ##############################\n",
    "\n",
    "        # for every run...\n",
    "        #######################################################\n",
    "        ### EVALUATE after all training for RUN is finished ###\n",
    "        #######################################################\n",
    "\n",
    "        model.eval()\n",
    "        likelihood.eval()\n",
    "\n",
    "        # Need gradients for autograd divergence: We clone and detach\n",
    "        x_test_grad = x_test.to(device).clone().requires_grad_(True)\n",
    "        x_train_grad = x_train.to(device).clone().requires_grad_(True)\n",
    "\n",
    "        # Underlying (latent) distribution and predictive distribution\n",
    "        dist_test = model(x_test_grad)\n",
    "        pred_dist_test = likelihood(dist_test)\n",
    "\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", gpytorch.utils.warnings.GPInputWarning)\n",
    "            dist_train = model(x_train_grad)\n",
    "            pred_dist_train = likelihood(dist_train)\n",
    "        \n",
    "        # Compute divergence field (from latent distribution)\n",
    "        test_div_field = compute_divergence_field(dist_test.mean, x_test_grad)\n",
    "        train_div_field = compute_divergence_field(dist_train.mean, x_train_grad)\n",
    "\n",
    "        # Only save mean_pred, covar_pred and divergence fields for the first run\n",
    "        if run == 0:\n",
    "\n",
    "            # (3) Since all epoch training is finished, we can save the losses over epochs\n",
    "            df_losses = pd.DataFrame({\n",
    "                'Epoch': list(range(train_losses_NLML_over_epochs.shape[0])), # pythonic indexing\n",
    "                'Train NLML': train_losses_NLML_over_epochs.tolist(),\n",
    "                'Train RMSE': train_losses_RMSE_over_epochs.tolist(),\n",
    "                'Test RMSE': test_losses_RMSE_over_epochs.tolist(),\n",
    "                # hyperparameters\n",
    "                'l1': l1_over_epochs.tolist(),\n",
    "                'l2': l2_over_epochs.tolist(),\n",
    "                'outputscale_var': outputscale_var_over_epochs.tolist(),\n",
    "                'noise_var': noise_var_over_epochs.tolist(),\n",
    "                })\n",
    "    \n",
    "\n",
    "        # Compute TRAIN metrics (convert tensors to float) for every run's tuned model\n",
    "        # NOTE: gpytorch outputs metrics per task\n",
    "        train_RMSE = torch.sqrt(gpytorch.metrics.mean_squared_error(\n",
    "            pred_dist_train, y_train.to(device)).mean()).item()\n",
    "        train_MAE = gpytorch.metrics.mean_absolute_error(\n",
    "            pred_dist_train, y_train.to(device)).mean().item()\n",
    "        train_NLL = gpytorch.metrics.negative_log_predictive_density(\n",
    "            pred_dist_train, y_train.to(device)).item()\n",
    "        train_QCE = quantile_coverage_error_2d(\n",
    "            pred_dist_train, y_train.to(device), quantile = 95.0).item()\n",
    "        ## NOTE: It is important to use the absolute value of the divergence field, since both positive and negative deviations are violations and shouldn't cancel each other out \n",
    "        train_MAD = train_div_field.abs().mean().item()\n",
    "\n",
    "        # Compute TEST metrics (convert tensors to float) for every run's tuned model\n",
    "        test_RMSE = torch.sqrt(gpytorch.metrics.mean_squared_error(\n",
    "            pred_dist_test, y_test.to(device)).mean()).item()\n",
    "        test_MAE = gpytorch.metrics.mean_absolute_error(\n",
    "            pred_dist_test, y_test.to(device)).mean().item()\n",
    "        test_NLL = gpytorch.metrics.negative_log_predictive_density(\n",
    "            pred_dist_test, y_test.to(device)).item()\n",
    "        test_QCE = quantile_coverage_error_2d(\n",
    "            pred_dist_test, y_test.to(device), quantile = 95.0).item()\n",
    "        ## NOTE: It is important to use the absolute value of the divergence field, since both positive and negative deviations are violations and shouldn't cancel each other out \n",
    "        test_MAD = test_div_field.abs().mean().item()\n",
    "\n",
    "        region_results.append([\n",
    "            run + 1,\n",
    "            train_RMSE, train_MAE, train_NLL, train_QCE, train_MAD,\n",
    "            test_RMSE, test_MAE, test_NLL, test_QCE, test_MAD\n",
    "        ])\n",
    "\n",
    "        # clean up\n",
    "        del dist_train, dist_test, pred_dist_train, pred_dist_test, test_div_field, train_div_field\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    ############################\n",
    "    ### END LOOP 2 over RUNS ###\n",
    "    ############################\n",
    "\n",
    "    print(f\"Test RMSE: {test_RMSE:.4f}, Test NLL: {test_NLL:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98fa8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # The train and test tensors have the following columns:\n",
    "    # [:, 0] = x\n",
    "    # [:, 1] = y\n",
    "    # [:, 2] = surface elevation (s)\n",
    "    # [:, 3] = ice flux in x direction (u)\n",
    "    # [:, 4] = ice flux in y direction (v)\n",
    "    # [:, 5] = ice velocity error in x direction (u_err)\n",
    "    # [:, 6] = ice velocity error in y direction (v_err)\n",
    "    # [:, 7] = ice velocity in x direction (u)\n",
    "    # [:, 8] = ice velocity in y direction (v)\n",
    "    # [:, 9] = thickness\n",
    "    # [:, 10] = source age\n",
    "    # [:, 11] = sqrt flux scale (used for scaling the fluxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3e3a65e",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (1072) must match the size of tensor b (536) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 8\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# factor = 7.5 / train[:, 11]\u001b[39;00m\n\u001b[1;32m      3\u001b[0m sigma_t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\n\u001b[1;32m      4\u001b[0m     factor \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog(train[:, \u001b[38;5;241m10\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m3\u001b[39m),\n\u001b[1;32m      5\u001b[0m     factor \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog(train[:, \u001b[38;5;241m10\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      6\u001b[0m ], dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \u001b[43msigma_t\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m11\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (1072) must match the size of tensor b (536) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "factor = 7.5 / train[:, 11]\n",
    "# factor = 7.5 / train[:, 11]\n",
    "sigma_t = torch.cat([\n",
    "    factor * torch.log(train[:, 10] + 3),\n",
    "    factor * torch.log(train[:, 10] + 3)\n",
    "], dim = 0)\n",
    "\n",
    "sigma_t * train[:, 11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d66ae198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0606, 0.0606, 0.0430, 0.0430, 0.0430, 0.0430, 0.0430, 0.0430, 0.0430,\n",
       "        0.0430, 0.0430, 0.0430, 0.0430, 0.0430, 0.0430, 0.0430, 0.0430, 0.0430,\n",
       "        0.0430, 0.0430, 0.0430, 0.0430, 0.0430, 0.0430, 0.0430, 0.0430, 0.0430,\n",
       "        0.0430, 0.0430, 0.0430, 0.0430, 0.0430, 0.0430, 0.0430, 0.0430, 0.0430,\n",
       "        0.0430, 0.0430, 0.0430, 0.0430, 0.0430, 0.0430, 0.0430, 0.0430, 0.0430,\n",
       "        0.0430, 0.0430, 0.0430, 0.0430, 0.0430, 0.0430, 0.0430, 0.0368, 0.0368,\n",
       "        0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368,\n",
       "        0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368,\n",
       "        0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368,\n",
       "        0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368,\n",
       "        0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368,\n",
       "        0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368,\n",
       "        0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368,\n",
       "        0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368,\n",
       "        0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368,\n",
       "        0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368,\n",
       "        0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368,\n",
       "        0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368,\n",
       "        0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368,\n",
       "        0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368,\n",
       "        0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368,\n",
       "        0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368,\n",
       "        0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368,\n",
       "        0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368,\n",
       "        0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368,\n",
       "        0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368,\n",
       "        0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368,\n",
       "        0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368,\n",
       "        0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368,\n",
       "        0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368,\n",
       "        0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368,\n",
       "        0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368,\n",
       "        0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368,\n",
       "        0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368,\n",
       "        0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368,\n",
       "        0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368,\n",
       "        0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368,\n",
       "        0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368,\n",
       "        0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368,\n",
       "        0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368,\n",
       "        0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368, 0.0368,\n",
       "        0.0368, 0.0368, 0.0368, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326,\n",
       "        0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326,\n",
       "        0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326,\n",
       "        0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326,\n",
       "        0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326,\n",
       "        0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326,\n",
       "        0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326,\n",
       "        0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.0184, 0.0184, 0.0184,\n",
       "        0.0184, 0.0184, 0.0184, 0.0184, 0.0184, 0.0184, 0.0184, 0.0184, 0.0184,\n",
       "        0.0184, 0.0184, 0.0184, 0.0184, 0.0184, 0.0184, 0.0184, 0.0184, 0.0184,\n",
       "        0.0184, 0.0184, 0.0184, 0.0184, 0.0184, 0.0184, 0.0184, 0.0184, 0.0184,\n",
       "        0.0184, 0.0184, 0.0184, 0.0184, 0.0184, 0.0184, 0.0184, 0.0184, 0.0184,\n",
       "        0.0184, 0.0184, 0.0184, 0.0184, 0.0184, 0.0184, 0.0184, 0.0184, 0.0184,\n",
       "        0.0184, 0.0184, 0.0184, 0.0184, 0.0184, 0.0184, 0.0184, 0.0184, 0.0184,\n",
       "        0.0184, 0.0386, 0.0386, 0.0386, 0.0386, 0.0386, 0.0386, 0.0386, 0.0386,\n",
       "        0.0386, 0.0386, 0.0386, 0.0386, 0.0386, 0.0386, 0.0386, 0.0386, 0.0386,\n",
       "        0.0386, 0.0386, 0.0386, 0.0386, 0.0386, 0.0386, 0.0386, 0.0386, 0.0386,\n",
       "        0.0386, 0.0386, 0.0386, 0.0386, 0.0386, 0.0386, 0.0386, 0.0386, 0.0386,\n",
       "        0.0386, 0.0386, 0.0386, 0.0386, 0.0386])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factor * torch.log(train[:, 10] + 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2370d932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lower noise var: 0.0000\n",
      "Upper noise var: 0.0008\n"
     ]
    }
   ],
   "source": [
    "### NOISE MODEL ###\n",
    "# Thickness^2 * error_uv^2 (t^2 * sigma_u^2, t^2 * sigma_v^2)\n",
    "noise_var_t_sq_times_uv_var = torch.cat([\n",
    "    (train[:, 9]**2 * train[:, 5]**2),\n",
    "    (train[:, 9]**2 * train[:, 6]**2),\n",
    "], dim = 0)\n",
    "\n",
    "# UV^2 * error_thickness^2 (u^2 * sigma_t^2, v^2 * sigma_t^2)\n",
    "# Calculate the factor for sigma_t (indpendent of scaling)\n",
    "factor = 7.5 / train[:, 11]\n",
    "# noise std level: only dependent on age, not depth, abou 15 m std\n",
    "sigma_t = torch.cat([\n",
    "    factor * torch.log(train[:, 10] + 3),\n",
    "    factor * torch.log(train[:, 10] + 3)\n",
    "], dim = 0)\n",
    "noise_var_uv_sq_times_t_var = (torch.concat((train[:, 7]**2, train[:, 8]**2), dim = 0) * sigma_t**2)\n",
    "\n",
    "# Combine via independed error propagation\n",
    "noise_var = noise_var_t_sq_times_uv_var + noise_var_uv_sq_times_t_var\n",
    "\n",
    "# Get quantiles for prior\n",
    "lower_noise_var = torch.quantile(noise_var, 0.05, dim = 0).item()\n",
    "upper_noise_var = torch.quantile(noise_var, 0.95, dim = 0).item()\n",
    "\n",
    "print(f\"Lower noise var: {lower_noise_var:.4f}\")\n",
    "print(f\"Upper noise var: {upper_noise_var:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8883c9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15.7113)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(factor * torch.log(train[:, 10] + 3) * train[:, 11]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54739e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.concat((train[:, 7]**2, train[:, 8]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2794b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_var_uv_sq_times_t_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7422cd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### NOISE MODEL ###\n",
    "# (t^2 * sigma_u^2, t^2 * sigma_v^2)\n",
    "noise_var_t_sq_times_uv_var = torch.cat((train[:, 9]**2 * train[:, 5])**2, (train[:, 9]**2 * train[:, 6])**2, dim = 0)\n",
    "# (u^2 * sigma_t^2, v^2 * sigma_t^2)\n",
    "train[:, 11]\n",
    "# Calculate the factor for sigma_t\n",
    "factor = 7.5 / train[:, 11]\n",
    "sigma_t = torch.cat((train[:, 9] * factor * torch.log(train[:, 7] + 3), train[:, 10] * factor * torch.log(train[:, 7] + 3)), dim = 0)\n",
    "noise_var_uv_sq_times_t_var = (torch.concat((train[:, 7]**2, train[:, 8]**2), dim = 0) * sigma_t**2)\n",
    "noise_var = noise_var_t_sq_times_uv_var + noise_var_uv_sq_times_t_var\n",
    "\n",
    "# noise variance (h * sigma_u)^2 and (h * sigma_v)^2 (tensor contains [h sig_u, h sig_v] stds)\n",
    "noise_var_h_times_uv_train = torch.concat((train[:, 5], train[:, 6]), dim = 0)**2\n",
    "# assume age dependent noise sigma_h on ice thickness measurements: with 0.1 ~10 - 20 m std (x1000 scaling)\n",
    "sigma_h = 0.01 * torch.log(train[:, 7] + 3)\n",
    "\n",
    "# calculate noise variance (u * sigma_h)^2 and (v * sigma_h)^2\n",
    "noise_var_uv_times_h_train = (torch.concat((train[:, 3], train[:, 4]), dim = 0) * torch.cat([sigma_h, sigma_h]))**2\n",
    "# combine noise variances for both dimensions\n",
    "noise_var = noise_var_h_times_uv_train + noise_var_uv_times_h_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a982479",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_noise_diag[:midpoint] + train_noise_diag[midpoint:])/2  # This is just to show the concatenation\n",
    "# torch.mean(train_noise_diag[:midpoint]), torch.mean(train_noise_diag[midpoint:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e4646c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### NOISE MODEL ###\n",
    "# TRAIN\n",
    "# noise variance (h * sigma_u)^2 and (h * sigma_v)^2 (tensor contains [h sig_u, h sig_v] stds)\n",
    "noise_var_h_times_uv_train = torch.concat((train[:, 5], train[:, 6]), dim = 0)**2\n",
    "# assume age dependent noise sigma_h on ice thickness measurements: with 0.1 ~10 - 20 m std (x1000 scaling)\n",
    "sigma_h = 0.01 * torch.log(train[:, 7] + 3)\n",
    "\n",
    "# calculate noise variance (u * sigma_h)^2 and (v * sigma_h)^2\n",
    "noise_var_uv_times_h_train = (torch.concat((train[:, 3], train[:, 4]), dim = 0) * torch.cat([sigma_h, sigma_h]))**2\n",
    "# combine noise variances for both dimensions\n",
    "noise_var = noise_var_h_times_uv_train + noise_var_uv_times_h_train\n",
    "\n",
    "# Print noise levels for train, formatted to 4 decimal places\n",
    "print(f\"Mean noise var: {noise_var.mean(dim = 0).item():.4f}\")\n",
    "print(f\"Std noise var: {noise_var.std(dim = 0).item():.4f}\")\n",
    "print(f\"Mean noise std: {torch.sqrt(noise_var).mean(dim = 0).item():.4f}\")\n",
    "print(f\"Std noise std: {torch.sqrt(noise_var).std(dim = 0).item():.4f}\")\n",
    "lower_noise_var = torch.quantile(noise_var, 0.10, dim = 0).item()\n",
    "upper_Noise_var = torch.quantile(noise_var, 0.90, dim = 0).item()\n",
    "\n",
    "print(f\"Lower noise var: {lower_noise_var:.4f}\")\n",
    "print(f\"Upper noise var: {upper_Noise_var:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9118af5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b957e062",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_var\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(torch.sqrt(noise_var).cpu().numpy(), bins = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f42ae3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_var_h_times_uv_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad92ffc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.01 * torch.log(train[:, 7] + 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
