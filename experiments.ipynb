{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dfGP + dfNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "=== CONVERGENCE ===\n",
      "Training inputs shape: torch.Size([196, 2])\n",
      "Training observations shape: torch.Size([196, 2])\n",
      "Training inputs dtype: torch.float32\n",
      "\n",
      "=== BRANCHING ===\n",
      "Training inputs shape: torch.Size([196, 2])\n",
      "Training observations shape: torch.Size([196, 2])\n",
      "Training inputs dtype: torch.float32\n",
      "\n",
      "=== MERGE ===\n",
      "Training inputs shape: torch.Size([196, 2])\n",
      "Training observations shape: torch.Size([196, 2])\n",
      "Training inputs dtype: torch.float32\n",
      "\n",
      "=== DEFLECTION ===\n",
      "Training inputs shape: torch.Size([196, 2])\n",
      "Training observations shape: torch.Size([196, 2])\n",
      "Training inputs dtype: torch.float32\n",
      "\n",
      "=== RIDGE ===\n",
      "Training inputs shape: torch.Size([196, 2])\n",
      "Training observations shape: torch.Size([196, 2])\n",
      "Training inputs dtype: torch.float32\n",
      "\n",
      "=== Generating test data ===\n",
      "=== CONVERGENCE ===\n",
      "Test inputs shape: torch.Size([400, 2])\n",
      "Test observations shape: torch.Size([400, 2])\n",
      "Test inputs dtype: torch.float32\n",
      "\n",
      "=== BRANCHING ===\n",
      "Test inputs shape: torch.Size([400, 2])\n",
      "Test observations shape: torch.Size([400, 2])\n",
      "Test inputs dtype: torch.float32\n",
      "\n",
      "=== MERGE ===\n",
      "Test inputs shape: torch.Size([400, 2])\n",
      "Test observations shape: torch.Size([400, 2])\n",
      "Test inputs dtype: torch.float32\n",
      "\n",
      "=== DEFLECTION ===\n",
      "Test inputs shape: torch.Size([400, 2])\n",
      "Test observations shape: torch.Size([400, 2])\n",
      "Test inputs dtype: torch.float32\n",
      "\n",
      "=== RIDGE ===\n",
      "Test inputs shape: torch.Size([400, 2])\n",
      "Test observations shape: torch.Size([400, 2])\n",
      "Test inputs dtype: torch.float32\n",
      "\n",
      "\n",
      "Training for CONVERGENCE...\n",
      "\n",
      "--- Training Run 1/10 ---\n",
      "\n",
      "Start Training\n",
      "convergence dfGPdfNN Run 1/10, Epoch 1/1000, Training Loss (NLML): -872.5280, (RMSE): 0.0155\n",
      "convergence dfGPdfNN Run 1/10, Epoch 2/1000, Training Loss (NLML): -885.1465, (RMSE): 0.0103\n",
      "convergence dfGPdfNN Run 1/10, Epoch 3/1000, Training Loss (NLML): -890.2666, (RMSE): 0.0085\n",
      "convergence dfGPdfNN Run 1/10, Epoch 4/1000, Training Loss (NLML): -894.5923, (RMSE): 0.0065\n",
      "convergence dfGPdfNN Run 1/10, Epoch 5/1000, Training Loss (NLML): -898.1437, (RMSE): 0.0047\n",
      "convergence dfGPdfNN Run 1/10, Epoch 6/1000, Training Loss (NLML): -900.1156, (RMSE): 0.0043\n",
      "convergence dfGPdfNN Run 1/10, Epoch 7/1000, Training Loss (NLML): -902.0502, (RMSE): 0.0040\n",
      "convergence dfGPdfNN Run 1/10, Epoch 8/1000, Training Loss (NLML): -904.1273, (RMSE): 0.0033\n",
      "convergence dfGPdfNN Run 1/10, Epoch 9/1000, Training Loss (NLML): -905.9165, (RMSE): 0.0027\n",
      "convergence dfGPdfNN Run 1/10, Epoch 10/1000, Training Loss (NLML): -907.2411, (RMSE): 0.0029\n",
      "convergence dfGPdfNN Run 1/10, Epoch 11/1000, Training Loss (NLML): -908.5232, (RMSE): 0.0031\n",
      "convergence dfGPdfNN Run 1/10, Epoch 12/1000, Training Loss (NLML): -910.0233, (RMSE): 0.0030\n",
      "convergence dfGPdfNN Run 1/10, Epoch 13/1000, Training Loss (NLML): -911.3365, (RMSE): 0.0031\n",
      "convergence dfGPdfNN Run 1/10, Epoch 14/1000, Training Loss (NLML): -912.6949, (RMSE): 0.0028\n",
      "convergence dfGPdfNN Run 1/10, Epoch 15/1000, Training Loss (NLML): -914.0663, (RMSE): 0.0025\n",
      "convergence dfGPdfNN Run 1/10, Epoch 16/1000, Training Loss (NLML): -915.3617, (RMSE): 0.0022\n",
      "convergence dfGPdfNN Run 1/10, Epoch 17/1000, Training Loss (NLML): -916.5851, (RMSE): 0.0018\n",
      "convergence dfGPdfNN Run 1/10, Epoch 18/1000, Training Loss (NLML): -917.7350, (RMSE): 0.0016\n",
      "convergence dfGPdfNN Run 1/10, Epoch 19/1000, Training Loss (NLML): -918.7980, (RMSE): 0.0014\n",
      "convergence dfGPdfNN Run 1/10, Epoch 20/1000, Training Loss (NLML): -919.8313, (RMSE): 0.0013\n",
      "convergence dfGPdfNN Run 1/10, Epoch 21/1000, Training Loss (NLML): -920.7861, (RMSE): 0.0014\n",
      "convergence dfGPdfNN Run 1/10, Epoch 22/1000, Training Loss (NLML): -921.6843, (RMSE): 0.0015\n",
      "convergence dfGPdfNN Run 1/10, Epoch 23/1000, Training Loss (NLML): -922.5854, (RMSE): 0.0015\n",
      "convergence dfGPdfNN Run 1/10, Epoch 24/1000, Training Loss (NLML): -923.4846, (RMSE): 0.0015\n",
      "convergence dfGPdfNN Run 1/10, Epoch 25/1000, Training Loss (NLML): -924.1892, (RMSE): 0.0019\n",
      "convergence dfGPdfNN Run 1/10, Epoch 26/1000, Training Loss (NLML): -925.0154, (RMSE): 0.0019\n",
      "convergence dfGPdfNN Run 1/10, Epoch 27/1000, Training Loss (NLML): -925.8232, (RMSE): 0.0018\n",
      "convergence dfGPdfNN Run 1/10, Epoch 28/1000, Training Loss (NLML): -926.5934, (RMSE): 0.0017\n",
      "convergence dfGPdfNN Run 1/10, Epoch 29/1000, Training Loss (NLML): -927.4438, (RMSE): 0.0013\n",
      "convergence dfGPdfNN Run 1/10, Epoch 30/1000, Training Loss (NLML): -928.1686, (RMSE): 0.0013\n",
      "convergence dfGPdfNN Run 1/10, Epoch 31/1000, Training Loss (NLML): -928.8569, (RMSE): 0.0012\n",
      "convergence dfGPdfNN Run 1/10, Epoch 32/1000, Training Loss (NLML): -929.5293, (RMSE): 0.0012\n",
      "convergence dfGPdfNN Run 1/10, Epoch 33/1000, Training Loss (NLML): -930.1710, (RMSE): 0.0012\n",
      "convergence dfGPdfNN Run 1/10, Epoch 34/1000, Training Loss (NLML): -930.8048, (RMSE): 0.0012\n",
      "convergence dfGPdfNN Run 1/10, Epoch 35/1000, Training Loss (NLML): -931.4128, (RMSE): 0.0011\n",
      "convergence dfGPdfNN Run 1/10, Epoch 36/1000, Training Loss (NLML): -932.0092, (RMSE): 0.0012\n",
      "convergence dfGPdfNN Run 1/10, Epoch 37/1000, Training Loss (NLML): -932.5428, (RMSE): 0.0013\n",
      "convergence dfGPdfNN Run 1/10, Epoch 38/1000, Training Loss (NLML): -933.0991, (RMSE): 0.0013\n",
      "convergence dfGPdfNN Run 1/10, Epoch 39/1000, Training Loss (NLML): -933.6332, (RMSE): 0.0013\n",
      "convergence dfGPdfNN Run 1/10, Epoch 40/1000, Training Loss (NLML): -934.1771, (RMSE): 0.0013\n",
      "convergence dfGPdfNN Run 1/10, Epoch 41/1000, Training Loss (NLML): -934.6892, (RMSE): 0.0013\n",
      "convergence dfGPdfNN Run 1/10, Epoch 42/1000, Training Loss (NLML): -935.1902, (RMSE): 0.0014\n",
      "convergence dfGPdfNN Run 1/10, Epoch 43/1000, Training Loss (NLML): -935.6678, (RMSE): 0.0014\n",
      "convergence dfGPdfNN Run 1/10, Epoch 44/1000, Training Loss (NLML): -936.1560, (RMSE): 0.0015\n",
      "convergence dfGPdfNN Run 1/10, Epoch 45/1000, Training Loss (NLML): -936.5992, (RMSE): 0.0016\n",
      "convergence dfGPdfNN Run 1/10, Epoch 46/1000, Training Loss (NLML): -937.0496, (RMSE): 0.0016\n",
      "convergence dfGPdfNN Run 1/10, Epoch 47/1000, Training Loss (NLML): -935.7340, (RMSE): 0.0048\n",
      "convergence dfGPdfNN Run 1/10, Epoch 48/1000, Training Loss (NLML): -937.8529, (RMSE): 0.0019\n",
      "convergence dfGPdfNN Run 1/10, Epoch 49/1000, Training Loss (NLML): -938.2825, (RMSE): 0.0018\n",
      "convergence dfGPdfNN Run 1/10, Epoch 50/1000, Training Loss (NLML): -938.6755, (RMSE): 0.0018\n",
      "convergence dfGPdfNN Run 1/10, Epoch 51/1000, Training Loss (NLML): -939.0425, (RMSE): 0.0018\n",
      "convergence dfGPdfNN Run 1/10, Epoch 52/1000, Training Loss (NLML): -939.3997, (RMSE): 0.0018\n",
      "convergence dfGPdfNN Run 1/10, Epoch 53/1000, Training Loss (NLML): -939.7600, (RMSE): 0.0017\n",
      "convergence dfGPdfNN Run 1/10, Epoch 54/1000, Training Loss (NLML): -940.1453, (RMSE): 0.0016\n",
      "convergence dfGPdfNN Run 1/10, Epoch 55/1000, Training Loss (NLML): -940.4917, (RMSE): 0.0017\n",
      "convergence dfGPdfNN Run 1/10, Epoch 56/1000, Training Loss (NLML): -940.8496, (RMSE): 0.0018\n",
      "convergence dfGPdfNN Run 1/10, Epoch 57/1000, Training Loss (NLML): -941.2150, (RMSE): 0.0018\n",
      "convergence dfGPdfNN Run 1/10, Epoch 58/1000, Training Loss (NLML): -941.5638, (RMSE): 0.0017\n",
      "convergence dfGPdfNN Run 1/10, Epoch 59/1000, Training Loss (NLML): -941.8904, (RMSE): 0.0018\n",
      "convergence dfGPdfNN Run 1/10, Epoch 60/1000, Training Loss (NLML): -942.2291, (RMSE): 0.0019\n",
      "convergence dfGPdfNN Run 1/10, Epoch 61/1000, Training Loss (NLML): -942.5751, (RMSE): 0.0019\n",
      "convergence dfGPdfNN Run 1/10, Epoch 62/1000, Training Loss (NLML): -942.8931, (RMSE): 0.0019\n",
      "convergence dfGPdfNN Run 1/10, Epoch 63/1000, Training Loss (NLML): -943.1965, (RMSE): 0.0020\n",
      "convergence dfGPdfNN Run 1/10, Epoch 64/1000, Training Loss (NLML): -943.4868, (RMSE): 0.0021\n",
      "convergence dfGPdfNN Run 1/10, Epoch 65/1000, Training Loss (NLML): -928.2678, (RMSE): 0.0138\n",
      "convergence dfGPdfNN Run 1/10, Epoch 66/1000, Training Loss (NLML): -944.0713, (RMSE): 0.0021\n",
      "convergence dfGPdfNN Run 1/10, Epoch 67/1000, Training Loss (NLML): -944.3673, (RMSE): 0.0019\n",
      "convergence dfGPdfNN Run 1/10, Epoch 68/1000, Training Loss (NLML): -944.5959, (RMSE): 0.0019\n",
      "convergence dfGPdfNN Run 1/10, Epoch 69/1000, Training Loss (NLML): -944.8250, (RMSE): 0.0019\n",
      "convergence dfGPdfNN Run 1/10, Epoch 70/1000, Training Loss (NLML): -945.0681, (RMSE): 0.0019\n",
      "convergence dfGPdfNN Run 1/10, Epoch 71/1000, Training Loss (NLML): -945.3119, (RMSE): 0.0020\n",
      "convergence dfGPdfNN Run 1/10, Epoch 72/1000, Training Loss (NLML): -945.5769, (RMSE): 0.0020\n",
      "convergence dfGPdfNN Run 1/10, Epoch 73/1000, Training Loss (NLML): -945.8470, (RMSE): 0.0020\n",
      "convergence dfGPdfNN Run 1/10, Epoch 74/1000, Training Loss (NLML): -946.0728, (RMSE): 0.0022\n",
      "convergence dfGPdfNN Run 1/10, Epoch 75/1000, Training Loss (NLML): -946.2739, (RMSE): 0.0024\n",
      "convergence dfGPdfNN Run 1/10, Epoch 76/1000, Training Loss (NLML): -946.4851, (RMSE): 0.0025\n",
      "convergence dfGPdfNN Run 1/10, Epoch 77/1000, Training Loss (NLML): -946.6863, (RMSE): 0.0026\n",
      "convergence dfGPdfNN Run 1/10, Epoch 78/1000, Training Loss (NLML): -946.9506, (RMSE): 0.0026\n",
      "convergence dfGPdfNN Run 1/10, Epoch 79/1000, Training Loss (NLML): -947.1442, (RMSE): 0.0028\n",
      "convergence dfGPdfNN Run 1/10, Epoch 80/1000, Training Loss (NLML): -947.3558, (RMSE): 0.0029\n",
      "convergence dfGPdfNN Run 1/10, Epoch 81/1000, Training Loss (NLML): -947.6013, (RMSE): 0.0030\n",
      "convergence dfGPdfNN Run 1/10, Epoch 82/1000, Training Loss (NLML): -947.7679, (RMSE): 0.0031\n",
      "convergence dfGPdfNN Run 1/10, Epoch 83/1000, Training Loss (NLML): -948.0024, (RMSE): 0.0031\n",
      "convergence dfGPdfNN Run 1/10, Epoch 84/1000, Training Loss (NLML): -948.2893, (RMSE): 0.0028\n",
      "convergence dfGPdfNN Run 1/10, Epoch 85/1000, Training Loss (NLML): -948.5219, (RMSE): 0.0027\n",
      "convergence dfGPdfNN Run 1/10, Epoch 86/1000, Training Loss (NLML): -948.7347, (RMSE): 0.0026\n",
      "convergence dfGPdfNN Run 1/10, Epoch 87/1000, Training Loss (NLML): -948.9027, (RMSE): 0.0027\n",
      "convergence dfGPdfNN Run 1/10, Epoch 88/1000, Training Loss (NLML): -949.1082, (RMSE): 0.0027\n",
      "convergence dfGPdfNN Run 1/10, Epoch 89/1000, Training Loss (NLML): -949.3054, (RMSE): 0.0028\n",
      "convergence dfGPdfNN Run 1/10, Epoch 90/1000, Training Loss (NLML): -949.5554, (RMSE): 0.0028\n",
      "convergence dfGPdfNN Run 1/10, Epoch 91/1000, Training Loss (NLML): -949.7257, (RMSE): 0.0029\n",
      "convergence dfGPdfNN Run 1/10, Epoch 92/1000, Training Loss (NLML): -949.8590, (RMSE): 0.0031\n",
      "convergence dfGPdfNN Run 1/10, Epoch 93/1000, Training Loss (NLML): -950.0238, (RMSE): 0.0031\n",
      "convergence dfGPdfNN Run 1/10, Epoch 94/1000, Training Loss (NLML): -950.2152, (RMSE): 0.0032\n",
      "convergence dfGPdfNN Run 1/10, Epoch 95/1000, Training Loss (NLML): -950.3555, (RMSE): 0.0032\n",
      "convergence dfGPdfNN Run 1/10, Epoch 96/1000, Training Loss (NLML): -950.4774, (RMSE): 0.0033\n",
      "convergence dfGPdfNN Run 1/10, Epoch 97/1000, Training Loss (NLML): -950.7323, (RMSE): 0.0032\n",
      "convergence dfGPdfNN Run 1/10, Epoch 98/1000, Training Loss (NLML): -950.9408, (RMSE): 0.0032\n",
      "convergence dfGPdfNN Run 1/10, Epoch 99/1000, Training Loss (NLML): -951.1245, (RMSE): 0.0032\n",
      "convergence dfGPdfNN Run 1/10, Epoch 100/1000, Training Loss (NLML): -951.3751, (RMSE): 0.0030\n",
      "convergence dfGPdfNN Run 1/10, Epoch 101/1000, Training Loss (NLML): -951.5427, (RMSE): 0.0030\n",
      "convergence dfGPdfNN Run 1/10, Epoch 102/1000, Training Loss (NLML): -951.7643, (RMSE): 0.0030\n",
      "convergence dfGPdfNN Run 1/10, Epoch 103/1000, Training Loss (NLML): -951.9022, (RMSE): 0.0030\n",
      "convergence dfGPdfNN Run 1/10, Epoch 104/1000, Training Loss (NLML): -952.0138, (RMSE): 0.0032\n",
      "convergence dfGPdfNN Run 1/10, Epoch 105/1000, Training Loss (NLML): -952.1779, (RMSE): 0.0032\n",
      "convergence dfGPdfNN Run 1/10, Epoch 106/1000, Training Loss (NLML): -952.3486, (RMSE): 0.0033\n",
      "convergence dfGPdfNN Run 1/10, Epoch 107/1000, Training Loss (NLML): -952.4998, (RMSE): 0.0034\n",
      "convergence dfGPdfNN Run 1/10, Epoch 108/1000, Training Loss (NLML): -952.6793, (RMSE): 0.0033\n",
      "convergence dfGPdfNN Run 1/10, Epoch 109/1000, Training Loss (NLML): -952.8691, (RMSE): 0.0032\n",
      "convergence dfGPdfNN Run 1/10, Epoch 110/1000, Training Loss (NLML): -953.0526, (RMSE): 0.0032\n",
      "convergence dfGPdfNN Run 1/10, Epoch 111/1000, Training Loss (NLML): -953.2086, (RMSE): 0.0032\n",
      "convergence dfGPdfNN Run 1/10, Epoch 112/1000, Training Loss (NLML): -953.3541, (RMSE): 0.0032\n",
      "convergence dfGPdfNN Run 1/10, Epoch 113/1000, Training Loss (NLML): -953.4880, (RMSE): 0.0032\n",
      "convergence dfGPdfNN Run 1/10, Epoch 114/1000, Training Loss (NLML): -953.5613, (RMSE): 0.0034\n",
      "convergence dfGPdfNN Run 1/10, Epoch 115/1000, Training Loss (NLML): -953.6387, (RMSE): 0.0035\n",
      "convergence dfGPdfNN Run 1/10, Epoch 116/1000, Training Loss (NLML): -953.7372, (RMSE): 0.0035\n",
      "convergence dfGPdfNN Run 1/10, Epoch 117/1000, Training Loss (NLML): -953.8479, (RMSE): 0.0035\n",
      "convergence dfGPdfNN Run 1/10, Epoch 118/1000, Training Loss (NLML): -953.8728, (RMSE): 0.0036\n",
      "convergence dfGPdfNN Run 1/10, Epoch 119/1000, Training Loss (NLML): -953.9812, (RMSE): 0.0037\n",
      "convergence dfGPdfNN Run 1/10, Epoch 120/1000, Training Loss (NLML): -954.0942, (RMSE): 0.0037\n",
      "convergence dfGPdfNN Run 1/10, Epoch 121/1000, Training Loss (NLML): -954.1841, (RMSE): 0.0038\n",
      "convergence dfGPdfNN Run 1/10, Epoch 122/1000, Training Loss (NLML): -954.2836, (RMSE): 0.0038\n",
      "convergence dfGPdfNN Run 1/10, Epoch 123/1000, Training Loss (NLML): -954.3979, (RMSE): 0.0037\n",
      "convergence dfGPdfNN Run 1/10, Epoch 124/1000, Training Loss (NLML): -954.5569, (RMSE): 0.0036\n",
      "convergence dfGPdfNN Run 1/10, Epoch 125/1000, Training Loss (NLML): -954.6747, (RMSE): 0.0036\n",
      "convergence dfGPdfNN Run 1/10, Epoch 126/1000, Training Loss (NLML): -954.8046, (RMSE): 0.0036\n",
      "convergence dfGPdfNN Run 1/10, Epoch 127/1000, Training Loss (NLML): -954.9042, (RMSE): 0.0038\n",
      "convergence dfGPdfNN Run 1/10, Epoch 128/1000, Training Loss (NLML): -954.9624, (RMSE): 0.0039\n",
      "convergence dfGPdfNN Run 1/10, Epoch 129/1000, Training Loss (NLML): -955.0887, (RMSE): 0.0040\n",
      "convergence dfGPdfNN Run 1/10, Epoch 130/1000, Training Loss (NLML): -955.2014, (RMSE): 0.0040\n",
      "convergence dfGPdfNN Run 1/10, Epoch 131/1000, Training Loss (NLML): -955.2517, (RMSE): 0.0040\n",
      "convergence dfGPdfNN Run 1/10, Epoch 132/1000, Training Loss (NLML): -955.3351, (RMSE): 0.0040\n",
      "convergence dfGPdfNN Run 1/10, Epoch 133/1000, Training Loss (NLML): -955.3635, (RMSE): 0.0039\n",
      "convergence dfGPdfNN Run 1/10, Epoch 134/1000, Training Loss (NLML): -955.3728, (RMSE): 0.0039\n",
      "convergence dfGPdfNN Run 1/10, Epoch 135/1000, Training Loss (NLML): -955.4326, (RMSE): 0.0039\n",
      "convergence dfGPdfNN Run 1/10, Epoch 136/1000, Training Loss (NLML): -955.3627, (RMSE): 0.0041\n",
      "convergence dfGPdfNN Run 1/10, Epoch 137/1000, Training Loss (NLML): -955.3574, (RMSE): 0.0043\n",
      "convergence dfGPdfNN Run 1/10, Epoch 138/1000, Training Loss (NLML): -955.3610, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 139/1000, Training Loss (NLML): -955.4832, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 140/1000, Training Loss (NLML): -955.6537, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 141/1000, Training Loss (NLML): -955.6326, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 142/1000, Training Loss (NLML): -955.5953, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 143/1000, Training Loss (NLML): -955.7168, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 144/1000, Training Loss (NLML): -955.7642, (RMSE): 0.0043\n",
      "convergence dfGPdfNN Run 1/10, Epoch 145/1000, Training Loss (NLML): -955.8892, (RMSE): 0.0042\n",
      "convergence dfGPdfNN Run 1/10, Epoch 146/1000, Training Loss (NLML): -955.9125, (RMSE): 0.0042\n",
      "convergence dfGPdfNN Run 1/10, Epoch 147/1000, Training Loss (NLML): -955.9355, (RMSE): 0.0043\n",
      "convergence dfGPdfNN Run 1/10, Epoch 148/1000, Training Loss (NLML): -956.0265, (RMSE): 0.0043\n",
      "convergence dfGPdfNN Run 1/10, Epoch 149/1000, Training Loss (NLML): -956.0170, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 150/1000, Training Loss (NLML): -956.0149, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 151/1000, Training Loss (NLML): -956.0156, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 152/1000, Training Loss (NLML): -956.0548, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 153/1000, Training Loss (NLML): -956.0385, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 154/1000, Training Loss (NLML): -956.0299, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 155/1000, Training Loss (NLML): -956.0150, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 156/1000, Training Loss (NLML): -956.0261, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 157/1000, Training Loss (NLML): -956.0974, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 158/1000, Training Loss (NLML): -956.1682, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 159/1000, Training Loss (NLML): -956.3142, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 160/1000, Training Loss (NLML): -956.3485, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 161/1000, Training Loss (NLML): -956.3938, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 162/1000, Training Loss (NLML): -956.2919, (RMSE): 0.0046\n",
      "convergence dfGPdfNN Run 1/10, Epoch 163/1000, Training Loss (NLML): -956.2678, (RMSE): 0.0046\n",
      "convergence dfGPdfNN Run 1/10, Epoch 164/1000, Training Loss (NLML): -956.2120, (RMSE): 0.0046\n",
      "convergence dfGPdfNN Run 1/10, Epoch 165/1000, Training Loss (NLML): -956.2002, (RMSE): 0.0046\n",
      "convergence dfGPdfNN Run 1/10, Epoch 166/1000, Training Loss (NLML): -956.2208, (RMSE): 0.0046\n",
      "convergence dfGPdfNN Run 1/10, Epoch 167/1000, Training Loss (NLML): -956.3088, (RMSE): 0.0046\n",
      "convergence dfGPdfNN Run 1/10, Epoch 168/1000, Training Loss (NLML): -956.2639, (RMSE): 0.0048\n",
      "convergence dfGPdfNN Run 1/10, Epoch 169/1000, Training Loss (NLML): -956.5691, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 170/1000, Training Loss (NLML): -956.6096, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 171/1000, Training Loss (NLML): -956.4233, (RMSE): 0.0047\n",
      "convergence dfGPdfNN Run 1/10, Epoch 172/1000, Training Loss (NLML): -956.1414, (RMSE): 0.0050\n",
      "convergence dfGPdfNN Run 1/10, Epoch 173/1000, Training Loss (NLML): -956.0785, (RMSE): 0.0049\n",
      "convergence dfGPdfNN Run 1/10, Epoch 174/1000, Training Loss (NLML): -956.1338, (RMSE): 0.0047\n",
      "convergence dfGPdfNN Run 1/10, Epoch 175/1000, Training Loss (NLML): -956.1337, (RMSE): 0.0047\n",
      "convergence dfGPdfNN Run 1/10, Epoch 176/1000, Training Loss (NLML): -956.1571, (RMSE): 0.0047\n",
      "convergence dfGPdfNN Run 1/10, Epoch 177/1000, Training Loss (NLML): -956.1993, (RMSE): 0.0047\n",
      "convergence dfGPdfNN Run 1/10, Epoch 178/1000, Training Loss (NLML): -956.2482, (RMSE): 0.0047\n",
      "convergence dfGPdfNN Run 1/10, Epoch 179/1000, Training Loss (NLML): -956.2887, (RMSE): 0.0048\n",
      "convergence dfGPdfNN Run 1/10, Epoch 180/1000, Training Loss (NLML): -956.2223, (RMSE): 0.0049\n",
      "convergence dfGPdfNN Run 1/10, Epoch 181/1000, Training Loss (NLML): -956.3541, (RMSE): 0.0050\n",
      "convergence dfGPdfNN Run 1/10, Epoch 182/1000, Training Loss (NLML): -956.4540, (RMSE): 0.0049\n",
      "convergence dfGPdfNN Run 1/10, Epoch 183/1000, Training Loss (NLML): -956.5464, (RMSE): 0.0048\n",
      "convergence dfGPdfNN Run 1/10, Epoch 184/1000, Training Loss (NLML): -956.5823, (RMSE): 0.0048\n",
      "convergence dfGPdfNN Run 1/10, Epoch 185/1000, Training Loss (NLML): -956.5422, (RMSE): 0.0048\n",
      "convergence dfGPdfNN Run 1/10, Epoch 186/1000, Training Loss (NLML): -956.5061, (RMSE): 0.0048\n",
      "convergence dfGPdfNN Run 1/10, Epoch 187/1000, Training Loss (NLML): -956.4307, (RMSE): 0.0048\n",
      "convergence dfGPdfNN Run 1/10, Epoch 188/1000, Training Loss (NLML): -956.5018, (RMSE): 0.0048\n",
      "convergence dfGPdfNN Run 1/10, Epoch 189/1000, Training Loss (NLML): -956.5309, (RMSE): 0.0048\n",
      "convergence dfGPdfNN Run 1/10, Epoch 190/1000, Training Loss (NLML): -956.6790, (RMSE): 0.0047\n",
      "convergence dfGPdfNN Run 1/10, Epoch 191/1000, Training Loss (NLML): -956.7841, (RMSE): 0.0047\n",
      "convergence dfGPdfNN Run 1/10, Epoch 192/1000, Training Loss (NLML): -956.9186, (RMSE): 0.0046\n",
      "convergence dfGPdfNN Run 1/10, Epoch 193/1000, Training Loss (NLML): -957.0262, (RMSE): 0.0046\n",
      "convergence dfGPdfNN Run 1/10, Epoch 194/1000, Training Loss (NLML): -957.1609, (RMSE): 0.0046\n",
      "convergence dfGPdfNN Run 1/10, Epoch 195/1000, Training Loss (NLML): -957.2275, (RMSE): 0.0046\n",
      "convergence dfGPdfNN Run 1/10, Epoch 196/1000, Training Loss (NLML): -957.2933, (RMSE): 0.0046\n",
      "convergence dfGPdfNN Run 1/10, Epoch 197/1000, Training Loss (NLML): -957.1798, (RMSE): 0.0048\n",
      "convergence dfGPdfNN Run 1/10, Epoch 198/1000, Training Loss (NLML): -957.0366, (RMSE): 0.0049\n",
      "convergence dfGPdfNN Run 1/10, Epoch 199/1000, Training Loss (NLML): -957.0278, (RMSE): 0.0049\n",
      "convergence dfGPdfNN Run 1/10, Epoch 200/1000, Training Loss (NLML): -956.9595, (RMSE): 0.0048\n",
      "convergence dfGPdfNN Run 1/10, Epoch 201/1000, Training Loss (NLML): -956.6886, (RMSE): 0.0049\n",
      "convergence dfGPdfNN Run 1/10, Epoch 202/1000, Training Loss (NLML): -956.6676, (RMSE): 0.0048\n",
      "convergence dfGPdfNN Run 1/10, Epoch 203/1000, Training Loss (NLML): -956.6550, (RMSE): 0.0048\n",
      "convergence dfGPdfNN Run 1/10, Epoch 204/1000, Training Loss (NLML): -956.6270, (RMSE): 0.0048\n",
      "convergence dfGPdfNN Run 1/10, Epoch 205/1000, Training Loss (NLML): -956.5840, (RMSE): 0.0048\n",
      "convergence dfGPdfNN Run 1/10, Epoch 206/1000, Training Loss (NLML): -956.6144, (RMSE): 0.0048\n",
      "convergence dfGPdfNN Run 1/10, Epoch 207/1000, Training Loss (NLML): -956.5504, (RMSE): 0.0049\n",
      "convergence dfGPdfNN Run 1/10, Epoch 208/1000, Training Loss (NLML): -956.5908, (RMSE): 0.0048\n",
      "convergence dfGPdfNN Run 1/10, Epoch 209/1000, Training Loss (NLML): -956.6241, (RMSE): 0.0048\n",
      "convergence dfGPdfNN Run 1/10, Epoch 210/1000, Training Loss (NLML): -956.6344, (RMSE): 0.0048\n",
      "convergence dfGPdfNN Run 1/10, Epoch 211/1000, Training Loss (NLML): -956.5892, (RMSE): 0.0049\n",
      "convergence dfGPdfNN Run 1/10, Epoch 212/1000, Training Loss (NLML): -956.6171, (RMSE): 0.0049\n",
      "convergence dfGPdfNN Run 1/10, Epoch 213/1000, Training Loss (NLML): -956.6389, (RMSE): 0.0049\n",
      "convergence dfGPdfNN Run 1/10, Epoch 214/1000, Training Loss (NLML): -956.6597, (RMSE): 0.0049\n",
      "convergence dfGPdfNN Run 1/10, Epoch 215/1000, Training Loss (NLML): -956.7866, (RMSE): 0.0047\n",
      "convergence dfGPdfNN Run 1/10, Epoch 216/1000, Training Loss (NLML): -956.8689, (RMSE): 0.0047\n",
      "convergence dfGPdfNN Run 1/10, Epoch 217/1000, Training Loss (NLML): -956.8540, (RMSE): 0.0047\n",
      "convergence dfGPdfNN Run 1/10, Epoch 218/1000, Training Loss (NLML): -956.7307, (RMSE): 0.0049\n",
      "convergence dfGPdfNN Run 1/10, Epoch 219/1000, Training Loss (NLML): -956.4622, (RMSE): 0.0052\n",
      "convergence dfGPdfNN Run 1/10, Epoch 220/1000, Training Loss (NLML): -956.5688, (RMSE): 0.0051\n",
      "convergence dfGPdfNN Run 1/10, Epoch 221/1000, Training Loss (NLML): -956.6970, (RMSE): 0.0049\n",
      "convergence dfGPdfNN Run 1/10, Epoch 222/1000, Training Loss (NLML): -956.7999, (RMSE): 0.0049\n",
      "convergence dfGPdfNN Run 1/10, Epoch 223/1000, Training Loss (NLML): -956.7765, (RMSE): 0.0049\n",
      "convergence dfGPdfNN Run 1/10, Epoch 224/1000, Training Loss (NLML): -956.8833, (RMSE): 0.0048\n",
      "convergence dfGPdfNN Run 1/10, Epoch 225/1000, Training Loss (NLML): -957.0195, (RMSE): 0.0047\n",
      "convergence dfGPdfNN Run 1/10, Epoch 226/1000, Training Loss (NLML): -957.1750, (RMSE): 0.0047\n",
      "convergence dfGPdfNN Run 1/10, Epoch 227/1000, Training Loss (NLML): -957.2867, (RMSE): 0.0046\n",
      "convergence dfGPdfNN Run 1/10, Epoch 228/1000, Training Loss (NLML): -957.4563, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 229/1000, Training Loss (NLML): -957.5436, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 230/1000, Training Loss (NLML): -957.5709, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 231/1000, Training Loss (NLML): -957.5338, (RMSE): 0.0046\n",
      "convergence dfGPdfNN Run 1/10, Epoch 232/1000, Training Loss (NLML): -957.4305, (RMSE): 0.0048\n",
      "convergence dfGPdfNN Run 1/10, Epoch 233/1000, Training Loss (NLML): -957.2900, (RMSE): 0.0050\n",
      "convergence dfGPdfNN Run 1/10, Epoch 234/1000, Training Loss (NLML): -957.2496, (RMSE): 0.0051\n",
      "convergence dfGPdfNN Run 1/10, Epoch 235/1000, Training Loss (NLML): -957.5381, (RMSE): 0.0048\n",
      "convergence dfGPdfNN Run 1/10, Epoch 236/1000, Training Loss (NLML): -957.6986, (RMSE): 0.0046\n",
      "convergence dfGPdfNN Run 1/10, Epoch 237/1000, Training Loss (NLML): -957.6816, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 238/1000, Training Loss (NLML): -957.5123, (RMSE): 0.0047\n",
      "convergence dfGPdfNN Run 1/10, Epoch 239/1000, Training Loss (NLML): -957.3258, (RMSE): 0.0048\n",
      "convergence dfGPdfNN Run 1/10, Epoch 240/1000, Training Loss (NLML): -957.1769, (RMSE): 0.0048\n",
      "convergence dfGPdfNN Run 1/10, Epoch 241/1000, Training Loss (NLML): -957.2217, (RMSE): 0.0048\n",
      "convergence dfGPdfNN Run 1/10, Epoch 242/1000, Training Loss (NLML): -957.2343, (RMSE): 0.0048\n",
      "convergence dfGPdfNN Run 1/10, Epoch 243/1000, Training Loss (NLML): -957.2931, (RMSE): 0.0047\n",
      "convergence dfGPdfNN Run 1/10, Epoch 244/1000, Training Loss (NLML): -957.3259, (RMSE): 0.0047\n",
      "convergence dfGPdfNN Run 1/10, Epoch 245/1000, Training Loss (NLML): -957.2842, (RMSE): 0.0048\n",
      "convergence dfGPdfNN Run 1/10, Epoch 246/1000, Training Loss (NLML): -957.2780, (RMSE): 0.0048\n",
      "convergence dfGPdfNN Run 1/10, Epoch 247/1000, Training Loss (NLML): -957.3481, (RMSE): 0.0048\n",
      "convergence dfGPdfNN Run 1/10, Epoch 248/1000, Training Loss (NLML): -957.3660, (RMSE): 0.0048\n",
      "convergence dfGPdfNN Run 1/10, Epoch 249/1000, Training Loss (NLML): -957.3879, (RMSE): 0.0047\n",
      "convergence dfGPdfNN Run 1/10, Epoch 250/1000, Training Loss (NLML): -957.5665, (RMSE): 0.0046\n",
      "convergence dfGPdfNN Run 1/10, Epoch 251/1000, Training Loss (NLML): -957.6351, (RMSE): 0.0046\n",
      "convergence dfGPdfNN Run 1/10, Epoch 252/1000, Training Loss (NLML): -957.7704, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 253/1000, Training Loss (NLML): -957.8214, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 254/1000, Training Loss (NLML): -957.9052, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 255/1000, Training Loss (NLML): -957.9435, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 256/1000, Training Loss (NLML): -957.9431, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 257/1000, Training Loss (NLML): -957.9299, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 258/1000, Training Loss (NLML): -957.8928, (RMSE): 0.0046\n",
      "convergence dfGPdfNN Run 1/10, Epoch 259/1000, Training Loss (NLML): -957.8020, (RMSE): 0.0047\n",
      "convergence dfGPdfNN Run 1/10, Epoch 260/1000, Training Loss (NLML): -957.8214, (RMSE): 0.0047\n",
      "convergence dfGPdfNN Run 1/10, Epoch 261/1000, Training Loss (NLML): -957.9572, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 262/1000, Training Loss (NLML): -958.0355, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 263/1000, Training Loss (NLML): -958.0897, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 264/1000, Training Loss (NLML): -958.1146, (RMSE): 0.0043\n",
      "convergence dfGPdfNN Run 1/10, Epoch 265/1000, Training Loss (NLML): -958.0903, (RMSE): 0.0043\n",
      "convergence dfGPdfNN Run 1/10, Epoch 266/1000, Training Loss (NLML): -958.1265, (RMSE): 0.0043\n",
      "convergence dfGPdfNN Run 1/10, Epoch 267/1000, Training Loss (NLML): -958.0261, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 268/1000, Training Loss (NLML): -957.8077, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 269/1000, Training Loss (NLML): -957.7916, (RMSE): 0.0046\n",
      "convergence dfGPdfNN Run 1/10, Epoch 270/1000, Training Loss (NLML): -957.8206, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 271/1000, Training Loss (NLML): -957.8230, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 272/1000, Training Loss (NLML): -957.8241, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 273/1000, Training Loss (NLML): -957.6819, (RMSE): 0.0047\n",
      "convergence dfGPdfNN Run 1/10, Epoch 274/1000, Training Loss (NLML): -957.7014, (RMSE): 0.0047\n",
      "convergence dfGPdfNN Run 1/10, Epoch 275/1000, Training Loss (NLML): -957.7025, (RMSE): 0.0047\n",
      "convergence dfGPdfNN Run 1/10, Epoch 276/1000, Training Loss (NLML): -957.7888, (RMSE): 0.0046\n",
      "convergence dfGPdfNN Run 1/10, Epoch 277/1000, Training Loss (NLML): -957.8101, (RMSE): 0.0046\n",
      "convergence dfGPdfNN Run 1/10, Epoch 278/1000, Training Loss (NLML): -957.9169, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 279/1000, Training Loss (NLML): -958.0576, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 280/1000, Training Loss (NLML): -958.1838, (RMSE): 0.0043\n",
      "convergence dfGPdfNN Run 1/10, Epoch 281/1000, Training Loss (NLML): -958.2427, (RMSE): 0.0043\n",
      "convergence dfGPdfNN Run 1/10, Epoch 282/1000, Training Loss (NLML): -958.2931, (RMSE): 0.0042\n",
      "convergence dfGPdfNN Run 1/10, Epoch 283/1000, Training Loss (NLML): -958.3361, (RMSE): 0.0042\n",
      "convergence dfGPdfNN Run 1/10, Epoch 284/1000, Training Loss (NLML): -958.3489, (RMSE): 0.0042\n",
      "convergence dfGPdfNN Run 1/10, Epoch 285/1000, Training Loss (NLML): -958.3766, (RMSE): 0.0042\n",
      "convergence dfGPdfNN Run 1/10, Epoch 286/1000, Training Loss (NLML): -958.3822, (RMSE): 0.0042\n",
      "convergence dfGPdfNN Run 1/10, Epoch 287/1000, Training Loss (NLML): -958.3397, (RMSE): 0.0043\n",
      "convergence dfGPdfNN Run 1/10, Epoch 288/1000, Training Loss (NLML): -958.3308, (RMSE): 0.0043\n",
      "convergence dfGPdfNN Run 1/10, Epoch 289/1000, Training Loss (NLML): -958.3754, (RMSE): 0.0043\n",
      "convergence dfGPdfNN Run 1/10, Epoch 290/1000, Training Loss (NLML): -958.3905, (RMSE): 0.0043\n",
      "convergence dfGPdfNN Run 1/10, Epoch 291/1000, Training Loss (NLML): -958.3646, (RMSE): 0.0043\n",
      "convergence dfGPdfNN Run 1/10, Epoch 292/1000, Training Loss (NLML): -958.3667, (RMSE): 0.0042\n",
      "convergence dfGPdfNN Run 1/10, Epoch 293/1000, Training Loss (NLML): -958.2125, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 294/1000, Training Loss (NLML): -958.1373, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 295/1000, Training Loss (NLML): -958.1033, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 296/1000, Training Loss (NLML): -958.0590, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 297/1000, Training Loss (NLML): -958.0173, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 298/1000, Training Loss (NLML): -958.0402, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 299/1000, Training Loss (NLML): -958.0566, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 300/1000, Training Loss (NLML): -958.0824, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 301/1000, Training Loss (NLML): -958.0847, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 302/1000, Training Loss (NLML): -958.0973, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 303/1000, Training Loss (NLML): -958.0994, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 304/1000, Training Loss (NLML): -958.2604, (RMSE): 0.0043\n",
      "convergence dfGPdfNN Run 1/10, Epoch 305/1000, Training Loss (NLML): -958.3208, (RMSE): 0.0043\n",
      "convergence dfGPdfNN Run 1/10, Epoch 306/1000, Training Loss (NLML): -958.4329, (RMSE): 0.0042\n",
      "convergence dfGPdfNN Run 1/10, Epoch 307/1000, Training Loss (NLML): -958.4663, (RMSE): 0.0042\n",
      "convergence dfGPdfNN Run 1/10, Epoch 308/1000, Training Loss (NLML): -958.5215, (RMSE): 0.0041\n",
      "convergence dfGPdfNN Run 1/10, Epoch 309/1000, Training Loss (NLML): -958.5977, (RMSE): 0.0041\n",
      "convergence dfGPdfNN Run 1/10, Epoch 310/1000, Training Loss (NLML): -958.6090, (RMSE): 0.0041\n",
      "convergence dfGPdfNN Run 1/10, Epoch 311/1000, Training Loss (NLML): -958.6300, (RMSE): 0.0041\n",
      "convergence dfGPdfNN Run 1/10, Epoch 312/1000, Training Loss (NLML): -958.6125, (RMSE): 0.0041\n",
      "convergence dfGPdfNN Run 1/10, Epoch 313/1000, Training Loss (NLML): -958.5535, (RMSE): 0.0042\n",
      "convergence dfGPdfNN Run 1/10, Epoch 314/1000, Training Loss (NLML): -958.5470, (RMSE): 0.0043\n",
      "convergence dfGPdfNN Run 1/10, Epoch 315/1000, Training Loss (NLML): -958.6036, (RMSE): 0.0042\n",
      "convergence dfGPdfNN Run 1/10, Epoch 316/1000, Training Loss (NLML): -958.6169, (RMSE): 0.0042\n",
      "convergence dfGPdfNN Run 1/10, Epoch 317/1000, Training Loss (NLML): -958.6537, (RMSE): 0.0041\n",
      "convergence dfGPdfNN Run 1/10, Epoch 318/1000, Training Loss (NLML): -958.7084, (RMSE): 0.0040\n",
      "convergence dfGPdfNN Run 1/10, Epoch 319/1000, Training Loss (NLML): -958.6823, (RMSE): 0.0041\n",
      "convergence dfGPdfNN Run 1/10, Epoch 320/1000, Training Loss (NLML): -958.6368, (RMSE): 0.0041\n",
      "convergence dfGPdfNN Run 1/10, Epoch 321/1000, Training Loss (NLML): -958.4847, (RMSE): 0.0042\n",
      "convergence dfGPdfNN Run 1/10, Epoch 322/1000, Training Loss (NLML): -958.3591, (RMSE): 0.0043\n",
      "convergence dfGPdfNN Run 1/10, Epoch 323/1000, Training Loss (NLML): -958.3224, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 324/1000, Training Loss (NLML): -958.2877, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 325/1000, Training Loss (NLML): -958.3014, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 326/1000, Training Loss (NLML): -958.2433, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 327/1000, Training Loss (NLML): -958.2104, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 328/1000, Training Loss (NLML): -958.1250, (RMSE): 0.0046\n",
      "convergence dfGPdfNN Run 1/10, Epoch 329/1000, Training Loss (NLML): -958.1497, (RMSE): 0.0046\n",
      "convergence dfGPdfNN Run 1/10, Epoch 330/1000, Training Loss (NLML): -958.2528, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 331/1000, Training Loss (NLML): -958.2866, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 332/1000, Training Loss (NLML): -958.2664, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 333/1000, Training Loss (NLML): -958.2817, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 334/1000, Training Loss (NLML): -958.2887, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 335/1000, Training Loss (NLML): -958.2988, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 336/1000, Training Loss (NLML): -958.3062, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 337/1000, Training Loss (NLML): -958.3149, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 338/1000, Training Loss (NLML): -958.2576, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 339/1000, Training Loss (NLML): -958.2649, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 340/1000, Training Loss (NLML): -958.2787, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 341/1000, Training Loss (NLML): -958.2877, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 342/1000, Training Loss (NLML): -958.2997, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 343/1000, Training Loss (NLML): -958.3130, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 344/1000, Training Loss (NLML): -958.3225, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 345/1000, Training Loss (NLML): -958.3442, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 346/1000, Training Loss (NLML): -958.3091, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 347/1000, Training Loss (NLML): -958.3215, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 348/1000, Training Loss (NLML): -958.3317, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 349/1000, Training Loss (NLML): -958.3446, (RMSE): 0.0045\n",
      "convergence dfGPdfNN Run 1/10, Epoch 350/1000, Training Loss (NLML): -958.3566, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 351/1000, Training Loss (NLML): -958.3542, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 352/1000, Training Loss (NLML): -958.3641, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 353/1000, Training Loss (NLML): -958.3801, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 354/1000, Training Loss (NLML): -958.4231, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 355/1000, Training Loss (NLML): -958.4393, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 356/1000, Training Loss (NLML): -958.4498, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 357/1000, Training Loss (NLML): -958.4475, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 358/1000, Training Loss (NLML): -958.4535, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 359/1000, Training Loss (NLML): -958.4562, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 360/1000, Training Loss (NLML): -958.4640, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 361/1000, Training Loss (NLML): -958.4948, (RMSE): 0.0044\n",
      "convergence dfGPdfNN Run 1/10, Epoch 362/1000, Training Loss (NLML): -958.5244, (RMSE): 0.0043\n",
      "convergence dfGPdfNN Run 1/10, Epoch 363/1000, Training Loss (NLML): -958.5389, (RMSE): 0.0043\n",
      "convergence dfGPdfNN Run 1/10, Epoch 364/1000, Training Loss (NLML): -958.5559, (RMSE): 0.0043\n",
      "convergence dfGPdfNN Run 1/10, Epoch 365/1000, Training Loss (NLML): -958.5773, (RMSE): 0.0043\n",
      "convergence dfGPdfNN Run 1/10, Epoch 366/1000, Training Loss (NLML): -958.5837, (RMSE): 0.0043\n",
      "convergence dfGPdfNN Run 1/10, Epoch 367/1000, Training Loss (NLML): -958.5891, (RMSE): 0.0043\n",
      "convergence dfGPdfNN Run 1/10, Epoch 368/1000, Training Loss (NLML): -958.6405, (RMSE): 0.0042\n",
      "Early stopping triggered after 368 epochs.\n",
      "\n",
      "--- Training Run 2/10 ---\n",
      "\n",
      "Start Training\n",
      "convergence dfGPdfNN Run 2/10, Epoch 1/1000, Training Loss (NLML): -884.1392\n",
      "convergence dfGPdfNN Run 2/10, Epoch 2/1000, Training Loss (NLML): -888.4862\n",
      "convergence dfGPdfNN Run 2/10, Epoch 3/1000, Training Loss (NLML): -891.8903\n",
      "convergence dfGPdfNN Run 2/10, Epoch 4/1000, Training Loss (NLML): -894.4111\n",
      "convergence dfGPdfNN Run 2/10, Epoch 5/1000, Training Loss (NLML): -896.5400\n",
      "convergence dfGPdfNN Run 2/10, Epoch 6/1000, Training Loss (NLML): -898.5034\n",
      "convergence dfGPdfNN Run 2/10, Epoch 7/1000, Training Loss (NLML): -900.2649\n",
      "convergence dfGPdfNN Run 2/10, Epoch 8/1000, Training Loss (NLML): -900.9675\n",
      "convergence dfGPdfNN Run 2/10, Epoch 9/1000, Training Loss (NLML): -903.3693\n",
      "convergence dfGPdfNN Run 2/10, Epoch 10/1000, Training Loss (NLML): -904.2816\n",
      "convergence dfGPdfNN Run 2/10, Epoch 11/1000, Training Loss (NLML): -905.8193\n",
      "convergence dfGPdfNN Run 2/10, Epoch 12/1000, Training Loss (NLML): -907.9288\n",
      "convergence dfGPdfNN Run 2/10, Epoch 13/1000, Training Loss (NLML): -909.6349\n",
      "convergence dfGPdfNN Run 2/10, Epoch 14/1000, Training Loss (NLML): -911.1056\n",
      "convergence dfGPdfNN Run 2/10, Epoch 15/1000, Training Loss (NLML): -912.5812\n",
      "convergence dfGPdfNN Run 2/10, Epoch 16/1000, Training Loss (NLML): -913.8862\n",
      "convergence dfGPdfNN Run 2/10, Epoch 17/1000, Training Loss (NLML): -915.0968\n",
      "convergence dfGPdfNN Run 2/10, Epoch 18/1000, Training Loss (NLML): -916.2499\n",
      "convergence dfGPdfNN Run 2/10, Epoch 19/1000, Training Loss (NLML): -917.1807\n",
      "convergence dfGPdfNN Run 2/10, Epoch 20/1000, Training Loss (NLML): -917.5493\n",
      "convergence dfGPdfNN Run 2/10, Epoch 21/1000, Training Loss (NLML): -918.6295\n",
      "convergence dfGPdfNN Run 2/10, Epoch 22/1000, Training Loss (NLML): -919.8401\n",
      "convergence dfGPdfNN Run 2/10, Epoch 23/1000, Training Loss (NLML): -920.8618\n",
      "convergence dfGPdfNN Run 2/10, Epoch 24/1000, Training Loss (NLML): -921.7246\n",
      "convergence dfGPdfNN Run 2/10, Epoch 25/1000, Training Loss (NLML): -922.7374\n",
      "convergence dfGPdfNN Run 2/10, Epoch 26/1000, Training Loss (NLML): -923.7542\n",
      "convergence dfGPdfNN Run 2/10, Epoch 27/1000, Training Loss (NLML): -924.7489\n",
      "convergence dfGPdfNN Run 2/10, Epoch 28/1000, Training Loss (NLML): -925.6122\n",
      "convergence dfGPdfNN Run 2/10, Epoch 29/1000, Training Loss (NLML): -926.4144\n",
      "convergence dfGPdfNN Run 2/10, Epoch 30/1000, Training Loss (NLML): -927.1986\n",
      "convergence dfGPdfNN Run 2/10, Epoch 31/1000, Training Loss (NLML): -927.8845\n",
      "convergence dfGPdfNN Run 2/10, Epoch 32/1000, Training Loss (NLML): -928.6355\n",
      "convergence dfGPdfNN Run 2/10, Epoch 33/1000, Training Loss (NLML): -929.3392\n",
      "convergence dfGPdfNN Run 2/10, Epoch 34/1000, Training Loss (NLML): -930.0126\n",
      "convergence dfGPdfNN Run 2/10, Epoch 35/1000, Training Loss (NLML): -930.6610\n",
      "convergence dfGPdfNN Run 2/10, Epoch 36/1000, Training Loss (NLML): -931.2747\n",
      "convergence dfGPdfNN Run 2/10, Epoch 37/1000, Training Loss (NLML): -931.8641\n",
      "convergence dfGPdfNN Run 2/10, Epoch 38/1000, Training Loss (NLML): -932.4507\n",
      "convergence dfGPdfNN Run 2/10, Epoch 39/1000, Training Loss (NLML): -933.0111\n",
      "convergence dfGPdfNN Run 2/10, Epoch 40/1000, Training Loss (NLML): -932.6791\n",
      "convergence dfGPdfNN Run 2/10, Epoch 41/1000, Training Loss (NLML): -932.7017\n",
      "convergence dfGPdfNN Run 2/10, Epoch 42/1000, Training Loss (NLML): -933.4495\n",
      "convergence dfGPdfNN Run 2/10, Epoch 43/1000, Training Loss (NLML): -934.3143\n",
      "convergence dfGPdfNN Run 2/10, Epoch 44/1000, Training Loss (NLML): -935.1333\n",
      "convergence dfGPdfNN Run 2/10, Epoch 45/1000, Training Loss (NLML): -935.8229\n",
      "convergence dfGPdfNN Run 2/10, Epoch 46/1000, Training Loss (NLML): -935.4401\n",
      "convergence dfGPdfNN Run 2/10, Epoch 47/1000, Training Loss (NLML): -935.8433\n",
      "convergence dfGPdfNN Run 2/10, Epoch 48/1000, Training Loss (NLML): -936.5574\n",
      "convergence dfGPdfNN Run 2/10, Epoch 49/1000, Training Loss (NLML): -937.1697\n",
      "convergence dfGPdfNN Run 2/10, Epoch 50/1000, Training Loss (NLML): -937.4215\n",
      "convergence dfGPdfNN Run 2/10, Epoch 51/1000, Training Loss (NLML): -938.2341\n",
      "convergence dfGPdfNN Run 2/10, Epoch 52/1000, Training Loss (NLML): -938.6252\n",
      "convergence dfGPdfNN Run 2/10, Epoch 53/1000, Training Loss (NLML): -938.9757\n",
      "convergence dfGPdfNN Run 2/10, Epoch 54/1000, Training Loss (NLML): -939.3551\n",
      "convergence dfGPdfNN Run 2/10, Epoch 55/1000, Training Loss (NLML): -939.7488\n",
      "convergence dfGPdfNN Run 2/10, Epoch 56/1000, Training Loss (NLML): -940.1638\n",
      "convergence dfGPdfNN Run 2/10, Epoch 57/1000, Training Loss (NLML): -940.3937\n",
      "convergence dfGPdfNN Run 2/10, Epoch 58/1000, Training Loss (NLML): -940.7716\n",
      "convergence dfGPdfNN Run 2/10, Epoch 59/1000, Training Loss (NLML): -941.1379\n",
      "convergence dfGPdfNN Run 2/10, Epoch 60/1000, Training Loss (NLML): -941.5439\n",
      "convergence dfGPdfNN Run 2/10, Epoch 61/1000, Training Loss (NLML): -941.8960\n",
      "convergence dfGPdfNN Run 2/10, Epoch 62/1000, Training Loss (NLML): -942.2166\n",
      "convergence dfGPdfNN Run 2/10, Epoch 63/1000, Training Loss (NLML): -942.5187\n",
      "convergence dfGPdfNN Run 2/10, Epoch 64/1000, Training Loss (NLML): -942.6304\n",
      "convergence dfGPdfNN Run 2/10, Epoch 65/1000, Training Loss (NLML): -942.6572\n",
      "convergence dfGPdfNN Run 2/10, Epoch 66/1000, Training Loss (NLML): -943.0045\n",
      "convergence dfGPdfNN Run 2/10, Epoch 67/1000, Training Loss (NLML): -943.4823\n",
      "convergence dfGPdfNN Run 2/10, Epoch 68/1000, Training Loss (NLML): -943.8713\n",
      "convergence dfGPdfNN Run 2/10, Epoch 69/1000, Training Loss (NLML): -944.1642\n",
      "convergence dfGPdfNN Run 2/10, Epoch 70/1000, Training Loss (NLML): -944.4237\n",
      "convergence dfGPdfNN Run 2/10, Epoch 71/1000, Training Loss (NLML): -944.6307\n",
      "convergence dfGPdfNN Run 2/10, Epoch 72/1000, Training Loss (NLML): -944.9451\n",
      "convergence dfGPdfNN Run 2/10, Epoch 73/1000, Training Loss (NLML): -945.2374\n",
      "convergence dfGPdfNN Run 2/10, Epoch 74/1000, Training Loss (NLML): -945.4055\n",
      "convergence dfGPdfNN Run 2/10, Epoch 75/1000, Training Loss (NLML): -945.8262\n",
      "convergence dfGPdfNN Run 2/10, Epoch 76/1000, Training Loss (NLML): -941.4781\n",
      "convergence dfGPdfNN Run 2/10, Epoch 77/1000, Training Loss (NLML): -938.9109\n",
      "convergence dfGPdfNN Run 2/10, Epoch 78/1000, Training Loss (NLML): -935.8883\n",
      "convergence dfGPdfNN Run 2/10, Epoch 79/1000, Training Loss (NLML): -937.2950\n",
      "convergence dfGPdfNN Run 2/10, Epoch 80/1000, Training Loss (NLML): -940.6377\n",
      "convergence dfGPdfNN Run 2/10, Epoch 81/1000, Training Loss (NLML): -943.8778\n",
      "convergence dfGPdfNN Run 2/10, Epoch 82/1000, Training Loss (NLML): -946.1470\n",
      "convergence dfGPdfNN Run 2/10, Epoch 83/1000, Training Loss (NLML): -946.9830\n",
      "convergence dfGPdfNN Run 2/10, Epoch 84/1000, Training Loss (NLML): -946.6055\n",
      "convergence dfGPdfNN Run 2/10, Epoch 85/1000, Training Loss (NLML): -946.2336\n",
      "convergence dfGPdfNN Run 2/10, Epoch 86/1000, Training Loss (NLML): -946.4335\n",
      "convergence dfGPdfNN Run 2/10, Epoch 87/1000, Training Loss (NLML): -947.0017\n",
      "convergence dfGPdfNN Run 2/10, Epoch 88/1000, Training Loss (NLML): -947.5354\n",
      "convergence dfGPdfNN Run 2/10, Epoch 89/1000, Training Loss (NLML): -947.8118\n",
      "convergence dfGPdfNN Run 2/10, Epoch 90/1000, Training Loss (NLML): -947.6787\n",
      "convergence dfGPdfNN Run 2/10, Epoch 91/1000, Training Loss (NLML): -948.5481\n",
      "convergence dfGPdfNN Run 2/10, Epoch 92/1000, Training Loss (NLML): -948.8546\n",
      "convergence dfGPdfNN Run 2/10, Epoch 93/1000, Training Loss (NLML): -949.0793\n",
      "convergence dfGPdfNN Run 2/10, Epoch 94/1000, Training Loss (NLML): -949.2687\n",
      "convergence dfGPdfNN Run 2/10, Epoch 95/1000, Training Loss (NLML): -949.1324\n",
      "convergence dfGPdfNN Run 2/10, Epoch 96/1000, Training Loss (NLML): -949.0305\n",
      "convergence dfGPdfNN Run 2/10, Epoch 97/1000, Training Loss (NLML): -946.2759\n",
      "convergence dfGPdfNN Run 2/10, Epoch 98/1000, Training Loss (NLML): -942.7100\n",
      "convergence dfGPdfNN Run 2/10, Epoch 99/1000, Training Loss (NLML): -945.7913\n",
      "convergence dfGPdfNN Run 2/10, Epoch 100/1000, Training Loss (NLML): -947.8796\n",
      "convergence dfGPdfNN Run 2/10, Epoch 101/1000, Training Loss (NLML): -948.5135\n",
      "convergence dfGPdfNN Run 2/10, Epoch 102/1000, Training Loss (NLML): -948.7762\n",
      "convergence dfGPdfNN Run 2/10, Epoch 103/1000, Training Loss (NLML): -949.3511\n",
      "convergence dfGPdfNN Run 2/10, Epoch 104/1000, Training Loss (NLML): -949.2986\n",
      "convergence dfGPdfNN Run 2/10, Epoch 105/1000, Training Loss (NLML): -949.4075\n",
      "convergence dfGPdfNN Run 2/10, Epoch 106/1000, Training Loss (NLML): -949.6696\n",
      "convergence dfGPdfNN Run 2/10, Epoch 107/1000, Training Loss (NLML): -949.6508\n",
      "convergence dfGPdfNN Run 2/10, Epoch 108/1000, Training Loss (NLML): -949.7371\n",
      "convergence dfGPdfNN Run 2/10, Epoch 109/1000, Training Loss (NLML): -949.7993\n",
      "convergence dfGPdfNN Run 2/10, Epoch 110/1000, Training Loss (NLML): -949.9652\n",
      "convergence dfGPdfNN Run 2/10, Epoch 111/1000, Training Loss (NLML): -950.0520\n",
      "convergence dfGPdfNN Run 2/10, Epoch 112/1000, Training Loss (NLML): -950.0496\n",
      "convergence dfGPdfNN Run 2/10, Epoch 113/1000, Training Loss (NLML): -950.2177\n",
      "convergence dfGPdfNN Run 2/10, Epoch 114/1000, Training Loss (NLML): -950.3857\n",
      "convergence dfGPdfNN Run 2/10, Epoch 115/1000, Training Loss (NLML): -950.5573\n",
      "convergence dfGPdfNN Run 2/10, Epoch 116/1000, Training Loss (NLML): -950.7557\n",
      "convergence dfGPdfNN Run 2/10, Epoch 117/1000, Training Loss (NLML): -950.9478\n",
      "convergence dfGPdfNN Run 2/10, Epoch 118/1000, Training Loss (NLML): -951.0614\n",
      "convergence dfGPdfNN Run 2/10, Epoch 119/1000, Training Loss (NLML): -951.1467\n",
      "convergence dfGPdfNN Run 2/10, Epoch 120/1000, Training Loss (NLML): -951.2554\n",
      "convergence dfGPdfNN Run 2/10, Epoch 121/1000, Training Loss (NLML): -951.3710\n",
      "convergence dfGPdfNN Run 2/10, Epoch 122/1000, Training Loss (NLML): -951.3480\n",
      "convergence dfGPdfNN Run 2/10, Epoch 123/1000, Training Loss (NLML): -951.4015\n",
      "convergence dfGPdfNN Run 2/10, Epoch 124/1000, Training Loss (NLML): -951.4147\n",
      "convergence dfGPdfNN Run 2/10, Epoch 125/1000, Training Loss (NLML): -951.5065\n",
      "convergence dfGPdfNN Run 2/10, Epoch 126/1000, Training Loss (NLML): -951.5786\n",
      "convergence dfGPdfNN Run 2/10, Epoch 127/1000, Training Loss (NLML): -951.6858\n",
      "convergence dfGPdfNN Run 2/10, Epoch 128/1000, Training Loss (NLML): -951.7946\n",
      "convergence dfGPdfNN Run 2/10, Epoch 129/1000, Training Loss (NLML): -952.3678\n",
      "convergence dfGPdfNN Run 2/10, Epoch 130/1000, Training Loss (NLML): -952.4220\n",
      "convergence dfGPdfNN Run 2/10, Epoch 131/1000, Training Loss (NLML): -952.5657\n",
      "convergence dfGPdfNN Run 2/10, Epoch 132/1000, Training Loss (NLML): -952.5841\n",
      "convergence dfGPdfNN Run 2/10, Epoch 133/1000, Training Loss (NLML): -952.5986\n",
      "convergence dfGPdfNN Run 2/10, Epoch 134/1000, Training Loss (NLML): -952.5803\n",
      "convergence dfGPdfNN Run 2/10, Epoch 135/1000, Training Loss (NLML): -952.4835\n",
      "convergence dfGPdfNN Run 2/10, Epoch 136/1000, Training Loss (NLML): -952.7825\n",
      "convergence dfGPdfNN Run 2/10, Epoch 137/1000, Training Loss (NLML): -952.9329\n",
      "convergence dfGPdfNN Run 2/10, Epoch 138/1000, Training Loss (NLML): -953.0382\n",
      "convergence dfGPdfNN Run 2/10, Epoch 139/1000, Training Loss (NLML): -953.1587\n",
      "convergence dfGPdfNN Run 2/10, Epoch 140/1000, Training Loss (NLML): -953.3057\n",
      "convergence dfGPdfNN Run 2/10, Epoch 141/1000, Training Loss (NLML): -953.1177\n",
      "convergence dfGPdfNN Run 2/10, Epoch 142/1000, Training Loss (NLML): -953.4656\n",
      "convergence dfGPdfNN Run 2/10, Epoch 143/1000, Training Loss (NLML): -953.5441\n",
      "convergence dfGPdfNN Run 2/10, Epoch 144/1000, Training Loss (NLML): -953.6326\n",
      "convergence dfGPdfNN Run 2/10, Epoch 145/1000, Training Loss (NLML): -953.9126\n",
      "convergence dfGPdfNN Run 2/10, Epoch 146/1000, Training Loss (NLML): -954.0845\n",
      "convergence dfGPdfNN Run 2/10, Epoch 147/1000, Training Loss (NLML): -954.1047\n",
      "convergence dfGPdfNN Run 2/10, Epoch 148/1000, Training Loss (NLML): -954.1453\n",
      "convergence dfGPdfNN Run 2/10, Epoch 149/1000, Training Loss (NLML): -938.4696\n",
      "convergence dfGPdfNN Run 2/10, Epoch 150/1000, Training Loss (NLML): -934.1567\n",
      "convergence dfGPdfNN Run 2/10, Epoch 151/1000, Training Loss (NLML): -935.1003\n",
      "convergence dfGPdfNN Run 2/10, Epoch 152/1000, Training Loss (NLML): -945.3390\n",
      "convergence dfGPdfNN Run 2/10, Epoch 153/1000, Training Loss (NLML): -951.6500\n",
      "convergence dfGPdfNN Run 2/10, Epoch 154/1000, Training Loss (NLML): -951.9564\n",
      "convergence dfGPdfNN Run 2/10, Epoch 155/1000, Training Loss (NLML): -951.7491\n",
      "convergence dfGPdfNN Run 2/10, Epoch 156/1000, Training Loss (NLML): -951.8667\n",
      "convergence dfGPdfNN Run 2/10, Epoch 157/1000, Training Loss (NLML): -952.0035\n",
      "convergence dfGPdfNN Run 2/10, Epoch 158/1000, Training Loss (NLML): -952.0995\n",
      "convergence dfGPdfNN Run 2/10, Epoch 159/1000, Training Loss (NLML): -951.0109\n",
      "convergence dfGPdfNN Run 2/10, Epoch 160/1000, Training Loss (NLML): -950.0646\n",
      "convergence dfGPdfNN Run 2/10, Epoch 161/1000, Training Loss (NLML): -951.8894\n",
      "convergence dfGPdfNN Run 2/10, Epoch 162/1000, Training Loss (NLML): -952.4316\n",
      "convergence dfGPdfNN Run 2/10, Epoch 163/1000, Training Loss (NLML): -952.1536\n",
      "convergence dfGPdfNN Run 2/10, Epoch 164/1000, Training Loss (NLML): -952.1614\n",
      "convergence dfGPdfNN Run 2/10, Epoch 165/1000, Training Loss (NLML): -952.1945\n",
      "convergence dfGPdfNN Run 2/10, Epoch 166/1000, Training Loss (NLML): -952.2982\n",
      "convergence dfGPdfNN Run 2/10, Epoch 167/1000, Training Loss (NLML): -952.3792\n",
      "convergence dfGPdfNN Run 2/10, Epoch 168/1000, Training Loss (NLML): -952.4281\n",
      "convergence dfGPdfNN Run 2/10, Epoch 169/1000, Training Loss (NLML): -952.5286\n",
      "convergence dfGPdfNN Run 2/10, Epoch 170/1000, Training Loss (NLML): -952.2020\n",
      "convergence dfGPdfNN Run 2/10, Epoch 171/1000, Training Loss (NLML): -952.6147\n",
      "convergence dfGPdfNN Run 2/10, Epoch 172/1000, Training Loss (NLML): -952.6622\n",
      "convergence dfGPdfNN Run 2/10, Epoch 173/1000, Training Loss (NLML): -952.6234\n",
      "convergence dfGPdfNN Run 2/10, Epoch 174/1000, Training Loss (NLML): -952.5167\n",
      "convergence dfGPdfNN Run 2/10, Epoch 175/1000, Training Loss (NLML): -952.5421\n",
      "convergence dfGPdfNN Run 2/10, Epoch 176/1000, Training Loss (NLML): -952.5757\n",
      "convergence dfGPdfNN Run 2/10, Epoch 177/1000, Training Loss (NLML): -952.6343\n",
      "convergence dfGPdfNN Run 2/10, Epoch 178/1000, Training Loss (NLML): -952.7463\n",
      "convergence dfGPdfNN Run 2/10, Epoch 179/1000, Training Loss (NLML): -952.9113\n",
      "convergence dfGPdfNN Run 2/10, Epoch 180/1000, Training Loss (NLML): -953.0247\n",
      "convergence dfGPdfNN Run 2/10, Epoch 181/1000, Training Loss (NLML): -953.1422\n",
      "convergence dfGPdfNN Run 2/10, Epoch 182/1000, Training Loss (NLML): -953.2468\n",
      "convergence dfGPdfNN Run 2/10, Epoch 183/1000, Training Loss (NLML): -953.2513\n",
      "convergence dfGPdfNN Run 2/10, Epoch 184/1000, Training Loss (NLML): -953.3241\n",
      "convergence dfGPdfNN Run 2/10, Epoch 185/1000, Training Loss (NLML): -953.4377\n",
      "convergence dfGPdfNN Run 2/10, Epoch 186/1000, Training Loss (NLML): -953.5171\n",
      "convergence dfGPdfNN Run 2/10, Epoch 187/1000, Training Loss (NLML): -953.5768\n",
      "convergence dfGPdfNN Run 2/10, Epoch 188/1000, Training Loss (NLML): -953.7278\n",
      "convergence dfGPdfNN Run 2/10, Epoch 189/1000, Training Loss (NLML): -953.7753\n",
      "convergence dfGPdfNN Run 2/10, Epoch 190/1000, Training Loss (NLML): -953.8853\n",
      "convergence dfGPdfNN Run 2/10, Epoch 191/1000, Training Loss (NLML): -954.0483\n",
      "convergence dfGPdfNN Run 2/10, Epoch 192/1000, Training Loss (NLML): -954.0840\n",
      "convergence dfGPdfNN Run 2/10, Epoch 193/1000, Training Loss (NLML): -954.1671\n",
      "convergence dfGPdfNN Run 2/10, Epoch 194/1000, Training Loss (NLML): -954.2560\n",
      "convergence dfGPdfNN Run 2/10, Epoch 195/1000, Training Loss (NLML): -954.3564\n",
      "convergence dfGPdfNN Run 2/10, Epoch 196/1000, Training Loss (NLML): -954.4069\n",
      "convergence dfGPdfNN Run 2/10, Epoch 197/1000, Training Loss (NLML): -954.4590\n",
      "convergence dfGPdfNN Run 2/10, Epoch 198/1000, Training Loss (NLML): -954.4596\n",
      "convergence dfGPdfNN Run 2/10, Epoch 199/1000, Training Loss (NLML): -954.3790\n",
      "convergence dfGPdfNN Run 2/10, Epoch 200/1000, Training Loss (NLML): -954.2498\n",
      "convergence dfGPdfNN Run 2/10, Epoch 201/1000, Training Loss (NLML): -954.4115\n",
      "convergence dfGPdfNN Run 2/10, Epoch 202/1000, Training Loss (NLML): -954.4680\n",
      "convergence dfGPdfNN Run 2/10, Epoch 203/1000, Training Loss (NLML): -953.3136\n",
      "convergence dfGPdfNN Run 2/10, Epoch 204/1000, Training Loss (NLML): -949.0040\n",
      "convergence dfGPdfNN Run 2/10, Epoch 205/1000, Training Loss (NLML): -950.4375\n",
      "convergence dfGPdfNN Run 2/10, Epoch 206/1000, Training Loss (NLML): -952.8220\n",
      "convergence dfGPdfNN Run 2/10, Epoch 207/1000, Training Loss (NLML): -953.5277\n",
      "convergence dfGPdfNN Run 2/10, Epoch 208/1000, Training Loss (NLML): -954.6201\n",
      "convergence dfGPdfNN Run 2/10, Epoch 209/1000, Training Loss (NLML): -954.7830\n",
      "convergence dfGPdfNN Run 2/10, Epoch 210/1000, Training Loss (NLML): -954.9269\n",
      "convergence dfGPdfNN Run 2/10, Epoch 211/1000, Training Loss (NLML): -954.8516\n",
      "convergence dfGPdfNN Run 2/10, Epoch 212/1000, Training Loss (NLML): -954.7261\n",
      "convergence dfGPdfNN Run 2/10, Epoch 213/1000, Training Loss (NLML): -954.7272\n",
      "convergence dfGPdfNN Run 2/10, Epoch 214/1000, Training Loss (NLML): -954.7609\n",
      "convergence dfGPdfNN Run 2/10, Epoch 215/1000, Training Loss (NLML): -954.8383\n",
      "convergence dfGPdfNN Run 2/10, Epoch 216/1000, Training Loss (NLML): -954.9709\n",
      "convergence dfGPdfNN Run 2/10, Epoch 217/1000, Training Loss (NLML): -955.1329\n",
      "convergence dfGPdfNN Run 2/10, Epoch 218/1000, Training Loss (NLML): -955.3032\n",
      "convergence dfGPdfNN Run 2/10, Epoch 219/1000, Training Loss (NLML): -955.3533\n",
      "convergence dfGPdfNN Run 2/10, Epoch 220/1000, Training Loss (NLML): -955.3832\n",
      "convergence dfGPdfNN Run 2/10, Epoch 221/1000, Training Loss (NLML): -955.3413\n",
      "convergence dfGPdfNN Run 2/10, Epoch 222/1000, Training Loss (NLML): -955.2300\n",
      "convergence dfGPdfNN Run 2/10, Epoch 223/1000, Training Loss (NLML): -955.3522\n",
      "convergence dfGPdfNN Run 2/10, Epoch 224/1000, Training Loss (NLML): -955.4636\n",
      "convergence dfGPdfNN Run 2/10, Epoch 225/1000, Training Loss (NLML): -955.5538\n",
      "convergence dfGPdfNN Run 2/10, Epoch 226/1000, Training Loss (NLML): -955.6119\n",
      "convergence dfGPdfNN Run 2/10, Epoch 227/1000, Training Loss (NLML): -938.8821\n",
      "convergence dfGPdfNN Run 2/10, Epoch 228/1000, Training Loss (NLML): -934.3165\n",
      "convergence dfGPdfNN Run 2/10, Epoch 229/1000, Training Loss (NLML): -955.1128\n",
      "convergence dfGPdfNN Run 2/10, Epoch 230/1000, Training Loss (NLML): -954.8199\n",
      "convergence dfGPdfNN Run 2/10, Epoch 231/1000, Training Loss (NLML): -954.6681\n",
      "convergence dfGPdfNN Run 2/10, Epoch 232/1000, Training Loss (NLML): -954.6116\n",
      "convergence dfGPdfNN Run 2/10, Epoch 233/1000, Training Loss (NLML): -954.5104\n",
      "convergence dfGPdfNN Run 2/10, Epoch 234/1000, Training Loss (NLML): -954.4236\n",
      "convergence dfGPdfNN Run 2/10, Epoch 235/1000, Training Loss (NLML): -954.3989\n",
      "convergence dfGPdfNN Run 2/10, Epoch 236/1000, Training Loss (NLML): -954.3401\n",
      "convergence dfGPdfNN Run 2/10, Epoch 237/1000, Training Loss (NLML): -954.2825\n",
      "convergence dfGPdfNN Run 2/10, Epoch 238/1000, Training Loss (NLML): -954.2333\n",
      "convergence dfGPdfNN Run 2/10, Epoch 239/1000, Training Loss (NLML): -954.2277\n",
      "convergence dfGPdfNN Run 2/10, Epoch 240/1000, Training Loss (NLML): -954.2198\n",
      "convergence dfGPdfNN Run 2/10, Epoch 241/1000, Training Loss (NLML): -954.2888\n",
      "convergence dfGPdfNN Run 2/10, Epoch 242/1000, Training Loss (NLML): -954.3280\n",
      "convergence dfGPdfNN Run 2/10, Epoch 243/1000, Training Loss (NLML): -954.3650\n",
      "convergence dfGPdfNN Run 2/10, Epoch 244/1000, Training Loss (NLML): -954.3749\n",
      "convergence dfGPdfNN Run 2/10, Epoch 245/1000, Training Loss (NLML): -954.4336\n",
      "convergence dfGPdfNN Run 2/10, Epoch 246/1000, Training Loss (NLML): -954.5104\n",
      "convergence dfGPdfNN Run 2/10, Epoch 247/1000, Training Loss (NLML): -953.9905\n",
      "convergence dfGPdfNN Run 2/10, Epoch 248/1000, Training Loss (NLML): -953.6650\n",
      "convergence dfGPdfNN Run 2/10, Epoch 249/1000, Training Loss (NLML): -953.9517\n",
      "convergence dfGPdfNN Run 2/10, Epoch 250/1000, Training Loss (NLML): -954.3372\n",
      "convergence dfGPdfNN Run 2/10, Epoch 251/1000, Training Loss (NLML): -954.4749\n",
      "convergence dfGPdfNN Run 2/10, Epoch 252/1000, Training Loss (NLML): -954.7255\n",
      "convergence dfGPdfNN Run 2/10, Epoch 253/1000, Training Loss (NLML): -954.8556\n",
      "convergence dfGPdfNN Run 2/10, Epoch 254/1000, Training Loss (NLML): -954.9696\n",
      "convergence dfGPdfNN Run 2/10, Epoch 255/1000, Training Loss (NLML): -955.0011\n",
      "convergence dfGPdfNN Run 2/10, Epoch 256/1000, Training Loss (NLML): -955.0908\n",
      "convergence dfGPdfNN Run 2/10, Epoch 257/1000, Training Loss (NLML): -955.1049\n",
      "convergence dfGPdfNN Run 2/10, Epoch 258/1000, Training Loss (NLML): -955.1158\n",
      "convergence dfGPdfNN Run 2/10, Epoch 259/1000, Training Loss (NLML): -955.1453\n",
      "convergence dfGPdfNN Run 2/10, Epoch 260/1000, Training Loss (NLML): -955.2008\n",
      "convergence dfGPdfNN Run 2/10, Epoch 261/1000, Training Loss (NLML): -955.2644\n",
      "convergence dfGPdfNN Run 2/10, Epoch 262/1000, Training Loss (NLML): -955.2933\n",
      "convergence dfGPdfNN Run 2/10, Epoch 263/1000, Training Loss (NLML): -955.3608\n",
      "convergence dfGPdfNN Run 2/10, Epoch 264/1000, Training Loss (NLML): -955.4368\n",
      "convergence dfGPdfNN Run 2/10, Epoch 265/1000, Training Loss (NLML): -955.5079\n",
      "convergence dfGPdfNN Run 2/10, Epoch 266/1000, Training Loss (NLML): -955.5620\n",
      "convergence dfGPdfNN Run 2/10, Epoch 267/1000, Training Loss (NLML): -955.6140\n",
      "convergence dfGPdfNN Run 2/10, Epoch 268/1000, Training Loss (NLML): -955.6647\n",
      "convergence dfGPdfNN Run 2/10, Epoch 269/1000, Training Loss (NLML): -955.7214\n",
      "convergence dfGPdfNN Run 2/10, Epoch 270/1000, Training Loss (NLML): -955.7795\n",
      "convergence dfGPdfNN Run 2/10, Epoch 271/1000, Training Loss (NLML): -955.8547\n",
      "convergence dfGPdfNN Run 2/10, Epoch 272/1000, Training Loss (NLML): -955.9163\n",
      "convergence dfGPdfNN Run 2/10, Epoch 273/1000, Training Loss (NLML): -956.0723\n",
      "convergence dfGPdfNN Run 2/10, Epoch 274/1000, Training Loss (NLML): -956.1361\n",
      "convergence dfGPdfNN Run 2/10, Epoch 275/1000, Training Loss (NLML): -956.1959\n",
      "convergence dfGPdfNN Run 2/10, Epoch 276/1000, Training Loss (NLML): -956.2517\n",
      "convergence dfGPdfNN Run 2/10, Epoch 277/1000, Training Loss (NLML): -956.3420\n",
      "convergence dfGPdfNN Run 2/10, Epoch 278/1000, Training Loss (NLML): -956.3163\n",
      "convergence dfGPdfNN Run 2/10, Epoch 279/1000, Training Loss (NLML): -956.3507\n",
      "convergence dfGPdfNN Run 2/10, Epoch 280/1000, Training Loss (NLML): -956.4590\n",
      "convergence dfGPdfNN Run 2/10, Epoch 281/1000, Training Loss (NLML): -956.5841\n",
      "convergence dfGPdfNN Run 2/10, Epoch 282/1000, Training Loss (NLML): -956.6237\n",
      "convergence dfGPdfNN Run 2/10, Epoch 283/1000, Training Loss (NLML): -956.6195\n",
      "convergence dfGPdfNN Run 2/10, Epoch 284/1000, Training Loss (NLML): -956.7686\n",
      "convergence dfGPdfNN Run 2/10, Epoch 285/1000, Training Loss (NLML): -956.8326\n",
      "convergence dfGPdfNN Run 2/10, Epoch 286/1000, Training Loss (NLML): -956.8977\n",
      "convergence dfGPdfNN Run 2/10, Epoch 287/1000, Training Loss (NLML): -956.9556\n",
      "convergence dfGPdfNN Run 2/10, Epoch 288/1000, Training Loss (NLML): -957.0397\n",
      "convergence dfGPdfNN Run 2/10, Epoch 289/1000, Training Loss (NLML): -957.0814\n",
      "convergence dfGPdfNN Run 2/10, Epoch 290/1000, Training Loss (NLML): -957.1311\n",
      "convergence dfGPdfNN Run 2/10, Epoch 291/1000, Training Loss (NLML): -957.1786\n",
      "convergence dfGPdfNN Run 2/10, Epoch 292/1000, Training Loss (NLML): -957.2865\n",
      "convergence dfGPdfNN Run 2/10, Epoch 293/1000, Training Loss (NLML): -957.3677\n",
      "convergence dfGPdfNN Run 2/10, Epoch 294/1000, Training Loss (NLML): -957.4432\n",
      "convergence dfGPdfNN Run 2/10, Epoch 295/1000, Training Loss (NLML): -957.4430\n",
      "convergence dfGPdfNN Run 2/10, Epoch 296/1000, Training Loss (NLML): -957.5061\n",
      "convergence dfGPdfNN Run 2/10, Epoch 297/1000, Training Loss (NLML): -957.5634\n",
      "convergence dfGPdfNN Run 2/10, Epoch 298/1000, Training Loss (NLML): -957.6178\n",
      "convergence dfGPdfNN Run 2/10, Epoch 299/1000, Training Loss (NLML): -957.6661\n",
      "convergence dfGPdfNN Run 2/10, Epoch 300/1000, Training Loss (NLML): -957.7861\n",
      "convergence dfGPdfNN Run 2/10, Epoch 301/1000, Training Loss (NLML): -957.8199\n",
      "convergence dfGPdfNN Run 2/10, Epoch 302/1000, Training Loss (NLML): -957.8658\n",
      "convergence dfGPdfNN Run 2/10, Epoch 303/1000, Training Loss (NLML): -957.8875\n",
      "convergence dfGPdfNN Run 2/10, Epoch 304/1000, Training Loss (NLML): -957.3560\n",
      "convergence dfGPdfNN Run 2/10, Epoch 305/1000, Training Loss (NLML): -957.8506\n",
      "convergence dfGPdfNN Run 2/10, Epoch 306/1000, Training Loss (NLML): -957.8634\n",
      "convergence dfGPdfNN Run 2/10, Epoch 307/1000, Training Loss (NLML): -957.9592\n",
      "convergence dfGPdfNN Run 2/10, Epoch 308/1000, Training Loss (NLML): -957.9838\n",
      "convergence dfGPdfNN Run 2/10, Epoch 309/1000, Training Loss (NLML): -958.0063\n",
      "convergence dfGPdfNN Run 2/10, Epoch 310/1000, Training Loss (NLML): -958.0200\n",
      "convergence dfGPdfNN Run 2/10, Epoch 311/1000, Training Loss (NLML): -958.0314\n",
      "convergence dfGPdfNN Run 2/10, Epoch 312/1000, Training Loss (NLML): -958.0574\n",
      "convergence dfGPdfNN Run 2/10, Epoch 313/1000, Training Loss (NLML): -958.0712\n",
      "convergence dfGPdfNN Run 2/10, Epoch 314/1000, Training Loss (NLML): -958.0802\n",
      "convergence dfGPdfNN Run 2/10, Epoch 315/1000, Training Loss (NLML): -958.1067\n",
      "convergence dfGPdfNN Run 2/10, Epoch 316/1000, Training Loss (NLML): -958.1179\n",
      "convergence dfGPdfNN Run 2/10, Epoch 317/1000, Training Loss (NLML): -958.1279\n",
      "convergence dfGPdfNN Run 2/10, Epoch 318/1000, Training Loss (NLML): -958.1140\n",
      "convergence dfGPdfNN Run 2/10, Epoch 319/1000, Training Loss (NLML): -958.1262\n",
      "convergence dfGPdfNN Run 2/10, Epoch 320/1000, Training Loss (NLML): -958.1349\n",
      "convergence dfGPdfNN Run 2/10, Epoch 321/1000, Training Loss (NLML): -958.1375\n",
      "convergence dfGPdfNN Run 2/10, Epoch 322/1000, Training Loss (NLML): -958.1464\n",
      "convergence dfGPdfNN Run 2/10, Epoch 323/1000, Training Loss (NLML): -958.1598\n",
      "convergence dfGPdfNN Run 2/10, Epoch 324/1000, Training Loss (NLML): -958.1721\n",
      "convergence dfGPdfNN Run 2/10, Epoch 325/1000, Training Loss (NLML): -958.1862\n",
      "convergence dfGPdfNN Run 2/10, Epoch 326/1000, Training Loss (NLML): -958.1976\n",
      "convergence dfGPdfNN Run 2/10, Epoch 327/1000, Training Loss (NLML): -958.2106\n",
      "convergence dfGPdfNN Run 2/10, Epoch 328/1000, Training Loss (NLML): -958.2272\n",
      "convergence dfGPdfNN Run 2/10, Epoch 329/1000, Training Loss (NLML): -958.2544\n",
      "convergence dfGPdfNN Run 2/10, Epoch 330/1000, Training Loss (NLML): -958.2571\n",
      "convergence dfGPdfNN Run 2/10, Epoch 331/1000, Training Loss (NLML): -958.2606\n",
      "convergence dfGPdfNN Run 2/10, Epoch 332/1000, Training Loss (NLML): -958.2714\n",
      "convergence dfGPdfNN Run 2/10, Epoch 333/1000, Training Loss (NLML): -958.2821\n",
      "convergence dfGPdfNN Run 2/10, Epoch 334/1000, Training Loss (NLML): -958.2926\n",
      "convergence dfGPdfNN Run 2/10, Epoch 335/1000, Training Loss (NLML): -958.3059\n",
      "convergence dfGPdfNN Run 2/10, Epoch 336/1000, Training Loss (NLML): -958.3191\n",
      "convergence dfGPdfNN Run 2/10, Epoch 337/1000, Training Loss (NLML): -958.3314\n",
      "convergence dfGPdfNN Run 2/10, Epoch 338/1000, Training Loss (NLML): -958.3416\n",
      "convergence dfGPdfNN Run 2/10, Epoch 339/1000, Training Loss (NLML): -958.3531\n",
      "convergence dfGPdfNN Run 2/10, Epoch 340/1000, Training Loss (NLML): -958.3646\n",
      "convergence dfGPdfNN Run 2/10, Epoch 341/1000, Training Loss (NLML): -958.3755\n",
      "convergence dfGPdfNN Run 2/10, Epoch 342/1000, Training Loss (NLML): -958.3997\n",
      "convergence dfGPdfNN Run 2/10, Epoch 343/1000, Training Loss (NLML): -958.4093\n",
      "convergence dfGPdfNN Run 2/10, Epoch 344/1000, Training Loss (NLML): -958.4197\n",
      "convergence dfGPdfNN Run 2/10, Epoch 345/1000, Training Loss (NLML): -958.4293\n",
      "convergence dfGPdfNN Run 2/10, Epoch 346/1000, Training Loss (NLML): -958.4390\n",
      "convergence dfGPdfNN Run 2/10, Epoch 347/1000, Training Loss (NLML): -958.4492\n",
      "convergence dfGPdfNN Run 2/10, Epoch 348/1000, Training Loss (NLML): -958.4464\n",
      "convergence dfGPdfNN Run 2/10, Epoch 349/1000, Training Loss (NLML): -958.4541\n",
      "convergence dfGPdfNN Run 2/10, Epoch 350/1000, Training Loss (NLML): -958.4658\n",
      "convergence dfGPdfNN Run 2/10, Epoch 351/1000, Training Loss (NLML): -958.4753\n",
      "convergence dfGPdfNN Run 2/10, Epoch 352/1000, Training Loss (NLML): -958.5000\n",
      "convergence dfGPdfNN Run 2/10, Epoch 353/1000, Training Loss (NLML): -958.5085\n",
      "convergence dfGPdfNN Run 2/10, Epoch 354/1000, Training Loss (NLML): -958.5198\n",
      "convergence dfGPdfNN Run 2/10, Epoch 355/1000, Training Loss (NLML): -958.5283\n",
      "convergence dfGPdfNN Run 2/10, Epoch 356/1000, Training Loss (NLML): -958.5383\n",
      "convergence dfGPdfNN Run 2/10, Epoch 357/1000, Training Loss (NLML): -958.5397\n",
      "convergence dfGPdfNN Run 2/10, Epoch 358/1000, Training Loss (NLML): -958.5455\n",
      "convergence dfGPdfNN Run 2/10, Epoch 359/1000, Training Loss (NLML): -958.5520\n",
      "convergence dfGPdfNN Run 2/10, Epoch 360/1000, Training Loss (NLML): -958.5615\n",
      "convergence dfGPdfNN Run 2/10, Epoch 361/1000, Training Loss (NLML): -958.5692\n",
      "convergence dfGPdfNN Run 2/10, Epoch 362/1000, Training Loss (NLML): -958.5934\n",
      "convergence dfGPdfNN Run 2/10, Epoch 363/1000, Training Loss (NLML): -958.6014\n",
      "convergence dfGPdfNN Run 2/10, Epoch 364/1000, Training Loss (NLML): -958.6111\n",
      "convergence dfGPdfNN Run 2/10, Epoch 365/1000, Training Loss (NLML): -958.6190\n",
      "convergence dfGPdfNN Run 2/10, Epoch 366/1000, Training Loss (NLML): -958.6187\n",
      "convergence dfGPdfNN Run 2/10, Epoch 367/1000, Training Loss (NLML): -958.6243\n",
      "convergence dfGPdfNN Run 2/10, Epoch 368/1000, Training Loss (NLML): -958.6331\n",
      "convergence dfGPdfNN Run 2/10, Epoch 369/1000, Training Loss (NLML): -958.6421\n",
      "convergence dfGPdfNN Run 2/10, Epoch 370/1000, Training Loss (NLML): -958.6531\n",
      "convergence dfGPdfNN Run 2/10, Epoch 371/1000, Training Loss (NLML): -958.6642\n",
      "convergence dfGPdfNN Run 2/10, Epoch 372/1000, Training Loss (NLML): -958.6798\n",
      "convergence dfGPdfNN Run 2/10, Epoch 373/1000, Training Loss (NLML): -958.6886\n",
      "convergence dfGPdfNN Run 2/10, Epoch 374/1000, Training Loss (NLML): -958.6973\n",
      "convergence dfGPdfNN Run 2/10, Epoch 375/1000, Training Loss (NLML): -958.6929\n",
      "convergence dfGPdfNN Run 2/10, Epoch 376/1000, Training Loss (NLML): -958.7006\n",
      "convergence dfGPdfNN Run 2/10, Epoch 377/1000, Training Loss (NLML): -958.7096\n",
      "convergence dfGPdfNN Run 2/10, Epoch 378/1000, Training Loss (NLML): -958.7184\n",
      "convergence dfGPdfNN Run 2/10, Epoch 379/1000, Training Loss (NLML): -958.7399\n",
      "convergence dfGPdfNN Run 2/10, Epoch 380/1000, Training Loss (NLML): -958.7480\n",
      "convergence dfGPdfNN Run 2/10, Epoch 381/1000, Training Loss (NLML): -958.7432\n",
      "convergence dfGPdfNN Run 2/10, Epoch 382/1000, Training Loss (NLML): -958.7520\n",
      "convergence dfGPdfNN Run 2/10, Epoch 383/1000, Training Loss (NLML): -958.7596\n",
      "convergence dfGPdfNN Run 2/10, Epoch 384/1000, Training Loss (NLML): -958.7738\n",
      "convergence dfGPdfNN Run 2/10, Epoch 385/1000, Training Loss (NLML): -958.7888\n",
      "convergence dfGPdfNN Run 2/10, Epoch 386/1000, Training Loss (NLML): -958.7852\n",
      "convergence dfGPdfNN Run 2/10, Epoch 387/1000, Training Loss (NLML): -958.7933\n",
      "convergence dfGPdfNN Run 2/10, Epoch 388/1000, Training Loss (NLML): -958.8073\n",
      "convergence dfGPdfNN Run 2/10, Epoch 389/1000, Training Loss (NLML): -958.8134\n",
      "convergence dfGPdfNN Run 2/10, Epoch 390/1000, Training Loss (NLML): -958.8230\n",
      "convergence dfGPdfNN Run 2/10, Epoch 391/1000, Training Loss (NLML): -958.8304\n",
      "convergence dfGPdfNN Run 2/10, Epoch 392/1000, Training Loss (NLML): -958.8380\n",
      "convergence dfGPdfNN Run 2/10, Epoch 393/1000, Training Loss (NLML): -958.8462\n",
      "convergence dfGPdfNN Run 2/10, Epoch 394/1000, Training Loss (NLML): -958.8474\n",
      "convergence dfGPdfNN Run 2/10, Epoch 395/1000, Training Loss (NLML): -958.8622\n",
      "convergence dfGPdfNN Run 2/10, Epoch 396/1000, Training Loss (NLML): -958.8759\n",
      "convergence dfGPdfNN Run 2/10, Epoch 397/1000, Training Loss (NLML): -958.8723\n",
      "convergence dfGPdfNN Run 2/10, Epoch 398/1000, Training Loss (NLML): -958.8853\n",
      "convergence dfGPdfNN Run 2/10, Epoch 399/1000, Training Loss (NLML): -958.8914\n",
      "convergence dfGPdfNN Run 2/10, Epoch 400/1000, Training Loss (NLML): -958.9010\n",
      "convergence dfGPdfNN Run 2/10, Epoch 401/1000, Training Loss (NLML): -958.9153\n",
      "convergence dfGPdfNN Run 2/10, Epoch 402/1000, Training Loss (NLML): -958.9108\n",
      "convergence dfGPdfNN Run 2/10, Epoch 403/1000, Training Loss (NLML): -958.9172\n",
      "convergence dfGPdfNN Run 2/10, Epoch 404/1000, Training Loss (NLML): -958.9371\n",
      "convergence dfGPdfNN Run 2/10, Epoch 405/1000, Training Loss (NLML): -958.9375\n",
      "convergence dfGPdfNN Run 2/10, Epoch 406/1000, Training Loss (NLML): -958.9456\n",
      "convergence dfGPdfNN Run 2/10, Epoch 407/1000, Training Loss (NLML): -958.9523\n",
      "convergence dfGPdfNN Run 2/10, Epoch 408/1000, Training Loss (NLML): -958.9542\n",
      "convergence dfGPdfNN Run 2/10, Epoch 409/1000, Training Loss (NLML): -958.9673\n",
      "convergence dfGPdfNN Run 2/10, Epoch 410/1000, Training Loss (NLML): -958.9810\n",
      "convergence dfGPdfNN Run 2/10, Epoch 411/1000, Training Loss (NLML): -958.9819\n",
      "convergence dfGPdfNN Run 2/10, Epoch 412/1000, Training Loss (NLML): -958.9844\n",
      "convergence dfGPdfNN Run 2/10, Epoch 413/1000, Training Loss (NLML): -958.9957\n",
      "convergence dfGPdfNN Run 2/10, Epoch 414/1000, Training Loss (NLML): -959.0027\n",
      "convergence dfGPdfNN Run 2/10, Epoch 415/1000, Training Loss (NLML): -959.0166\n",
      "convergence dfGPdfNN Run 2/10, Epoch 416/1000, Training Loss (NLML): -959.0109\n",
      "convergence dfGPdfNN Run 2/10, Epoch 417/1000, Training Loss (NLML): -959.0181\n",
      "convergence dfGPdfNN Run 2/10, Epoch 418/1000, Training Loss (NLML): -959.0365\n",
      "convergence dfGPdfNN Run 2/10, Epoch 419/1000, Training Loss (NLML): -959.0446\n",
      "convergence dfGPdfNN Run 2/10, Epoch 420/1000, Training Loss (NLML): -959.0382\n",
      "convergence dfGPdfNN Run 2/10, Epoch 421/1000, Training Loss (NLML): -959.0468\n",
      "convergence dfGPdfNN Run 2/10, Epoch 422/1000, Training Loss (NLML): -959.0568\n",
      "convergence dfGPdfNN Run 2/10, Epoch 423/1000, Training Loss (NLML): -959.0713\n",
      "convergence dfGPdfNN Run 2/10, Epoch 424/1000, Training Loss (NLML): -959.0781\n",
      "convergence dfGPdfNN Run 2/10, Epoch 425/1000, Training Loss (NLML): -959.0775\n",
      "convergence dfGPdfNN Run 2/10, Epoch 426/1000, Training Loss (NLML): -959.0801\n",
      "convergence dfGPdfNN Run 2/10, Epoch 427/1000, Training Loss (NLML): -959.0928\n",
      "convergence dfGPdfNN Run 2/10, Epoch 428/1000, Training Loss (NLML): -959.1063\n",
      "convergence dfGPdfNN Run 2/10, Epoch 429/1000, Training Loss (NLML): -959.1128\n",
      "convergence dfGPdfNN Run 2/10, Epoch 430/1000, Training Loss (NLML): -959.1129\n",
      "convergence dfGPdfNN Run 2/10, Epoch 431/1000, Training Loss (NLML): -959.1135\n",
      "convergence dfGPdfNN Run 2/10, Epoch 432/1000, Training Loss (NLML): -959.1324\n",
      "convergence dfGPdfNN Run 2/10, Epoch 433/1000, Training Loss (NLML): -959.1381\n",
      "convergence dfGPdfNN Run 2/10, Epoch 434/1000, Training Loss (NLML): -959.1322\n",
      "convergence dfGPdfNN Run 2/10, Epoch 435/1000, Training Loss (NLML): -959.1438\n",
      "convergence dfGPdfNN Run 2/10, Epoch 436/1000, Training Loss (NLML): -959.1591\n",
      "convergence dfGPdfNN Run 2/10, Epoch 437/1000, Training Loss (NLML): -959.1582\n",
      "convergence dfGPdfNN Run 2/10, Epoch 438/1000, Training Loss (NLML): -959.1583\n",
      "convergence dfGPdfNN Run 2/10, Epoch 439/1000, Training Loss (NLML): -959.1782\n",
      "convergence dfGPdfNN Run 2/10, Epoch 440/1000, Training Loss (NLML): -959.1769\n",
      "convergence dfGPdfNN Run 2/10, Epoch 441/1000, Training Loss (NLML): -959.1792\n",
      "convergence dfGPdfNN Run 2/10, Epoch 442/1000, Training Loss (NLML): -959.1890\n",
      "convergence dfGPdfNN Run 2/10, Epoch 443/1000, Training Loss (NLML): -959.1948\n",
      "convergence dfGPdfNN Run 2/10, Epoch 444/1000, Training Loss (NLML): -959.2086\n",
      "convergence dfGPdfNN Run 2/10, Epoch 445/1000, Training Loss (NLML): -959.2037\n",
      "convergence dfGPdfNN Run 2/10, Epoch 446/1000, Training Loss (NLML): -959.2095\n",
      "convergence dfGPdfNN Run 2/10, Epoch 447/1000, Training Loss (NLML): -959.2203\n",
      "convergence dfGPdfNN Run 2/10, Epoch 448/1000, Training Loss (NLML): -959.2321\n",
      "convergence dfGPdfNN Run 2/10, Epoch 449/1000, Training Loss (NLML): -959.2405\n",
      "convergence dfGPdfNN Run 2/10, Epoch 450/1000, Training Loss (NLML): -959.2338\n",
      "convergence dfGPdfNN Run 2/10, Epoch 451/1000, Training Loss (NLML): -959.2400\n",
      "convergence dfGPdfNN Run 2/10, Epoch 452/1000, Training Loss (NLML): -959.2467\n",
      "convergence dfGPdfNN Run 2/10, Epoch 453/1000, Training Loss (NLML): -959.2557\n",
      "convergence dfGPdfNN Run 2/10, Epoch 454/1000, Training Loss (NLML): -959.2688\n",
      "convergence dfGPdfNN Run 2/10, Epoch 455/1000, Training Loss (NLML): -959.2754\n",
      "convergence dfGPdfNN Run 2/10, Epoch 456/1000, Training Loss (NLML): -959.2812\n",
      "convergence dfGPdfNN Run 2/10, Epoch 457/1000, Training Loss (NLML): -959.2776\n",
      "convergence dfGPdfNN Run 2/10, Epoch 458/1000, Training Loss (NLML): -959.2826\n",
      "convergence dfGPdfNN Run 2/10, Epoch 459/1000, Training Loss (NLML): -959.2885\n",
      "convergence dfGPdfNN Run 2/10, Epoch 460/1000, Training Loss (NLML): -959.3063\n",
      "convergence dfGPdfNN Run 2/10, Epoch 461/1000, Training Loss (NLML): -959.3119\n",
      "convergence dfGPdfNN Run 2/10, Epoch 462/1000, Training Loss (NLML): -959.3113\n",
      "convergence dfGPdfNN Run 2/10, Epoch 463/1000, Training Loss (NLML): -959.3125\n",
      "convergence dfGPdfNN Run 2/10, Epoch 464/1000, Training Loss (NLML): -959.3226\n",
      "convergence dfGPdfNN Run 2/10, Epoch 465/1000, Training Loss (NLML): -959.3336\n",
      "convergence dfGPdfNN Run 2/10, Epoch 466/1000, Training Loss (NLML): -959.3411\n",
      "convergence dfGPdfNN Run 2/10, Epoch 467/1000, Training Loss (NLML): -959.3372\n",
      "convergence dfGPdfNN Run 2/10, Epoch 468/1000, Training Loss (NLML): -959.3413\n",
      "convergence dfGPdfNN Run 2/10, Epoch 469/1000, Training Loss (NLML): -959.3500\n",
      "convergence dfGPdfNN Run 2/10, Epoch 470/1000, Training Loss (NLML): -959.3640\n",
      "convergence dfGPdfNN Run 2/10, Epoch 471/1000, Training Loss (NLML): -959.3689\n",
      "convergence dfGPdfNN Run 2/10, Epoch 472/1000, Training Loss (NLML): -959.3645\n",
      "convergence dfGPdfNN Run 2/10, Epoch 473/1000, Training Loss (NLML): -959.3689\n",
      "convergence dfGPdfNN Run 2/10, Epoch 474/1000, Training Loss (NLML): -959.3857\n",
      "convergence dfGPdfNN Run 2/10, Epoch 475/1000, Training Loss (NLML): -959.3854\n",
      "convergence dfGPdfNN Run 2/10, Epoch 476/1000, Training Loss (NLML): -959.3981\n",
      "convergence dfGPdfNN Run 2/10, Epoch 477/1000, Training Loss (NLML): -959.3912\n",
      "convergence dfGPdfNN Run 2/10, Epoch 478/1000, Training Loss (NLML): -959.4374\n",
      "convergence dfGPdfNN Run 2/10, Epoch 479/1000, Training Loss (NLML): -959.4144\n",
      "convergence dfGPdfNN Run 2/10, Epoch 480/1000, Training Loss (NLML): -959.4205\n",
      "convergence dfGPdfNN Run 2/10, Epoch 481/1000, Training Loss (NLML): -959.4175\n",
      "convergence dfGPdfNN Run 2/10, Epoch 482/1000, Training Loss (NLML): -959.4240\n",
      "convergence dfGPdfNN Run 2/10, Epoch 483/1000, Training Loss (NLML): -959.4288\n",
      "convergence dfGPdfNN Run 2/10, Epoch 484/1000, Training Loss (NLML): -959.4349\n",
      "convergence dfGPdfNN Run 2/10, Epoch 485/1000, Training Loss (NLML): -959.5157\n",
      "convergence dfGPdfNN Run 2/10, Epoch 486/1000, Training Loss (NLML): -959.4456\n",
      "convergence dfGPdfNN Run 2/10, Epoch 487/1000, Training Loss (NLML): -959.4515\n",
      "convergence dfGPdfNN Run 2/10, Epoch 488/1000, Training Loss (NLML): -959.4553\n",
      "convergence dfGPdfNN Run 2/10, Epoch 489/1000, Training Loss (NLML): -959.4668\n",
      "convergence dfGPdfNN Run 2/10, Epoch 490/1000, Training Loss (NLML): -959.4734\n",
      "convergence dfGPdfNN Run 2/10, Epoch 491/1000, Training Loss (NLML): -959.5112\n",
      "convergence dfGPdfNN Run 2/10, Epoch 492/1000, Training Loss (NLML): -959.5122\n",
      "convergence dfGPdfNN Run 2/10, Epoch 493/1000, Training Loss (NLML): -959.5232\n",
      "convergence dfGPdfNN Run 2/10, Epoch 494/1000, Training Loss (NLML): -959.4944\n",
      "convergence dfGPdfNN Run 2/10, Epoch 495/1000, Training Loss (NLML): -959.4979\n",
      "convergence dfGPdfNN Run 2/10, Epoch 496/1000, Training Loss (NLML): -959.4967\n",
      "convergence dfGPdfNN Run 2/10, Epoch 497/1000, Training Loss (NLML): -959.4968\n",
      "convergence dfGPdfNN Run 2/10, Epoch 498/1000, Training Loss (NLML): -959.5543\n",
      "convergence dfGPdfNN Run 2/10, Epoch 499/1000, Training Loss (NLML): -959.5592\n",
      "convergence dfGPdfNN Run 2/10, Epoch 500/1000, Training Loss (NLML): -959.5540\n",
      "convergence dfGPdfNN Run 2/10, Epoch 501/1000, Training Loss (NLML): -959.5188\n",
      "convergence dfGPdfNN Run 2/10, Epoch 502/1000, Training Loss (NLML): -959.5282\n",
      "convergence dfGPdfNN Run 2/10, Epoch 503/1000, Training Loss (NLML): -959.5399\n",
      "convergence dfGPdfNN Run 2/10, Epoch 504/1000, Training Loss (NLML): -959.5850\n",
      "convergence dfGPdfNN Run 2/10, Epoch 505/1000, Training Loss (NLML): -959.5798\n",
      "convergence dfGPdfNN Run 2/10, Epoch 506/1000, Training Loss (NLML): -959.5840\n",
      "convergence dfGPdfNN Run 2/10, Epoch 507/1000, Training Loss (NLML): -959.5471\n",
      "convergence dfGPdfNN Run 2/10, Epoch 508/1000, Training Loss (NLML): -959.5582\n",
      "convergence dfGPdfNN Run 2/10, Epoch 509/1000, Training Loss (NLML): -959.6094\n",
      "convergence dfGPdfNN Run 2/10, Epoch 510/1000, Training Loss (NLML): -959.6139\n",
      "convergence dfGPdfNN Run 2/10, Epoch 511/1000, Training Loss (NLML): -959.6090\n",
      "convergence dfGPdfNN Run 2/10, Epoch 512/1000, Training Loss (NLML): -959.5729\n",
      "convergence dfGPdfNN Run 2/10, Epoch 513/1000, Training Loss (NLML): -959.5775\n",
      "convergence dfGPdfNN Run 2/10, Epoch 514/1000, Training Loss (NLML): -959.6324\n",
      "convergence dfGPdfNN Run 2/10, Epoch 515/1000, Training Loss (NLML): -959.6707\n",
      "convergence dfGPdfNN Run 2/10, Epoch 516/1000, Training Loss (NLML): -959.6354\n",
      "convergence dfGPdfNN Run 2/10, Epoch 517/1000, Training Loss (NLML): -959.5961\n",
      "convergence dfGPdfNN Run 2/10, Epoch 518/1000, Training Loss (NLML): -959.6057\n",
      "convergence dfGPdfNN Run 2/10, Epoch 519/1000, Training Loss (NLML): -959.6572\n",
      "convergence dfGPdfNN Run 2/10, Epoch 520/1000, Training Loss (NLML): -959.6613\n",
      "convergence dfGPdfNN Run 2/10, Epoch 521/1000, Training Loss (NLML): -959.6619\n",
      "convergence dfGPdfNN Run 2/10, Epoch 522/1000, Training Loss (NLML): -959.6606\n",
      "convergence dfGPdfNN Run 2/10, Epoch 523/1000, Training Loss (NLML): -959.6664\n",
      "convergence dfGPdfNN Run 2/10, Epoch 524/1000, Training Loss (NLML): -959.6892\n",
      "convergence dfGPdfNN Run 2/10, Epoch 525/1000, Training Loss (NLML): -959.6530\n",
      "convergence dfGPdfNN Run 2/10, Epoch 526/1000, Training Loss (NLML): -959.6570\n",
      "convergence dfGPdfNN Run 2/10, Epoch 527/1000, Training Loss (NLML): -959.7025\n",
      "convergence dfGPdfNN Run 2/10, Epoch 528/1000, Training Loss (NLML): -959.6998\n",
      "convergence dfGPdfNN Run 2/10, Epoch 529/1000, Training Loss (NLML): -959.7030\n",
      "convergence dfGPdfNN Run 2/10, Epoch 530/1000, Training Loss (NLML): -959.7087\n",
      "convergence dfGPdfNN Run 2/10, Epoch 531/1000, Training Loss (NLML): -959.6733\n",
      "convergence dfGPdfNN Run 2/10, Epoch 532/1000, Training Loss (NLML): -959.6844\n",
      "convergence dfGPdfNN Run 2/10, Epoch 533/1000, Training Loss (NLML): -959.7607\n",
      "convergence dfGPdfNN Run 2/10, Epoch 534/1000, Training Loss (NLML): -959.7330\n",
      "convergence dfGPdfNN Run 2/10, Epoch 535/1000, Training Loss (NLML): -959.6968\n",
      "convergence dfGPdfNN Run 2/10, Epoch 536/1000, Training Loss (NLML): -959.7362\n",
      "convergence dfGPdfNN Run 2/10, Epoch 537/1000, Training Loss (NLML): -959.7395\n",
      "convergence dfGPdfNN Run 2/10, Epoch 538/1000, Training Loss (NLML): -959.7449\n",
      "convergence dfGPdfNN Run 2/10, Epoch 539/1000, Training Loss (NLML): -959.7068\n",
      "convergence dfGPdfNN Run 2/10, Epoch 540/1000, Training Loss (NLML): -959.7515\n",
      "convergence dfGPdfNN Run 2/10, Epoch 541/1000, Training Loss (NLML): -959.7965\n",
      "convergence dfGPdfNN Run 2/10, Epoch 542/1000, Training Loss (NLML): -959.7703\n",
      "convergence dfGPdfNN Run 2/10, Epoch 543/1000, Training Loss (NLML): -959.7328\n",
      "convergence dfGPdfNN Run 2/10, Epoch 544/1000, Training Loss (NLML): -959.7292\n",
      "convergence dfGPdfNN Run 2/10, Epoch 545/1000, Training Loss (NLML): -959.7733\n",
      "convergence dfGPdfNN Run 2/10, Epoch 546/1000, Training Loss (NLML): -959.7795\n",
      "convergence dfGPdfNN Run 2/10, Epoch 547/1000, Training Loss (NLML): -959.8152\n",
      "convergence dfGPdfNN Run 2/10, Epoch 548/1000, Training Loss (NLML): -959.7872\n",
      "convergence dfGPdfNN Run 2/10, Epoch 549/1000, Training Loss (NLML): -959.7909\n",
      "convergence dfGPdfNN Run 2/10, Epoch 550/1000, Training Loss (NLML): -959.7535\n",
      "convergence dfGPdfNN Run 2/10, Epoch 551/1000, Training Loss (NLML): -959.7910\n",
      "convergence dfGPdfNN Run 2/10, Epoch 552/1000, Training Loss (NLML): -959.7965\n",
      "convergence dfGPdfNN Run 2/10, Epoch 553/1000, Training Loss (NLML): -959.8350\n",
      "convergence dfGPdfNN Run 2/10, Epoch 554/1000, Training Loss (NLML): -959.8429\n",
      "convergence dfGPdfNN Run 2/10, Epoch 555/1000, Training Loss (NLML): -959.8167\n",
      "convergence dfGPdfNN Run 2/10, Epoch 556/1000, Training Loss (NLML): -959.8153\n",
      "convergence dfGPdfNN Run 2/10, Epoch 557/1000, Training Loss (NLML): -959.8179\n",
      "convergence dfGPdfNN Run 2/10, Epoch 558/1000, Training Loss (NLML): -959.8274\n",
      "convergence dfGPdfNN Run 2/10, Epoch 559/1000, Training Loss (NLML): -959.8318\n",
      "convergence dfGPdfNN Run 2/10, Epoch 560/1000, Training Loss (NLML): -959.8767\n",
      "convergence dfGPdfNN Run 2/10, Epoch 561/1000, Training Loss (NLML): -959.8438\n",
      "convergence dfGPdfNN Run 2/10, Epoch 562/1000, Training Loss (NLML): -959.8475\n",
      "convergence dfGPdfNN Run 2/10, Epoch 563/1000, Training Loss (NLML): -959.8481\n",
      "convergence dfGPdfNN Run 2/10, Epoch 564/1000, Training Loss (NLML): -959.8560\n",
      "convergence dfGPdfNN Run 2/10, Epoch 565/1000, Training Loss (NLML): -959.8639\n",
      "convergence dfGPdfNN Run 2/10, Epoch 566/1000, Training Loss (NLML): -959.8644\n",
      "convergence dfGPdfNN Run 2/10, Epoch 567/1000, Training Loss (NLML): -959.8662\n",
      "convergence dfGPdfNN Run 2/10, Epoch 568/1000, Training Loss (NLML): -959.8722\n",
      "convergence dfGPdfNN Run 2/10, Epoch 569/1000, Training Loss (NLML): -959.9076\n",
      "convergence dfGPdfNN Run 2/10, Epoch 570/1000, Training Loss (NLML): -959.8771\n",
      "convergence dfGPdfNN Run 2/10, Epoch 571/1000, Training Loss (NLML): -959.8866\n",
      "convergence dfGPdfNN Run 2/10, Epoch 572/1000, Training Loss (NLML): -959.8950\n",
      "convergence dfGPdfNN Run 2/10, Epoch 573/1000, Training Loss (NLML): -959.8887\n",
      "convergence dfGPdfNN Run 2/10, Epoch 574/1000, Training Loss (NLML): -959.8929\n",
      "convergence dfGPdfNN Run 2/10, Epoch 575/1000, Training Loss (NLML): -959.8955\n",
      "convergence dfGPdfNN Run 2/10, Epoch 576/1000, Training Loss (NLML): -959.9098\n",
      "convergence dfGPdfNN Run 2/10, Epoch 577/1000, Training Loss (NLML): -959.9452\n",
      "convergence dfGPdfNN Run 2/10, Epoch 578/1000, Training Loss (NLML): -959.9081\n",
      "convergence dfGPdfNN Run 2/10, Epoch 579/1000, Training Loss (NLML): -959.9115\n",
      "convergence dfGPdfNN Run 2/10, Epoch 580/1000, Training Loss (NLML): -959.9171\n",
      "convergence dfGPdfNN Run 2/10, Epoch 581/1000, Training Loss (NLML): -959.9214\n",
      "convergence dfGPdfNN Run 2/10, Epoch 582/1000, Training Loss (NLML): -959.9595\n",
      "convergence dfGPdfNN Run 2/10, Epoch 583/1000, Training Loss (NLML): -959.9362\n",
      "convergence dfGPdfNN Run 2/10, Epoch 584/1000, Training Loss (NLML): -959.9406\n",
      "convergence dfGPdfNN Run 2/10, Epoch 585/1000, Training Loss (NLML): -959.9369\n",
      "convergence dfGPdfNN Run 2/10, Epoch 586/1000, Training Loss (NLML): -959.9393\n",
      "convergence dfGPdfNN Run 2/10, Epoch 587/1000, Training Loss (NLML): -959.9796\n",
      "convergence dfGPdfNN Run 2/10, Epoch 588/1000, Training Loss (NLML): -959.9886\n",
      "convergence dfGPdfNN Run 2/10, Epoch 589/1000, Training Loss (NLML): -959.9495\n",
      "convergence dfGPdfNN Run 2/10, Epoch 590/1000, Training Loss (NLML): -959.9570\n",
      "convergence dfGPdfNN Run 2/10, Epoch 591/1000, Training Loss (NLML): -959.9655\n",
      "convergence dfGPdfNN Run 2/10, Epoch 592/1000, Training Loss (NLML): -959.9656\n",
      "convergence dfGPdfNN Run 2/10, Epoch 593/1000, Training Loss (NLML): -960.0012\n",
      "convergence dfGPdfNN Run 2/10, Epoch 594/1000, Training Loss (NLML): -960.0048\n",
      "convergence dfGPdfNN Run 2/10, Epoch 595/1000, Training Loss (NLML): -959.9756\n",
      "convergence dfGPdfNN Run 2/10, Epoch 596/1000, Training Loss (NLML): -959.9845\n",
      "convergence dfGPdfNN Run 2/10, Epoch 597/1000, Training Loss (NLML): -959.9800\n",
      "convergence dfGPdfNN Run 2/10, Epoch 598/1000, Training Loss (NLML): -960.0260\n",
      "convergence dfGPdfNN Run 2/10, Epoch 599/1000, Training Loss (NLML): -960.0205\n",
      "convergence dfGPdfNN Run 2/10, Epoch 600/1000, Training Loss (NLML): -959.9961\n",
      "convergence dfGPdfNN Run 2/10, Epoch 601/1000, Training Loss (NLML): -959.9991\n",
      "convergence dfGPdfNN Run 2/10, Epoch 602/1000, Training Loss (NLML): -960.0070\n",
      "convergence dfGPdfNN Run 2/10, Epoch 603/1000, Training Loss (NLML): -960.0374\n",
      "convergence dfGPdfNN Run 2/10, Epoch 604/1000, Training Loss (NLML): -960.0382\n",
      "convergence dfGPdfNN Run 2/10, Epoch 605/1000, Training Loss (NLML): -960.0083\n",
      "convergence dfGPdfNN Run 2/10, Epoch 606/1000, Training Loss (NLML): -960.0128\n",
      "convergence dfGPdfNN Run 2/10, Epoch 607/1000, Training Loss (NLML): -960.0576\n",
      "convergence dfGPdfNN Run 2/10, Epoch 608/1000, Training Loss (NLML): -960.0568\n",
      "convergence dfGPdfNN Run 2/10, Epoch 609/1000, Training Loss (NLML): -960.0234\n",
      "convergence dfGPdfNN Run 2/10, Epoch 610/1000, Training Loss (NLML): -960.0272\n",
      "convergence dfGPdfNN Run 2/10, Epoch 611/1000, Training Loss (NLML): -960.0641\n",
      "convergence dfGPdfNN Run 2/10, Epoch 612/1000, Training Loss (NLML): -960.0669\n",
      "convergence dfGPdfNN Run 2/10, Epoch 613/1000, Training Loss (NLML): -960.0376\n",
      "convergence dfGPdfNN Run 2/10, Epoch 614/1000, Training Loss (NLML): -960.0781\n",
      "convergence dfGPdfNN Run 2/10, Epoch 615/1000, Training Loss (NLML): -960.0815\n",
      "convergence dfGPdfNN Run 2/10, Epoch 616/1000, Training Loss (NLML): -960.0474\n",
      "convergence dfGPdfNN Run 2/10, Epoch 617/1000, Training Loss (NLML): -960.0505\n",
      "convergence dfGPdfNN Run 2/10, Epoch 618/1000, Training Loss (NLML): -960.0914\n",
      "convergence dfGPdfNN Run 2/10, Epoch 619/1000, Training Loss (NLML): -960.0955\n",
      "convergence dfGPdfNN Run 2/10, Epoch 620/1000, Training Loss (NLML): -960.0995\n",
      "convergence dfGPdfNN Run 2/10, Epoch 621/1000, Training Loss (NLML): -960.0995\n",
      "convergence dfGPdfNN Run 2/10, Epoch 622/1000, Training Loss (NLML): -960.0682\n",
      "convergence dfGPdfNN Run 2/10, Epoch 623/1000, Training Loss (NLML): -960.0775\n",
      "convergence dfGPdfNN Run 2/10, Epoch 624/1000, Training Loss (NLML): -960.1179\n",
      "convergence dfGPdfNN Run 2/10, Epoch 625/1000, Training Loss (NLML): -960.1063\n",
      "convergence dfGPdfNN Run 2/10, Epoch 626/1000, Training Loss (NLML): -960.1094\n",
      "convergence dfGPdfNN Run 2/10, Epoch 627/1000, Training Loss (NLML): -960.1129\n",
      "convergence dfGPdfNN Run 2/10, Epoch 628/1000, Training Loss (NLML): -960.1318\n",
      "convergence dfGPdfNN Run 2/10, Epoch 629/1000, Training Loss (NLML): -960.0957\n",
      "convergence dfGPdfNN Run 2/10, Epoch 630/1000, Training Loss (NLML): -960.1339\n",
      "convergence dfGPdfNN Run 2/10, Epoch 631/1000, Training Loss (NLML): -960.0983\n",
      "convergence dfGPdfNN Run 2/10, Epoch 632/1000, Training Loss (NLML): -960.1440\n",
      "convergence dfGPdfNN Run 2/10, Epoch 633/1000, Training Loss (NLML): -960.1442\n",
      "convergence dfGPdfNN Run 2/10, Epoch 634/1000, Training Loss (NLML): -960.1423\n",
      "convergence dfGPdfNN Run 2/10, Epoch 635/1000, Training Loss (NLML): -960.1451\n",
      "convergence dfGPdfNN Run 2/10, Epoch 636/1000, Training Loss (NLML): -960.1190\n",
      "convergence dfGPdfNN Run 2/10, Epoch 637/1000, Training Loss (NLML): -960.1255\n",
      "convergence dfGPdfNN Run 2/10, Epoch 638/1000, Training Loss (NLML): -960.1631\n",
      "convergence dfGPdfNN Run 2/10, Epoch 639/1000, Training Loss (NLML): -960.1603\n",
      "convergence dfGPdfNN Run 2/10, Epoch 640/1000, Training Loss (NLML): -960.1633\n",
      "convergence dfGPdfNN Run 2/10, Epoch 641/1000, Training Loss (NLML): -960.1650\n",
      "convergence dfGPdfNN Run 2/10, Epoch 642/1000, Training Loss (NLML): -960.1699\n",
      "convergence dfGPdfNN Run 2/10, Epoch 643/1000, Training Loss (NLML): -960.1727\n",
      "convergence dfGPdfNN Run 2/10, Epoch 644/1000, Training Loss (NLML): -960.1838\n",
      "convergence dfGPdfNN Run 2/10, Epoch 645/1000, Training Loss (NLML): -960.1501\n",
      "convergence dfGPdfNN Run 2/10, Epoch 646/1000, Training Loss (NLML): -960.1495\n",
      "convergence dfGPdfNN Run 2/10, Epoch 647/1000, Training Loss (NLML): -960.1863\n",
      "convergence dfGPdfNN Run 2/10, Epoch 648/1000, Training Loss (NLML): -960.1880\n",
      "convergence dfGPdfNN Run 2/10, Epoch 649/1000, Training Loss (NLML): -960.1852\n",
      "convergence dfGPdfNN Run 2/10, Epoch 650/1000, Training Loss (NLML): -960.1936\n",
      "convergence dfGPdfNN Run 2/10, Epoch 651/1000, Training Loss (NLML): -960.1986\n",
      "convergence dfGPdfNN Run 2/10, Epoch 652/1000, Training Loss (NLML): -960.1962\n",
      "convergence dfGPdfNN Run 2/10, Epoch 653/1000, Training Loss (NLML): -960.2041\n",
      "convergence dfGPdfNN Run 2/10, Epoch 654/1000, Training Loss (NLML): -960.2083\n",
      "convergence dfGPdfNN Run 2/10, Epoch 655/1000, Training Loss (NLML): -960.2141\n",
      "convergence dfGPdfNN Run 2/10, Epoch 656/1000, Training Loss (NLML): -960.2180\n",
      "convergence dfGPdfNN Run 2/10, Epoch 657/1000, Training Loss (NLML): -960.2205\n",
      "convergence dfGPdfNN Run 2/10, Epoch 658/1000, Training Loss (NLML): -960.2235\n",
      "convergence dfGPdfNN Run 2/10, Epoch 659/1000, Training Loss (NLML): -960.1874\n",
      "convergence dfGPdfNN Run 2/10, Epoch 660/1000, Training Loss (NLML): -960.2290\n",
      "convergence dfGPdfNN Run 2/10, Epoch 661/1000, Training Loss (NLML): -960.2338\n",
      "convergence dfGPdfNN Run 2/10, Epoch 662/1000, Training Loss (NLML): -960.2358\n",
      "convergence dfGPdfNN Run 2/10, Epoch 663/1000, Training Loss (NLML): -960.2351\n",
      "convergence dfGPdfNN Run 2/10, Epoch 664/1000, Training Loss (NLML): -960.2421\n",
      "convergence dfGPdfNN Run 2/10, Epoch 665/1000, Training Loss (NLML): -960.2445\n",
      "convergence dfGPdfNN Run 2/10, Epoch 666/1000, Training Loss (NLML): -960.2472\n",
      "convergence dfGPdfNN Run 2/10, Epoch 667/1000, Training Loss (NLML): -960.2487\n",
      "convergence dfGPdfNN Run 2/10, Epoch 668/1000, Training Loss (NLML): -960.2509\n",
      "convergence dfGPdfNN Run 2/10, Epoch 669/1000, Training Loss (NLML): -960.2609\n",
      "convergence dfGPdfNN Run 2/10, Epoch 670/1000, Training Loss (NLML): -960.2605\n",
      "convergence dfGPdfNN Run 2/10, Epoch 671/1000, Training Loss (NLML): -960.2582\n",
      "convergence dfGPdfNN Run 2/10, Epoch 672/1000, Training Loss (NLML): -960.2622\n",
      "convergence dfGPdfNN Run 2/10, Epoch 673/1000, Training Loss (NLML): -960.2666\n",
      "convergence dfGPdfNN Run 2/10, Epoch 674/1000, Training Loss (NLML): -960.2731\n",
      "convergence dfGPdfNN Run 2/10, Epoch 675/1000, Training Loss (NLML): -960.2795\n",
      "convergence dfGPdfNN Run 2/10, Epoch 676/1000, Training Loss (NLML): -960.2754\n",
      "convergence dfGPdfNN Run 2/10, Epoch 677/1000, Training Loss (NLML): -960.2765\n",
      "convergence dfGPdfNN Run 2/10, Epoch 678/1000, Training Loss (NLML): -960.2843\n",
      "convergence dfGPdfNN Run 2/10, Epoch 679/1000, Training Loss (NLML): -960.2809\n",
      "convergence dfGPdfNN Run 2/10, Epoch 680/1000, Training Loss (NLML): -960.2842\n",
      "convergence dfGPdfNN Run 2/10, Epoch 681/1000, Training Loss (NLML): -960.2872\n",
      "convergence dfGPdfNN Run 2/10, Epoch 682/1000, Training Loss (NLML): -960.2864\n",
      "convergence dfGPdfNN Run 2/10, Epoch 683/1000, Training Loss (NLML): -960.2947\n",
      "convergence dfGPdfNN Run 2/10, Epoch 684/1000, Training Loss (NLML): -960.3019\n",
      "convergence dfGPdfNN Run 2/10, Epoch 685/1000, Training Loss (NLML): -960.3083\n",
      "convergence dfGPdfNN Run 2/10, Epoch 686/1000, Training Loss (NLML): -960.3073\n",
      "convergence dfGPdfNN Run 2/10, Epoch 687/1000, Training Loss (NLML): -960.3071\n",
      "convergence dfGPdfNN Run 2/10, Epoch 688/1000, Training Loss (NLML): -960.3098\n",
      "convergence dfGPdfNN Run 2/10, Epoch 689/1000, Training Loss (NLML): -960.3135\n",
      "convergence dfGPdfNN Run 2/10, Epoch 690/1000, Training Loss (NLML): -960.3229\n",
      "convergence dfGPdfNN Run 2/10, Epoch 691/1000, Training Loss (NLML): -960.3243\n",
      "convergence dfGPdfNN Run 2/10, Epoch 692/1000, Training Loss (NLML): -960.3223\n",
      "convergence dfGPdfNN Run 2/10, Epoch 693/1000, Training Loss (NLML): -960.3245\n",
      "convergence dfGPdfNN Run 2/10, Epoch 694/1000, Training Loss (NLML): -960.3260\n",
      "convergence dfGPdfNN Run 2/10, Epoch 695/1000, Training Loss (NLML): -960.3308\n",
      "convergence dfGPdfNN Run 2/10, Epoch 696/1000, Training Loss (NLML): -960.3350\n",
      "convergence dfGPdfNN Run 2/10, Epoch 697/1000, Training Loss (NLML): -960.3373\n",
      "convergence dfGPdfNN Run 2/10, Epoch 698/1000, Training Loss (NLML): -960.3451\n",
      "convergence dfGPdfNN Run 2/10, Epoch 699/1000, Training Loss (NLML): -960.3417\n",
      "convergence dfGPdfNN Run 2/10, Epoch 700/1000, Training Loss (NLML): -960.3439\n",
      "convergence dfGPdfNN Run 2/10, Epoch 701/1000, Training Loss (NLML): -960.3462\n",
      "convergence dfGPdfNN Run 2/10, Epoch 702/1000, Training Loss (NLML): -960.3535\n",
      "convergence dfGPdfNN Run 2/10, Epoch 703/1000, Training Loss (NLML): -960.3560\n",
      "convergence dfGPdfNN Run 2/10, Epoch 704/1000, Training Loss (NLML): -960.3582\n",
      "convergence dfGPdfNN Run 2/10, Epoch 705/1000, Training Loss (NLML): -960.3593\n",
      "convergence dfGPdfNN Run 2/10, Epoch 706/1000, Training Loss (NLML): -960.3654\n",
      "convergence dfGPdfNN Run 2/10, Epoch 707/1000, Training Loss (NLML): -960.3641\n",
      "convergence dfGPdfNN Run 2/10, Epoch 708/1000, Training Loss (NLML): -960.3680\n",
      "convergence dfGPdfNN Run 2/10, Epoch 709/1000, Training Loss (NLML): -960.3708\n",
      "convergence dfGPdfNN Run 2/10, Epoch 710/1000, Training Loss (NLML): -960.3733\n",
      "convergence dfGPdfNN Run 2/10, Epoch 711/1000, Training Loss (NLML): -960.3760\n",
      "convergence dfGPdfNN Run 2/10, Epoch 712/1000, Training Loss (NLML): -960.3796\n",
      "convergence dfGPdfNN Run 2/10, Epoch 713/1000, Training Loss (NLML): -960.3820\n",
      "convergence dfGPdfNN Run 2/10, Epoch 714/1000, Training Loss (NLML): -960.3763\n",
      "convergence dfGPdfNN Run 2/10, Epoch 715/1000, Training Loss (NLML): -960.3810\n",
      "convergence dfGPdfNN Run 2/10, Epoch 716/1000, Training Loss (NLML): -960.3893\n",
      "convergence dfGPdfNN Run 2/10, Epoch 717/1000, Training Loss (NLML): -960.3909\n",
      "convergence dfGPdfNN Run 2/10, Epoch 718/1000, Training Loss (NLML): -960.3997\n",
      "convergence dfGPdfNN Run 2/10, Epoch 719/1000, Training Loss (NLML): -960.3983\n",
      "convergence dfGPdfNN Run 2/10, Epoch 720/1000, Training Loss (NLML): -960.4028\n",
      "convergence dfGPdfNN Run 2/10, Epoch 721/1000, Training Loss (NLML): -960.4021\n",
      "convergence dfGPdfNN Run 2/10, Epoch 722/1000, Training Loss (NLML): -960.4052\n",
      "convergence dfGPdfNN Run 2/10, Epoch 723/1000, Training Loss (NLML): -960.4059\n",
      "convergence dfGPdfNN Run 2/10, Epoch 724/1000, Training Loss (NLML): -960.4138\n",
      "convergence dfGPdfNN Run 2/10, Epoch 725/1000, Training Loss (NLML): -960.4156\n",
      "convergence dfGPdfNN Run 2/10, Epoch 726/1000, Training Loss (NLML): -960.4183\n",
      "convergence dfGPdfNN Run 2/10, Epoch 727/1000, Training Loss (NLML): -960.4209\n",
      "convergence dfGPdfNN Run 2/10, Epoch 728/1000, Training Loss (NLML): -960.4192\n",
      "convergence dfGPdfNN Run 2/10, Epoch 729/1000, Training Loss (NLML): -960.4259\n",
      "convergence dfGPdfNN Run 2/10, Epoch 730/1000, Training Loss (NLML): -960.4247\n",
      "convergence dfGPdfNN Run 2/10, Epoch 731/1000, Training Loss (NLML): -960.4314\n",
      "convergence dfGPdfNN Run 2/10, Epoch 732/1000, Training Loss (NLML): -960.4327\n",
      "convergence dfGPdfNN Run 2/10, Epoch 733/1000, Training Loss (NLML): -960.4330\n",
      "convergence dfGPdfNN Run 2/10, Epoch 734/1000, Training Loss (NLML): -960.4404\n",
      "convergence dfGPdfNN Run 2/10, Epoch 735/1000, Training Loss (NLML): -960.4430\n",
      "convergence dfGPdfNN Run 2/10, Epoch 736/1000, Training Loss (NLML): -960.4414\n",
      "convergence dfGPdfNN Run 2/10, Epoch 737/1000, Training Loss (NLML): -960.4447\n",
      "convergence dfGPdfNN Run 2/10, Epoch 738/1000, Training Loss (NLML): -960.4495\n",
      "convergence dfGPdfNN Run 2/10, Epoch 739/1000, Training Loss (NLML): -960.4524\n",
      "convergence dfGPdfNN Run 2/10, Epoch 740/1000, Training Loss (NLML): -960.4543\n",
      "convergence dfGPdfNN Run 2/10, Epoch 741/1000, Training Loss (NLML): -960.4543\n",
      "convergence dfGPdfNN Run 2/10, Epoch 742/1000, Training Loss (NLML): -960.4579\n",
      "convergence dfGPdfNN Run 2/10, Epoch 743/1000, Training Loss (NLML): -960.4631\n",
      "convergence dfGPdfNN Run 2/10, Epoch 744/1000, Training Loss (NLML): -960.4650\n",
      "convergence dfGPdfNN Run 2/10, Epoch 745/1000, Training Loss (NLML): -960.4631\n",
      "convergence dfGPdfNN Run 2/10, Epoch 746/1000, Training Loss (NLML): -960.4618\n",
      "convergence dfGPdfNN Run 2/10, Epoch 747/1000, Training Loss (NLML): -960.4635\n",
      "convergence dfGPdfNN Run 2/10, Epoch 748/1000, Training Loss (NLML): -960.4669\n",
      "convergence dfGPdfNN Run 2/10, Epoch 749/1000, Training Loss (NLML): -960.4786\n",
      "convergence dfGPdfNN Run 2/10, Epoch 750/1000, Training Loss (NLML): -960.4799\n",
      "convergence dfGPdfNN Run 2/10, Epoch 751/1000, Training Loss (NLML): -960.4797\n",
      "convergence dfGPdfNN Run 2/10, Epoch 752/1000, Training Loss (NLML): -960.4827\n",
      "convergence dfGPdfNN Run 2/10, Epoch 753/1000, Training Loss (NLML): -960.4886\n",
      "convergence dfGPdfNN Run 2/10, Epoch 754/1000, Training Loss (NLML): -960.4901\n",
      "convergence dfGPdfNN Run 2/10, Epoch 755/1000, Training Loss (NLML): -960.4916\n",
      "convergence dfGPdfNN Run 2/10, Epoch 756/1000, Training Loss (NLML): -960.4921\n",
      "convergence dfGPdfNN Run 2/10, Epoch 757/1000, Training Loss (NLML): -960.4948\n",
      "convergence dfGPdfNN Run 2/10, Epoch 758/1000, Training Loss (NLML): -960.4951\n",
      "convergence dfGPdfNN Run 2/10, Epoch 759/1000, Training Loss (NLML): -960.4994\n",
      "convergence dfGPdfNN Run 2/10, Epoch 760/1000, Training Loss (NLML): -960.5013\n",
      "convergence dfGPdfNN Run 2/10, Epoch 761/1000, Training Loss (NLML): -960.5107\n",
      "convergence dfGPdfNN Run 2/10, Epoch 762/1000, Training Loss (NLML): -960.5109\n",
      "convergence dfGPdfNN Run 2/10, Epoch 763/1000, Training Loss (NLML): -960.5084\n",
      "convergence dfGPdfNN Run 2/10, Epoch 764/1000, Training Loss (NLML): -960.5125\n",
      "convergence dfGPdfNN Run 2/10, Epoch 765/1000, Training Loss (NLML): -960.5182\n",
      "convergence dfGPdfNN Run 2/10, Epoch 766/1000, Training Loss (NLML): -960.5190\n",
      "convergence dfGPdfNN Run 2/10, Epoch 767/1000, Training Loss (NLML): -960.5226\n",
      "convergence dfGPdfNN Run 2/10, Epoch 768/1000, Training Loss (NLML): -960.5242\n",
      "convergence dfGPdfNN Run 2/10, Epoch 769/1000, Training Loss (NLML): -960.5236\n",
      "convergence dfGPdfNN Run 2/10, Epoch 770/1000, Training Loss (NLML): -960.5275\n",
      "convergence dfGPdfNN Run 2/10, Epoch 771/1000, Training Loss (NLML): -960.5326\n",
      "convergence dfGPdfNN Run 2/10, Epoch 772/1000, Training Loss (NLML): -960.5342\n",
      "convergence dfGPdfNN Run 2/10, Epoch 773/1000, Training Loss (NLML): -960.5376\n",
      "convergence dfGPdfNN Run 2/10, Epoch 774/1000, Training Loss (NLML): -960.5359\n",
      "convergence dfGPdfNN Run 2/10, Epoch 775/1000, Training Loss (NLML): -960.5387\n",
      "convergence dfGPdfNN Run 2/10, Epoch 776/1000, Training Loss (NLML): -960.5404\n",
      "convergence dfGPdfNN Run 2/10, Epoch 777/1000, Training Loss (NLML): -960.5464\n",
      "convergence dfGPdfNN Run 2/10, Epoch 778/1000, Training Loss (NLML): -960.5481\n",
      "convergence dfGPdfNN Run 2/10, Epoch 779/1000, Training Loss (NLML): -960.5518\n",
      "convergence dfGPdfNN Run 2/10, Epoch 780/1000, Training Loss (NLML): -960.5504\n",
      "convergence dfGPdfNN Run 2/10, Epoch 781/1000, Training Loss (NLML): -960.5519\n",
      "convergence dfGPdfNN Run 2/10, Epoch 782/1000, Training Loss (NLML): -960.5544\n",
      "convergence dfGPdfNN Run 2/10, Epoch 783/1000, Training Loss (NLML): -960.5574\n",
      "convergence dfGPdfNN Run 2/10, Epoch 784/1000, Training Loss (NLML): -960.5588\n",
      "convergence dfGPdfNN Run 2/10, Epoch 785/1000, Training Loss (NLML): -960.5605\n",
      "convergence dfGPdfNN Run 2/10, Epoch 786/1000, Training Loss (NLML): -960.5620\n",
      "convergence dfGPdfNN Run 2/10, Epoch 787/1000, Training Loss (NLML): -960.5623\n",
      "convergence dfGPdfNN Run 2/10, Epoch 788/1000, Training Loss (NLML): -960.5597\n",
      "convergence dfGPdfNN Run 2/10, Epoch 789/1000, Training Loss (NLML): -960.5850\n",
      "convergence dfGPdfNN Run 2/10, Epoch 790/1000, Training Loss (NLML): -960.5897\n",
      "convergence dfGPdfNN Run 2/10, Epoch 791/1000, Training Loss (NLML): -960.5787\n",
      "convergence dfGPdfNN Run 2/10, Epoch 792/1000, Training Loss (NLML): -960.5789\n",
      "convergence dfGPdfNN Run 2/10, Epoch 793/1000, Training Loss (NLML): -960.5778\n",
      "convergence dfGPdfNN Run 2/10, Epoch 794/1000, Training Loss (NLML): -960.5791\n",
      "convergence dfGPdfNN Run 2/10, Epoch 795/1000, Training Loss (NLML): -960.5817\n",
      "convergence dfGPdfNN Run 2/10, Epoch 796/1000, Training Loss (NLML): -960.5850\n",
      "convergence dfGPdfNN Run 2/10, Epoch 797/1000, Training Loss (NLML): -960.5919\n",
      "convergence dfGPdfNN Run 2/10, Epoch 798/1000, Training Loss (NLML): -960.5957\n",
      "convergence dfGPdfNN Run 2/10, Epoch 799/1000, Training Loss (NLML): -960.5906\n",
      "convergence dfGPdfNN Run 2/10, Epoch 800/1000, Training Loss (NLML): -960.6086\n",
      "convergence dfGPdfNN Run 2/10, Epoch 801/1000, Training Loss (NLML): -960.5947\n",
      "convergence dfGPdfNN Run 2/10, Epoch 802/1000, Training Loss (NLML): -960.5996\n",
      "convergence dfGPdfNN Run 2/10, Epoch 803/1000, Training Loss (NLML): -960.6053\n",
      "convergence dfGPdfNN Run 2/10, Epoch 804/1000, Training Loss (NLML): -960.6077\n",
      "convergence dfGPdfNN Run 2/10, Epoch 805/1000, Training Loss (NLML): -960.6096\n",
      "convergence dfGPdfNN Run 2/10, Epoch 806/1000, Training Loss (NLML): -960.6057\n",
      "convergence dfGPdfNN Run 2/10, Epoch 807/1000, Training Loss (NLML): -960.6077\n",
      "convergence dfGPdfNN Run 2/10, Epoch 808/1000, Training Loss (NLML): -960.6104\n",
      "convergence dfGPdfNN Run 2/10, Epoch 809/1000, Training Loss (NLML): -960.6169\n",
      "convergence dfGPdfNN Run 2/10, Epoch 810/1000, Training Loss (NLML): -960.6378\n",
      "convergence dfGPdfNN Run 2/10, Epoch 811/1000, Training Loss (NLML): -960.6422\n",
      "convergence dfGPdfNN Run 2/10, Epoch 812/1000, Training Loss (NLML): -960.6195\n",
      "convergence dfGPdfNN Run 2/10, Epoch 813/1000, Training Loss (NLML): -960.6211\n",
      "convergence dfGPdfNN Run 2/10, Epoch 814/1000, Training Loss (NLML): -960.6250\n",
      "convergence dfGPdfNN Run 2/10, Epoch 815/1000, Training Loss (NLML): -960.6320\n",
      "convergence dfGPdfNN Run 2/10, Epoch 816/1000, Training Loss (NLML): -960.6349\n",
      "convergence dfGPdfNN Run 2/10, Epoch 817/1000, Training Loss (NLML): -960.6473\n",
      "convergence dfGPdfNN Run 2/10, Epoch 818/1000, Training Loss (NLML): -960.6501\n",
      "convergence dfGPdfNN Run 2/10, Epoch 819/1000, Training Loss (NLML): -960.6547\n",
      "convergence dfGPdfNN Run 2/10, Epoch 820/1000, Training Loss (NLML): -960.6405\n",
      "convergence dfGPdfNN Run 2/10, Epoch 821/1000, Training Loss (NLML): -960.6416\n",
      "convergence dfGPdfNN Run 2/10, Epoch 822/1000, Training Loss (NLML): -960.6471\n",
      "convergence dfGPdfNN Run 2/10, Epoch 823/1000, Training Loss (NLML): -960.6440\n",
      "convergence dfGPdfNN Run 2/10, Epoch 824/1000, Training Loss (NLML): -960.6654\n",
      "convergence dfGPdfNN Run 2/10, Epoch 825/1000, Training Loss (NLML): -960.6676\n",
      "convergence dfGPdfNN Run 2/10, Epoch 826/1000, Training Loss (NLML): -960.6698\n",
      "convergence dfGPdfNN Run 2/10, Epoch 827/1000, Training Loss (NLML): -960.6736\n",
      "convergence dfGPdfNN Run 2/10, Epoch 828/1000, Training Loss (NLML): -960.6533\n",
      "convergence dfGPdfNN Run 2/10, Epoch 829/1000, Training Loss (NLML): -960.6593\n",
      "convergence dfGPdfNN Run 2/10, Epoch 830/1000, Training Loss (NLML): -960.6593\n",
      "convergence dfGPdfNN Run 2/10, Epoch 831/1000, Training Loss (NLML): -960.6644\n",
      "convergence dfGPdfNN Run 2/10, Epoch 832/1000, Training Loss (NLML): -960.6835\n",
      "convergence dfGPdfNN Run 2/10, Epoch 833/1000, Training Loss (NLML): -960.6940\n",
      "convergence dfGPdfNN Run 2/10, Epoch 834/1000, Training Loss (NLML): -960.6851\n",
      "convergence dfGPdfNN Run 2/10, Epoch 835/1000, Training Loss (NLML): -960.6731\n",
      "convergence dfGPdfNN Run 2/10, Epoch 836/1000, Training Loss (NLML): -960.6747\n",
      "convergence dfGPdfNN Run 2/10, Epoch 837/1000, Training Loss (NLML): -960.6766\n",
      "convergence dfGPdfNN Run 2/10, Epoch 838/1000, Training Loss (NLML): -960.7014\n",
      "convergence dfGPdfNN Run 2/10, Epoch 839/1000, Training Loss (NLML): -960.7004\n",
      "convergence dfGPdfNN Run 2/10, Epoch 840/1000, Training Loss (NLML): -960.6974\n",
      "convergence dfGPdfNN Run 2/10, Epoch 841/1000, Training Loss (NLML): -960.6998\n",
      "convergence dfGPdfNN Run 2/10, Epoch 842/1000, Training Loss (NLML): -960.7030\n",
      "convergence dfGPdfNN Run 2/10, Epoch 843/1000, Training Loss (NLML): -960.7104\n",
      "convergence dfGPdfNN Run 2/10, Epoch 844/1000, Training Loss (NLML): -960.6896\n",
      "convergence dfGPdfNN Run 2/10, Epoch 845/1000, Training Loss (NLML): -960.6904\n",
      "convergence dfGPdfNN Run 2/10, Epoch 846/1000, Training Loss (NLML): -960.7123\n",
      "convergence dfGPdfNN Run 2/10, Epoch 847/1000, Training Loss (NLML): -960.7150\n",
      "convergence dfGPdfNN Run 2/10, Epoch 848/1000, Training Loss (NLML): -960.7151\n",
      "convergence dfGPdfNN Run 2/10, Epoch 849/1000, Training Loss (NLML): -960.7162\n",
      "convergence dfGPdfNN Run 2/10, Epoch 850/1000, Training Loss (NLML): -960.7178\n",
      "convergence dfGPdfNN Run 2/10, Epoch 851/1000, Training Loss (NLML): -960.7235\n",
      "convergence dfGPdfNN Run 2/10, Epoch 852/1000, Training Loss (NLML): -960.7273\n",
      "convergence dfGPdfNN Run 2/10, Epoch 853/1000, Training Loss (NLML): -960.7238\n",
      "convergence dfGPdfNN Run 2/10, Epoch 854/1000, Training Loss (NLML): -960.7087\n",
      "convergence dfGPdfNN Run 2/10, Epoch 855/1000, Training Loss (NLML): -960.7313\n",
      "convergence dfGPdfNN Run 2/10, Epoch 856/1000, Training Loss (NLML): -960.7328\n",
      "convergence dfGPdfNN Run 2/10, Epoch 857/1000, Training Loss (NLML): -960.7336\n",
      "convergence dfGPdfNN Run 2/10, Epoch 858/1000, Training Loss (NLML): -960.7507\n",
      "convergence dfGPdfNN Run 2/10, Epoch 859/1000, Training Loss (NLML): -960.7382\n",
      "convergence dfGPdfNN Run 2/10, Epoch 860/1000, Training Loss (NLML): -960.7373\n",
      "convergence dfGPdfNN Run 2/10, Epoch 861/1000, Training Loss (NLML): -960.7428\n",
      "convergence dfGPdfNN Run 2/10, Epoch 862/1000, Training Loss (NLML): -960.7449\n",
      "convergence dfGPdfNN Run 2/10, Epoch 863/1000, Training Loss (NLML): -960.7466\n",
      "convergence dfGPdfNN Run 2/10, Epoch 864/1000, Training Loss (NLML): -960.7485\n",
      "convergence dfGPdfNN Run 2/10, Epoch 865/1000, Training Loss (NLML): -960.7522\n",
      "convergence dfGPdfNN Run 2/10, Epoch 866/1000, Training Loss (NLML): -960.7671\n",
      "convergence dfGPdfNN Run 2/10, Epoch 867/1000, Training Loss (NLML): -960.7506\n",
      "convergence dfGPdfNN Run 2/10, Epoch 868/1000, Training Loss (NLML): -960.7574\n",
      "convergence dfGPdfNN Run 2/10, Epoch 869/1000, Training Loss (NLML): -960.7579\n",
      "convergence dfGPdfNN Run 2/10, Epoch 870/1000, Training Loss (NLML): -960.7607\n",
      "convergence dfGPdfNN Run 2/10, Epoch 871/1000, Training Loss (NLML): -960.7651\n",
      "convergence dfGPdfNN Run 2/10, Epoch 872/1000, Training Loss (NLML): -960.7629\n",
      "convergence dfGPdfNN Run 2/10, Epoch 873/1000, Training Loss (NLML): -960.7816\n",
      "convergence dfGPdfNN Run 2/10, Epoch 874/1000, Training Loss (NLML): -960.7687\n",
      "convergence dfGPdfNN Run 2/10, Epoch 875/1000, Training Loss (NLML): -960.7709\n",
      "convergence dfGPdfNN Run 2/10, Epoch 876/1000, Training Loss (NLML): -960.7693\n",
      "convergence dfGPdfNN Run 2/10, Epoch 877/1000, Training Loss (NLML): -960.7766\n",
      "convergence dfGPdfNN Run 2/10, Epoch 878/1000, Training Loss (NLML): -960.7957\n",
      "convergence dfGPdfNN Run 2/10, Epoch 879/1000, Training Loss (NLML): -960.7727\n",
      "convergence dfGPdfNN Run 2/10, Epoch 880/1000, Training Loss (NLML): -960.7692\n",
      "convergence dfGPdfNN Run 2/10, Epoch 881/1000, Training Loss (NLML): -960.7780\n",
      "convergence dfGPdfNN Run 2/10, Epoch 882/1000, Training Loss (NLML): -960.7863\n",
      "convergence dfGPdfNN Run 2/10, Epoch 883/1000, Training Loss (NLML): -960.8048\n",
      "convergence dfGPdfNN Run 2/10, Epoch 884/1000, Training Loss (NLML): -960.7876\n",
      "convergence dfGPdfNN Run 2/10, Epoch 885/1000, Training Loss (NLML): -960.7670\n",
      "convergence dfGPdfNN Run 2/10, Epoch 886/1000, Training Loss (NLML): -960.7878\n",
      "convergence dfGPdfNN Run 2/10, Epoch 887/1000, Training Loss (NLML): -960.8094\n",
      "convergence dfGPdfNN Run 2/10, Epoch 888/1000, Training Loss (NLML): -960.7925\n",
      "convergence dfGPdfNN Run 2/10, Epoch 889/1000, Training Loss (NLML): -960.7922\n",
      "convergence dfGPdfNN Run 2/10, Epoch 890/1000, Training Loss (NLML): -960.7928\n",
      "convergence dfGPdfNN Run 2/10, Epoch 891/1000, Training Loss (NLML): -960.7766\n",
      "convergence dfGPdfNN Run 2/10, Epoch 892/1000, Training Loss (NLML): -960.8126\n",
      "convergence dfGPdfNN Run 2/10, Epoch 893/1000, Training Loss (NLML): -960.8158\n",
      "convergence dfGPdfNN Run 2/10, Epoch 894/1000, Training Loss (NLML): -960.7816\n",
      "convergence dfGPdfNN Run 2/10, Epoch 895/1000, Training Loss (NLML): -960.7844\n",
      "convergence dfGPdfNN Run 2/10, Epoch 896/1000, Training Loss (NLML): -960.8217\n",
      "convergence dfGPdfNN Run 2/10, Epoch 897/1000, Training Loss (NLML): -960.8077\n",
      "convergence dfGPdfNN Run 2/10, Epoch 898/1000, Training Loss (NLML): -960.7926\n",
      "convergence dfGPdfNN Run 2/10, Epoch 899/1000, Training Loss (NLML): -960.8232\n",
      "convergence dfGPdfNN Run 2/10, Epoch 900/1000, Training Loss (NLML): -960.7947\n",
      "convergence dfGPdfNN Run 2/10, Epoch 901/1000, Training Loss (NLML): -960.8274\n",
      "convergence dfGPdfNN Run 2/10, Epoch 902/1000, Training Loss (NLML): -960.8010\n",
      "convergence dfGPdfNN Run 2/10, Epoch 903/1000, Training Loss (NLML): -960.8052\n",
      "convergence dfGPdfNN Run 2/10, Epoch 904/1000, Training Loss (NLML): -960.8337\n",
      "convergence dfGPdfNN Run 2/10, Epoch 905/1000, Training Loss (NLML): -960.8324\n",
      "convergence dfGPdfNN Run 2/10, Epoch 906/1000, Training Loss (NLML): -960.8348\n",
      "convergence dfGPdfNN Run 2/10, Epoch 907/1000, Training Loss (NLML): -960.8073\n",
      "convergence dfGPdfNN Run 2/10, Epoch 908/1000, Training Loss (NLML): -960.8112\n",
      "convergence dfGPdfNN Run 2/10, Epoch 909/1000, Training Loss (NLML): -960.8452\n",
      "convergence dfGPdfNN Run 2/10, Epoch 910/1000, Training Loss (NLML): -960.8488\n",
      "convergence dfGPdfNN Run 2/10, Epoch 911/1000, Training Loss (NLML): -960.8312\n",
      "convergence dfGPdfNN Run 2/10, Epoch 912/1000, Training Loss (NLML): -960.8346\n",
      "convergence dfGPdfNN Run 2/10, Epoch 913/1000, Training Loss (NLML): -960.8342\n",
      "convergence dfGPdfNN Run 2/10, Epoch 914/1000, Training Loss (NLML): -960.8489\n",
      "convergence dfGPdfNN Run 2/10, Epoch 915/1000, Training Loss (NLML): -960.8433\n",
      "convergence dfGPdfNN Run 2/10, Epoch 916/1000, Training Loss (NLML): -960.8446\n",
      "convergence dfGPdfNN Run 2/10, Epoch 917/1000, Training Loss (NLML): -960.8475\n",
      "convergence dfGPdfNN Run 2/10, Epoch 918/1000, Training Loss (NLML): -960.8560\n",
      "convergence dfGPdfNN Run 2/10, Epoch 919/1000, Training Loss (NLML): -960.8285\n",
      "convergence dfGPdfNN Run 2/10, Epoch 920/1000, Training Loss (NLML): -960.8591\n",
      "convergence dfGPdfNN Run 2/10, Epoch 921/1000, Training Loss (NLML): -960.8652\n",
      "convergence dfGPdfNN Run 2/10, Epoch 922/1000, Training Loss (NLML): -960.8391\n",
      "convergence dfGPdfNN Run 2/10, Epoch 923/1000, Training Loss (NLML): -960.8590\n",
      "convergence dfGPdfNN Run 2/10, Epoch 924/1000, Training Loss (NLML): -960.8685\n",
      "convergence dfGPdfNN Run 2/10, Epoch 925/1000, Training Loss (NLML): -960.8687\n",
      "convergence dfGPdfNN Run 2/10, Epoch 926/1000, Training Loss (NLML): -960.8700\n",
      "convergence dfGPdfNN Run 2/10, Epoch 927/1000, Training Loss (NLML): -960.8453\n",
      "convergence dfGPdfNN Run 2/10, Epoch 928/1000, Training Loss (NLML): -960.8514\n",
      "convergence dfGPdfNN Run 2/10, Epoch 929/1000, Training Loss (NLML): -960.8833\n",
      "convergence dfGPdfNN Run 2/10, Epoch 930/1000, Training Loss (NLML): -960.8811\n",
      "convergence dfGPdfNN Run 2/10, Epoch 931/1000, Training Loss (NLML): -960.8762\n",
      "convergence dfGPdfNN Run 2/10, Epoch 932/1000, Training Loss (NLML): -960.8785\n",
      "convergence dfGPdfNN Run 2/10, Epoch 933/1000, Training Loss (NLML): -960.8792\n",
      "convergence dfGPdfNN Run 2/10, Epoch 934/1000, Training Loss (NLML): -960.8506\n",
      "convergence dfGPdfNN Run 2/10, Epoch 935/1000, Training Loss (NLML): -960.8712\n",
      "convergence dfGPdfNN Run 2/10, Epoch 936/1000, Training Loss (NLML): -960.8909\n",
      "convergence dfGPdfNN Run 2/10, Epoch 937/1000, Training Loss (NLML): -960.8928\n",
      "convergence dfGPdfNN Run 2/10, Epoch 938/1000, Training Loss (NLML): -960.8865\n",
      "convergence dfGPdfNN Run 2/10, Epoch 939/1000, Training Loss (NLML): -960.8879\n",
      "convergence dfGPdfNN Run 2/10, Epoch 940/1000, Training Loss (NLML): -960.8706\n",
      "convergence dfGPdfNN Run 2/10, Epoch 941/1000, Training Loss (NLML): -960.8740\n",
      "convergence dfGPdfNN Run 2/10, Epoch 942/1000, Training Loss (NLML): -960.8748\n",
      "convergence dfGPdfNN Run 2/10, Epoch 943/1000, Training Loss (NLML): -960.8889\n",
      "convergence dfGPdfNN Run 2/10, Epoch 944/1000, Training Loss (NLML): -960.8925\n",
      "convergence dfGPdfNN Run 2/10, Epoch 945/1000, Training Loss (NLML): -960.8959\n",
      "convergence dfGPdfNN Run 2/10, Epoch 946/1000, Training Loss (NLML): -960.8981\n",
      "convergence dfGPdfNN Run 2/10, Epoch 947/1000, Training Loss (NLML): -960.8859\n",
      "convergence dfGPdfNN Run 2/10, Epoch 948/1000, Training Loss (NLML): -960.8718\n",
      "convergence dfGPdfNN Run 2/10, Epoch 949/1000, Training Loss (NLML): -960.9016\n",
      "convergence dfGPdfNN Run 2/10, Epoch 950/1000, Training Loss (NLML): -960.9022\n",
      "convergence dfGPdfNN Run 2/10, Epoch 951/1000, Training Loss (NLML): -960.9058\n",
      "convergence dfGPdfNN Run 2/10, Epoch 952/1000, Training Loss (NLML): -960.9092\n",
      "convergence dfGPdfNN Run 2/10, Epoch 953/1000, Training Loss (NLML): -960.8956\n",
      "convergence dfGPdfNN Run 2/10, Epoch 954/1000, Training Loss (NLML): -960.8969\n",
      "convergence dfGPdfNN Run 2/10, Epoch 955/1000, Training Loss (NLML): -960.9116\n",
      "convergence dfGPdfNN Run 2/10, Epoch 956/1000, Training Loss (NLML): -960.9050\n",
      "convergence dfGPdfNN Run 2/10, Epoch 957/1000, Training Loss (NLML): -960.9039\n",
      "convergence dfGPdfNN Run 2/10, Epoch 958/1000, Training Loss (NLML): -960.9092\n",
      "convergence dfGPdfNN Run 2/10, Epoch 959/1000, Training Loss (NLML): -960.8953\n",
      "convergence dfGPdfNN Run 2/10, Epoch 960/1000, Training Loss (NLML): -960.8960\n",
      "convergence dfGPdfNN Run 2/10, Epoch 961/1000, Training Loss (NLML): -960.9120\n",
      "convergence dfGPdfNN Run 2/10, Epoch 962/1000, Training Loss (NLML): -960.9125\n",
      "convergence dfGPdfNN Run 2/10, Epoch 963/1000, Training Loss (NLML): -960.9164\n",
      "convergence dfGPdfNN Run 2/10, Epoch 964/1000, Training Loss (NLML): -960.9180\n",
      "convergence dfGPdfNN Run 2/10, Epoch 965/1000, Training Loss (NLML): -960.9189\n",
      "convergence dfGPdfNN Run 2/10, Epoch 966/1000, Training Loss (NLML): -960.9076\n",
      "convergence dfGPdfNN Run 2/10, Epoch 967/1000, Training Loss (NLML): -960.9152\n",
      "convergence dfGPdfNN Run 2/10, Epoch 968/1000, Training Loss (NLML): -960.9128\n",
      "convergence dfGPdfNN Run 2/10, Epoch 969/1000, Training Loss (NLML): -960.9268\n",
      "convergence dfGPdfNN Run 2/10, Epoch 970/1000, Training Loss (NLML): -960.9261\n",
      "convergence dfGPdfNN Run 2/10, Epoch 971/1000, Training Loss (NLML): -960.9258\n",
      "convergence dfGPdfNN Run 2/10, Epoch 972/1000, Training Loss (NLML): -960.9285\n",
      "convergence dfGPdfNN Run 2/10, Epoch 973/1000, Training Loss (NLML): -960.9324\n",
      "convergence dfGPdfNN Run 2/10, Epoch 974/1000, Training Loss (NLML): -960.9321\n",
      "convergence dfGPdfNN Run 2/10, Epoch 975/1000, Training Loss (NLML): -960.9360\n",
      "convergence dfGPdfNN Run 2/10, Epoch 976/1000, Training Loss (NLML): -960.9355\n",
      "convergence dfGPdfNN Run 2/10, Epoch 977/1000, Training Loss (NLML): -960.9382\n",
      "convergence dfGPdfNN Run 2/10, Epoch 978/1000, Training Loss (NLML): -960.9395\n",
      "convergence dfGPdfNN Run 2/10, Epoch 979/1000, Training Loss (NLML): -960.9370\n",
      "convergence dfGPdfNN Run 2/10, Epoch 980/1000, Training Loss (NLML): -960.9375\n",
      "convergence dfGPdfNN Run 2/10, Epoch 981/1000, Training Loss (NLML): -960.9410\n",
      "convergence dfGPdfNN Run 2/10, Epoch 982/1000, Training Loss (NLML): -960.9337\n",
      "convergence dfGPdfNN Run 2/10, Epoch 983/1000, Training Loss (NLML): -960.9337\n",
      "convergence dfGPdfNN Run 2/10, Epoch 984/1000, Training Loss (NLML): -960.9348\n",
      "convergence dfGPdfNN Run 2/10, Epoch 985/1000, Training Loss (NLML): -960.9296\n",
      "convergence dfGPdfNN Run 2/10, Epoch 986/1000, Training Loss (NLML): -960.9254\n",
      "convergence dfGPdfNN Run 2/10, Epoch 987/1000, Training Loss (NLML): -960.9263\n",
      "convergence dfGPdfNN Run 2/10, Epoch 988/1000, Training Loss (NLML): -960.9309\n",
      "convergence dfGPdfNN Run 2/10, Epoch 989/1000, Training Loss (NLML): -960.9333\n",
      "convergence dfGPdfNN Run 2/10, Epoch 990/1000, Training Loss (NLML): -960.9353\n",
      "convergence dfGPdfNN Run 2/10, Epoch 991/1000, Training Loss (NLML): -960.9302\n",
      "convergence dfGPdfNN Run 2/10, Epoch 992/1000, Training Loss (NLML): -960.9396\n",
      "convergence dfGPdfNN Run 2/10, Epoch 993/1000, Training Loss (NLML): -960.9390\n",
      "convergence dfGPdfNN Run 2/10, Epoch 994/1000, Training Loss (NLML): -960.9377\n",
      "convergence dfGPdfNN Run 2/10, Epoch 995/1000, Training Loss (NLML): -960.9395\n",
      "convergence dfGPdfNN Run 2/10, Epoch 996/1000, Training Loss (NLML): -960.9373\n",
      "convergence dfGPdfNN Run 2/10, Epoch 997/1000, Training Loss (NLML): -960.9320\n",
      "convergence dfGPdfNN Run 2/10, Epoch 998/1000, Training Loss (NLML): -960.9431\n",
      "convergence dfGPdfNN Run 2/10, Epoch 999/1000, Training Loss (NLML): -960.9371\n",
      "convergence dfGPdfNN Run 2/10, Epoch 1000/1000, Training Loss (NLML): -960.9424\n",
      "\n",
      "--- Training Run 3/10 ---\n",
      "\n",
      "Start Training\n",
      "convergence dfGPdfNN Run 3/10, Epoch 1/1000, Training Loss (NLML): -862.1488\n",
      "convergence dfGPdfNN Run 3/10, Epoch 2/1000, Training Loss (NLML): -875.7240\n",
      "convergence dfGPdfNN Run 3/10, Epoch 3/1000, Training Loss (NLML): -882.3160\n",
      "convergence dfGPdfNN Run 3/10, Epoch 4/1000, Training Loss (NLML): -885.6136\n",
      "convergence dfGPdfNN Run 3/10, Epoch 5/1000, Training Loss (NLML): -888.4983\n",
      "convergence dfGPdfNN Run 3/10, Epoch 6/1000, Training Loss (NLML): -891.2635\n",
      "convergence dfGPdfNN Run 3/10, Epoch 7/1000, Training Loss (NLML): -894.0785\n",
      "convergence dfGPdfNN Run 3/10, Epoch 8/1000, Training Loss (NLML): -896.0087\n",
      "convergence dfGPdfNN Run 3/10, Epoch 9/1000, Training Loss (NLML): -898.1326\n",
      "convergence dfGPdfNN Run 3/10, Epoch 10/1000, Training Loss (NLML): -900.1550\n",
      "convergence dfGPdfNN Run 3/10, Epoch 11/1000, Training Loss (NLML): -901.8853\n",
      "convergence dfGPdfNN Run 3/10, Epoch 12/1000, Training Loss (NLML): -903.6483\n",
      "convergence dfGPdfNN Run 3/10, Epoch 13/1000, Training Loss (NLML): -905.3789\n",
      "convergence dfGPdfNN Run 3/10, Epoch 14/1000, Training Loss (NLML): -907.1729\n",
      "convergence dfGPdfNN Run 3/10, Epoch 15/1000, Training Loss (NLML): -908.7870\n",
      "convergence dfGPdfNN Run 3/10, Epoch 16/1000, Training Loss (NLML): -910.3483\n",
      "convergence dfGPdfNN Run 3/10, Epoch 17/1000, Training Loss (NLML): -911.8600\n",
      "convergence dfGPdfNN Run 3/10, Epoch 18/1000, Training Loss (NLML): -913.1165\n",
      "convergence dfGPdfNN Run 3/10, Epoch 19/1000, Training Loss (NLML): -914.3374\n",
      "convergence dfGPdfNN Run 3/10, Epoch 20/1000, Training Loss (NLML): -915.7462\n",
      "convergence dfGPdfNN Run 3/10, Epoch 21/1000, Training Loss (NLML): -917.0553\n",
      "convergence dfGPdfNN Run 3/10, Epoch 22/1000, Training Loss (NLML): -918.1902\n",
      "convergence dfGPdfNN Run 3/10, Epoch 23/1000, Training Loss (NLML): -919.1698\n",
      "convergence dfGPdfNN Run 3/10, Epoch 24/1000, Training Loss (NLML): -920.3804\n",
      "convergence dfGPdfNN Run 3/10, Epoch 25/1000, Training Loss (NLML): -921.3977\n",
      "convergence dfGPdfNN Run 3/10, Epoch 26/1000, Training Loss (NLML): -922.4009\n",
      "convergence dfGPdfNN Run 3/10, Epoch 27/1000, Training Loss (NLML): -923.1918\n",
      "convergence dfGPdfNN Run 3/10, Epoch 28/1000, Training Loss (NLML): -923.5127\n",
      "convergence dfGPdfNN Run 3/10, Epoch 29/1000, Training Loss (NLML): -924.9617\n",
      "convergence dfGPdfNN Run 3/10, Epoch 30/1000, Training Loss (NLML): -925.9597\n",
      "convergence dfGPdfNN Run 3/10, Epoch 31/1000, Training Loss (NLML): -926.7715\n",
      "convergence dfGPdfNN Run 3/10, Epoch 32/1000, Training Loss (NLML): -927.5536\n",
      "convergence dfGPdfNN Run 3/10, Epoch 33/1000, Training Loss (NLML): -927.9907\n",
      "convergence dfGPdfNN Run 3/10, Epoch 34/1000, Training Loss (NLML): -928.9550\n",
      "convergence dfGPdfNN Run 3/10, Epoch 35/1000, Training Loss (NLML): -929.6284\n",
      "convergence dfGPdfNN Run 3/10, Epoch 36/1000, Training Loss (NLML): -930.3464\n",
      "convergence dfGPdfNN Run 3/10, Epoch 37/1000, Training Loss (NLML): -931.0604\n",
      "convergence dfGPdfNN Run 3/10, Epoch 38/1000, Training Loss (NLML): -931.6627\n",
      "convergence dfGPdfNN Run 3/10, Epoch 39/1000, Training Loss (NLML): -932.2719\n",
      "convergence dfGPdfNN Run 3/10, Epoch 40/1000, Training Loss (NLML): -932.8600\n",
      "convergence dfGPdfNN Run 3/10, Epoch 41/1000, Training Loss (NLML): -933.2101\n",
      "convergence dfGPdfNN Run 3/10, Epoch 42/1000, Training Loss (NLML): -933.7808\n",
      "convergence dfGPdfNN Run 3/10, Epoch 43/1000, Training Loss (NLML): -934.5194\n",
      "convergence dfGPdfNN Run 3/10, Epoch 44/1000, Training Loss (NLML): -934.9761\n",
      "convergence dfGPdfNN Run 3/10, Epoch 45/1000, Training Loss (NLML): -935.4098\n",
      "convergence dfGPdfNN Run 3/10, Epoch 46/1000, Training Loss (NLML): -936.0702\n",
      "convergence dfGPdfNN Run 3/10, Epoch 47/1000, Training Loss (NLML): -936.5494\n",
      "convergence dfGPdfNN Run 3/10, Epoch 48/1000, Training Loss (NLML): -936.8783\n",
      "convergence dfGPdfNN Run 3/10, Epoch 49/1000, Training Loss (NLML): -937.2791\n",
      "convergence dfGPdfNN Run 3/10, Epoch 50/1000, Training Loss (NLML): -937.7358\n",
      "convergence dfGPdfNN Run 3/10, Epoch 51/1000, Training Loss (NLML): -937.3933\n",
      "convergence dfGPdfNN Run 3/10, Epoch 52/1000, Training Loss (NLML): -938.6793\n",
      "convergence dfGPdfNN Run 3/10, Epoch 53/1000, Training Loss (NLML): -938.9773\n",
      "convergence dfGPdfNN Run 3/10, Epoch 54/1000, Training Loss (NLML): -939.1362\n",
      "convergence dfGPdfNN Run 3/10, Epoch 55/1000, Training Loss (NLML): -939.9271\n",
      "convergence dfGPdfNN Run 3/10, Epoch 56/1000, Training Loss (NLML): -940.3271\n",
      "convergence dfGPdfNN Run 3/10, Epoch 57/1000, Training Loss (NLML): -940.6824\n",
      "convergence dfGPdfNN Run 3/10, Epoch 58/1000, Training Loss (NLML): -940.9139\n",
      "convergence dfGPdfNN Run 3/10, Epoch 59/1000, Training Loss (NLML): -941.2456\n",
      "convergence dfGPdfNN Run 3/10, Epoch 60/1000, Training Loss (NLML): -941.2520\n",
      "convergence dfGPdfNN Run 3/10, Epoch 61/1000, Training Loss (NLML): -941.8751\n",
      "convergence dfGPdfNN Run 3/10, Epoch 62/1000, Training Loss (NLML): -942.3672\n",
      "convergence dfGPdfNN Run 3/10, Epoch 63/1000, Training Loss (NLML): -942.1173\n",
      "convergence dfGPdfNN Run 3/10, Epoch 64/1000, Training Loss (NLML): -942.9634\n",
      "convergence dfGPdfNN Run 3/10, Epoch 65/1000, Training Loss (NLML): -943.1914\n",
      "convergence dfGPdfNN Run 3/10, Epoch 66/1000, Training Loss (NLML): -943.5547\n",
      "convergence dfGPdfNN Run 3/10, Epoch 67/1000, Training Loss (NLML): -943.6008\n",
      "convergence dfGPdfNN Run 3/10, Epoch 68/1000, Training Loss (NLML): -943.7786\n",
      "convergence dfGPdfNN Run 3/10, Epoch 69/1000, Training Loss (NLML): -944.4091\n",
      "convergence dfGPdfNN Run 3/10, Epoch 70/1000, Training Loss (NLML): -944.0654\n",
      "convergence dfGPdfNN Run 3/10, Epoch 71/1000, Training Loss (NLML): -944.7344\n",
      "convergence dfGPdfNN Run 3/10, Epoch 72/1000, Training Loss (NLML): -944.9376\n",
      "convergence dfGPdfNN Run 3/10, Epoch 73/1000, Training Loss (NLML): -945.2551\n",
      "convergence dfGPdfNN Run 3/10, Epoch 74/1000, Training Loss (NLML): -945.5725\n",
      "convergence dfGPdfNN Run 3/10, Epoch 75/1000, Training Loss (NLML): -946.0028\n",
      "convergence dfGPdfNN Run 3/10, Epoch 76/1000, Training Loss (NLML): -945.9460\n",
      "convergence dfGPdfNN Run 3/10, Epoch 77/1000, Training Loss (NLML): -943.5714\n",
      "convergence dfGPdfNN Run 3/10, Epoch 78/1000, Training Loss (NLML): -945.9829\n",
      "convergence dfGPdfNN Run 3/10, Epoch 79/1000, Training Loss (NLML): -945.4730\n",
      "convergence dfGPdfNN Run 3/10, Epoch 80/1000, Training Loss (NLML): -946.7584\n",
      "convergence dfGPdfNN Run 3/10, Epoch 81/1000, Training Loss (NLML): -946.9357\n",
      "convergence dfGPdfNN Run 3/10, Epoch 82/1000, Training Loss (NLML): -947.1116\n",
      "convergence dfGPdfNN Run 3/10, Epoch 83/1000, Training Loss (NLML): -947.2727\n",
      "convergence dfGPdfNN Run 3/10, Epoch 84/1000, Training Loss (NLML): -947.6954\n",
      "convergence dfGPdfNN Run 3/10, Epoch 85/1000, Training Loss (NLML): -947.8717\n",
      "convergence dfGPdfNN Run 3/10, Epoch 86/1000, Training Loss (NLML): -948.0789\n",
      "convergence dfGPdfNN Run 3/10, Epoch 87/1000, Training Loss (NLML): -947.0686\n",
      "convergence dfGPdfNN Run 3/10, Epoch 88/1000, Training Loss (NLML): -948.3057\n",
      "convergence dfGPdfNN Run 3/10, Epoch 89/1000, Training Loss (NLML): -948.6558\n",
      "convergence dfGPdfNN Run 3/10, Epoch 90/1000, Training Loss (NLML): -947.8103\n",
      "convergence dfGPdfNN Run 3/10, Epoch 91/1000, Training Loss (NLML): -949.0024\n",
      "convergence dfGPdfNN Run 3/10, Epoch 92/1000, Training Loss (NLML): -948.9150\n",
      "convergence dfGPdfNN Run 3/10, Epoch 93/1000, Training Loss (NLML): -949.3092\n",
      "convergence dfGPdfNN Run 3/10, Epoch 94/1000, Training Loss (NLML): -943.7252\n",
      "convergence dfGPdfNN Run 3/10, Epoch 95/1000, Training Loss (NLML): -949.6289\n",
      "convergence dfGPdfNN Run 3/10, Epoch 96/1000, Training Loss (NLML): -949.6085\n",
      "convergence dfGPdfNN Run 3/10, Epoch 97/1000, Training Loss (NLML): -949.5721\n",
      "convergence dfGPdfNN Run 3/10, Epoch 98/1000, Training Loss (NLML): -949.7584\n",
      "convergence dfGPdfNN Run 3/10, Epoch 99/1000, Training Loss (NLML): -949.9359\n",
      "convergence dfGPdfNN Run 3/10, Epoch 100/1000, Training Loss (NLML): -950.0370\n",
      "convergence dfGPdfNN Run 3/10, Epoch 101/1000, Training Loss (NLML): -949.6040\n",
      "convergence dfGPdfNN Run 3/10, Epoch 102/1000, Training Loss (NLML): -949.4521\n",
      "convergence dfGPdfNN Run 3/10, Epoch 103/1000, Training Loss (NLML): -949.6877\n",
      "convergence dfGPdfNN Run 3/10, Epoch 104/1000, Training Loss (NLML): -950.3147\n",
      "convergence dfGPdfNN Run 3/10, Epoch 105/1000, Training Loss (NLML): -950.4655\n",
      "convergence dfGPdfNN Run 3/10, Epoch 106/1000, Training Loss (NLML): -950.5851\n",
      "convergence dfGPdfNN Run 3/10, Epoch 107/1000, Training Loss (NLML): -950.7599\n",
      "convergence dfGPdfNN Run 3/10, Epoch 108/1000, Training Loss (NLML): -950.4789\n",
      "convergence dfGPdfNN Run 3/10, Epoch 109/1000, Training Loss (NLML): -950.5601\n",
      "convergence dfGPdfNN Run 3/10, Epoch 110/1000, Training Loss (NLML): -951.1090\n",
      "convergence dfGPdfNN Run 3/10, Epoch 111/1000, Training Loss (NLML): -950.4209\n",
      "convergence dfGPdfNN Run 3/10, Epoch 112/1000, Training Loss (NLML): -951.2168\n",
      "convergence dfGPdfNN Run 3/10, Epoch 113/1000, Training Loss (NLML): -951.3761\n",
      "convergence dfGPdfNN Run 3/10, Epoch 114/1000, Training Loss (NLML): -951.4969\n",
      "convergence dfGPdfNN Run 3/10, Epoch 115/1000, Training Loss (NLML): -951.6798\n",
      "convergence dfGPdfNN Run 3/10, Epoch 116/1000, Training Loss (NLML): -951.2542\n",
      "convergence dfGPdfNN Run 3/10, Epoch 117/1000, Training Loss (NLML): -947.3206\n",
      "convergence dfGPdfNN Run 3/10, Epoch 118/1000, Training Loss (NLML): -951.6111\n",
      "convergence dfGPdfNN Run 3/10, Epoch 119/1000, Training Loss (NLML): -952.0695\n",
      "convergence dfGPdfNN Run 3/10, Epoch 120/1000, Training Loss (NLML): -952.3500\n",
      "convergence dfGPdfNN Run 3/10, Epoch 121/1000, Training Loss (NLML): -951.2341\n",
      "convergence dfGPdfNN Run 3/10, Epoch 122/1000, Training Loss (NLML): -951.7855\n",
      "convergence dfGPdfNN Run 3/10, Epoch 123/1000, Training Loss (NLML): -952.7742\n",
      "convergence dfGPdfNN Run 3/10, Epoch 124/1000, Training Loss (NLML): -952.4769\n",
      "convergence dfGPdfNN Run 3/10, Epoch 125/1000, Training Loss (NLML): -952.1654\n",
      "convergence dfGPdfNN Run 3/10, Epoch 126/1000, Training Loss (NLML): -951.3602\n",
      "convergence dfGPdfNN Run 3/10, Epoch 127/1000, Training Loss (NLML): -952.0878\n",
      "convergence dfGPdfNN Run 3/10, Epoch 128/1000, Training Loss (NLML): -952.1210\n",
      "convergence dfGPdfNN Run 3/10, Epoch 129/1000, Training Loss (NLML): -952.8850\n",
      "convergence dfGPdfNN Run 3/10, Epoch 130/1000, Training Loss (NLML): -952.9512\n",
      "convergence dfGPdfNN Run 3/10, Epoch 131/1000, Training Loss (NLML): -952.9750\n",
      "convergence dfGPdfNN Run 3/10, Epoch 132/1000, Training Loss (NLML): -953.1869\n",
      "convergence dfGPdfNN Run 3/10, Epoch 133/1000, Training Loss (NLML): -953.0184\n",
      "convergence dfGPdfNN Run 3/10, Epoch 134/1000, Training Loss (NLML): -952.6907\n",
      "convergence dfGPdfNN Run 3/10, Epoch 135/1000, Training Loss (NLML): -951.7029\n",
      "convergence dfGPdfNN Run 3/10, Epoch 136/1000, Training Loss (NLML): -951.7831\n",
      "convergence dfGPdfNN Run 3/10, Epoch 137/1000, Training Loss (NLML): -953.0813\n",
      "convergence dfGPdfNN Run 3/10, Epoch 138/1000, Training Loss (NLML): -953.0818\n",
      "convergence dfGPdfNN Run 3/10, Epoch 139/1000, Training Loss (NLML): -952.6069\n",
      "convergence dfGPdfNN Run 3/10, Epoch 140/1000, Training Loss (NLML): -952.8323\n",
      "convergence dfGPdfNN Run 3/10, Epoch 141/1000, Training Loss (NLML): -953.1631\n",
      "convergence dfGPdfNN Run 3/10, Epoch 142/1000, Training Loss (NLML): -950.2722\n",
      "convergence dfGPdfNN Run 3/10, Epoch 143/1000, Training Loss (NLML): -953.1754\n",
      "convergence dfGPdfNN Run 3/10, Epoch 144/1000, Training Loss (NLML): -953.2831\n",
      "convergence dfGPdfNN Run 3/10, Epoch 145/1000, Training Loss (NLML): -953.5039\n",
      "convergence dfGPdfNN Run 3/10, Epoch 146/1000, Training Loss (NLML): -953.4917\n",
      "convergence dfGPdfNN Run 3/10, Epoch 147/1000, Training Loss (NLML): -953.5406\n",
      "convergence dfGPdfNN Run 3/10, Epoch 148/1000, Training Loss (NLML): -953.8207\n",
      "convergence dfGPdfNN Run 3/10, Epoch 149/1000, Training Loss (NLML): -953.8260\n",
      "convergence dfGPdfNN Run 3/10, Epoch 150/1000, Training Loss (NLML): -950.8331\n",
      "convergence dfGPdfNN Run 3/10, Epoch 151/1000, Training Loss (NLML): -953.6515\n",
      "convergence dfGPdfNN Run 3/10, Epoch 152/1000, Training Loss (NLML): -953.7815\n",
      "convergence dfGPdfNN Run 3/10, Epoch 153/1000, Training Loss (NLML): -953.7491\n",
      "convergence dfGPdfNN Run 3/10, Epoch 154/1000, Training Loss (NLML): -953.7318\n",
      "convergence dfGPdfNN Run 3/10, Epoch 155/1000, Training Loss (NLML): -953.9240\n",
      "convergence dfGPdfNN Run 3/10, Epoch 156/1000, Training Loss (NLML): -954.1317\n",
      "convergence dfGPdfNN Run 3/10, Epoch 157/1000, Training Loss (NLML): -954.2185\n",
      "convergence dfGPdfNN Run 3/10, Epoch 158/1000, Training Loss (NLML): -954.2472\n",
      "convergence dfGPdfNN Run 3/10, Epoch 159/1000, Training Loss (NLML): -954.1592\n",
      "convergence dfGPdfNN Run 3/10, Epoch 160/1000, Training Loss (NLML): -954.5420\n",
      "convergence dfGPdfNN Run 3/10, Epoch 161/1000, Training Loss (NLML): -954.5183\n",
      "convergence dfGPdfNN Run 3/10, Epoch 162/1000, Training Loss (NLML): -954.5089\n",
      "convergence dfGPdfNN Run 3/10, Epoch 163/1000, Training Loss (NLML): -954.5095\n",
      "convergence dfGPdfNN Run 3/10, Epoch 164/1000, Training Loss (NLML): -954.4666\n",
      "convergence dfGPdfNN Run 3/10, Epoch 165/1000, Training Loss (NLML): -954.3955\n",
      "convergence dfGPdfNN Run 3/10, Epoch 166/1000, Training Loss (NLML): -953.8506\n",
      "convergence dfGPdfNN Run 3/10, Epoch 167/1000, Training Loss (NLML): -952.0356\n",
      "convergence dfGPdfNN Run 3/10, Epoch 168/1000, Training Loss (NLML): -954.3069\n",
      "convergence dfGPdfNN Run 3/10, Epoch 169/1000, Training Loss (NLML): -954.2358\n",
      "convergence dfGPdfNN Run 3/10, Epoch 170/1000, Training Loss (NLML): -954.3728\n",
      "convergence dfGPdfNN Run 3/10, Epoch 171/1000, Training Loss (NLML): -954.4771\n",
      "convergence dfGPdfNN Run 3/10, Epoch 172/1000, Training Loss (NLML): -954.4951\n",
      "convergence dfGPdfNN Run 3/10, Epoch 173/1000, Training Loss (NLML): -954.4301\n",
      "convergence dfGPdfNN Run 3/10, Epoch 174/1000, Training Loss (NLML): -954.2971\n",
      "convergence dfGPdfNN Run 3/10, Epoch 175/1000, Training Loss (NLML): -954.2517\n",
      "convergence dfGPdfNN Run 3/10, Epoch 176/1000, Training Loss (NLML): -954.2513\n",
      "convergence dfGPdfNN Run 3/10, Epoch 177/1000, Training Loss (NLML): -954.2623\n",
      "convergence dfGPdfNN Run 3/10, Epoch 178/1000, Training Loss (NLML): -954.5474\n",
      "convergence dfGPdfNN Run 3/10, Epoch 179/1000, Training Loss (NLML): -954.6447\n",
      "convergence dfGPdfNN Run 3/10, Epoch 180/1000, Training Loss (NLML): -954.7396\n",
      "convergence dfGPdfNN Run 3/10, Epoch 181/1000, Training Loss (NLML): -954.6473\n",
      "convergence dfGPdfNN Run 3/10, Epoch 182/1000, Training Loss (NLML): -954.5198\n",
      "convergence dfGPdfNN Run 3/10, Epoch 183/1000, Training Loss (NLML): -954.6082\n",
      "convergence dfGPdfNN Run 3/10, Epoch 184/1000, Training Loss (NLML): -954.2898\n",
      "convergence dfGPdfNN Run 3/10, Epoch 185/1000, Training Loss (NLML): -954.1307\n",
      "convergence dfGPdfNN Run 3/10, Epoch 186/1000, Training Loss (NLML): -953.8308\n",
      "convergence dfGPdfNN Run 3/10, Epoch 187/1000, Training Loss (NLML): -954.2990\n",
      "convergence dfGPdfNN Run 3/10, Epoch 188/1000, Training Loss (NLML): -954.4215\n",
      "convergence dfGPdfNN Run 3/10, Epoch 189/1000, Training Loss (NLML): -954.4783\n",
      "convergence dfGPdfNN Run 3/10, Epoch 190/1000, Training Loss (NLML): -954.5798\n",
      "convergence dfGPdfNN Run 3/10, Epoch 191/1000, Training Loss (NLML): -954.6792\n",
      "convergence dfGPdfNN Run 3/10, Epoch 192/1000, Training Loss (NLML): -954.3210\n",
      "convergence dfGPdfNN Run 3/10, Epoch 193/1000, Training Loss (NLML): -954.9576\n",
      "convergence dfGPdfNN Run 3/10, Epoch 194/1000, Training Loss (NLML): -955.0562\n",
      "convergence dfGPdfNN Run 3/10, Epoch 195/1000, Training Loss (NLML): -955.1069\n",
      "convergence dfGPdfNN Run 3/10, Epoch 196/1000, Training Loss (NLML): -955.1553\n",
      "convergence dfGPdfNN Run 3/10, Epoch 197/1000, Training Loss (NLML): -955.1455\n",
      "convergence dfGPdfNN Run 3/10, Epoch 198/1000, Training Loss (NLML): -955.2961\n",
      "convergence dfGPdfNN Run 3/10, Epoch 199/1000, Training Loss (NLML): -955.4453\n",
      "convergence dfGPdfNN Run 3/10, Epoch 200/1000, Training Loss (NLML): -955.3357\n",
      "convergence dfGPdfNN Run 3/10, Epoch 201/1000, Training Loss (NLML): -955.0853\n",
      "convergence dfGPdfNN Run 3/10, Epoch 202/1000, Training Loss (NLML): -954.9969\n",
      "convergence dfGPdfNN Run 3/10, Epoch 203/1000, Training Loss (NLML): -954.9963\n",
      "convergence dfGPdfNN Run 3/10, Epoch 204/1000, Training Loss (NLML): -955.1462\n",
      "convergence dfGPdfNN Run 3/10, Epoch 205/1000, Training Loss (NLML): -955.1581\n",
      "convergence dfGPdfNN Run 3/10, Epoch 206/1000, Training Loss (NLML): -955.2266\n",
      "convergence dfGPdfNN Run 3/10, Epoch 207/1000, Training Loss (NLML): -955.2262\n",
      "convergence dfGPdfNN Run 3/10, Epoch 208/1000, Training Loss (NLML): -955.2006\n",
      "convergence dfGPdfNN Run 3/10, Epoch 209/1000, Training Loss (NLML): -955.1886\n",
      "convergence dfGPdfNN Run 3/10, Epoch 210/1000, Training Loss (NLML): -955.2122\n",
      "convergence dfGPdfNN Run 3/10, Epoch 211/1000, Training Loss (NLML): -955.2821\n",
      "convergence dfGPdfNN Run 3/10, Epoch 212/1000, Training Loss (NLML): -955.3226\n",
      "convergence dfGPdfNN Run 3/10, Epoch 213/1000, Training Loss (NLML): -955.3622\n",
      "convergence dfGPdfNN Run 3/10, Epoch 214/1000, Training Loss (NLML): -954.6223\n",
      "convergence dfGPdfNN Run 3/10, Epoch 215/1000, Training Loss (NLML): -955.3849\n",
      "convergence dfGPdfNN Run 3/10, Epoch 216/1000, Training Loss (NLML): -955.4181\n",
      "convergence dfGPdfNN Run 3/10, Epoch 217/1000, Training Loss (NLML): -955.4536\n",
      "convergence dfGPdfNN Run 3/10, Epoch 218/1000, Training Loss (NLML): -955.6212\n",
      "convergence dfGPdfNN Run 3/10, Epoch 219/1000, Training Loss (NLML): -955.5275\n",
      "convergence dfGPdfNN Run 3/10, Epoch 220/1000, Training Loss (NLML): -955.6102\n",
      "convergence dfGPdfNN Run 3/10, Epoch 221/1000, Training Loss (NLML): -955.6555\n",
      "convergence dfGPdfNN Run 3/10, Epoch 222/1000, Training Loss (NLML): -955.7472\n",
      "convergence dfGPdfNN Run 3/10, Epoch 223/1000, Training Loss (NLML): -955.7660\n",
      "convergence dfGPdfNN Run 3/10, Epoch 224/1000, Training Loss (NLML): -955.8093\n",
      "convergence dfGPdfNN Run 3/10, Epoch 225/1000, Training Loss (NLML): -955.8425\n",
      "convergence dfGPdfNN Run 3/10, Epoch 226/1000, Training Loss (NLML): -955.8737\n",
      "convergence dfGPdfNN Run 3/10, Epoch 227/1000, Training Loss (NLML): -955.9758\n",
      "convergence dfGPdfNN Run 3/10, Epoch 228/1000, Training Loss (NLML): -955.9030\n",
      "convergence dfGPdfNN Run 3/10, Epoch 229/1000, Training Loss (NLML): -955.9872\n",
      "convergence dfGPdfNN Run 3/10, Epoch 230/1000, Training Loss (NLML): -955.9409\n",
      "convergence dfGPdfNN Run 3/10, Epoch 231/1000, Training Loss (NLML): -956.0750\n",
      "convergence dfGPdfNN Run 3/10, Epoch 232/1000, Training Loss (NLML): -956.0302\n",
      "convergence dfGPdfNN Run 3/10, Epoch 233/1000, Training Loss (NLML): -955.8485\n",
      "convergence dfGPdfNN Run 3/10, Epoch 234/1000, Training Loss (NLML): -955.6946\n",
      "convergence dfGPdfNN Run 3/10, Epoch 235/1000, Training Loss (NLML): -955.6533\n",
      "convergence dfGPdfNN Run 3/10, Epoch 236/1000, Training Loss (NLML): -955.6614\n",
      "convergence dfGPdfNN Run 3/10, Epoch 237/1000, Training Loss (NLML): -955.6562\n",
      "convergence dfGPdfNN Run 3/10, Epoch 238/1000, Training Loss (NLML): -955.7284\n",
      "convergence dfGPdfNN Run 3/10, Epoch 239/1000, Training Loss (NLML): -955.8073\n",
      "convergence dfGPdfNN Run 3/10, Epoch 240/1000, Training Loss (NLML): -955.7795\n",
      "convergence dfGPdfNN Run 3/10, Epoch 241/1000, Training Loss (NLML): -955.7529\n",
      "convergence dfGPdfNN Run 3/10, Epoch 242/1000, Training Loss (NLML): -955.7053\n",
      "convergence dfGPdfNN Run 3/10, Epoch 243/1000, Training Loss (NLML): -955.6958\n",
      "convergence dfGPdfNN Run 3/10, Epoch 244/1000, Training Loss (NLML): -955.6105\n",
      "convergence dfGPdfNN Run 3/10, Epoch 245/1000, Training Loss (NLML): -955.8787\n",
      "convergence dfGPdfNN Run 3/10, Epoch 246/1000, Training Loss (NLML): -955.9944\n",
      "convergence dfGPdfNN Run 3/10, Epoch 247/1000, Training Loss (NLML): -956.0260\n",
      "convergence dfGPdfNN Run 3/10, Epoch 248/1000, Training Loss (NLML): -955.9736\n",
      "convergence dfGPdfNN Run 3/10, Epoch 249/1000, Training Loss (NLML): -956.0066\n",
      "convergence dfGPdfNN Run 3/10, Epoch 250/1000, Training Loss (NLML): -956.0031\n",
      "convergence dfGPdfNN Run 3/10, Epoch 251/1000, Training Loss (NLML): -956.0339\n",
      "convergence dfGPdfNN Run 3/10, Epoch 252/1000, Training Loss (NLML): -956.0658\n",
      "convergence dfGPdfNN Run 3/10, Epoch 253/1000, Training Loss (NLML): -956.0750\n",
      "convergence dfGPdfNN Run 3/10, Epoch 254/1000, Training Loss (NLML): -956.0426\n",
      "convergence dfGPdfNN Run 3/10, Epoch 255/1000, Training Loss (NLML): -956.0592\n",
      "convergence dfGPdfNN Run 3/10, Epoch 256/1000, Training Loss (NLML): -956.0496\n",
      "convergence dfGPdfNN Run 3/10, Epoch 257/1000, Training Loss (NLML): -956.0753\n",
      "convergence dfGPdfNN Run 3/10, Epoch 258/1000, Training Loss (NLML): -956.0917\n",
      "convergence dfGPdfNN Run 3/10, Epoch 259/1000, Training Loss (NLML): -956.1669\n",
      "convergence dfGPdfNN Run 3/10, Epoch 260/1000, Training Loss (NLML): -956.2454\n",
      "convergence dfGPdfNN Run 3/10, Epoch 261/1000, Training Loss (NLML): -956.2844\n",
      "convergence dfGPdfNN Run 3/10, Epoch 262/1000, Training Loss (NLML): -956.3314\n",
      "convergence dfGPdfNN Run 3/10, Epoch 263/1000, Training Loss (NLML): -956.3668\n",
      "convergence dfGPdfNN Run 3/10, Epoch 264/1000, Training Loss (NLML): -956.3834\n",
      "convergence dfGPdfNN Run 3/10, Epoch 265/1000, Training Loss (NLML): -956.4131\n",
      "convergence dfGPdfNN Run 3/10, Epoch 266/1000, Training Loss (NLML): -956.4686\n",
      "convergence dfGPdfNN Run 3/10, Epoch 267/1000, Training Loss (NLML): -956.4313\n",
      "convergence dfGPdfNN Run 3/10, Epoch 268/1000, Training Loss (NLML): -956.4160\n",
      "convergence dfGPdfNN Run 3/10, Epoch 269/1000, Training Loss (NLML): -956.5463\n",
      "convergence dfGPdfNN Run 3/10, Epoch 270/1000, Training Loss (NLML): -956.5873\n",
      "convergence dfGPdfNN Run 3/10, Epoch 271/1000, Training Loss (NLML): -956.6075\n",
      "convergence dfGPdfNN Run 3/10, Epoch 272/1000, Training Loss (NLML): -956.6017\n",
      "convergence dfGPdfNN Run 3/10, Epoch 273/1000, Training Loss (NLML): -956.6536\n",
      "convergence dfGPdfNN Run 3/10, Epoch 274/1000, Training Loss (NLML): -956.6759\n",
      "convergence dfGPdfNN Run 3/10, Epoch 275/1000, Training Loss (NLML): -956.7223\n",
      "convergence dfGPdfNN Run 3/10, Epoch 276/1000, Training Loss (NLML): -956.7577\n",
      "convergence dfGPdfNN Run 3/10, Epoch 277/1000, Training Loss (NLML): -956.8018\n",
      "convergence dfGPdfNN Run 3/10, Epoch 278/1000, Training Loss (NLML): -956.7366\n",
      "convergence dfGPdfNN Run 3/10, Epoch 279/1000, Training Loss (NLML): -956.7550\n",
      "convergence dfGPdfNN Run 3/10, Epoch 280/1000, Training Loss (NLML): -956.6602\n",
      "convergence dfGPdfNN Run 3/10, Epoch 281/1000, Training Loss (NLML): -956.6848\n",
      "convergence dfGPdfNN Run 3/10, Epoch 282/1000, Training Loss (NLML): -956.7357\n",
      "convergence dfGPdfNN Run 3/10, Epoch 283/1000, Training Loss (NLML): -956.7628\n",
      "convergence dfGPdfNN Run 3/10, Epoch 284/1000, Training Loss (NLML): -956.7710\n",
      "convergence dfGPdfNN Run 3/10, Epoch 285/1000, Training Loss (NLML): -956.7987\n",
      "convergence dfGPdfNN Run 3/10, Epoch 286/1000, Training Loss (NLML): -956.8184\n",
      "convergence dfGPdfNN Run 3/10, Epoch 287/1000, Training Loss (NLML): -956.8317\n",
      "convergence dfGPdfNN Run 3/10, Epoch 288/1000, Training Loss (NLML): -956.8453\n",
      "convergence dfGPdfNN Run 3/10, Epoch 289/1000, Training Loss (NLML): -956.8499\n",
      "convergence dfGPdfNN Run 3/10, Epoch 290/1000, Training Loss (NLML): -956.8760\n",
      "convergence dfGPdfNN Run 3/10, Epoch 291/1000, Training Loss (NLML): -956.9417\n",
      "convergence dfGPdfNN Run 3/10, Epoch 292/1000, Training Loss (NLML): -956.9611\n",
      "convergence dfGPdfNN Run 3/10, Epoch 293/1000, Training Loss (NLML): -956.9834\n",
      "convergence dfGPdfNN Run 3/10, Epoch 294/1000, Training Loss (NLML): -956.9757\n",
      "convergence dfGPdfNN Run 3/10, Epoch 295/1000, Training Loss (NLML): -956.9360\n",
      "convergence dfGPdfNN Run 3/10, Epoch 296/1000, Training Loss (NLML): -956.9659\n",
      "convergence dfGPdfNN Run 3/10, Epoch 297/1000, Training Loss (NLML): -956.9741\n",
      "convergence dfGPdfNN Run 3/10, Epoch 298/1000, Training Loss (NLML): -956.9642\n",
      "convergence dfGPdfNN Run 3/10, Epoch 299/1000, Training Loss (NLML): -956.9875\n",
      "convergence dfGPdfNN Run 3/10, Epoch 300/1000, Training Loss (NLML): -956.9973\n",
      "convergence dfGPdfNN Run 3/10, Epoch 301/1000, Training Loss (NLML): -956.9813\n",
      "convergence dfGPdfNN Run 3/10, Epoch 302/1000, Training Loss (NLML): -957.0361\n",
      "convergence dfGPdfNN Run 3/10, Epoch 303/1000, Training Loss (NLML): -957.0804\n",
      "convergence dfGPdfNN Run 3/10, Epoch 304/1000, Training Loss (NLML): -957.1075\n",
      "convergence dfGPdfNN Run 3/10, Epoch 305/1000, Training Loss (NLML): -957.1315\n",
      "convergence dfGPdfNN Run 3/10, Epoch 306/1000, Training Loss (NLML): -957.1177\n",
      "convergence dfGPdfNN Run 3/10, Epoch 307/1000, Training Loss (NLML): -957.1261\n",
      "convergence dfGPdfNN Run 3/10, Epoch 308/1000, Training Loss (NLML): -957.1873\n",
      "convergence dfGPdfNN Run 3/10, Epoch 309/1000, Training Loss (NLML): -957.2079\n",
      "convergence dfGPdfNN Run 3/10, Epoch 310/1000, Training Loss (NLML): -957.2551\n",
      "convergence dfGPdfNN Run 3/10, Epoch 311/1000, Training Loss (NLML): -957.3062\n",
      "convergence dfGPdfNN Run 3/10, Epoch 312/1000, Training Loss (NLML): -957.2625\n",
      "convergence dfGPdfNN Run 3/10, Epoch 313/1000, Training Loss (NLML): -957.3060\n",
      "convergence dfGPdfNN Run 3/10, Epoch 314/1000, Training Loss (NLML): -957.3346\n",
      "convergence dfGPdfNN Run 3/10, Epoch 315/1000, Training Loss (NLML): -957.3156\n",
      "convergence dfGPdfNN Run 3/10, Epoch 316/1000, Training Loss (NLML): -957.3300\n",
      "convergence dfGPdfNN Run 3/10, Epoch 317/1000, Training Loss (NLML): -957.3448\n",
      "convergence dfGPdfNN Run 3/10, Epoch 318/1000, Training Loss (NLML): -957.3621\n",
      "convergence dfGPdfNN Run 3/10, Epoch 319/1000, Training Loss (NLML): -957.3949\n",
      "convergence dfGPdfNN Run 3/10, Epoch 320/1000, Training Loss (NLML): -957.3958\n",
      "convergence dfGPdfNN Run 3/10, Epoch 321/1000, Training Loss (NLML): -957.4039\n",
      "convergence dfGPdfNN Run 3/10, Epoch 322/1000, Training Loss (NLML): -957.4093\n",
      "convergence dfGPdfNN Run 3/10, Epoch 323/1000, Training Loss (NLML): -957.3973\n",
      "convergence dfGPdfNN Run 3/10, Epoch 324/1000, Training Loss (NLML): -957.4095\n",
      "convergence dfGPdfNN Run 3/10, Epoch 325/1000, Training Loss (NLML): -957.3928\n",
      "convergence dfGPdfNN Run 3/10, Epoch 326/1000, Training Loss (NLML): -957.4111\n",
      "convergence dfGPdfNN Run 3/10, Epoch 327/1000, Training Loss (NLML): -957.4720\n",
      "convergence dfGPdfNN Run 3/10, Epoch 328/1000, Training Loss (NLML): -957.5353\n",
      "convergence dfGPdfNN Run 3/10, Epoch 329/1000, Training Loss (NLML): -957.5494\n",
      "convergence dfGPdfNN Run 3/10, Epoch 330/1000, Training Loss (NLML): -957.6005\n",
      "convergence dfGPdfNN Run 3/10, Epoch 331/1000, Training Loss (NLML): -957.5912\n",
      "convergence dfGPdfNN Run 3/10, Epoch 332/1000, Training Loss (NLML): -957.6851\n",
      "convergence dfGPdfNN Run 3/10, Epoch 333/1000, Training Loss (NLML): -957.6837\n",
      "convergence dfGPdfNN Run 3/10, Epoch 334/1000, Training Loss (NLML): -957.7095\n",
      "convergence dfGPdfNN Run 3/10, Epoch 335/1000, Training Loss (NLML): -957.7537\n",
      "convergence dfGPdfNN Run 3/10, Epoch 336/1000, Training Loss (NLML): -957.6721\n",
      "convergence dfGPdfNN Run 3/10, Epoch 337/1000, Training Loss (NLML): -957.7484\n",
      "convergence dfGPdfNN Run 3/10, Epoch 338/1000, Training Loss (NLML): -957.7495\n",
      "convergence dfGPdfNN Run 3/10, Epoch 339/1000, Training Loss (NLML): -957.6686\n",
      "convergence dfGPdfNN Run 3/10, Epoch 340/1000, Training Loss (NLML): -957.6702\n",
      "convergence dfGPdfNN Run 3/10, Epoch 341/1000, Training Loss (NLML): -957.6786\n",
      "convergence dfGPdfNN Run 3/10, Epoch 342/1000, Training Loss (NLML): -957.6810\n",
      "convergence dfGPdfNN Run 3/10, Epoch 343/1000, Training Loss (NLML): -957.6740\n",
      "convergence dfGPdfNN Run 3/10, Epoch 344/1000, Training Loss (NLML): -957.6842\n",
      "convergence dfGPdfNN Run 3/10, Epoch 345/1000, Training Loss (NLML): -957.6472\n",
      "convergence dfGPdfNN Run 3/10, Epoch 346/1000, Training Loss (NLML): -957.6007\n",
      "convergence dfGPdfNN Run 3/10, Epoch 347/1000, Training Loss (NLML): -957.6992\n",
      "convergence dfGPdfNN Run 3/10, Epoch 348/1000, Training Loss (NLML): -957.6407\n",
      "convergence dfGPdfNN Run 3/10, Epoch 349/1000, Training Loss (NLML): -957.6545\n",
      "convergence dfGPdfNN Run 3/10, Epoch 350/1000, Training Loss (NLML): -957.6570\n",
      "convergence dfGPdfNN Run 3/10, Epoch 351/1000, Training Loss (NLML): -957.6769\n",
      "convergence dfGPdfNN Run 3/10, Epoch 352/1000, Training Loss (NLML): -957.7065\n",
      "convergence dfGPdfNN Run 3/10, Epoch 353/1000, Training Loss (NLML): -957.7405\n",
      "convergence dfGPdfNN Run 3/10, Epoch 354/1000, Training Loss (NLML): -957.8361\n",
      "convergence dfGPdfNN Run 3/10, Epoch 355/1000, Training Loss (NLML): -957.8417\n",
      "convergence dfGPdfNN Run 3/10, Epoch 356/1000, Training Loss (NLML): -957.8640\n",
      "convergence dfGPdfNN Run 3/10, Epoch 357/1000, Training Loss (NLML): -957.8778\n",
      "convergence dfGPdfNN Run 3/10, Epoch 358/1000, Training Loss (NLML): -957.8950\n",
      "convergence dfGPdfNN Run 3/10, Epoch 359/1000, Training Loss (NLML): -957.8292\n",
      "convergence dfGPdfNN Run 3/10, Epoch 360/1000, Training Loss (NLML): -957.7992\n",
      "convergence dfGPdfNN Run 3/10, Epoch 361/1000, Training Loss (NLML): -951.5458\n",
      "convergence dfGPdfNN Run 3/10, Epoch 362/1000, Training Loss (NLML): -957.7136\n",
      "convergence dfGPdfNN Run 3/10, Epoch 363/1000, Training Loss (NLML): -957.6984\n",
      "convergence dfGPdfNN Run 3/10, Epoch 364/1000, Training Loss (NLML): -957.6688\n",
      "convergence dfGPdfNN Run 3/10, Epoch 365/1000, Training Loss (NLML): -957.6826\n",
      "convergence dfGPdfNN Run 3/10, Epoch 366/1000, Training Loss (NLML): -957.6888\n",
      "convergence dfGPdfNN Run 3/10, Epoch 367/1000, Training Loss (NLML): -957.7130\n",
      "convergence dfGPdfNN Run 3/10, Epoch 368/1000, Training Loss (NLML): -957.6975\n",
      "convergence dfGPdfNN Run 3/10, Epoch 369/1000, Training Loss (NLML): -957.7587\n",
      "convergence dfGPdfNN Run 3/10, Epoch 370/1000, Training Loss (NLML): -957.8225\n",
      "convergence dfGPdfNN Run 3/10, Epoch 371/1000, Training Loss (NLML): -957.4102\n",
      "convergence dfGPdfNN Run 3/10, Epoch 372/1000, Training Loss (NLML): -957.3112\n",
      "convergence dfGPdfNN Run 3/10, Epoch 373/1000, Training Loss (NLML): -957.6036\n",
      "convergence dfGPdfNN Run 3/10, Epoch 374/1000, Training Loss (NLML): -957.5497\n",
      "convergence dfGPdfNN Run 3/10, Epoch 375/1000, Training Loss (NLML): -957.6599\n",
      "convergence dfGPdfNN Run 3/10, Epoch 376/1000, Training Loss (NLML): -957.6865\n",
      "convergence dfGPdfNN Run 3/10, Epoch 377/1000, Training Loss (NLML): -957.7551\n",
      "convergence dfGPdfNN Run 3/10, Epoch 378/1000, Training Loss (NLML): -957.6165\n",
      "convergence dfGPdfNN Run 3/10, Epoch 379/1000, Training Loss (NLML): -956.2737\n",
      "convergence dfGPdfNN Run 3/10, Epoch 380/1000, Training Loss (NLML): -957.1881\n",
      "convergence dfGPdfNN Run 3/10, Epoch 381/1000, Training Loss (NLML): -957.3746\n",
      "convergence dfGPdfNN Run 3/10, Epoch 382/1000, Training Loss (NLML): -957.6918\n",
      "convergence dfGPdfNN Run 3/10, Epoch 383/1000, Training Loss (NLML): -957.3279\n",
      "convergence dfGPdfNN Run 3/10, Epoch 384/1000, Training Loss (NLML): -957.9563\n",
      "convergence dfGPdfNN Run 3/10, Epoch 385/1000, Training Loss (NLML): -957.8054\n",
      "convergence dfGPdfNN Run 3/10, Epoch 386/1000, Training Loss (NLML): -957.8174\n",
      "convergence dfGPdfNN Run 3/10, Epoch 387/1000, Training Loss (NLML): -956.8274\n",
      "convergence dfGPdfNN Run 3/10, Epoch 388/1000, Training Loss (NLML): -957.7538\n",
      "convergence dfGPdfNN Run 3/10, Epoch 389/1000, Training Loss (NLML): -955.3022\n",
      "convergence dfGPdfNN Run 3/10, Epoch 390/1000, Training Loss (NLML): -958.0382\n",
      "convergence dfGPdfNN Run 3/10, Epoch 391/1000, Training Loss (NLML): -957.9829\n",
      "convergence dfGPdfNN Run 3/10, Epoch 392/1000, Training Loss (NLML): -957.7993\n",
      "convergence dfGPdfNN Run 3/10, Epoch 393/1000, Training Loss (NLML): -957.9845\n",
      "convergence dfGPdfNN Run 3/10, Epoch 394/1000, Training Loss (NLML): -957.9496\n",
      "convergence dfGPdfNN Run 3/10, Epoch 395/1000, Training Loss (NLML): -958.0581\n",
      "convergence dfGPdfNN Run 3/10, Epoch 396/1000, Training Loss (NLML): -958.0110\n",
      "convergence dfGPdfNN Run 3/10, Epoch 397/1000, Training Loss (NLML): -958.0220\n",
      "convergence dfGPdfNN Run 3/10, Epoch 398/1000, Training Loss (NLML): -958.0522\n",
      "convergence dfGPdfNN Run 3/10, Epoch 399/1000, Training Loss (NLML): -958.1394\n",
      "convergence dfGPdfNN Run 3/10, Epoch 400/1000, Training Loss (NLML): -958.1327\n",
      "convergence dfGPdfNN Run 3/10, Epoch 401/1000, Training Loss (NLML): -958.1610\n",
      "convergence dfGPdfNN Run 3/10, Epoch 402/1000, Training Loss (NLML): -958.1038\n",
      "convergence dfGPdfNN Run 3/10, Epoch 403/1000, Training Loss (NLML): -958.1394\n",
      "convergence dfGPdfNN Run 3/10, Epoch 404/1000, Training Loss (NLML): -951.3452\n",
      "convergence dfGPdfNN Run 3/10, Epoch 405/1000, Training Loss (NLML): -957.1007\n",
      "convergence dfGPdfNN Run 3/10, Epoch 406/1000, Training Loss (NLML): -958.0033\n",
      "convergence dfGPdfNN Run 3/10, Epoch 407/1000, Training Loss (NLML): -957.4066\n",
      "convergence dfGPdfNN Run 3/10, Epoch 408/1000, Training Loss (NLML): -957.6941\n",
      "convergence dfGPdfNN Run 3/10, Epoch 409/1000, Training Loss (NLML): -957.9098\n",
      "convergence dfGPdfNN Run 3/10, Epoch 410/1000, Training Loss (NLML): -957.4297\n",
      "convergence dfGPdfNN Run 3/10, Epoch 411/1000, Training Loss (NLML): -957.8580\n",
      "convergence dfGPdfNN Run 3/10, Epoch 412/1000, Training Loss (NLML): -957.5896\n",
      "convergence dfGPdfNN Run 3/10, Epoch 413/1000, Training Loss (NLML): -957.3014\n",
      "convergence dfGPdfNN Run 3/10, Epoch 414/1000, Training Loss (NLML): -957.3799\n",
      "convergence dfGPdfNN Run 3/10, Epoch 415/1000, Training Loss (NLML): -957.4869\n",
      "convergence dfGPdfNN Run 3/10, Epoch 416/1000, Training Loss (NLML): -957.4886\n",
      "convergence dfGPdfNN Run 3/10, Epoch 417/1000, Training Loss (NLML): -957.7599\n",
      "convergence dfGPdfNN Run 3/10, Epoch 418/1000, Training Loss (NLML): -957.8043\n",
      "convergence dfGPdfNN Run 3/10, Epoch 419/1000, Training Loss (NLML): -957.7461\n",
      "convergence dfGPdfNN Run 3/10, Epoch 420/1000, Training Loss (NLML): -957.8025\n",
      "convergence dfGPdfNN Run 3/10, Epoch 421/1000, Training Loss (NLML): -957.8739\n",
      "convergence dfGPdfNN Run 3/10, Epoch 422/1000, Training Loss (NLML): -957.9432\n",
      "convergence dfGPdfNN Run 3/10, Epoch 423/1000, Training Loss (NLML): -958.0470\n",
      "convergence dfGPdfNN Run 3/10, Epoch 424/1000, Training Loss (NLML): -958.0872\n",
      "convergence dfGPdfNN Run 3/10, Epoch 425/1000, Training Loss (NLML): -958.1122\n",
      "convergence dfGPdfNN Run 3/10, Epoch 426/1000, Training Loss (NLML): -958.2325\n",
      "convergence dfGPdfNN Run 3/10, Epoch 427/1000, Training Loss (NLML): -958.3650\n",
      "convergence dfGPdfNN Run 3/10, Epoch 428/1000, Training Loss (NLML): -958.4230\n",
      "convergence dfGPdfNN Run 3/10, Epoch 429/1000, Training Loss (NLML): -958.4843\n",
      "convergence dfGPdfNN Run 3/10, Epoch 430/1000, Training Loss (NLML): -958.4747\n",
      "convergence dfGPdfNN Run 3/10, Epoch 431/1000, Training Loss (NLML): -958.2946\n",
      "convergence dfGPdfNN Run 3/10, Epoch 432/1000, Training Loss (NLML): -956.9827\n",
      "convergence dfGPdfNN Run 3/10, Epoch 433/1000, Training Loss (NLML): -954.5803\n",
      "convergence dfGPdfNN Run 3/10, Epoch 434/1000, Training Loss (NLML): -957.8860\n",
      "convergence dfGPdfNN Run 3/10, Epoch 435/1000, Training Loss (NLML): -958.0693\n",
      "convergence dfGPdfNN Run 3/10, Epoch 436/1000, Training Loss (NLML): -958.0378\n",
      "convergence dfGPdfNN Run 3/10, Epoch 437/1000, Training Loss (NLML): -958.0444\n",
      "convergence dfGPdfNN Run 3/10, Epoch 438/1000, Training Loss (NLML): -958.0299\n",
      "convergence dfGPdfNN Run 3/10, Epoch 439/1000, Training Loss (NLML): -958.1935\n",
      "convergence dfGPdfNN Run 3/10, Epoch 440/1000, Training Loss (NLML): -958.2948\n",
      "convergence dfGPdfNN Run 3/10, Epoch 441/1000, Training Loss (NLML): -958.2703\n",
      "convergence dfGPdfNN Run 3/10, Epoch 442/1000, Training Loss (NLML): -958.2070\n",
      "convergence dfGPdfNN Run 3/10, Epoch 443/1000, Training Loss (NLML): -958.1593\n",
      "convergence dfGPdfNN Run 3/10, Epoch 444/1000, Training Loss (NLML): -958.1323\n",
      "convergence dfGPdfNN Run 3/10, Epoch 445/1000, Training Loss (NLML): -958.1486\n",
      "convergence dfGPdfNN Run 3/10, Epoch 446/1000, Training Loss (NLML): -958.1224\n",
      "convergence dfGPdfNN Run 3/10, Epoch 447/1000, Training Loss (NLML): -958.1781\n",
      "convergence dfGPdfNN Run 3/10, Epoch 448/1000, Training Loss (NLML): -958.1954\n",
      "convergence dfGPdfNN Run 3/10, Epoch 449/1000, Training Loss (NLML): -958.3285\n",
      "convergence dfGPdfNN Run 3/10, Epoch 450/1000, Training Loss (NLML): -958.4644\n",
      "convergence dfGPdfNN Run 3/10, Epoch 451/1000, Training Loss (NLML): -958.4420\n",
      "convergence dfGPdfNN Run 3/10, Epoch 452/1000, Training Loss (NLML): -958.0505\n",
      "convergence dfGPdfNN Run 3/10, Epoch 453/1000, Training Loss (NLML): -958.0115\n",
      "convergence dfGPdfNN Run 3/10, Epoch 454/1000, Training Loss (NLML): -958.3735\n",
      "convergence dfGPdfNN Run 3/10, Epoch 455/1000, Training Loss (NLML): -958.5946\n",
      "convergence dfGPdfNN Run 3/10, Epoch 456/1000, Training Loss (NLML): -958.6984\n",
      "convergence dfGPdfNN Run 3/10, Epoch 457/1000, Training Loss (NLML): -958.7087\n",
      "convergence dfGPdfNN Run 3/10, Epoch 458/1000, Training Loss (NLML): -958.6979\n",
      "convergence dfGPdfNN Run 3/10, Epoch 459/1000, Training Loss (NLML): -958.7086\n",
      "convergence dfGPdfNN Run 3/10, Epoch 460/1000, Training Loss (NLML): -958.6874\n",
      "convergence dfGPdfNN Run 3/10, Epoch 461/1000, Training Loss (NLML): -958.5977\n",
      "convergence dfGPdfNN Run 3/10, Epoch 462/1000, Training Loss (NLML): -958.5897\n",
      "convergence dfGPdfNN Run 3/10, Epoch 463/1000, Training Loss (NLML): -958.5835\n",
      "convergence dfGPdfNN Run 3/10, Epoch 464/1000, Training Loss (NLML): -958.5695\n",
      "convergence dfGPdfNN Run 3/10, Epoch 465/1000, Training Loss (NLML): -958.6927\n",
      "convergence dfGPdfNN Run 3/10, Epoch 466/1000, Training Loss (NLML): -958.7900\n",
      "convergence dfGPdfNN Run 3/10, Epoch 467/1000, Training Loss (NLML): -958.8159\n",
      "convergence dfGPdfNN Run 3/10, Epoch 468/1000, Training Loss (NLML): -958.8230\n",
      "convergence dfGPdfNN Run 3/10, Epoch 469/1000, Training Loss (NLML): -958.8198\n",
      "convergence dfGPdfNN Run 3/10, Epoch 470/1000, Training Loss (NLML): -958.7383\n",
      "convergence dfGPdfNN Run 3/10, Epoch 471/1000, Training Loss (NLML): -958.6801\n",
      "convergence dfGPdfNN Run 3/10, Epoch 472/1000, Training Loss (NLML): -958.6410\n",
      "convergence dfGPdfNN Run 3/10, Epoch 473/1000, Training Loss (NLML): -958.7034\n",
      "convergence dfGPdfNN Run 3/10, Epoch 474/1000, Training Loss (NLML): -958.7864\n",
      "convergence dfGPdfNN Run 3/10, Epoch 475/1000, Training Loss (NLML): -958.8215\n",
      "convergence dfGPdfNN Run 3/10, Epoch 476/1000, Training Loss (NLML): -958.7968\n",
      "convergence dfGPdfNN Run 3/10, Epoch 477/1000, Training Loss (NLML): -958.8575\n",
      "convergence dfGPdfNN Run 3/10, Epoch 478/1000, Training Loss (NLML): -958.8173\n",
      "convergence dfGPdfNN Run 3/10, Epoch 479/1000, Training Loss (NLML): -958.7987\n",
      "convergence dfGPdfNN Run 3/10, Epoch 480/1000, Training Loss (NLML): -958.8076\n",
      "convergence dfGPdfNN Run 3/10, Epoch 481/1000, Training Loss (NLML): -958.8101\n",
      "convergence dfGPdfNN Run 3/10, Epoch 482/1000, Training Loss (NLML): -958.8142\n",
      "convergence dfGPdfNN Run 3/10, Epoch 483/1000, Training Loss (NLML): -958.8165\n",
      "convergence dfGPdfNN Run 3/10, Epoch 484/1000, Training Loss (NLML): -958.8263\n",
      "convergence dfGPdfNN Run 3/10, Epoch 485/1000, Training Loss (NLML): -958.8368\n",
      "convergence dfGPdfNN Run 3/10, Epoch 486/1000, Training Loss (NLML): -958.8455\n",
      "convergence dfGPdfNN Run 3/10, Epoch 487/1000, Training Loss (NLML): -958.8872\n",
      "convergence dfGPdfNN Run 3/10, Epoch 488/1000, Training Loss (NLML): -958.8923\n",
      "convergence dfGPdfNN Run 3/10, Epoch 489/1000, Training Loss (NLML): -958.8970\n",
      "convergence dfGPdfNN Run 3/10, Epoch 490/1000, Training Loss (NLML): -958.8708\n",
      "convergence dfGPdfNN Run 3/10, Epoch 491/1000, Training Loss (NLML): -958.8831\n",
      "convergence dfGPdfNN Run 3/10, Epoch 492/1000, Training Loss (NLML): -958.8901\n",
      "convergence dfGPdfNN Run 3/10, Epoch 493/1000, Training Loss (NLML): -958.8959\n",
      "convergence dfGPdfNN Run 3/10, Epoch 494/1000, Training Loss (NLML): -958.9054\n",
      "convergence dfGPdfNN Run 3/10, Epoch 495/1000, Training Loss (NLML): -958.9095\n",
      "convergence dfGPdfNN Run 3/10, Epoch 496/1000, Training Loss (NLML): -958.9169\n",
      "convergence dfGPdfNN Run 3/10, Epoch 497/1000, Training Loss (NLML): -958.9255\n",
      "convergence dfGPdfNN Run 3/10, Epoch 498/1000, Training Loss (NLML): -958.9327\n",
      "convergence dfGPdfNN Run 3/10, Epoch 499/1000, Training Loss (NLML): -958.9700\n",
      "convergence dfGPdfNN Run 3/10, Epoch 500/1000, Training Loss (NLML): -959.0239\n",
      "convergence dfGPdfNN Run 3/10, Epoch 501/1000, Training Loss (NLML): -958.9827\n",
      "convergence dfGPdfNN Run 3/10, Epoch 502/1000, Training Loss (NLML): -958.9611\n",
      "convergence dfGPdfNN Run 3/10, Epoch 503/1000, Training Loss (NLML): -958.9670\n",
      "convergence dfGPdfNN Run 3/10, Epoch 504/1000, Training Loss (NLML): -958.9746\n",
      "convergence dfGPdfNN Run 3/10, Epoch 505/1000, Training Loss (NLML): -958.9797\n",
      "convergence dfGPdfNN Run 3/10, Epoch 506/1000, Training Loss (NLML): -958.9880\n",
      "convergence dfGPdfNN Run 3/10, Epoch 507/1000, Training Loss (NLML): -958.9913\n",
      "convergence dfGPdfNN Run 3/10, Epoch 508/1000, Training Loss (NLML): -958.9988\n",
      "convergence dfGPdfNN Run 3/10, Epoch 509/1000, Training Loss (NLML): -959.0054\n",
      "convergence dfGPdfNN Run 3/10, Epoch 510/1000, Training Loss (NLML): -959.0410\n",
      "convergence dfGPdfNN Run 3/10, Epoch 511/1000, Training Loss (NLML): -959.0935\n",
      "convergence dfGPdfNN Run 3/10, Epoch 512/1000, Training Loss (NLML): -959.0557\n",
      "convergence dfGPdfNN Run 3/10, Epoch 513/1000, Training Loss (NLML): -959.0326\n",
      "convergence dfGPdfNN Run 3/10, Epoch 514/1000, Training Loss (NLML): -959.0397\n",
      "convergence dfGPdfNN Run 3/10, Epoch 515/1000, Training Loss (NLML): -959.0469\n",
      "convergence dfGPdfNN Run 3/10, Epoch 516/1000, Training Loss (NLML): -959.0530\n",
      "convergence dfGPdfNN Run 3/10, Epoch 517/1000, Training Loss (NLML): -959.0588\n",
      "convergence dfGPdfNN Run 3/10, Epoch 518/1000, Training Loss (NLML): -959.0651\n",
      "convergence dfGPdfNN Run 3/10, Epoch 519/1000, Training Loss (NLML): -959.0712\n",
      "convergence dfGPdfNN Run 3/10, Epoch 520/1000, Training Loss (NLML): -959.1069\n",
      "convergence dfGPdfNN Run 3/10, Epoch 521/1000, Training Loss (NLML): -959.1128\n",
      "convergence dfGPdfNN Run 3/10, Epoch 522/1000, Training Loss (NLML): -959.1190\n",
      "convergence dfGPdfNN Run 3/10, Epoch 523/1000, Training Loss (NLML): -959.1267\n",
      "convergence dfGPdfNN Run 3/10, Epoch 524/1000, Training Loss (NLML): -959.1018\n",
      "convergence dfGPdfNN Run 3/10, Epoch 525/1000, Training Loss (NLML): -959.1125\n",
      "convergence dfGPdfNN Run 3/10, Epoch 526/1000, Training Loss (NLML): -959.1174\n",
      "convergence dfGPdfNN Run 3/10, Epoch 527/1000, Training Loss (NLML): -959.1261\n",
      "convergence dfGPdfNN Run 3/10, Epoch 528/1000, Training Loss (NLML): -959.1322\n",
      "convergence dfGPdfNN Run 3/10, Epoch 529/1000, Training Loss (NLML): -959.1660\n",
      "convergence dfGPdfNN Run 3/10, Epoch 530/1000, Training Loss (NLML): -959.1718\n",
      "convergence dfGPdfNN Run 3/10, Epoch 531/1000, Training Loss (NLML): -959.1793\n",
      "convergence dfGPdfNN Run 3/10, Epoch 532/1000, Training Loss (NLML): -959.1837\n",
      "convergence dfGPdfNN Run 3/10, Epoch 533/1000, Training Loss (NLML): -959.1620\n",
      "convergence dfGPdfNN Run 3/10, Epoch 534/1000, Training Loss (NLML): -959.1682\n",
      "convergence dfGPdfNN Run 3/10, Epoch 535/1000, Training Loss (NLML): -959.1730\n",
      "convergence dfGPdfNN Run 3/10, Epoch 536/1000, Training Loss (NLML): -959.1808\n",
      "convergence dfGPdfNN Run 3/10, Epoch 537/1000, Training Loss (NLML): -959.2163\n",
      "convergence dfGPdfNN Run 3/10, Epoch 538/1000, Training Loss (NLML): -959.2222\n",
      "convergence dfGPdfNN Run 3/10, Epoch 539/1000, Training Loss (NLML): -959.2281\n",
      "convergence dfGPdfNN Run 3/10, Epoch 540/1000, Training Loss (NLML): -959.2030\n",
      "convergence dfGPdfNN Run 3/10, Epoch 541/1000, Training Loss (NLML): -959.2112\n",
      "convergence dfGPdfNN Run 3/10, Epoch 542/1000, Training Loss (NLML): -959.2156\n",
      "convergence dfGPdfNN Run 3/10, Epoch 543/1000, Training Loss (NLML): -959.2487\n",
      "convergence dfGPdfNN Run 3/10, Epoch 544/1000, Training Loss (NLML): -959.2546\n",
      "convergence dfGPdfNN Run 3/10, Epoch 545/1000, Training Loss (NLML): -959.2617\n",
      "convergence dfGPdfNN Run 3/10, Epoch 546/1000, Training Loss (NLML): -959.2362\n",
      "convergence dfGPdfNN Run 3/10, Epoch 547/1000, Training Loss (NLML): -959.2407\n",
      "convergence dfGPdfNN Run 3/10, Epoch 548/1000, Training Loss (NLML): -959.2766\n",
      "convergence dfGPdfNN Run 3/10, Epoch 549/1000, Training Loss (NLML): -959.2533\n",
      "convergence dfGPdfNN Run 3/10, Epoch 550/1000, Training Loss (NLML): -959.2871\n",
      "convergence dfGPdfNN Run 3/10, Epoch 551/1000, Training Loss (NLML): -959.2939\n",
      "convergence dfGPdfNN Run 3/10, Epoch 552/1000, Training Loss (NLML): -959.2703\n",
      "convergence dfGPdfNN Run 3/10, Epoch 553/1000, Training Loss (NLML): -959.2772\n",
      "convergence dfGPdfNN Run 3/10, Epoch 554/1000, Training Loss (NLML): -959.3096\n",
      "convergence dfGPdfNN Run 3/10, Epoch 555/1000, Training Loss (NLML): -959.3152\n",
      "convergence dfGPdfNN Run 3/10, Epoch 556/1000, Training Loss (NLML): -959.3207\n",
      "convergence dfGPdfNN Run 3/10, Epoch 557/1000, Training Loss (NLML): -959.2983\n",
      "convergence dfGPdfNN Run 3/10, Epoch 558/1000, Training Loss (NLML): -959.3038\n",
      "convergence dfGPdfNN Run 3/10, Epoch 559/1000, Training Loss (NLML): -959.3402\n",
      "convergence dfGPdfNN Run 3/10, Epoch 560/1000, Training Loss (NLML): -959.3466\n",
      "convergence dfGPdfNN Run 3/10, Epoch 561/1000, Training Loss (NLML): -959.3528\n",
      "convergence dfGPdfNN Run 3/10, Epoch 562/1000, Training Loss (NLML): -959.3573\n",
      "convergence dfGPdfNN Run 3/10, Epoch 563/1000, Training Loss (NLML): -959.3345\n",
      "convergence dfGPdfNN Run 3/10, Epoch 564/1000, Training Loss (NLML): -959.3685\n",
      "convergence dfGPdfNN Run 3/10, Epoch 565/1000, Training Loss (NLML): -959.3734\n",
      "convergence dfGPdfNN Run 3/10, Epoch 566/1000, Training Loss (NLML): -959.3788\n",
      "convergence dfGPdfNN Run 3/10, Epoch 567/1000, Training Loss (NLML): -959.3851\n",
      "convergence dfGPdfNN Run 3/10, Epoch 568/1000, Training Loss (NLML): -959.3607\n",
      "convergence dfGPdfNN Run 3/10, Epoch 569/1000, Training Loss (NLML): -959.3955\n",
      "convergence dfGPdfNN Run 3/10, Epoch 570/1000, Training Loss (NLML): -959.3999\n",
      "convergence dfGPdfNN Run 3/10, Epoch 571/1000, Training Loss (NLML): -959.4059\n",
      "convergence dfGPdfNN Run 3/10, Epoch 572/1000, Training Loss (NLML): -959.4106\n",
      "convergence dfGPdfNN Run 3/10, Epoch 573/1000, Training Loss (NLML): -959.3879\n",
      "convergence dfGPdfNN Run 3/10, Epoch 574/1000, Training Loss (NLML): -959.4215\n",
      "convergence dfGPdfNN Run 3/10, Epoch 575/1000, Training Loss (NLML): -959.4255\n",
      "convergence dfGPdfNN Run 3/10, Epoch 576/1000, Training Loss (NLML): -959.4371\n",
      "convergence dfGPdfNN Run 3/10, Epoch 577/1000, Training Loss (NLML): -959.4427\n",
      "convergence dfGPdfNN Run 3/10, Epoch 578/1000, Training Loss (NLML): -959.4467\n",
      "convergence dfGPdfNN Run 3/10, Epoch 579/1000, Training Loss (NLML): -959.4525\n",
      "convergence dfGPdfNN Run 3/10, Epoch 580/1000, Training Loss (NLML): -959.4587\n",
      "convergence dfGPdfNN Run 3/10, Epoch 581/1000, Training Loss (NLML): -959.4631\n",
      "convergence dfGPdfNN Run 3/10, Epoch 582/1000, Training Loss (NLML): -959.4697\n",
      "convergence dfGPdfNN Run 3/10, Epoch 583/1000, Training Loss (NLML): -959.4736\n",
      "convergence dfGPdfNN Run 3/10, Epoch 584/1000, Training Loss (NLML): -959.4774\n",
      "convergence dfGPdfNN Run 3/10, Epoch 585/1000, Training Loss (NLML): -959.4838\n",
      "convergence dfGPdfNN Run 3/10, Epoch 586/1000, Training Loss (NLML): -959.4896\n",
      "convergence dfGPdfNN Run 3/10, Epoch 587/1000, Training Loss (NLML): -959.4935\n",
      "convergence dfGPdfNN Run 3/10, Epoch 588/1000, Training Loss (NLML): -959.4806\n",
      "convergence dfGPdfNN Run 3/10, Epoch 589/1000, Training Loss (NLML): -959.4866\n",
      "convergence dfGPdfNN Run 3/10, Epoch 590/1000, Training Loss (NLML): -959.5355\n",
      "convergence dfGPdfNN Run 3/10, Epoch 591/1000, Training Loss (NLML): -959.4962\n",
      "convergence dfGPdfNN Run 3/10, Epoch 592/1000, Training Loss (NLML): -959.4725\n",
      "convergence dfGPdfNN Run 3/10, Epoch 593/1000, Training Loss (NLML): -959.4832\n",
      "convergence dfGPdfNN Run 3/10, Epoch 594/1000, Training Loss (NLML): -959.5603\n",
      "convergence dfGPdfNN Run 3/10, Epoch 595/1000, Training Loss (NLML): -959.5662\n",
      "convergence dfGPdfNN Run 3/10, Epoch 596/1000, Training Loss (NLML): -959.5724\n",
      "convergence dfGPdfNN Run 3/10, Epoch 597/1000, Training Loss (NLML): -959.5325\n",
      "convergence dfGPdfNN Run 3/10, Epoch 598/1000, Training Loss (NLML): -959.5092\n",
      "convergence dfGPdfNN Run 3/10, Epoch 599/1000, Training Loss (NLML): -959.5121\n",
      "convergence dfGPdfNN Run 3/10, Epoch 600/1000, Training Loss (NLML): -959.5475\n",
      "convergence dfGPdfNN Run 3/10, Epoch 601/1000, Training Loss (NLML): -959.5964\n",
      "convergence dfGPdfNN Run 3/10, Epoch 602/1000, Training Loss (NLML): -959.6023\n",
      "convergence dfGPdfNN Run 3/10, Epoch 603/1000, Training Loss (NLML): -959.6072\n",
      "convergence dfGPdfNN Run 3/10, Epoch 604/1000, Training Loss (NLML): -959.5676\n",
      "convergence dfGPdfNN Run 3/10, Epoch 605/1000, Training Loss (NLML): -959.5436\n",
      "convergence dfGPdfNN Run 3/10, Epoch 606/1000, Training Loss (NLML): -959.5490\n",
      "convergence dfGPdfNN Run 3/10, Epoch 607/1000, Training Loss (NLML): -959.5802\n",
      "convergence dfGPdfNN Run 3/10, Epoch 608/1000, Training Loss (NLML): -959.6307\n",
      "convergence dfGPdfNN Run 3/10, Epoch 609/1000, Training Loss (NLML): -959.6345\n",
      "convergence dfGPdfNN Run 3/10, Epoch 610/1000, Training Loss (NLML): -959.6414\n",
      "convergence dfGPdfNN Run 3/10, Epoch 611/1000, Training Loss (NLML): -959.6016\n",
      "convergence dfGPdfNN Run 3/10, Epoch 612/1000, Training Loss (NLML): -959.5787\n",
      "convergence dfGPdfNN Run 3/10, Epoch 613/1000, Training Loss (NLML): -959.5868\n",
      "convergence dfGPdfNN Run 3/10, Epoch 614/1000, Training Loss (NLML): -959.6655\n",
      "convergence dfGPdfNN Run 3/10, Epoch 615/1000, Training Loss (NLML): -959.6707\n",
      "convergence dfGPdfNN Run 3/10, Epoch 616/1000, Training Loss (NLML): -959.6752\n",
      "convergence dfGPdfNN Run 3/10, Epoch 617/1000, Training Loss (NLML): -959.6364\n",
      "convergence dfGPdfNN Run 3/10, Epoch 618/1000, Training Loss (NLML): -959.6083\n",
      "convergence dfGPdfNN Run 3/10, Epoch 619/1000, Training Loss (NLML): -959.6832\n",
      "convergence dfGPdfNN Run 3/10, Epoch 620/1000, Training Loss (NLML): -959.6898\n",
      "convergence dfGPdfNN Run 3/10, Epoch 621/1000, Training Loss (NLML): -959.6478\n",
      "convergence dfGPdfNN Run 3/10, Epoch 622/1000, Training Loss (NLML): -959.6371\n",
      "convergence dfGPdfNN Run 3/10, Epoch 623/1000, Training Loss (NLML): -959.6807\n",
      "convergence dfGPdfNN Run 3/10, Epoch 624/1000, Training Loss (NLML): -959.6465\n",
      "convergence dfGPdfNN Run 3/10, Epoch 625/1000, Training Loss (NLML): -959.6484\n",
      "convergence dfGPdfNN Run 3/10, Epoch 626/1000, Training Loss (NLML): -959.6542\n",
      "convergence dfGPdfNN Run 3/10, Epoch 627/1000, Training Loss (NLML): -959.7003\n",
      "convergence dfGPdfNN Run 3/10, Epoch 628/1000, Training Loss (NLML): -959.7073\n",
      "convergence dfGPdfNN Run 3/10, Epoch 629/1000, Training Loss (NLML): -959.7115\n",
      "convergence dfGPdfNN Run 3/10, Epoch 630/1000, Training Loss (NLML): -959.6418\n",
      "convergence dfGPdfNN Run 3/10, Epoch 631/1000, Training Loss (NLML): -959.6791\n",
      "convergence dfGPdfNN Run 3/10, Epoch 632/1000, Training Loss (NLML): -959.7213\n",
      "convergence dfGPdfNN Run 3/10, Epoch 633/1000, Training Loss (NLML): -959.7274\n",
      "convergence dfGPdfNN Run 3/10, Epoch 634/1000, Training Loss (NLML): -959.6875\n",
      "convergence dfGPdfNN Run 3/10, Epoch 635/1000, Training Loss (NLML): -959.6958\n",
      "convergence dfGPdfNN Run 3/10, Epoch 636/1000, Training Loss (NLML): -959.7413\n",
      "convergence dfGPdfNN Run 3/10, Epoch 637/1000, Training Loss (NLML): -959.7021\n",
      "convergence dfGPdfNN Run 3/10, Epoch 638/1000, Training Loss (NLML): -959.7068\n",
      "convergence dfGPdfNN Run 3/10, Epoch 639/1000, Training Loss (NLML): -959.7546\n",
      "convergence dfGPdfNN Run 3/10, Epoch 640/1000, Training Loss (NLML): -959.7585\n",
      "convergence dfGPdfNN Run 3/10, Epoch 641/1000, Training Loss (NLML): -959.7640\n",
      "convergence dfGPdfNN Run 3/10, Epoch 642/1000, Training Loss (NLML): -959.6973\n",
      "convergence dfGPdfNN Run 3/10, Epoch 643/1000, Training Loss (NLML): -959.7301\n",
      "convergence dfGPdfNN Run 3/10, Epoch 644/1000, Training Loss (NLML): -959.7749\n",
      "convergence dfGPdfNN Run 3/10, Epoch 645/1000, Training Loss (NLML): -959.7803\n",
      "convergence dfGPdfNN Run 3/10, Epoch 646/1000, Training Loss (NLML): -959.7849\n",
      "convergence dfGPdfNN Run 3/10, Epoch 647/1000, Training Loss (NLML): -959.7461\n",
      "convergence dfGPdfNN Run 3/10, Epoch 648/1000, Training Loss (NLML): -959.7185\n",
      "convergence dfGPdfNN Run 3/10, Epoch 649/1000, Training Loss (NLML): -959.7931\n",
      "convergence dfGPdfNN Run 3/10, Epoch 650/1000, Training Loss (NLML): -959.8007\n",
      "convergence dfGPdfNN Run 3/10, Epoch 651/1000, Training Loss (NLML): -959.8020\n",
      "convergence dfGPdfNN Run 3/10, Epoch 652/1000, Training Loss (NLML): -959.7623\n",
      "convergence dfGPdfNN Run 3/10, Epoch 653/1000, Training Loss (NLML): -959.7683\n",
      "convergence dfGPdfNN Run 3/10, Epoch 654/1000, Training Loss (NLML): -959.7715\n",
      "convergence dfGPdfNN Run 3/10, Epoch 655/1000, Training Loss (NLML): -959.8192\n",
      "convergence dfGPdfNN Run 3/10, Epoch 656/1000, Training Loss (NLML): -959.8228\n",
      "convergence dfGPdfNN Run 3/10, Epoch 657/1000, Training Loss (NLML): -959.8260\n",
      "convergence dfGPdfNN Run 3/10, Epoch 658/1000, Training Loss (NLML): -959.7612\n",
      "convergence dfGPdfNN Run 3/10, Epoch 659/1000, Training Loss (NLML): -959.7906\n",
      "convergence dfGPdfNN Run 3/10, Epoch 660/1000, Training Loss (NLML): -959.8383\n",
      "convergence dfGPdfNN Run 3/10, Epoch 661/1000, Training Loss (NLML): -959.8433\n",
      "convergence dfGPdfNN Run 3/10, Epoch 662/1000, Training Loss (NLML): -959.8475\n",
      "convergence dfGPdfNN Run 3/10, Epoch 663/1000, Training Loss (NLML): -959.8512\n",
      "convergence dfGPdfNN Run 3/10, Epoch 664/1000, Training Loss (NLML): -959.7837\n",
      "convergence dfGPdfNN Run 3/10, Epoch 665/1000, Training Loss (NLML): -959.8243\n",
      "convergence dfGPdfNN Run 3/10, Epoch 666/1000, Training Loss (NLML): -959.8715\n",
      "convergence dfGPdfNN Run 3/10, Epoch 667/1000, Training Loss (NLML): -959.8741\n",
      "convergence dfGPdfNN Run 3/10, Epoch 668/1000, Training Loss (NLML): -959.8804\n",
      "convergence dfGPdfNN Run 3/10, Epoch 669/1000, Training Loss (NLML): -959.8838\n",
      "convergence dfGPdfNN Run 3/10, Epoch 670/1000, Training Loss (NLML): -959.8870\n",
      "convergence dfGPdfNN Run 3/10, Epoch 671/1000, Training Loss (NLML): -959.8480\n",
      "convergence dfGPdfNN Run 3/10, Epoch 672/1000, Training Loss (NLML): -959.8522\n",
      "convergence dfGPdfNN Run 3/10, Epoch 673/1000, Training Loss (NLML): -959.8955\n",
      "convergence dfGPdfNN Run 3/10, Epoch 674/1000, Training Loss (NLML): -959.9009\n",
      "convergence dfGPdfNN Run 3/10, Epoch 675/1000, Training Loss (NLML): -959.9030\n",
      "convergence dfGPdfNN Run 3/10, Epoch 676/1000, Training Loss (NLML): -959.9093\n",
      "convergence dfGPdfNN Run 3/10, Epoch 677/1000, Training Loss (NLML): -959.9128\n",
      "convergence dfGPdfNN Run 3/10, Epoch 678/1000, Training Loss (NLML): -959.9154\n",
      "convergence dfGPdfNN Run 3/10, Epoch 679/1000, Training Loss (NLML): -959.8508\n",
      "convergence dfGPdfNN Run 3/10, Epoch 680/1000, Training Loss (NLML): -959.9167\n",
      "convergence dfGPdfNN Run 3/10, Epoch 681/1000, Training Loss (NLML): -959.9220\n",
      "convergence dfGPdfNN Run 3/10, Epoch 682/1000, Training Loss (NLML): -959.9261\n",
      "convergence dfGPdfNN Run 3/10, Epoch 683/1000, Training Loss (NLML): -959.9292\n",
      "convergence dfGPdfNN Run 3/10, Epoch 684/1000, Training Loss (NLML): -959.9310\n",
      "convergence dfGPdfNN Run 3/10, Epoch 685/1000, Training Loss (NLML): -959.8665\n",
      "convergence dfGPdfNN Run 3/10, Epoch 686/1000, Training Loss (NLML): -959.9382\n",
      "convergence dfGPdfNN Run 3/10, Epoch 687/1000, Training Loss (NLML): -959.9427\n",
      "convergence dfGPdfNN Run 3/10, Epoch 688/1000, Training Loss (NLML): -959.9476\n",
      "convergence dfGPdfNN Run 3/10, Epoch 689/1000, Training Loss (NLML): -959.9497\n",
      "convergence dfGPdfNN Run 3/10, Epoch 690/1000, Training Loss (NLML): -959.9138\n",
      "convergence dfGPdfNN Run 3/10, Epoch 691/1000, Training Loss (NLML): -959.9573\n",
      "convergence dfGPdfNN Run 3/10, Epoch 692/1000, Training Loss (NLML): -959.9626\n",
      "convergence dfGPdfNN Run 3/10, Epoch 693/1000, Training Loss (NLML): -959.9232\n",
      "convergence dfGPdfNN Run 3/10, Epoch 694/1000, Training Loss (NLML): -959.9679\n",
      "convergence dfGPdfNN Run 3/10, Epoch 695/1000, Training Loss (NLML): -959.9719\n",
      "convergence dfGPdfNN Run 3/10, Epoch 696/1000, Training Loss (NLML): -959.9760\n",
      "convergence dfGPdfNN Run 3/10, Epoch 697/1000, Training Loss (NLML): -959.9790\n",
      "convergence dfGPdfNN Run 3/10, Epoch 698/1000, Training Loss (NLML): -959.9829\n",
      "convergence dfGPdfNN Run 3/10, Epoch 699/1000, Training Loss (NLML): -959.9238\n",
      "convergence dfGPdfNN Run 3/10, Epoch 700/1000, Training Loss (NLML): -959.9955\n",
      "convergence dfGPdfNN Run 3/10, Epoch 701/1000, Training Loss (NLML): -960.0002\n",
      "convergence dfGPdfNN Run 3/10, Epoch 702/1000, Training Loss (NLML): -960.0013\n",
      "convergence dfGPdfNN Run 3/10, Epoch 703/1000, Training Loss (NLML): -960.0073\n",
      "convergence dfGPdfNN Run 3/10, Epoch 704/1000, Training Loss (NLML): -960.0045\n",
      "convergence dfGPdfNN Run 3/10, Epoch 705/1000, Training Loss (NLML): -960.0077\n",
      "convergence dfGPdfNN Run 3/10, Epoch 706/1000, Training Loss (NLML): -960.0128\n",
      "convergence dfGPdfNN Run 3/10, Epoch 707/1000, Training Loss (NLML): -960.0151\n",
      "convergence dfGPdfNN Run 3/10, Epoch 708/1000, Training Loss (NLML): -960.0190\n",
      "convergence dfGPdfNN Run 3/10, Epoch 709/1000, Training Loss (NLML): -959.9541\n",
      "convergence dfGPdfNN Run 3/10, Epoch 710/1000, Training Loss (NLML): -960.0265\n",
      "convergence dfGPdfNN Run 3/10, Epoch 711/1000, Training Loss (NLML): -960.0292\n",
      "convergence dfGPdfNN Run 3/10, Epoch 712/1000, Training Loss (NLML): -960.0330\n",
      "convergence dfGPdfNN Run 3/10, Epoch 713/1000, Training Loss (NLML): -960.0367\n",
      "convergence dfGPdfNN Run 3/10, Epoch 714/1000, Training Loss (NLML): -960.0392\n",
      "convergence dfGPdfNN Run 3/10, Epoch 715/1000, Training Loss (NLML): -960.0491\n",
      "convergence dfGPdfNN Run 3/10, Epoch 716/1000, Training Loss (NLML): -960.0519\n",
      "convergence dfGPdfNN Run 3/10, Epoch 717/1000, Training Loss (NLML): -960.0557\n",
      "convergence dfGPdfNN Run 3/10, Epoch 718/1000, Training Loss (NLML): -960.0531\n",
      "convergence dfGPdfNN Run 3/10, Epoch 719/1000, Training Loss (NLML): -960.0548\n",
      "convergence dfGPdfNN Run 3/10, Epoch 720/1000, Training Loss (NLML): -960.0604\n",
      "convergence dfGPdfNN Run 3/10, Epoch 721/1000, Training Loss (NLML): -960.0621\n",
      "convergence dfGPdfNN Run 3/10, Epoch 722/1000, Training Loss (NLML): -960.0671\n",
      "convergence dfGPdfNN Run 3/10, Epoch 723/1000, Training Loss (NLML): -960.0306\n",
      "convergence dfGPdfNN Run 3/10, Epoch 724/1000, Training Loss (NLML): -960.0720\n",
      "convergence dfGPdfNN Run 3/10, Epoch 725/1000, Training Loss (NLML): -960.0773\n",
      "convergence dfGPdfNN Run 3/10, Epoch 726/1000, Training Loss (NLML): -960.0863\n",
      "convergence dfGPdfNN Run 3/10, Epoch 727/1000, Training Loss (NLML): -960.0909\n",
      "convergence dfGPdfNN Run 3/10, Epoch 728/1000, Training Loss (NLML): -960.0878\n",
      "convergence dfGPdfNN Run 3/10, Epoch 729/1000, Training Loss (NLML): -960.0892\n",
      "convergence dfGPdfNN Run 3/10, Epoch 730/1000, Training Loss (NLML): -960.0942\n",
      "convergence dfGPdfNN Run 3/10, Epoch 731/1000, Training Loss (NLML): -960.0968\n",
      "convergence dfGPdfNN Run 3/10, Epoch 732/1000, Training Loss (NLML): -960.0992\n",
      "convergence dfGPdfNN Run 3/10, Epoch 733/1000, Training Loss (NLML): -960.1034\n",
      "convergence dfGPdfNN Run 3/10, Epoch 734/1000, Training Loss (NLML): -960.1062\n",
      "convergence dfGPdfNN Run 3/10, Epoch 735/1000, Training Loss (NLML): -960.1110\n",
      "convergence dfGPdfNN Run 3/10, Epoch 736/1000, Training Loss (NLML): -960.1204\n",
      "convergence dfGPdfNN Run 3/10, Epoch 737/1000, Training Loss (NLML): -960.1221\n",
      "convergence dfGPdfNN Run 3/10, Epoch 738/1000, Training Loss (NLML): -960.1257\n",
      "convergence dfGPdfNN Run 3/10, Epoch 739/1000, Training Loss (NLML): -960.1289\n",
      "convergence dfGPdfNN Run 3/10, Epoch 740/1000, Training Loss (NLML): -960.1265\n",
      "convergence dfGPdfNN Run 3/10, Epoch 741/1000, Training Loss (NLML): -960.1306\n",
      "convergence dfGPdfNN Run 3/10, Epoch 742/1000, Training Loss (NLML): -960.1313\n",
      "convergence dfGPdfNN Run 3/10, Epoch 743/1000, Training Loss (NLML): -960.1366\n",
      "convergence dfGPdfNN Run 3/10, Epoch 744/1000, Training Loss (NLML): -960.1375\n",
      "convergence dfGPdfNN Run 3/10, Epoch 745/1000, Training Loss (NLML): -960.1412\n",
      "convergence dfGPdfNN Run 3/10, Epoch 746/1000, Training Loss (NLML): -960.1448\n",
      "convergence dfGPdfNN Run 3/10, Epoch 747/1000, Training Loss (NLML): -960.1478\n",
      "convergence dfGPdfNN Run 3/10, Epoch 748/1000, Training Loss (NLML): -960.1510\n",
      "convergence dfGPdfNN Run 3/10, Epoch 749/1000, Training Loss (NLML): -960.1549\n",
      "convergence dfGPdfNN Run 3/10, Epoch 750/1000, Training Loss (NLML): -960.1583\n",
      "convergence dfGPdfNN Run 3/10, Epoch 751/1000, Training Loss (NLML): -960.1611\n",
      "convergence dfGPdfNN Run 3/10, Epoch 752/1000, Training Loss (NLML): -960.1630\n",
      "convergence dfGPdfNN Run 3/10, Epoch 753/1000, Training Loss (NLML): -960.1674\n",
      "convergence dfGPdfNN Run 3/10, Epoch 754/1000, Training Loss (NLML): -960.1761\n",
      "convergence dfGPdfNN Run 3/10, Epoch 755/1000, Training Loss (NLML): -960.1790\n",
      "convergence dfGPdfNN Run 3/10, Epoch 756/1000, Training Loss (NLML): -960.1813\n",
      "convergence dfGPdfNN Run 3/10, Epoch 757/1000, Training Loss (NLML): -960.1798\n",
      "convergence dfGPdfNN Run 3/10, Epoch 758/1000, Training Loss (NLML): -960.1832\n",
      "convergence dfGPdfNN Run 3/10, Epoch 759/1000, Training Loss (NLML): -960.1852\n",
      "convergence dfGPdfNN Run 3/10, Epoch 760/1000, Training Loss (NLML): -960.1877\n",
      "convergence dfGPdfNN Run 3/10, Epoch 761/1000, Training Loss (NLML): -960.1917\n",
      "convergence dfGPdfNN Run 3/10, Epoch 762/1000, Training Loss (NLML): -960.1945\n",
      "convergence dfGPdfNN Run 3/10, Epoch 763/1000, Training Loss (NLML): -960.1913\n",
      "convergence dfGPdfNN Run 3/10, Epoch 764/1000, Training Loss (NLML): -960.1956\n",
      "convergence dfGPdfNN Run 3/10, Epoch 765/1000, Training Loss (NLML): -960.1989\n",
      "convergence dfGPdfNN Run 3/10, Epoch 766/1000, Training Loss (NLML): -960.2029\n",
      "convergence dfGPdfNN Run 3/10, Epoch 767/1000, Training Loss (NLML): -960.2052\n",
      "convergence dfGPdfNN Run 3/10, Epoch 768/1000, Training Loss (NLML): -960.2080\n",
      "convergence dfGPdfNN Run 3/10, Epoch 769/1000, Training Loss (NLML): -960.2118\n",
      "convergence dfGPdfNN Run 3/10, Epoch 770/1000, Training Loss (NLML): -960.2145\n",
      "convergence dfGPdfNN Run 3/10, Epoch 771/1000, Training Loss (NLML): -960.2170\n",
      "convergence dfGPdfNN Run 3/10, Epoch 772/1000, Training Loss (NLML): -960.2202\n",
      "convergence dfGPdfNN Run 3/10, Epoch 773/1000, Training Loss (NLML): -960.2218\n",
      "convergence dfGPdfNN Run 3/10, Epoch 774/1000, Training Loss (NLML): -960.2260\n",
      "convergence dfGPdfNN Run 3/10, Epoch 775/1000, Training Loss (NLML): -960.2284\n",
      "convergence dfGPdfNN Run 3/10, Epoch 776/1000, Training Loss (NLML): -960.2313\n",
      "convergence dfGPdfNN Run 3/10, Epoch 777/1000, Training Loss (NLML): -960.2347\n",
      "convergence dfGPdfNN Run 3/10, Epoch 778/1000, Training Loss (NLML): -960.2380\n",
      "convergence dfGPdfNN Run 3/10, Epoch 779/1000, Training Loss (NLML): -960.2408\n",
      "convergence dfGPdfNN Run 3/10, Epoch 780/1000, Training Loss (NLML): -960.2397\n",
      "convergence dfGPdfNN Run 3/10, Epoch 781/1000, Training Loss (NLML): -960.2418\n",
      "convergence dfGPdfNN Run 3/10, Epoch 782/1000, Training Loss (NLML): -960.2443\n",
      "convergence dfGPdfNN Run 3/10, Epoch 783/1000, Training Loss (NLML): -960.2473\n",
      "convergence dfGPdfNN Run 3/10, Epoch 784/1000, Training Loss (NLML): -960.2520\n",
      "convergence dfGPdfNN Run 3/10, Epoch 785/1000, Training Loss (NLML): -960.2543\n",
      "convergence dfGPdfNN Run 3/10, Epoch 786/1000, Training Loss (NLML): -960.2567\n",
      "convergence dfGPdfNN Run 3/10, Epoch 787/1000, Training Loss (NLML): -960.2592\n",
      "convergence dfGPdfNN Run 3/10, Epoch 788/1000, Training Loss (NLML): -960.2635\n",
      "convergence dfGPdfNN Run 3/10, Epoch 789/1000, Training Loss (NLML): -960.2654\n",
      "convergence dfGPdfNN Run 3/10, Epoch 790/1000, Training Loss (NLML): -960.2675\n",
      "convergence dfGPdfNN Run 3/10, Epoch 791/1000, Training Loss (NLML): -960.2711\n",
      "convergence dfGPdfNN Run 3/10, Epoch 792/1000, Training Loss (NLML): -960.2734\n",
      "convergence dfGPdfNN Run 3/10, Epoch 793/1000, Training Loss (NLML): -960.2745\n",
      "convergence dfGPdfNN Run 3/10, Epoch 794/1000, Training Loss (NLML): -960.2815\n",
      "convergence dfGPdfNN Run 3/10, Epoch 795/1000, Training Loss (NLML): -960.2853\n",
      "convergence dfGPdfNN Run 3/10, Epoch 796/1000, Training Loss (NLML): -960.2877\n",
      "convergence dfGPdfNN Run 3/10, Epoch 797/1000, Training Loss (NLML): -960.2898\n",
      "convergence dfGPdfNN Run 3/10, Epoch 798/1000, Training Loss (NLML): -960.3035\n",
      "convergence dfGPdfNN Run 3/10, Epoch 799/1000, Training Loss (NLML): -960.2965\n",
      "convergence dfGPdfNN Run 3/10, Epoch 800/1000, Training Loss (NLML): -960.2981\n",
      "convergence dfGPdfNN Run 3/10, Epoch 801/1000, Training Loss (NLML): -960.3014\n",
      "convergence dfGPdfNN Run 3/10, Epoch 802/1000, Training Loss (NLML): -960.2905\n",
      "convergence dfGPdfNN Run 3/10, Epoch 803/1000, Training Loss (NLML): -960.3048\n",
      "convergence dfGPdfNN Run 3/10, Epoch 804/1000, Training Loss (NLML): -960.2958\n",
      "convergence dfGPdfNN Run 3/10, Epoch 805/1000, Training Loss (NLML): -960.3002\n",
      "convergence dfGPdfNN Run 3/10, Epoch 806/1000, Training Loss (NLML): -960.3030\n",
      "convergence dfGPdfNN Run 3/10, Epoch 807/1000, Training Loss (NLML): -960.3175\n",
      "convergence dfGPdfNN Run 3/10, Epoch 808/1000, Training Loss (NLML): -960.3206\n",
      "convergence dfGPdfNN Run 3/10, Epoch 809/1000, Training Loss (NLML): -960.3242\n",
      "convergence dfGPdfNN Run 3/10, Epoch 810/1000, Training Loss (NLML): -960.3145\n",
      "convergence dfGPdfNN Run 3/10, Epoch 811/1000, Training Loss (NLML): -960.3167\n",
      "convergence dfGPdfNN Run 3/10, Epoch 812/1000, Training Loss (NLML): -960.3202\n",
      "convergence dfGPdfNN Run 3/10, Epoch 813/1000, Training Loss (NLML): -960.3239\n",
      "convergence dfGPdfNN Run 3/10, Epoch 814/1000, Training Loss (NLML): -960.3256\n",
      "convergence dfGPdfNN Run 3/10, Epoch 815/1000, Training Loss (NLML): -960.3408\n",
      "convergence dfGPdfNN Run 3/10, Epoch 816/1000, Training Loss (NLML): -960.3411\n",
      "convergence dfGPdfNN Run 3/10, Epoch 817/1000, Training Loss (NLML): -960.3461\n",
      "convergence dfGPdfNN Run 3/10, Epoch 818/1000, Training Loss (NLML): -960.3356\n",
      "convergence dfGPdfNN Run 3/10, Epoch 819/1000, Training Loss (NLML): -960.3397\n",
      "convergence dfGPdfNN Run 3/10, Epoch 820/1000, Training Loss (NLML): -960.3542\n",
      "convergence dfGPdfNN Run 3/10, Epoch 821/1000, Training Loss (NLML): -960.3562\n",
      "convergence dfGPdfNN Run 3/10, Epoch 822/1000, Training Loss (NLML): -960.3480\n",
      "convergence dfGPdfNN Run 3/10, Epoch 823/1000, Training Loss (NLML): -960.3511\n",
      "convergence dfGPdfNN Run 3/10, Epoch 824/1000, Training Loss (NLML): -960.3545\n",
      "convergence dfGPdfNN Run 3/10, Epoch 825/1000, Training Loss (NLML): -960.3920\n",
      "convergence dfGPdfNN Run 3/10, Epoch 826/1000, Training Loss (NLML): -960.3586\n",
      "convergence dfGPdfNN Run 3/10, Epoch 827/1000, Training Loss (NLML): -960.3621\n",
      "convergence dfGPdfNN Run 3/10, Epoch 828/1000, Training Loss (NLML): -960.3638\n",
      "convergence dfGPdfNN Run 3/10, Epoch 829/1000, Training Loss (NLML): -960.3783\n",
      "convergence dfGPdfNN Run 3/10, Epoch 830/1000, Training Loss (NLML): -960.3794\n",
      "convergence dfGPdfNN Run 3/10, Epoch 831/1000, Training Loss (NLML): -960.3833\n",
      "convergence dfGPdfNN Run 3/10, Epoch 832/1000, Training Loss (NLML): -960.3737\n",
      "convergence dfGPdfNN Run 3/10, Epoch 833/1000, Training Loss (NLML): -960.3774\n",
      "convergence dfGPdfNN Run 3/10, Epoch 834/1000, Training Loss (NLML): -960.3807\n",
      "convergence dfGPdfNN Run 3/10, Epoch 835/1000, Training Loss (NLML): -960.4199\n",
      "convergence dfGPdfNN Run 3/10, Epoch 836/1000, Training Loss (NLML): -960.3939\n",
      "convergence dfGPdfNN Run 3/10, Epoch 837/1000, Training Loss (NLML): -960.3853\n",
      "convergence dfGPdfNN Run 3/10, Epoch 838/1000, Training Loss (NLML): -960.3866\n",
      "convergence dfGPdfNN Run 3/10, Epoch 839/1000, Training Loss (NLML): -960.4253\n",
      "convergence dfGPdfNN Run 3/10, Epoch 840/1000, Training Loss (NLML): -960.4292\n",
      "convergence dfGPdfNN Run 3/10, Epoch 841/1000, Training Loss (NLML): -960.3943\n",
      "convergence dfGPdfNN Run 3/10, Epoch 842/1000, Training Loss (NLML): -960.3977\n",
      "convergence dfGPdfNN Run 3/10, Epoch 843/1000, Training Loss (NLML): -960.3986\n",
      "convergence dfGPdfNN Run 3/10, Epoch 844/1000, Training Loss (NLML): -960.4012\n",
      "convergence dfGPdfNN Run 3/10, Epoch 845/1000, Training Loss (NLML): -960.4014\n",
      "convergence dfGPdfNN Run 3/10, Epoch 846/1000, Training Loss (NLML): -960.4312\n",
      "convergence dfGPdfNN Run 3/10, Epoch 847/1000, Training Loss (NLML): -960.4091\n",
      "convergence dfGPdfNN Run 3/10, Epoch 848/1000, Training Loss (NLML): -960.4125\n",
      "convergence dfGPdfNN Run 3/10, Epoch 849/1000, Training Loss (NLML): -960.4117\n",
      "convergence dfGPdfNN Run 3/10, Epoch 850/1000, Training Loss (NLML): -960.4399\n",
      "convergence dfGPdfNN Run 3/10, Epoch 851/1000, Training Loss (NLML): -960.4425\n",
      "convergence dfGPdfNN Run 3/10, Epoch 852/1000, Training Loss (NLML): -960.4193\n",
      "convergence dfGPdfNN Run 3/10, Epoch 853/1000, Training Loss (NLML): -960.4174\n",
      "convergence dfGPdfNN Run 3/10, Epoch 854/1000, Training Loss (NLML): -960.4169\n",
      "convergence dfGPdfNN Run 3/10, Epoch 855/1000, Training Loss (NLML): -960.4480\n",
      "convergence dfGPdfNN Run 3/10, Epoch 856/1000, Training Loss (NLML): -960.4425\n",
      "convergence dfGPdfNN Run 3/10, Epoch 857/1000, Training Loss (NLML): -960.4446\n",
      "convergence dfGPdfNN Run 3/10, Epoch 858/1000, Training Loss (NLML): -960.4181\n",
      "convergence dfGPdfNN Run 3/10, Epoch 859/1000, Training Loss (NLML): -960.4199\n",
      "convergence dfGPdfNN Run 3/10, Epoch 860/1000, Training Loss (NLML): -960.4230\n",
      "convergence dfGPdfNN Run 3/10, Epoch 861/1000, Training Loss (NLML): -960.4517\n",
      "convergence dfGPdfNN Run 3/10, Epoch 862/1000, Training Loss (NLML): -960.4553\n",
      "convergence dfGPdfNN Run 3/10, Epoch 863/1000, Training Loss (NLML): -960.4580\n",
      "convergence dfGPdfNN Run 3/10, Epoch 864/1000, Training Loss (NLML): -960.4604\n",
      "convergence dfGPdfNN Run 3/10, Epoch 865/1000, Training Loss (NLML): -960.4596\n",
      "convergence dfGPdfNN Run 3/10, Epoch 866/1000, Training Loss (NLML): -960.4386\n",
      "convergence dfGPdfNN Run 3/10, Epoch 867/1000, Training Loss (NLML): -960.4414\n",
      "convergence dfGPdfNN Run 3/10, Epoch 868/1000, Training Loss (NLML): -960.4678\n",
      "convergence dfGPdfNN Run 3/10, Epoch 869/1000, Training Loss (NLML): -960.4718\n",
      "convergence dfGPdfNN Run 3/10, Epoch 870/1000, Training Loss (NLML): -960.4753\n",
      "convergence dfGPdfNN Run 3/10, Epoch 871/1000, Training Loss (NLML): -960.4777\n",
      "convergence dfGPdfNN Run 3/10, Epoch 872/1000, Training Loss (NLML): -960.4739\n",
      "convergence dfGPdfNN Run 3/10, Epoch 873/1000, Training Loss (NLML): -960.4493\n",
      "convergence dfGPdfNN Run 3/10, Epoch 874/1000, Training Loss (NLML): -960.4792\n",
      "convergence dfGPdfNN Run 3/10, Epoch 875/1000, Training Loss (NLML): -960.4819\n",
      "convergence dfGPdfNN Run 3/10, Epoch 876/1000, Training Loss (NLML): -960.4832\n",
      "convergence dfGPdfNN Run 3/10, Epoch 877/1000, Training Loss (NLML): -960.4868\n",
      "convergence dfGPdfNN Run 3/10, Epoch 878/1000, Training Loss (NLML): -960.4642\n",
      "convergence dfGPdfNN Run 3/10, Epoch 879/1000, Training Loss (NLML): -960.4926\n",
      "convergence dfGPdfNN Run 3/10, Epoch 880/1000, Training Loss (NLML): -960.4886\n",
      "convergence dfGPdfNN Run 3/10, Epoch 881/1000, Training Loss (NLML): -960.4911\n",
      "convergence dfGPdfNN Run 3/10, Epoch 882/1000, Training Loss (NLML): -960.4957\n",
      "convergence dfGPdfNN Run 3/10, Epoch 883/1000, Training Loss (NLML): -960.4971\n",
      "convergence dfGPdfNN Run 3/10, Epoch 884/1000, Training Loss (NLML): -960.4991\n",
      "convergence dfGPdfNN Run 3/10, Epoch 885/1000, Training Loss (NLML): -960.5037\n",
      "convergence dfGPdfNN Run 3/10, Epoch 886/1000, Training Loss (NLML): -960.4775\n",
      "convergence dfGPdfNN Run 3/10, Epoch 887/1000, Training Loss (NLML): -960.5061\n",
      "convergence dfGPdfNN Run 3/10, Epoch 888/1000, Training Loss (NLML): -960.5099\n",
      "convergence dfGPdfNN Run 3/10, Epoch 889/1000, Training Loss (NLML): -960.5105\n",
      "convergence dfGPdfNN Run 3/10, Epoch 890/1000, Training Loss (NLML): -960.5150\n",
      "convergence dfGPdfNN Run 3/10, Epoch 891/1000, Training Loss (NLML): -960.5139\n",
      "convergence dfGPdfNN Run 3/10, Epoch 892/1000, Training Loss (NLML): -960.5214\n",
      "convergence dfGPdfNN Run 3/10, Epoch 893/1000, Training Loss (NLML): -960.5215\n",
      "convergence dfGPdfNN Run 3/10, Epoch 894/1000, Training Loss (NLML): -960.5259\n",
      "convergence dfGPdfNN Run 3/10, Epoch 895/1000, Training Loss (NLML): -960.5254\n",
      "convergence dfGPdfNN Run 3/10, Epoch 896/1000, Training Loss (NLML): -960.5283\n",
      "convergence dfGPdfNN Run 3/10, Epoch 897/1000, Training Loss (NLML): -960.5306\n",
      "convergence dfGPdfNN Run 3/10, Epoch 898/1000, Training Loss (NLML): -960.5331\n",
      "convergence dfGPdfNN Run 3/10, Epoch 899/1000, Training Loss (NLML): -960.5325\n",
      "convergence dfGPdfNN Run 3/10, Epoch 900/1000, Training Loss (NLML): -960.5338\n",
      "convergence dfGPdfNN Run 3/10, Epoch 901/1000, Training Loss (NLML): -960.5358\n",
      "convergence dfGPdfNN Run 3/10, Epoch 902/1000, Training Loss (NLML): -960.5380\n",
      "convergence dfGPdfNN Run 3/10, Epoch 903/1000, Training Loss (NLML): -960.5139\n",
      "convergence dfGPdfNN Run 3/10, Epoch 904/1000, Training Loss (NLML): -960.5426\n",
      "convergence dfGPdfNN Run 3/10, Epoch 905/1000, Training Loss (NLML): -960.5468\n",
      "convergence dfGPdfNN Run 3/10, Epoch 906/1000, Training Loss (NLML): -960.5460\n",
      "convergence dfGPdfNN Run 3/10, Epoch 907/1000, Training Loss (NLML): -960.5521\n",
      "convergence dfGPdfNN Run 3/10, Epoch 908/1000, Training Loss (NLML): -960.5548\n",
      "convergence dfGPdfNN Run 3/10, Epoch 909/1000, Training Loss (NLML): -960.5555\n",
      "convergence dfGPdfNN Run 3/10, Epoch 910/1000, Training Loss (NLML): -960.5581\n",
      "convergence dfGPdfNN Run 3/10, Epoch 911/1000, Training Loss (NLML): -960.5336\n",
      "convergence dfGPdfNN Run 3/10, Epoch 912/1000, Training Loss (NLML): -960.5623\n",
      "convergence dfGPdfNN Run 3/10, Epoch 913/1000, Training Loss (NLML): -960.5658\n",
      "convergence dfGPdfNN Run 3/10, Epoch 914/1000, Training Loss (NLML): -960.5680\n",
      "convergence dfGPdfNN Run 3/10, Epoch 915/1000, Training Loss (NLML): -960.5669\n",
      "convergence dfGPdfNN Run 3/10, Epoch 916/1000, Training Loss (NLML): -960.5698\n",
      "convergence dfGPdfNN Run 3/10, Epoch 917/1000, Training Loss (NLML): -960.5720\n",
      "convergence dfGPdfNN Run 3/10, Epoch 918/1000, Training Loss (NLML): -960.5745\n",
      "convergence dfGPdfNN Run 3/10, Epoch 919/1000, Training Loss (NLML): -960.5764\n",
      "convergence dfGPdfNN Run 3/10, Epoch 920/1000, Training Loss (NLML): -960.5531\n",
      "convergence dfGPdfNN Run 3/10, Epoch 921/1000, Training Loss (NLML): -960.5847\n",
      "convergence dfGPdfNN Run 3/10, Epoch 922/1000, Training Loss (NLML): -960.5839\n",
      "convergence dfGPdfNN Run 3/10, Epoch 923/1000, Training Loss (NLML): -960.5885\n",
      "convergence dfGPdfNN Run 3/10, Epoch 924/1000, Training Loss (NLML): -960.5880\n",
      "convergence dfGPdfNN Run 3/10, Epoch 925/1000, Training Loss (NLML): -960.5897\n",
      "convergence dfGPdfNN Run 3/10, Epoch 926/1000, Training Loss (NLML): -960.5941\n",
      "convergence dfGPdfNN Run 3/10, Epoch 927/1000, Training Loss (NLML): -960.5947\n",
      "convergence dfGPdfNN Run 3/10, Epoch 928/1000, Training Loss (NLML): -960.5980\n",
      "convergence dfGPdfNN Run 3/10, Epoch 929/1000, Training Loss (NLML): -960.5991\n",
      "convergence dfGPdfNN Run 3/10, Epoch 930/1000, Training Loss (NLML): -960.6017\n",
      "convergence dfGPdfNN Run 3/10, Epoch 931/1000, Training Loss (NLML): -960.6039\n",
      "convergence dfGPdfNN Run 3/10, Epoch 932/1000, Training Loss (NLML): -960.6077\n",
      "convergence dfGPdfNN Run 3/10, Epoch 933/1000, Training Loss (NLML): -960.6085\n",
      "convergence dfGPdfNN Run 3/10, Epoch 934/1000, Training Loss (NLML): -960.6101\n",
      "convergence dfGPdfNN Run 3/10, Epoch 935/1000, Training Loss (NLML): -960.6141\n",
      "convergence dfGPdfNN Run 3/10, Epoch 936/1000, Training Loss (NLML): -960.6177\n",
      "convergence dfGPdfNN Run 3/10, Epoch 937/1000, Training Loss (NLML): -960.6178\n",
      "convergence dfGPdfNN Run 3/10, Epoch 938/1000, Training Loss (NLML): -960.6219\n",
      "convergence dfGPdfNN Run 3/10, Epoch 939/1000, Training Loss (NLML): -960.6248\n",
      "convergence dfGPdfNN Run 3/10, Epoch 940/1000, Training Loss (NLML): -960.6271\n",
      "convergence dfGPdfNN Run 3/10, Epoch 941/1000, Training Loss (NLML): -960.6290\n",
      "convergence dfGPdfNN Run 3/10, Epoch 942/1000, Training Loss (NLML): -960.6298\n",
      "convergence dfGPdfNN Run 3/10, Epoch 943/1000, Training Loss (NLML): -960.6317\n",
      "convergence dfGPdfNN Run 3/10, Epoch 944/1000, Training Loss (NLML): -960.6366\n",
      "convergence dfGPdfNN Run 3/10, Epoch 945/1000, Training Loss (NLML): -960.6360\n",
      "convergence dfGPdfNN Run 3/10, Epoch 946/1000, Training Loss (NLML): -960.6384\n",
      "convergence dfGPdfNN Run 3/10, Epoch 947/1000, Training Loss (NLML): -960.6433\n",
      "convergence dfGPdfNN Run 3/10, Epoch 948/1000, Training Loss (NLML): -960.6433\n",
      "convergence dfGPdfNN Run 3/10, Epoch 949/1000, Training Loss (NLML): -960.6459\n",
      "convergence dfGPdfNN Run 3/10, Epoch 950/1000, Training Loss (NLML): -960.6476\n",
      "convergence dfGPdfNN Run 3/10, Epoch 951/1000, Training Loss (NLML): -960.6498\n",
      "convergence dfGPdfNN Run 3/10, Epoch 952/1000, Training Loss (NLML): -960.6497\n",
      "convergence dfGPdfNN Run 3/10, Epoch 953/1000, Training Loss (NLML): -960.6532\n",
      "convergence dfGPdfNN Run 3/10, Epoch 954/1000, Training Loss (NLML): -960.6561\n",
      "convergence dfGPdfNN Run 3/10, Epoch 955/1000, Training Loss (NLML): -960.6570\n",
      "convergence dfGPdfNN Run 3/10, Epoch 956/1000, Training Loss (NLML): -960.6616\n",
      "convergence dfGPdfNN Run 3/10, Epoch 957/1000, Training Loss (NLML): -960.6632\n",
      "convergence dfGPdfNN Run 3/10, Epoch 958/1000, Training Loss (NLML): -960.6633\n",
      "convergence dfGPdfNN Run 3/10, Epoch 959/1000, Training Loss (NLML): -960.6658\n",
      "convergence dfGPdfNN Run 3/10, Epoch 960/1000, Training Loss (NLML): -960.6678\n",
      "convergence dfGPdfNN Run 3/10, Epoch 961/1000, Training Loss (NLML): -960.6702\n",
      "convergence dfGPdfNN Run 3/10, Epoch 962/1000, Training Loss (NLML): -960.6752\n",
      "convergence dfGPdfNN Run 3/10, Epoch 963/1000, Training Loss (NLML): -960.6729\n",
      "convergence dfGPdfNN Run 3/10, Epoch 964/1000, Training Loss (NLML): -960.6771\n",
      "convergence dfGPdfNN Run 3/10, Epoch 965/1000, Training Loss (NLML): -960.6779\n",
      "convergence dfGPdfNN Run 3/10, Epoch 966/1000, Training Loss (NLML): -960.6792\n",
      "convergence dfGPdfNN Run 3/10, Epoch 967/1000, Training Loss (NLML): -960.6819\n",
      "convergence dfGPdfNN Run 3/10, Epoch 968/1000, Training Loss (NLML): -960.6864\n",
      "convergence dfGPdfNN Run 3/10, Epoch 969/1000, Training Loss (NLML): -960.6863\n",
      "convergence dfGPdfNN Run 3/10, Epoch 970/1000, Training Loss (NLML): -960.6885\n",
      "convergence dfGPdfNN Run 3/10, Epoch 971/1000, Training Loss (NLML): -960.6881\n",
      "convergence dfGPdfNN Run 3/10, Epoch 972/1000, Training Loss (NLML): -960.6918\n",
      "convergence dfGPdfNN Run 3/10, Epoch 973/1000, Training Loss (NLML): -960.6930\n",
      "convergence dfGPdfNN Run 3/10, Epoch 974/1000, Training Loss (NLML): -960.6960\n",
      "convergence dfGPdfNN Run 3/10, Epoch 975/1000, Training Loss (NLML): -960.6978\n",
      "convergence dfGPdfNN Run 3/10, Epoch 976/1000, Training Loss (NLML): -960.7008\n",
      "convergence dfGPdfNN Run 3/10, Epoch 977/1000, Training Loss (NLML): -960.7036\n",
      "convergence dfGPdfNN Run 3/10, Epoch 978/1000, Training Loss (NLML): -960.7054\n",
      "convergence dfGPdfNN Run 3/10, Epoch 979/1000, Training Loss (NLML): -960.7086\n",
      "convergence dfGPdfNN Run 3/10, Epoch 980/1000, Training Loss (NLML): -960.7113\n",
      "convergence dfGPdfNN Run 3/10, Epoch 981/1000, Training Loss (NLML): -960.7101\n",
      "convergence dfGPdfNN Run 3/10, Epoch 982/1000, Training Loss (NLML): -960.7126\n",
      "convergence dfGPdfNN Run 3/10, Epoch 983/1000, Training Loss (NLML): -960.7144\n",
      "convergence dfGPdfNN Run 3/10, Epoch 984/1000, Training Loss (NLML): -960.7181\n",
      "convergence dfGPdfNN Run 3/10, Epoch 985/1000, Training Loss (NLML): -960.7198\n",
      "convergence dfGPdfNN Run 3/10, Epoch 986/1000, Training Loss (NLML): -960.7198\n",
      "convergence dfGPdfNN Run 3/10, Epoch 987/1000, Training Loss (NLML): -960.7228\n",
      "convergence dfGPdfNN Run 3/10, Epoch 988/1000, Training Loss (NLML): -960.7240\n",
      "convergence dfGPdfNN Run 3/10, Epoch 989/1000, Training Loss (NLML): -960.7285\n",
      "convergence dfGPdfNN Run 3/10, Epoch 990/1000, Training Loss (NLML): -960.7306\n",
      "convergence dfGPdfNN Run 3/10, Epoch 991/1000, Training Loss (NLML): -960.7299\n",
      "convergence dfGPdfNN Run 3/10, Epoch 992/1000, Training Loss (NLML): -960.7324\n",
      "convergence dfGPdfNN Run 3/10, Epoch 993/1000, Training Loss (NLML): -960.7347\n",
      "convergence dfGPdfNN Run 3/10, Epoch 994/1000, Training Loss (NLML): -960.7356\n",
      "convergence dfGPdfNN Run 3/10, Epoch 995/1000, Training Loss (NLML): -960.7360\n",
      "convergence dfGPdfNN Run 3/10, Epoch 996/1000, Training Loss (NLML): -960.7407\n",
      "convergence dfGPdfNN Run 3/10, Epoch 997/1000, Training Loss (NLML): -960.7418\n",
      "convergence dfGPdfNN Run 3/10, Epoch 998/1000, Training Loss (NLML): -960.7440\n",
      "convergence dfGPdfNN Run 3/10, Epoch 999/1000, Training Loss (NLML): -960.7485\n",
      "convergence dfGPdfNN Run 3/10, Epoch 1000/1000, Training Loss (NLML): -960.7482\n",
      "\n",
      "--- Training Run 4/10 ---\n",
      "\n",
      "Start Training\n",
      "convergence dfGPdfNN Run 4/10, Epoch 1/1000, Training Loss (NLML): -811.8303\n",
      "convergence dfGPdfNN Run 4/10, Epoch 2/1000, Training Loss (NLML): -822.9938\n",
      "convergence dfGPdfNN Run 4/10, Epoch 3/1000, Training Loss (NLML): -837.1634\n",
      "convergence dfGPdfNN Run 4/10, Epoch 4/1000, Training Loss (NLML): -844.3440\n",
      "convergence dfGPdfNN Run 4/10, Epoch 5/1000, Training Loss (NLML): -850.3025\n",
      "convergence dfGPdfNN Run 4/10, Epoch 6/1000, Training Loss (NLML): -854.8527\n",
      "convergence dfGPdfNN Run 4/10, Epoch 7/1000, Training Loss (NLML): -858.9056\n",
      "convergence dfGPdfNN Run 4/10, Epoch 8/1000, Training Loss (NLML): -862.5701\n",
      "convergence dfGPdfNN Run 4/10, Epoch 9/1000, Training Loss (NLML): -866.3110\n",
      "convergence dfGPdfNN Run 4/10, Epoch 10/1000, Training Loss (NLML): -869.7727\n",
      "convergence dfGPdfNN Run 4/10, Epoch 11/1000, Training Loss (NLML): -872.9858\n",
      "convergence dfGPdfNN Run 4/10, Epoch 12/1000, Training Loss (NLML): -875.6328\n",
      "convergence dfGPdfNN Run 4/10, Epoch 13/1000, Training Loss (NLML): -877.2495\n",
      "convergence dfGPdfNN Run 4/10, Epoch 14/1000, Training Loss (NLML): -881.2917\n",
      "convergence dfGPdfNN Run 4/10, Epoch 15/1000, Training Loss (NLML): -883.8160\n",
      "convergence dfGPdfNN Run 4/10, Epoch 16/1000, Training Loss (NLML): -886.0479\n",
      "convergence dfGPdfNN Run 4/10, Epoch 17/1000, Training Loss (NLML): -888.4878\n",
      "convergence dfGPdfNN Run 4/10, Epoch 18/1000, Training Loss (NLML): -890.7140\n",
      "convergence dfGPdfNN Run 4/10, Epoch 19/1000, Training Loss (NLML): -892.7094\n",
      "convergence dfGPdfNN Run 4/10, Epoch 20/1000, Training Loss (NLML): -894.8656\n",
      "convergence dfGPdfNN Run 4/10, Epoch 21/1000, Training Loss (NLML): -896.6597\n",
      "convergence dfGPdfNN Run 4/10, Epoch 22/1000, Training Loss (NLML): -898.3228\n",
      "convergence dfGPdfNN Run 4/10, Epoch 23/1000, Training Loss (NLML): -900.0111\n",
      "convergence dfGPdfNN Run 4/10, Epoch 24/1000, Training Loss (NLML): -901.5420\n",
      "convergence dfGPdfNN Run 4/10, Epoch 25/1000, Training Loss (NLML): -903.1541\n",
      "convergence dfGPdfNN Run 4/10, Epoch 26/1000, Training Loss (NLML): -904.6365\n",
      "convergence dfGPdfNN Run 4/10, Epoch 27/1000, Training Loss (NLML): -906.0446\n",
      "convergence dfGPdfNN Run 4/10, Epoch 28/1000, Training Loss (NLML): -907.1010\n",
      "convergence dfGPdfNN Run 4/10, Epoch 29/1000, Training Loss (NLML): -908.5927\n",
      "convergence dfGPdfNN Run 4/10, Epoch 30/1000, Training Loss (NLML): -909.6791\n",
      "convergence dfGPdfNN Run 4/10, Epoch 31/1000, Training Loss (NLML): -910.9086\n",
      "convergence dfGPdfNN Run 4/10, Epoch 32/1000, Training Loss (NLML): -912.0243\n",
      "convergence dfGPdfNN Run 4/10, Epoch 33/1000, Training Loss (NLML): -913.1228\n",
      "convergence dfGPdfNN Run 4/10, Epoch 34/1000, Training Loss (NLML): -914.2223\n",
      "convergence dfGPdfNN Run 4/10, Epoch 35/1000, Training Loss (NLML): -915.1504\n",
      "convergence dfGPdfNN Run 4/10, Epoch 36/1000, Training Loss (NLML): -916.0437\n",
      "convergence dfGPdfNN Run 4/10, Epoch 37/1000, Training Loss (NLML): -917.0448\n",
      "convergence dfGPdfNN Run 4/10, Epoch 38/1000, Training Loss (NLML): -918.0267\n",
      "convergence dfGPdfNN Run 4/10, Epoch 39/1000, Training Loss (NLML): -918.8810\n",
      "convergence dfGPdfNN Run 4/10, Epoch 40/1000, Training Loss (NLML): -919.7405\n",
      "convergence dfGPdfNN Run 4/10, Epoch 41/1000, Training Loss (NLML): -920.3763\n",
      "convergence dfGPdfNN Run 4/10, Epoch 42/1000, Training Loss (NLML): -921.1678\n",
      "convergence dfGPdfNN Run 4/10, Epoch 43/1000, Training Loss (NLML): -922.0577\n",
      "convergence dfGPdfNN Run 4/10, Epoch 44/1000, Training Loss (NLML): -922.7451\n",
      "convergence dfGPdfNN Run 4/10, Epoch 45/1000, Training Loss (NLML): -923.4324\n",
      "convergence dfGPdfNN Run 4/10, Epoch 46/1000, Training Loss (NLML): -923.3020\n",
      "convergence dfGPdfNN Run 4/10, Epoch 47/1000, Training Loss (NLML): -924.8895\n",
      "convergence dfGPdfNN Run 4/10, Epoch 48/1000, Training Loss (NLML): -925.4696\n",
      "convergence dfGPdfNN Run 4/10, Epoch 49/1000, Training Loss (NLML): -926.1232\n",
      "convergence dfGPdfNN Run 4/10, Epoch 50/1000, Training Loss (NLML): -926.6283\n",
      "convergence dfGPdfNN Run 4/10, Epoch 51/1000, Training Loss (NLML): -927.2404\n",
      "convergence dfGPdfNN Run 4/10, Epoch 52/1000, Training Loss (NLML): -927.9092\n",
      "convergence dfGPdfNN Run 4/10, Epoch 53/1000, Training Loss (NLML): -928.3727\n",
      "convergence dfGPdfNN Run 4/10, Epoch 54/1000, Training Loss (NLML): -929.0747\n",
      "convergence dfGPdfNN Run 4/10, Epoch 55/1000, Training Loss (NLML): -929.6144\n",
      "convergence dfGPdfNN Run 4/10, Epoch 56/1000, Training Loss (NLML): -930.1503\n",
      "convergence dfGPdfNN Run 4/10, Epoch 57/1000, Training Loss (NLML): -930.4282\n",
      "convergence dfGPdfNN Run 4/10, Epoch 58/1000, Training Loss (NLML): -931.0300\n",
      "convergence dfGPdfNN Run 4/10, Epoch 59/1000, Training Loss (NLML): -931.5048\n",
      "convergence dfGPdfNN Run 4/10, Epoch 60/1000, Training Loss (NLML): -932.1930\n",
      "convergence dfGPdfNN Run 4/10, Epoch 61/1000, Training Loss (NLML): -932.4858\n",
      "convergence dfGPdfNN Run 4/10, Epoch 62/1000, Training Loss (NLML): -932.9037\n",
      "convergence dfGPdfNN Run 4/10, Epoch 63/1000, Training Loss (NLML): -933.4979\n",
      "convergence dfGPdfNN Run 4/10, Epoch 64/1000, Training Loss (NLML): -933.8284\n",
      "convergence dfGPdfNN Run 4/10, Epoch 65/1000, Training Loss (NLML): -934.0266\n",
      "convergence dfGPdfNN Run 4/10, Epoch 66/1000, Training Loss (NLML): -934.7029\n",
      "convergence dfGPdfNN Run 4/10, Epoch 67/1000, Training Loss (NLML): -935.1796\n",
      "convergence dfGPdfNN Run 4/10, Epoch 68/1000, Training Loss (NLML): -935.7373\n",
      "convergence dfGPdfNN Run 4/10, Epoch 69/1000, Training Loss (NLML): -935.5475\n",
      "convergence dfGPdfNN Run 4/10, Epoch 70/1000, Training Loss (NLML): -935.5260\n",
      "convergence dfGPdfNN Run 4/10, Epoch 71/1000, Training Loss (NLML): -936.8728\n",
      "convergence dfGPdfNN Run 4/10, Epoch 72/1000, Training Loss (NLML): -937.0959\n",
      "convergence dfGPdfNN Run 4/10, Epoch 73/1000, Training Loss (NLML): -937.4742\n",
      "convergence dfGPdfNN Run 4/10, Epoch 74/1000, Training Loss (NLML): -937.8669\n",
      "convergence dfGPdfNN Run 4/10, Epoch 75/1000, Training Loss (NLML): -938.0839\n",
      "convergence dfGPdfNN Run 4/10, Epoch 76/1000, Training Loss (NLML): -938.6250\n",
      "convergence dfGPdfNN Run 4/10, Epoch 77/1000, Training Loss (NLML): -938.9347\n",
      "convergence dfGPdfNN Run 4/10, Epoch 78/1000, Training Loss (NLML): -939.2949\n",
      "convergence dfGPdfNN Run 4/10, Epoch 79/1000, Training Loss (NLML): -937.7091\n",
      "convergence dfGPdfNN Run 4/10, Epoch 80/1000, Training Loss (NLML): -939.9634\n",
      "convergence dfGPdfNN Run 4/10, Epoch 81/1000, Training Loss (NLML): -940.3339\n",
      "convergence dfGPdfNN Run 4/10, Epoch 82/1000, Training Loss (NLML): -940.6006\n",
      "convergence dfGPdfNN Run 4/10, Epoch 83/1000, Training Loss (NLML): -940.7725\n",
      "convergence dfGPdfNN Run 4/10, Epoch 84/1000, Training Loss (NLML): -941.0643\n",
      "convergence dfGPdfNN Run 4/10, Epoch 85/1000, Training Loss (NLML): -941.5674\n",
      "convergence dfGPdfNN Run 4/10, Epoch 86/1000, Training Loss (NLML): -941.1898\n",
      "convergence dfGPdfNN Run 4/10, Epoch 87/1000, Training Loss (NLML): -942.0966\n",
      "convergence dfGPdfNN Run 4/10, Epoch 88/1000, Training Loss (NLML): -942.5079\n",
      "convergence dfGPdfNN Run 4/10, Epoch 89/1000, Training Loss (NLML): -942.8043\n",
      "convergence dfGPdfNN Run 4/10, Epoch 90/1000, Training Loss (NLML): -942.5254\n",
      "convergence dfGPdfNN Run 4/10, Epoch 91/1000, Training Loss (NLML): -943.2687\n",
      "convergence dfGPdfNN Run 4/10, Epoch 92/1000, Training Loss (NLML): -942.9536\n",
      "convergence dfGPdfNN Run 4/10, Epoch 93/1000, Training Loss (NLML): -943.7313\n",
      "convergence dfGPdfNN Run 4/10, Epoch 94/1000, Training Loss (NLML): -944.0018\n",
      "convergence dfGPdfNN Run 4/10, Epoch 95/1000, Training Loss (NLML): -943.6136\n",
      "convergence dfGPdfNN Run 4/10, Epoch 96/1000, Training Loss (NLML): -943.9668\n",
      "convergence dfGPdfNN Run 4/10, Epoch 97/1000, Training Loss (NLML): -944.2930\n",
      "convergence dfGPdfNN Run 4/10, Epoch 98/1000, Training Loss (NLML): -945.2031\n",
      "convergence dfGPdfNN Run 4/10, Epoch 99/1000, Training Loss (NLML): -945.4313\n",
      "convergence dfGPdfNN Run 4/10, Epoch 100/1000, Training Loss (NLML): -944.9412\n",
      "convergence dfGPdfNN Run 4/10, Epoch 101/1000, Training Loss (NLML): -945.8781\n",
      "convergence dfGPdfNN Run 4/10, Epoch 102/1000, Training Loss (NLML): -946.1121\n",
      "convergence dfGPdfNN Run 4/10, Epoch 103/1000, Training Loss (NLML): -946.2729\n",
      "convergence dfGPdfNN Run 4/10, Epoch 104/1000, Training Loss (NLML): -946.5876\n",
      "convergence dfGPdfNN Run 4/10, Epoch 105/1000, Training Loss (NLML): -946.7527\n",
      "convergence dfGPdfNN Run 4/10, Epoch 106/1000, Training Loss (NLML): -946.9163\n",
      "convergence dfGPdfNN Run 4/10, Epoch 107/1000, Training Loss (NLML): -943.9717\n",
      "convergence dfGPdfNN Run 4/10, Epoch 108/1000, Training Loss (NLML): -947.1613\n",
      "convergence dfGPdfNN Run 4/10, Epoch 109/1000, Training Loss (NLML): -946.8120\n",
      "convergence dfGPdfNN Run 4/10, Epoch 110/1000, Training Loss (NLML): -947.3479\n",
      "convergence dfGPdfNN Run 4/10, Epoch 111/1000, Training Loss (NLML): -947.6077\n",
      "convergence dfGPdfNN Run 4/10, Epoch 112/1000, Training Loss (NLML): -947.8138\n",
      "convergence dfGPdfNN Run 4/10, Epoch 113/1000, Training Loss (NLML): -948.2982\n",
      "convergence dfGPdfNN Run 4/10, Epoch 114/1000, Training Loss (NLML): -948.4767\n",
      "convergence dfGPdfNN Run 4/10, Epoch 115/1000, Training Loss (NLML): -947.2401\n",
      "convergence dfGPdfNN Run 4/10, Epoch 116/1000, Training Loss (NLML): -948.4041\n",
      "convergence dfGPdfNN Run 4/10, Epoch 117/1000, Training Loss (NLML): -949.1064\n",
      "convergence dfGPdfNN Run 4/10, Epoch 118/1000, Training Loss (NLML): -949.2312\n",
      "convergence dfGPdfNN Run 4/10, Epoch 119/1000, Training Loss (NLML): -949.2971\n",
      "convergence dfGPdfNN Run 4/10, Epoch 120/1000, Training Loss (NLML): -948.9745\n",
      "convergence dfGPdfNN Run 4/10, Epoch 121/1000, Training Loss (NLML): -949.3009\n",
      "convergence dfGPdfNN Run 4/10, Epoch 122/1000, Training Loss (NLML): -949.0763\n",
      "convergence dfGPdfNN Run 4/10, Epoch 123/1000, Training Loss (NLML): -949.6499\n",
      "convergence dfGPdfNN Run 4/10, Epoch 124/1000, Training Loss (NLML): -949.0607\n",
      "convergence dfGPdfNN Run 4/10, Epoch 125/1000, Training Loss (NLML): -949.7397\n",
      "convergence dfGPdfNN Run 4/10, Epoch 126/1000, Training Loss (NLML): -949.8802\n",
      "convergence dfGPdfNN Run 4/10, Epoch 127/1000, Training Loss (NLML): -950.0011\n",
      "convergence dfGPdfNN Run 4/10, Epoch 128/1000, Training Loss (NLML): -950.1268\n",
      "convergence dfGPdfNN Run 4/10, Epoch 129/1000, Training Loss (NLML): -950.1497\n",
      "convergence dfGPdfNN Run 4/10, Epoch 130/1000, Training Loss (NLML): -946.6564\n",
      "convergence dfGPdfNN Run 4/10, Epoch 131/1000, Training Loss (NLML): -950.0089\n",
      "convergence dfGPdfNN Run 4/10, Epoch 132/1000, Training Loss (NLML): -950.0278\n",
      "convergence dfGPdfNN Run 4/10, Epoch 133/1000, Training Loss (NLML): -950.2621\n",
      "convergence dfGPdfNN Run 4/10, Epoch 134/1000, Training Loss (NLML): -950.2571\n",
      "convergence dfGPdfNN Run 4/10, Epoch 135/1000, Training Loss (NLML): -950.3485\n",
      "convergence dfGPdfNN Run 4/10, Epoch 136/1000, Training Loss (NLML): -950.4136\n",
      "convergence dfGPdfNN Run 4/10, Epoch 137/1000, Training Loss (NLML): -950.4233\n",
      "convergence dfGPdfNN Run 4/10, Epoch 138/1000, Training Loss (NLML): -950.3195\n",
      "convergence dfGPdfNN Run 4/10, Epoch 139/1000, Training Loss (NLML): -946.6348\n",
      "convergence dfGPdfNN Run 4/10, Epoch 140/1000, Training Loss (NLML): -948.5078\n",
      "convergence dfGPdfNN Run 4/10, Epoch 141/1000, Training Loss (NLML): -950.0879\n",
      "convergence dfGPdfNN Run 4/10, Epoch 142/1000, Training Loss (NLML): -950.2557\n",
      "convergence dfGPdfNN Run 4/10, Epoch 143/1000, Training Loss (NLML): -949.9177\n",
      "convergence dfGPdfNN Run 4/10, Epoch 144/1000, Training Loss (NLML): -950.4052\n",
      "convergence dfGPdfNN Run 4/10, Epoch 145/1000, Training Loss (NLML): -950.5068\n",
      "convergence dfGPdfNN Run 4/10, Epoch 146/1000, Training Loss (NLML): -950.5051\n",
      "convergence dfGPdfNN Run 4/10, Epoch 147/1000, Training Loss (NLML): -948.3989\n",
      "convergence dfGPdfNN Run 4/10, Epoch 148/1000, Training Loss (NLML): -950.8213\n",
      "convergence dfGPdfNN Run 4/10, Epoch 149/1000, Training Loss (NLML): -950.8197\n",
      "convergence dfGPdfNN Run 4/10, Epoch 150/1000, Training Loss (NLML): -950.9763\n",
      "convergence dfGPdfNN Run 4/10, Epoch 151/1000, Training Loss (NLML): -950.2195\n",
      "convergence dfGPdfNN Run 4/10, Epoch 152/1000, Training Loss (NLML): -949.0127\n",
      "convergence dfGPdfNN Run 4/10, Epoch 153/1000, Training Loss (NLML): -951.4403\n",
      "convergence dfGPdfNN Run 4/10, Epoch 154/1000, Training Loss (NLML): -951.0707\n",
      "convergence dfGPdfNN Run 4/10, Epoch 155/1000, Training Loss (NLML): -950.4314\n",
      "convergence dfGPdfNN Run 4/10, Epoch 156/1000, Training Loss (NLML): -950.6023\n",
      "convergence dfGPdfNN Run 4/10, Epoch 157/1000, Training Loss (NLML): -948.9901\n",
      "convergence dfGPdfNN Run 4/10, Epoch 158/1000, Training Loss (NLML): -951.1455\n",
      "convergence dfGPdfNN Run 4/10, Epoch 159/1000, Training Loss (NLML): -949.0957\n",
      "convergence dfGPdfNN Run 4/10, Epoch 160/1000, Training Loss (NLML): -951.4532\n",
      "convergence dfGPdfNN Run 4/10, Epoch 161/1000, Training Loss (NLML): -951.4700\n",
      "convergence dfGPdfNN Run 4/10, Epoch 162/1000, Training Loss (NLML): -951.6677\n",
      "convergence dfGPdfNN Run 4/10, Epoch 163/1000, Training Loss (NLML): -951.4659\n",
      "convergence dfGPdfNN Run 4/10, Epoch 164/1000, Training Loss (NLML): -951.4142\n",
      "convergence dfGPdfNN Run 4/10, Epoch 165/1000, Training Loss (NLML): -951.3424\n",
      "convergence dfGPdfNN Run 4/10, Epoch 166/1000, Training Loss (NLML): -951.4429\n",
      "convergence dfGPdfNN Run 4/10, Epoch 167/1000, Training Loss (NLML): -951.5320\n",
      "convergence dfGPdfNN Run 4/10, Epoch 168/1000, Training Loss (NLML): -951.7421\n",
      "convergence dfGPdfNN Run 4/10, Epoch 169/1000, Training Loss (NLML): -951.9098\n",
      "convergence dfGPdfNN Run 4/10, Epoch 170/1000, Training Loss (NLML): -952.1315\n",
      "convergence dfGPdfNN Run 4/10, Epoch 171/1000, Training Loss (NLML): -952.1433\n",
      "convergence dfGPdfNN Run 4/10, Epoch 172/1000, Training Loss (NLML): -952.2120\n",
      "convergence dfGPdfNN Run 4/10, Epoch 173/1000, Training Loss (NLML): -952.2455\n",
      "convergence dfGPdfNN Run 4/10, Epoch 174/1000, Training Loss (NLML): -951.5808\n",
      "convergence dfGPdfNN Run 4/10, Epoch 175/1000, Training Loss (NLML): -951.5527\n",
      "convergence dfGPdfNN Run 4/10, Epoch 176/1000, Training Loss (NLML): -951.0195\n",
      "convergence dfGPdfNN Run 4/10, Epoch 177/1000, Training Loss (NLML): -951.8140\n",
      "convergence dfGPdfNN Run 4/10, Epoch 178/1000, Training Loss (NLML): -951.9553\n",
      "convergence dfGPdfNN Run 4/10, Epoch 179/1000, Training Loss (NLML): -952.0137\n",
      "convergence dfGPdfNN Run 4/10, Epoch 180/1000, Training Loss (NLML): -952.0450\n",
      "convergence dfGPdfNN Run 4/10, Epoch 181/1000, Training Loss (NLML): -952.2048\n",
      "convergence dfGPdfNN Run 4/10, Epoch 182/1000, Training Loss (NLML): -952.1975\n",
      "convergence dfGPdfNN Run 4/10, Epoch 183/1000, Training Loss (NLML): -952.2488\n",
      "convergence dfGPdfNN Run 4/10, Epoch 184/1000, Training Loss (NLML): -952.1566\n",
      "convergence dfGPdfNN Run 4/10, Epoch 185/1000, Training Loss (NLML): -951.9203\n",
      "convergence dfGPdfNN Run 4/10, Epoch 186/1000, Training Loss (NLML): -952.1909\n",
      "convergence dfGPdfNN Run 4/10, Epoch 187/1000, Training Loss (NLML): -952.2097\n",
      "convergence dfGPdfNN Run 4/10, Epoch 188/1000, Training Loss (NLML): -952.2584\n",
      "convergence dfGPdfNN Run 4/10, Epoch 189/1000, Training Loss (NLML): -952.2081\n",
      "convergence dfGPdfNN Run 4/10, Epoch 190/1000, Training Loss (NLML): -952.1979\n",
      "convergence dfGPdfNN Run 4/10, Epoch 191/1000, Training Loss (NLML): -952.2191\n",
      "convergence dfGPdfNN Run 4/10, Epoch 192/1000, Training Loss (NLML): -952.2570\n",
      "convergence dfGPdfNN Run 4/10, Epoch 193/1000, Training Loss (NLML): -946.7457\n",
      "convergence dfGPdfNN Run 4/10, Epoch 194/1000, Training Loss (NLML): -952.1134\n",
      "convergence dfGPdfNN Run 4/10, Epoch 195/1000, Training Loss (NLML): -952.1053\n",
      "convergence dfGPdfNN Run 4/10, Epoch 196/1000, Training Loss (NLML): -952.2703\n",
      "convergence dfGPdfNN Run 4/10, Epoch 197/1000, Training Loss (NLML): -952.2378\n",
      "convergence dfGPdfNN Run 4/10, Epoch 198/1000, Training Loss (NLML): -952.1964\n",
      "convergence dfGPdfNN Run 4/10, Epoch 199/1000, Training Loss (NLML): -952.2366\n",
      "convergence dfGPdfNN Run 4/10, Epoch 200/1000, Training Loss (NLML): -952.2196\n",
      "convergence dfGPdfNN Run 4/10, Epoch 201/1000, Training Loss (NLML): -952.0801\n",
      "convergence dfGPdfNN Run 4/10, Epoch 202/1000, Training Loss (NLML): -952.1571\n",
      "convergence dfGPdfNN Run 4/10, Epoch 203/1000, Training Loss (NLML): -952.2872\n",
      "convergence dfGPdfNN Run 4/10, Epoch 204/1000, Training Loss (NLML): -952.1252\n",
      "convergence dfGPdfNN Run 4/10, Epoch 205/1000, Training Loss (NLML): -952.3184\n",
      "convergence dfGPdfNN Run 4/10, Epoch 206/1000, Training Loss (NLML): -952.6252\n",
      "convergence dfGPdfNN Run 4/10, Epoch 207/1000, Training Loss (NLML): -952.3732\n",
      "convergence dfGPdfNN Run 4/10, Epoch 208/1000, Training Loss (NLML): -952.4043\n",
      "convergence dfGPdfNN Run 4/10, Epoch 209/1000, Training Loss (NLML): -951.5668\n",
      "convergence dfGPdfNN Run 4/10, Epoch 210/1000, Training Loss (NLML): -952.7744\n",
      "convergence dfGPdfNN Run 4/10, Epoch 211/1000, Training Loss (NLML): -952.4648\n",
      "convergence dfGPdfNN Run 4/10, Epoch 212/1000, Training Loss (NLML): -951.7958\n",
      "convergence dfGPdfNN Run 4/10, Epoch 213/1000, Training Loss (NLML): -952.2146\n",
      "convergence dfGPdfNN Run 4/10, Epoch 214/1000, Training Loss (NLML): -952.3313\n",
      "convergence dfGPdfNN Run 4/10, Epoch 215/1000, Training Loss (NLML): -952.6740\n",
      "convergence dfGPdfNN Run 4/10, Epoch 216/1000, Training Loss (NLML): -952.6096\n",
      "convergence dfGPdfNN Run 4/10, Epoch 217/1000, Training Loss (NLML): -952.4144\n",
      "convergence dfGPdfNN Run 4/10, Epoch 218/1000, Training Loss (NLML): -952.4613\n",
      "convergence dfGPdfNN Run 4/10, Epoch 219/1000, Training Loss (NLML): -952.5693\n",
      "convergence dfGPdfNN Run 4/10, Epoch 220/1000, Training Loss (NLML): -952.6571\n",
      "convergence dfGPdfNN Run 4/10, Epoch 221/1000, Training Loss (NLML): -952.7469\n",
      "convergence dfGPdfNN Run 4/10, Epoch 222/1000, Training Loss (NLML): -952.8623\n",
      "convergence dfGPdfNN Run 4/10, Epoch 223/1000, Training Loss (NLML): -952.9082\n",
      "convergence dfGPdfNN Run 4/10, Epoch 224/1000, Training Loss (NLML): -952.7776\n",
      "convergence dfGPdfNN Run 4/10, Epoch 225/1000, Training Loss (NLML): -952.7860\n",
      "convergence dfGPdfNN Run 4/10, Epoch 226/1000, Training Loss (NLML): -953.1516\n",
      "convergence dfGPdfNN Run 4/10, Epoch 227/1000, Training Loss (NLML): -953.2683\n",
      "convergence dfGPdfNN Run 4/10, Epoch 228/1000, Training Loss (NLML): -952.8292\n",
      "convergence dfGPdfNN Run 4/10, Epoch 229/1000, Training Loss (NLML): -952.1499\n",
      "convergence dfGPdfNN Run 4/10, Epoch 230/1000, Training Loss (NLML): -952.7168\n",
      "convergence dfGPdfNN Run 4/10, Epoch 231/1000, Training Loss (NLML): -953.0321\n",
      "convergence dfGPdfNN Run 4/10, Epoch 232/1000, Training Loss (NLML): -952.5977\n",
      "convergence dfGPdfNN Run 4/10, Epoch 233/1000, Training Loss (NLML): -952.1125\n",
      "convergence dfGPdfNN Run 4/10, Epoch 234/1000, Training Loss (NLML): -951.8710\n",
      "convergence dfGPdfNN Run 4/10, Epoch 235/1000, Training Loss (NLML): -952.6138\n",
      "convergence dfGPdfNN Run 4/10, Epoch 236/1000, Training Loss (NLML): -952.7323\n",
      "convergence dfGPdfNN Run 4/10, Epoch 237/1000, Training Loss (NLML): -952.8232\n",
      "convergence dfGPdfNN Run 4/10, Epoch 238/1000, Training Loss (NLML): -952.4569\n",
      "convergence dfGPdfNN Run 4/10, Epoch 239/1000, Training Loss (NLML): -952.9673\n",
      "convergence dfGPdfNN Run 4/10, Epoch 240/1000, Training Loss (NLML): -952.9924\n",
      "convergence dfGPdfNN Run 4/10, Epoch 241/1000, Training Loss (NLML): -953.0906\n",
      "convergence dfGPdfNN Run 4/10, Epoch 242/1000, Training Loss (NLML): -953.1136\n",
      "convergence dfGPdfNN Run 4/10, Epoch 243/1000, Training Loss (NLML): -953.0245\n",
      "convergence dfGPdfNN Run 4/10, Epoch 244/1000, Training Loss (NLML): -947.2738\n",
      "convergence dfGPdfNN Run 4/10, Epoch 245/1000, Training Loss (NLML): -952.7725\n",
      "convergence dfGPdfNN Run 4/10, Epoch 246/1000, Training Loss (NLML): -952.4006\n",
      "convergence dfGPdfNN Run 4/10, Epoch 247/1000, Training Loss (NLML): -951.9788\n",
      "convergence dfGPdfNN Run 4/10, Epoch 248/1000, Training Loss (NLML): -952.8508\n",
      "convergence dfGPdfNN Run 4/10, Epoch 249/1000, Training Loss (NLML): -952.7777\n",
      "convergence dfGPdfNN Run 4/10, Epoch 250/1000, Training Loss (NLML): -953.1620\n",
      "convergence dfGPdfNN Run 4/10, Epoch 251/1000, Training Loss (NLML): -953.2308\n",
      "convergence dfGPdfNN Run 4/10, Epoch 252/1000, Training Loss (NLML): -953.0485\n",
      "convergence dfGPdfNN Run 4/10, Epoch 253/1000, Training Loss (NLML): -953.0375\n",
      "convergence dfGPdfNN Run 4/10, Epoch 254/1000, Training Loss (NLML): -952.7184\n",
      "convergence dfGPdfNN Run 4/10, Epoch 255/1000, Training Loss (NLML): -951.5707\n",
      "convergence dfGPdfNN Run 4/10, Epoch 256/1000, Training Loss (NLML): -953.6484\n",
      "convergence dfGPdfNN Run 4/10, Epoch 257/1000, Training Loss (NLML): -952.8201\n",
      "convergence dfGPdfNN Run 4/10, Epoch 258/1000, Training Loss (NLML): -952.7410\n",
      "convergence dfGPdfNN Run 4/10, Epoch 259/1000, Training Loss (NLML): -953.1290\n",
      "convergence dfGPdfNN Run 4/10, Epoch 260/1000, Training Loss (NLML): -953.2858\n",
      "convergence dfGPdfNN Run 4/10, Epoch 261/1000, Training Loss (NLML): -953.1300\n",
      "convergence dfGPdfNN Run 4/10, Epoch 262/1000, Training Loss (NLML): -953.0848\n",
      "convergence dfGPdfNN Run 4/10, Epoch 263/1000, Training Loss (NLML): -951.6229\n",
      "convergence dfGPdfNN Run 4/10, Epoch 264/1000, Training Loss (NLML): -951.8977\n",
      "convergence dfGPdfNN Run 4/10, Epoch 265/1000, Training Loss (NLML): -953.0243\n",
      "convergence dfGPdfNN Run 4/10, Epoch 266/1000, Training Loss (NLML): -952.7600\n",
      "convergence dfGPdfNN Run 4/10, Epoch 267/1000, Training Loss (NLML): -952.7074\n",
      "convergence dfGPdfNN Run 4/10, Epoch 268/1000, Training Loss (NLML): -952.8286\n",
      "convergence dfGPdfNN Run 4/10, Epoch 269/1000, Training Loss (NLML): -952.7262\n",
      "convergence dfGPdfNN Run 4/10, Epoch 270/1000, Training Loss (NLML): -952.4758\n",
      "convergence dfGPdfNN Run 4/10, Epoch 271/1000, Training Loss (NLML): -952.1213\n",
      "convergence dfGPdfNN Run 4/10, Epoch 272/1000, Training Loss (NLML): -952.2982\n",
      "convergence dfGPdfNN Run 4/10, Epoch 273/1000, Training Loss (NLML): -952.9602\n",
      "convergence dfGPdfNN Run 4/10, Epoch 274/1000, Training Loss (NLML): -953.1002\n",
      "convergence dfGPdfNN Run 4/10, Epoch 275/1000, Training Loss (NLML): -953.3767\n",
      "convergence dfGPdfNN Run 4/10, Epoch 276/1000, Training Loss (NLML): -953.0706\n",
      "convergence dfGPdfNN Run 4/10, Epoch 277/1000, Training Loss (NLML): -953.1316\n",
      "convergence dfGPdfNN Run 4/10, Epoch 278/1000, Training Loss (NLML): -952.9697\n",
      "convergence dfGPdfNN Run 4/10, Epoch 279/1000, Training Loss (NLML): -953.5162\n",
      "convergence dfGPdfNN Run 4/10, Epoch 280/1000, Training Loss (NLML): -951.6671\n",
      "convergence dfGPdfNN Run 4/10, Epoch 281/1000, Training Loss (NLML): -953.7386\n",
      "convergence dfGPdfNN Run 4/10, Epoch 282/1000, Training Loss (NLML): -954.2715\n",
      "convergence dfGPdfNN Run 4/10, Epoch 283/1000, Training Loss (NLML): -954.0223\n",
      "convergence dfGPdfNN Run 4/10, Epoch 284/1000, Training Loss (NLML): -953.8314\n",
      "convergence dfGPdfNN Run 4/10, Epoch 285/1000, Training Loss (NLML): -953.7545\n",
      "convergence dfGPdfNN Run 4/10, Epoch 286/1000, Training Loss (NLML): -953.8398\n",
      "convergence dfGPdfNN Run 4/10, Epoch 287/1000, Training Loss (NLML): -953.4191\n",
      "convergence dfGPdfNN Run 4/10, Epoch 288/1000, Training Loss (NLML): -954.4579\n",
      "convergence dfGPdfNN Run 4/10, Epoch 289/1000, Training Loss (NLML): -954.3405\n",
      "convergence dfGPdfNN Run 4/10, Epoch 290/1000, Training Loss (NLML): -954.3356\n",
      "convergence dfGPdfNN Run 4/10, Epoch 291/1000, Training Loss (NLML): -954.3555\n",
      "convergence dfGPdfNN Run 4/10, Epoch 292/1000, Training Loss (NLML): -954.4762\n",
      "convergence dfGPdfNN Run 4/10, Epoch 293/1000, Training Loss (NLML): -954.6273\n",
      "convergence dfGPdfNN Run 4/10, Epoch 294/1000, Training Loss (NLML): -954.5557\n",
      "convergence dfGPdfNN Run 4/10, Epoch 295/1000, Training Loss (NLML): -954.5183\n",
      "convergence dfGPdfNN Run 4/10, Epoch 296/1000, Training Loss (NLML): -954.3439\n",
      "convergence dfGPdfNN Run 4/10, Epoch 297/1000, Training Loss (NLML): -954.3405\n",
      "convergence dfGPdfNN Run 4/10, Epoch 298/1000, Training Loss (NLML): -954.4773\n",
      "convergence dfGPdfNN Run 4/10, Epoch 299/1000, Training Loss (NLML): -952.7699\n",
      "convergence dfGPdfNN Run 4/10, Epoch 300/1000, Training Loss (NLML): -954.7856\n",
      "convergence dfGPdfNN Run 4/10, Epoch 301/1000, Training Loss (NLML): -954.8376\n",
      "convergence dfGPdfNN Run 4/10, Epoch 302/1000, Training Loss (NLML): -954.8917\n",
      "convergence dfGPdfNN Run 4/10, Epoch 303/1000, Training Loss (NLML): -954.8773\n",
      "convergence dfGPdfNN Run 4/10, Epoch 304/1000, Training Loss (NLML): -954.9089\n",
      "convergence dfGPdfNN Run 4/10, Epoch 305/1000, Training Loss (NLML): -954.9351\n",
      "convergence dfGPdfNN Run 4/10, Epoch 306/1000, Training Loss (NLML): -954.9645\n",
      "convergence dfGPdfNN Run 4/10, Epoch 307/1000, Training Loss (NLML): -954.9744\n",
      "convergence dfGPdfNN Run 4/10, Epoch 308/1000, Training Loss (NLML): -955.0299\n",
      "convergence dfGPdfNN Run 4/10, Epoch 309/1000, Training Loss (NLML): -955.0558\n",
      "convergence dfGPdfNN Run 4/10, Epoch 310/1000, Training Loss (NLML): -954.9681\n",
      "convergence dfGPdfNN Run 4/10, Epoch 311/1000, Training Loss (NLML): -954.8158\n",
      "convergence dfGPdfNN Run 4/10, Epoch 312/1000, Training Loss (NLML): -954.7711\n",
      "convergence dfGPdfNN Run 4/10, Epoch 313/1000, Training Loss (NLML): -954.5173\n",
      "convergence dfGPdfNN Run 4/10, Epoch 314/1000, Training Loss (NLML): -954.8750\n",
      "convergence dfGPdfNN Run 4/10, Epoch 315/1000, Training Loss (NLML): -954.8656\n",
      "convergence dfGPdfNN Run 4/10, Epoch 316/1000, Training Loss (NLML): -953.4421\n",
      "convergence dfGPdfNN Run 4/10, Epoch 317/1000, Training Loss (NLML): -954.9021\n",
      "convergence dfGPdfNN Run 4/10, Epoch 318/1000, Training Loss (NLML): -954.9197\n",
      "convergence dfGPdfNN Run 4/10, Epoch 319/1000, Training Loss (NLML): -954.9622\n",
      "convergence dfGPdfNN Run 4/10, Epoch 320/1000, Training Loss (NLML): -954.9995\n",
      "convergence dfGPdfNN Run 4/10, Epoch 321/1000, Training Loss (NLML): -955.1750\n",
      "convergence dfGPdfNN Run 4/10, Epoch 322/1000, Training Loss (NLML): -955.3483\n",
      "convergence dfGPdfNN Run 4/10, Epoch 323/1000, Training Loss (NLML): -955.3824\n",
      "convergence dfGPdfNN Run 4/10, Epoch 324/1000, Training Loss (NLML): -955.3456\n",
      "convergence dfGPdfNN Run 4/10, Epoch 325/1000, Training Loss (NLML): -955.2725\n",
      "convergence dfGPdfNN Run 4/10, Epoch 326/1000, Training Loss (NLML): -955.2570\n",
      "convergence dfGPdfNN Run 4/10, Epoch 327/1000, Training Loss (NLML): -955.2849\n",
      "convergence dfGPdfNN Run 4/10, Epoch 328/1000, Training Loss (NLML): -955.3701\n",
      "convergence dfGPdfNN Run 4/10, Epoch 329/1000, Training Loss (NLML): -955.3163\n",
      "convergence dfGPdfNN Run 4/10, Epoch 330/1000, Training Loss (NLML): -955.1903\n",
      "convergence dfGPdfNN Run 4/10, Epoch 331/1000, Training Loss (NLML): -955.1344\n",
      "convergence dfGPdfNN Run 4/10, Epoch 332/1000, Training Loss (NLML): -955.1099\n",
      "convergence dfGPdfNN Run 4/10, Epoch 333/1000, Training Loss (NLML): -955.0918\n",
      "convergence dfGPdfNN Run 4/10, Epoch 334/1000, Training Loss (NLML): -955.1042\n",
      "convergence dfGPdfNN Run 4/10, Epoch 335/1000, Training Loss (NLML): -954.7035\n",
      "convergence dfGPdfNN Run 4/10, Epoch 336/1000, Training Loss (NLML): -955.1139\n",
      "convergence dfGPdfNN Run 4/10, Epoch 337/1000, Training Loss (NLML): -955.1964\n",
      "convergence dfGPdfNN Run 4/10, Epoch 338/1000, Training Loss (NLML): -955.2809\n",
      "convergence dfGPdfNN Run 4/10, Epoch 339/1000, Training Loss (NLML): -955.4644\n",
      "convergence dfGPdfNN Run 4/10, Epoch 340/1000, Training Loss (NLML): -955.6937\n",
      "convergence dfGPdfNN Run 4/10, Epoch 341/1000, Training Loss (NLML): -954.9232\n",
      "convergence dfGPdfNN Run 4/10, Epoch 342/1000, Training Loss (NLML): -955.6179\n",
      "convergence dfGPdfNN Run 4/10, Epoch 343/1000, Training Loss (NLML): -955.4167\n",
      "convergence dfGPdfNN Run 4/10, Epoch 344/1000, Training Loss (NLML): -955.2716\n",
      "convergence dfGPdfNN Run 4/10, Epoch 345/1000, Training Loss (NLML): -955.2958\n",
      "convergence dfGPdfNN Run 4/10, Epoch 346/1000, Training Loss (NLML): -955.3320\n",
      "convergence dfGPdfNN Run 4/10, Epoch 347/1000, Training Loss (NLML): -955.4368\n",
      "convergence dfGPdfNN Run 4/10, Epoch 348/1000, Training Loss (NLML): -955.6138\n",
      "convergence dfGPdfNN Run 4/10, Epoch 349/1000, Training Loss (NLML): -955.7350\n",
      "convergence dfGPdfNN Run 4/10, Epoch 350/1000, Training Loss (NLML): -955.7424\n",
      "convergence dfGPdfNN Run 4/10, Epoch 351/1000, Training Loss (NLML): -955.7955\n",
      "convergence dfGPdfNN Run 4/10, Epoch 352/1000, Training Loss (NLML): -955.8245\n",
      "convergence dfGPdfNN Run 4/10, Epoch 353/1000, Training Loss (NLML): -955.9102\n",
      "convergence dfGPdfNN Run 4/10, Epoch 354/1000, Training Loss (NLML): -955.9357\n",
      "convergence dfGPdfNN Run 4/10, Epoch 355/1000, Training Loss (NLML): -955.8744\n",
      "convergence dfGPdfNN Run 4/10, Epoch 356/1000, Training Loss (NLML): -955.8817\n",
      "convergence dfGPdfNN Run 4/10, Epoch 357/1000, Training Loss (NLML): -955.7910\n",
      "convergence dfGPdfNN Run 4/10, Epoch 358/1000, Training Loss (NLML): -955.7882\n",
      "convergence dfGPdfNN Run 4/10, Epoch 359/1000, Training Loss (NLML): -955.9489\n",
      "convergence dfGPdfNN Run 4/10, Epoch 360/1000, Training Loss (NLML): -955.9752\n",
      "convergence dfGPdfNN Run 4/10, Epoch 361/1000, Training Loss (NLML): -956.0881\n",
      "convergence dfGPdfNN Run 4/10, Epoch 362/1000, Training Loss (NLML): -956.1033\n",
      "convergence dfGPdfNN Run 4/10, Epoch 363/1000, Training Loss (NLML): -956.0940\n",
      "convergence dfGPdfNN Run 4/10, Epoch 364/1000, Training Loss (NLML): -956.1205\n",
      "convergence dfGPdfNN Run 4/10, Epoch 365/1000, Training Loss (NLML): -956.0914\n",
      "convergence dfGPdfNN Run 4/10, Epoch 366/1000, Training Loss (NLML): -956.0283\n",
      "convergence dfGPdfNN Run 4/10, Epoch 367/1000, Training Loss (NLML): -955.8434\n",
      "convergence dfGPdfNN Run 4/10, Epoch 368/1000, Training Loss (NLML): -955.7592\n",
      "convergence dfGPdfNN Run 4/10, Epoch 369/1000, Training Loss (NLML): -955.8059\n",
      "convergence dfGPdfNN Run 4/10, Epoch 370/1000, Training Loss (NLML): -955.8287\n",
      "convergence dfGPdfNN Run 4/10, Epoch 371/1000, Training Loss (NLML): -955.8575\n",
      "convergence dfGPdfNN Run 4/10, Epoch 372/1000, Training Loss (NLML): -955.8717\n",
      "convergence dfGPdfNN Run 4/10, Epoch 373/1000, Training Loss (NLML): -955.8873\n",
      "convergence dfGPdfNN Run 4/10, Epoch 374/1000, Training Loss (NLML): -955.7565\n",
      "convergence dfGPdfNN Run 4/10, Epoch 375/1000, Training Loss (NLML): -955.9451\n",
      "convergence dfGPdfNN Run 4/10, Epoch 376/1000, Training Loss (NLML): -955.4320\n",
      "convergence dfGPdfNN Run 4/10, Epoch 377/1000, Training Loss (NLML): -956.2065\n",
      "convergence dfGPdfNN Run 4/10, Epoch 378/1000, Training Loss (NLML): -956.2996\n",
      "convergence dfGPdfNN Run 4/10, Epoch 379/1000, Training Loss (NLML): -956.2322\n",
      "convergence dfGPdfNN Run 4/10, Epoch 380/1000, Training Loss (NLML): -956.2240\n",
      "convergence dfGPdfNN Run 4/10, Epoch 381/1000, Training Loss (NLML): -956.2632\n",
      "convergence dfGPdfNN Run 4/10, Epoch 382/1000, Training Loss (NLML): -956.1675\n",
      "convergence dfGPdfNN Run 4/10, Epoch 383/1000, Training Loss (NLML): -955.9596\n",
      "convergence dfGPdfNN Run 4/10, Epoch 384/1000, Training Loss (NLML): -955.9292\n",
      "convergence dfGPdfNN Run 4/10, Epoch 385/1000, Training Loss (NLML): -956.0167\n",
      "convergence dfGPdfNN Run 4/10, Epoch 386/1000, Training Loss (NLML): -955.9927\n",
      "convergence dfGPdfNN Run 4/10, Epoch 387/1000, Training Loss (NLML): -955.9004\n",
      "convergence dfGPdfNN Run 4/10, Epoch 388/1000, Training Loss (NLML): -955.8131\n",
      "convergence dfGPdfNN Run 4/10, Epoch 389/1000, Training Loss (NLML): -955.9740\n",
      "convergence dfGPdfNN Run 4/10, Epoch 390/1000, Training Loss (NLML): -955.9154\n",
      "convergence dfGPdfNN Run 4/10, Epoch 391/1000, Training Loss (NLML): -955.9198\n",
      "convergence dfGPdfNN Run 4/10, Epoch 392/1000, Training Loss (NLML): -955.9990\n",
      "convergence dfGPdfNN Run 4/10, Epoch 393/1000, Training Loss (NLML): -956.0006\n",
      "convergence dfGPdfNN Run 4/10, Epoch 394/1000, Training Loss (NLML): -956.0380\n",
      "convergence dfGPdfNN Run 4/10, Epoch 395/1000, Training Loss (NLML): -955.7980\n",
      "convergence dfGPdfNN Run 4/10, Epoch 396/1000, Training Loss (NLML): -956.1912\n",
      "convergence dfGPdfNN Run 4/10, Epoch 397/1000, Training Loss (NLML): -956.1860\n",
      "convergence dfGPdfNN Run 4/10, Epoch 398/1000, Training Loss (NLML): -956.4475\n",
      "convergence dfGPdfNN Run 4/10, Epoch 399/1000, Training Loss (NLML): -956.5521\n",
      "convergence dfGPdfNN Run 4/10, Epoch 400/1000, Training Loss (NLML): -954.8030\n",
      "convergence dfGPdfNN Run 4/10, Epoch 401/1000, Training Loss (NLML): -956.5250\n",
      "convergence dfGPdfNN Run 4/10, Epoch 402/1000, Training Loss (NLML): -956.5098\n",
      "convergence dfGPdfNN Run 4/10, Epoch 403/1000, Training Loss (NLML): -956.4553\n",
      "convergence dfGPdfNN Run 4/10, Epoch 404/1000, Training Loss (NLML): -956.4742\n",
      "convergence dfGPdfNN Run 4/10, Epoch 405/1000, Training Loss (NLML): -956.4760\n",
      "convergence dfGPdfNN Run 4/10, Epoch 406/1000, Training Loss (NLML): -956.5442\n",
      "convergence dfGPdfNN Run 4/10, Epoch 407/1000, Training Loss (NLML): -956.5537\n",
      "convergence dfGPdfNN Run 4/10, Epoch 408/1000, Training Loss (NLML): -956.5073\n",
      "convergence dfGPdfNN Run 4/10, Epoch 409/1000, Training Loss (NLML): -956.4227\n",
      "convergence dfGPdfNN Run 4/10, Epoch 410/1000, Training Loss (NLML): -956.2551\n",
      "convergence dfGPdfNN Run 4/10, Epoch 411/1000, Training Loss (NLML): -955.3494\n",
      "convergence dfGPdfNN Run 4/10, Epoch 412/1000, Training Loss (NLML): -956.2324\n",
      "convergence dfGPdfNN Run 4/10, Epoch 413/1000, Training Loss (NLML): -956.2524\n",
      "convergence dfGPdfNN Run 4/10, Epoch 414/1000, Training Loss (NLML): -956.2985\n",
      "convergence dfGPdfNN Run 4/10, Epoch 415/1000, Training Loss (NLML): -956.3444\n",
      "convergence dfGPdfNN Run 4/10, Epoch 416/1000, Training Loss (NLML): -956.5485\n",
      "convergence dfGPdfNN Run 4/10, Epoch 417/1000, Training Loss (NLML): -956.7697\n",
      "convergence dfGPdfNN Run 4/10, Epoch 418/1000, Training Loss (NLML): -956.8547\n",
      "convergence dfGPdfNN Run 4/10, Epoch 419/1000, Training Loss (NLML): -956.9114\n",
      "convergence dfGPdfNN Run 4/10, Epoch 420/1000, Training Loss (NLML): -956.8579\n",
      "convergence dfGPdfNN Run 4/10, Epoch 421/1000, Training Loss (NLML): -955.0405\n",
      "convergence dfGPdfNN Run 4/10, Epoch 422/1000, Training Loss (NLML): -956.5421\n",
      "convergence dfGPdfNN Run 4/10, Epoch 423/1000, Training Loss (NLML): -956.3947\n",
      "convergence dfGPdfNN Run 4/10, Epoch 424/1000, Training Loss (NLML): -956.4858\n",
      "convergence dfGPdfNN Run 4/10, Epoch 425/1000, Training Loss (NLML): -956.5746\n",
      "convergence dfGPdfNN Run 4/10, Epoch 426/1000, Training Loss (NLML): -956.7133\n",
      "convergence dfGPdfNN Run 4/10, Epoch 427/1000, Training Loss (NLML): -956.8276\n",
      "convergence dfGPdfNN Run 4/10, Epoch 428/1000, Training Loss (NLML): -956.5077\n",
      "convergence dfGPdfNN Run 4/10, Epoch 429/1000, Training Loss (NLML): -956.9580\n",
      "convergence dfGPdfNN Run 4/10, Epoch 430/1000, Training Loss (NLML): -956.9788\n",
      "convergence dfGPdfNN Run 4/10, Epoch 431/1000, Training Loss (NLML): -957.0033\n",
      "convergence dfGPdfNN Run 4/10, Epoch 432/1000, Training Loss (NLML): -957.0281\n",
      "convergence dfGPdfNN Run 4/10, Epoch 433/1000, Training Loss (NLML): -956.8865\n",
      "convergence dfGPdfNN Run 4/10, Epoch 434/1000, Training Loss (NLML): -956.8218\n",
      "convergence dfGPdfNN Run 4/10, Epoch 435/1000, Training Loss (NLML): -956.7699\n",
      "convergence dfGPdfNN Run 4/10, Epoch 436/1000, Training Loss (NLML): -956.7791\n",
      "convergence dfGPdfNN Run 4/10, Epoch 437/1000, Training Loss (NLML): -956.7042\n",
      "convergence dfGPdfNN Run 4/10, Epoch 438/1000, Training Loss (NLML): -956.8842\n",
      "convergence dfGPdfNN Run 4/10, Epoch 439/1000, Training Loss (NLML): -956.9941\n",
      "convergence dfGPdfNN Run 4/10, Epoch 440/1000, Training Loss (NLML): -957.0099\n",
      "convergence dfGPdfNN Run 4/10, Epoch 441/1000, Training Loss (NLML): -957.0895\n",
      "convergence dfGPdfNN Run 4/10, Epoch 442/1000, Training Loss (NLML): -957.1222\n",
      "convergence dfGPdfNN Run 4/10, Epoch 443/1000, Training Loss (NLML): -957.1470\n",
      "convergence dfGPdfNN Run 4/10, Epoch 444/1000, Training Loss (NLML): -957.1866\n",
      "convergence dfGPdfNN Run 4/10, Epoch 445/1000, Training Loss (NLML): -957.1913\n",
      "convergence dfGPdfNN Run 4/10, Epoch 446/1000, Training Loss (NLML): -957.2163\n",
      "convergence dfGPdfNN Run 4/10, Epoch 447/1000, Training Loss (NLML): -957.2236\n",
      "convergence dfGPdfNN Run 4/10, Epoch 448/1000, Training Loss (NLML): -957.1765\n",
      "convergence dfGPdfNN Run 4/10, Epoch 449/1000, Training Loss (NLML): -957.1572\n",
      "convergence dfGPdfNN Run 4/10, Epoch 450/1000, Training Loss (NLML): -957.2313\n",
      "convergence dfGPdfNN Run 4/10, Epoch 451/1000, Training Loss (NLML): -957.2340\n",
      "convergence dfGPdfNN Run 4/10, Epoch 452/1000, Training Loss (NLML): -957.2581\n",
      "convergence dfGPdfNN Run 4/10, Epoch 453/1000, Training Loss (NLML): -957.2747\n",
      "convergence dfGPdfNN Run 4/10, Epoch 454/1000, Training Loss (NLML): -957.3938\n",
      "convergence dfGPdfNN Run 4/10, Epoch 455/1000, Training Loss (NLML): -957.3939\n",
      "convergence dfGPdfNN Run 4/10, Epoch 456/1000, Training Loss (NLML): -957.4017\n",
      "convergence dfGPdfNN Run 4/10, Epoch 457/1000, Training Loss (NLML): -957.4323\n",
      "convergence dfGPdfNN Run 4/10, Epoch 458/1000, Training Loss (NLML): -957.1241\n",
      "convergence dfGPdfNN Run 4/10, Epoch 459/1000, Training Loss (NLML): -957.4124\n",
      "convergence dfGPdfNN Run 4/10, Epoch 460/1000, Training Loss (NLML): -957.4261\n",
      "convergence dfGPdfNN Run 4/10, Epoch 461/1000, Training Loss (NLML): -957.4517\n",
      "convergence dfGPdfNN Run 4/10, Epoch 462/1000, Training Loss (NLML): -957.3260\n",
      "convergence dfGPdfNN Run 4/10, Epoch 463/1000, Training Loss (NLML): -957.3516\n",
      "convergence dfGPdfNN Run 4/10, Epoch 464/1000, Training Loss (NLML): -957.3567\n",
      "convergence dfGPdfNN Run 4/10, Epoch 465/1000, Training Loss (NLML): -957.3624\n",
      "convergence dfGPdfNN Run 4/10, Epoch 466/1000, Training Loss (NLML): -957.4685\n",
      "convergence dfGPdfNN Run 4/10, Epoch 467/1000, Training Loss (NLML): -957.5085\n",
      "convergence dfGPdfNN Run 4/10, Epoch 468/1000, Training Loss (NLML): -957.5242\n",
      "convergence dfGPdfNN Run 4/10, Epoch 469/1000, Training Loss (NLML): -957.5344\n",
      "convergence dfGPdfNN Run 4/10, Epoch 470/1000, Training Loss (NLML): -957.5518\n",
      "convergence dfGPdfNN Run 4/10, Epoch 471/1000, Training Loss (NLML): -957.5735\n",
      "convergence dfGPdfNN Run 4/10, Epoch 472/1000, Training Loss (NLML): -957.5432\n",
      "convergence dfGPdfNN Run 4/10, Epoch 473/1000, Training Loss (NLML): -957.5538\n",
      "convergence dfGPdfNN Run 4/10, Epoch 474/1000, Training Loss (NLML): -957.5023\n",
      "convergence dfGPdfNN Run 4/10, Epoch 475/1000, Training Loss (NLML): -957.5121\n",
      "convergence dfGPdfNN Run 4/10, Epoch 476/1000, Training Loss (NLML): -957.5173\n",
      "convergence dfGPdfNN Run 4/10, Epoch 477/1000, Training Loss (NLML): -957.5529\n",
      "convergence dfGPdfNN Run 4/10, Epoch 478/1000, Training Loss (NLML): -957.5658\n",
      "convergence dfGPdfNN Run 4/10, Epoch 479/1000, Training Loss (NLML): -957.5942\n",
      "convergence dfGPdfNN Run 4/10, Epoch 480/1000, Training Loss (NLML): -957.5880\n",
      "convergence dfGPdfNN Run 4/10, Epoch 481/1000, Training Loss (NLML): -957.6050\n",
      "convergence dfGPdfNN Run 4/10, Epoch 482/1000, Training Loss (NLML): -957.6096\n",
      "convergence dfGPdfNN Run 4/10, Epoch 483/1000, Training Loss (NLML): -957.4865\n",
      "convergence dfGPdfNN Run 4/10, Epoch 484/1000, Training Loss (NLML): -957.4969\n",
      "convergence dfGPdfNN Run 4/10, Epoch 485/1000, Training Loss (NLML): -957.5361\n",
      "convergence dfGPdfNN Run 4/10, Epoch 486/1000, Training Loss (NLML): -957.4370\n",
      "convergence dfGPdfNN Run 4/10, Epoch 487/1000, Training Loss (NLML): -957.4528\n",
      "convergence dfGPdfNN Run 4/10, Epoch 488/1000, Training Loss (NLML): -957.5592\n",
      "convergence dfGPdfNN Run 4/10, Epoch 489/1000, Training Loss (NLML): -957.0073\n",
      "convergence dfGPdfNN Run 4/10, Epoch 490/1000, Training Loss (NLML): -957.6216\n",
      "convergence dfGPdfNN Run 4/10, Epoch 491/1000, Training Loss (NLML): -957.6033\n",
      "convergence dfGPdfNN Run 4/10, Epoch 492/1000, Training Loss (NLML): -956.5020\n",
      "convergence dfGPdfNN Run 4/10, Epoch 493/1000, Training Loss (NLML): -957.5798\n",
      "convergence dfGPdfNN Run 4/10, Epoch 494/1000, Training Loss (NLML): -957.5922\n",
      "convergence dfGPdfNN Run 4/10, Epoch 495/1000, Training Loss (NLML): -957.5935\n",
      "convergence dfGPdfNN Run 4/10, Epoch 496/1000, Training Loss (NLML): -957.5884\n",
      "convergence dfGPdfNN Run 4/10, Epoch 497/1000, Training Loss (NLML): -957.6056\n",
      "convergence dfGPdfNN Run 4/10, Epoch 498/1000, Training Loss (NLML): -957.6454\n",
      "convergence dfGPdfNN Run 4/10, Epoch 499/1000, Training Loss (NLML): -957.6547\n",
      "convergence dfGPdfNN Run 4/10, Epoch 500/1000, Training Loss (NLML): -957.6641\n",
      "convergence dfGPdfNN Run 4/10, Epoch 501/1000, Training Loss (NLML): -957.6765\n",
      "convergence dfGPdfNN Run 4/10, Epoch 502/1000, Training Loss (NLML): -955.7958\n",
      "convergence dfGPdfNN Run 4/10, Epoch 503/1000, Training Loss (NLML): -957.6903\n",
      "convergence dfGPdfNN Run 4/10, Epoch 504/1000, Training Loss (NLML): -957.6759\n",
      "convergence dfGPdfNN Run 4/10, Epoch 505/1000, Training Loss (NLML): -957.6683\n",
      "convergence dfGPdfNN Run 4/10, Epoch 506/1000, Training Loss (NLML): -957.6418\n",
      "convergence dfGPdfNN Run 4/10, Epoch 507/1000, Training Loss (NLML): -957.6561\n",
      "convergence dfGPdfNN Run 4/10, Epoch 508/1000, Training Loss (NLML): -957.6769\n",
      "convergence dfGPdfNN Run 4/10, Epoch 509/1000, Training Loss (NLML): -957.6772\n",
      "convergence dfGPdfNN Run 4/10, Epoch 510/1000, Training Loss (NLML): -957.6633\n",
      "convergence dfGPdfNN Run 4/10, Epoch 511/1000, Training Loss (NLML): -957.6764\n",
      "convergence dfGPdfNN Run 4/10, Epoch 512/1000, Training Loss (NLML): -957.6783\n",
      "convergence dfGPdfNN Run 4/10, Epoch 513/1000, Training Loss (NLML): -957.6738\n",
      "convergence dfGPdfNN Run 4/10, Epoch 514/1000, Training Loss (NLML): -957.6976\n",
      "convergence dfGPdfNN Run 4/10, Epoch 515/1000, Training Loss (NLML): -957.7360\n",
      "convergence dfGPdfNN Run 4/10, Epoch 516/1000, Training Loss (NLML): -957.7717\n",
      "convergence dfGPdfNN Run 4/10, Epoch 517/1000, Training Loss (NLML): -957.7827\n",
      "convergence dfGPdfNN Run 4/10, Epoch 518/1000, Training Loss (NLML): -956.9376\n",
      "convergence dfGPdfNN Run 4/10, Epoch 519/1000, Training Loss (NLML): -957.7957\n",
      "convergence dfGPdfNN Run 4/10, Epoch 520/1000, Training Loss (NLML): -957.8433\n",
      "convergence dfGPdfNN Run 4/10, Epoch 521/1000, Training Loss (NLML): -957.8743\n",
      "convergence dfGPdfNN Run 4/10, Epoch 522/1000, Training Loss (NLML): -957.8878\n",
      "convergence dfGPdfNN Run 4/10, Epoch 523/1000, Training Loss (NLML): -957.8795\n",
      "convergence dfGPdfNN Run 4/10, Epoch 524/1000, Training Loss (NLML): -957.8896\n",
      "convergence dfGPdfNN Run 4/10, Epoch 525/1000, Training Loss (NLML): -957.7823\n",
      "convergence dfGPdfNN Run 4/10, Epoch 526/1000, Training Loss (NLML): -957.7903\n",
      "convergence dfGPdfNN Run 4/10, Epoch 527/1000, Training Loss (NLML): -957.8260\n",
      "convergence dfGPdfNN Run 4/10, Epoch 528/1000, Training Loss (NLML): -957.4467\n",
      "convergence dfGPdfNN Run 4/10, Epoch 529/1000, Training Loss (NLML): -957.8500\n",
      "convergence dfGPdfNN Run 4/10, Epoch 530/1000, Training Loss (NLML): -957.8333\n",
      "convergence dfGPdfNN Run 4/10, Epoch 531/1000, Training Loss (NLML): -957.8491\n",
      "convergence dfGPdfNN Run 4/10, Epoch 532/1000, Training Loss (NLML): -957.8639\n",
      "convergence dfGPdfNN Run 4/10, Epoch 533/1000, Training Loss (NLML): -957.8724\n",
      "convergence dfGPdfNN Run 4/10, Epoch 534/1000, Training Loss (NLML): -957.8818\n",
      "convergence dfGPdfNN Run 4/10, Epoch 535/1000, Training Loss (NLML): -957.8483\n",
      "convergence dfGPdfNN Run 4/10, Epoch 536/1000, Training Loss (NLML): -957.8538\n",
      "convergence dfGPdfNN Run 4/10, Epoch 537/1000, Training Loss (NLML): -957.8617\n",
      "convergence dfGPdfNN Run 4/10, Epoch 538/1000, Training Loss (NLML): -957.8734\n",
      "convergence dfGPdfNN Run 4/10, Epoch 539/1000, Training Loss (NLML): -957.8827\n",
      "convergence dfGPdfNN Run 4/10, Epoch 540/1000, Training Loss (NLML): -957.8822\n",
      "convergence dfGPdfNN Run 4/10, Epoch 541/1000, Training Loss (NLML): -957.9194\n",
      "convergence dfGPdfNN Run 4/10, Epoch 542/1000, Training Loss (NLML): -957.9282\n",
      "convergence dfGPdfNN Run 4/10, Epoch 543/1000, Training Loss (NLML): -957.7931\n",
      "convergence dfGPdfNN Run 4/10, Epoch 544/1000, Training Loss (NLML): -957.9540\n",
      "convergence dfGPdfNN Run 4/10, Epoch 545/1000, Training Loss (NLML): -957.9006\n",
      "convergence dfGPdfNN Run 4/10, Epoch 546/1000, Training Loss (NLML): -957.9102\n",
      "convergence dfGPdfNN Run 4/10, Epoch 547/1000, Training Loss (NLML): -957.8903\n",
      "convergence dfGPdfNN Run 4/10, Epoch 548/1000, Training Loss (NLML): -957.8737\n",
      "convergence dfGPdfNN Run 4/10, Epoch 549/1000, Training Loss (NLML): -957.8883\n",
      "convergence dfGPdfNN Run 4/10, Epoch 550/1000, Training Loss (NLML): -957.8976\n",
      "convergence dfGPdfNN Run 4/10, Epoch 551/1000, Training Loss (NLML): -957.9171\n",
      "convergence dfGPdfNN Run 4/10, Epoch 552/1000, Training Loss (NLML): -957.9266\n",
      "convergence dfGPdfNN Run 4/10, Epoch 553/1000, Training Loss (NLML): -957.9312\n",
      "convergence dfGPdfNN Run 4/10, Epoch 554/1000, Training Loss (NLML): -957.9376\n",
      "convergence dfGPdfNN Run 4/10, Epoch 555/1000, Training Loss (NLML): -958.0276\n",
      "convergence dfGPdfNN Run 4/10, Epoch 556/1000, Training Loss (NLML): -958.0359\n",
      "convergence dfGPdfNN Run 4/10, Epoch 557/1000, Training Loss (NLML): -958.1123\n",
      "convergence dfGPdfNN Run 4/10, Epoch 558/1000, Training Loss (NLML): -958.1295\n",
      "convergence dfGPdfNN Run 4/10, Epoch 559/1000, Training Loss (NLML): -958.1317\n",
      "convergence dfGPdfNN Run 4/10, Epoch 560/1000, Training Loss (NLML): -958.1223\n",
      "convergence dfGPdfNN Run 4/10, Epoch 561/1000, Training Loss (NLML): -958.1390\n",
      "convergence dfGPdfNN Run 4/10, Epoch 562/1000, Training Loss (NLML): -958.1445\n",
      "convergence dfGPdfNN Run 4/10, Epoch 563/1000, Training Loss (NLML): -958.1490\n",
      "convergence dfGPdfNN Run 4/10, Epoch 564/1000, Training Loss (NLML): -958.1558\n",
      "convergence dfGPdfNN Run 4/10, Epoch 565/1000, Training Loss (NLML): -958.1641\n",
      "convergence dfGPdfNN Run 4/10, Epoch 566/1000, Training Loss (NLML): -957.8820\n",
      "convergence dfGPdfNN Run 4/10, Epoch 567/1000, Training Loss (NLML): -958.1819\n",
      "convergence dfGPdfNN Run 4/10, Epoch 568/1000, Training Loss (NLML): -958.1912\n",
      "convergence dfGPdfNN Run 4/10, Epoch 569/1000, Training Loss (NLML): -958.2029\n",
      "convergence dfGPdfNN Run 4/10, Epoch 570/1000, Training Loss (NLML): -958.2111\n",
      "convergence dfGPdfNN Run 4/10, Epoch 571/1000, Training Loss (NLML): -958.1472\n",
      "convergence dfGPdfNN Run 4/10, Epoch 572/1000, Training Loss (NLML): -956.9908\n",
      "convergence dfGPdfNN Run 4/10, Epoch 573/1000, Training Loss (NLML): -958.2325\n",
      "convergence dfGPdfNN Run 4/10, Epoch 574/1000, Training Loss (NLML): -958.2382\n",
      "convergence dfGPdfNN Run 4/10, Epoch 575/1000, Training Loss (NLML): -958.2416\n",
      "convergence dfGPdfNN Run 4/10, Epoch 576/1000, Training Loss (NLML): -958.2141\n",
      "convergence dfGPdfNN Run 4/10, Epoch 577/1000, Training Loss (NLML): -958.1989\n",
      "convergence dfGPdfNN Run 4/10, Epoch 578/1000, Training Loss (NLML): -958.2026\n",
      "convergence dfGPdfNN Run 4/10, Epoch 579/1000, Training Loss (NLML): -958.2222\n",
      "convergence dfGPdfNN Run 4/10, Epoch 580/1000, Training Loss (NLML): -958.2355\n",
      "convergence dfGPdfNN Run 4/10, Epoch 581/1000, Training Loss (NLML): -958.1904\n",
      "convergence dfGPdfNN Run 4/10, Epoch 582/1000, Training Loss (NLML): -958.2504\n",
      "convergence dfGPdfNN Run 4/10, Epoch 583/1000, Training Loss (NLML): -958.2003\n",
      "convergence dfGPdfNN Run 4/10, Epoch 584/1000, Training Loss (NLML): -958.2433\n",
      "convergence dfGPdfNN Run 4/10, Epoch 585/1000, Training Loss (NLML): -958.2635\n",
      "convergence dfGPdfNN Run 4/10, Epoch 586/1000, Training Loss (NLML): -958.2799\n",
      "convergence dfGPdfNN Run 4/10, Epoch 587/1000, Training Loss (NLML): -958.2915\n",
      "convergence dfGPdfNN Run 4/10, Epoch 588/1000, Training Loss (NLML): -958.1525\n",
      "convergence dfGPdfNN Run 4/10, Epoch 589/1000, Training Loss (NLML): -958.2169\n",
      "convergence dfGPdfNN Run 4/10, Epoch 590/1000, Training Loss (NLML): -958.2251\n",
      "convergence dfGPdfNN Run 4/10, Epoch 591/1000, Training Loss (NLML): -958.2126\n",
      "convergence dfGPdfNN Run 4/10, Epoch 592/1000, Training Loss (NLML): -958.2234\n",
      "convergence dfGPdfNN Run 4/10, Epoch 593/1000, Training Loss (NLML): -958.3052\n",
      "convergence dfGPdfNN Run 4/10, Epoch 594/1000, Training Loss (NLML): -958.3623\n",
      "convergence dfGPdfNN Run 4/10, Epoch 595/1000, Training Loss (NLML): -958.3018\n",
      "convergence dfGPdfNN Run 4/10, Epoch 596/1000, Training Loss (NLML): -958.3103\n",
      "convergence dfGPdfNN Run 4/10, Epoch 597/1000, Training Loss (NLML): -958.3130\n",
      "convergence dfGPdfNN Run 4/10, Epoch 598/1000, Training Loss (NLML): -958.3254\n",
      "convergence dfGPdfNN Run 4/10, Epoch 599/1000, Training Loss (NLML): -958.2881\n",
      "convergence dfGPdfNN Run 4/10, Epoch 600/1000, Training Loss (NLML): -958.2987\n",
      "convergence dfGPdfNN Run 4/10, Epoch 601/1000, Training Loss (NLML): -958.3073\n",
      "convergence dfGPdfNN Run 4/10, Epoch 602/1000, Training Loss (NLML): -958.3134\n",
      "convergence dfGPdfNN Run 4/10, Epoch 603/1000, Training Loss (NLML): -958.3264\n",
      "convergence dfGPdfNN Run 4/10, Epoch 604/1000, Training Loss (NLML): -958.3887\n",
      "convergence dfGPdfNN Run 4/10, Epoch 605/1000, Training Loss (NLML): -958.3601\n",
      "convergence dfGPdfNN Run 4/10, Epoch 606/1000, Training Loss (NLML): -958.3463\n",
      "convergence dfGPdfNN Run 4/10, Epoch 607/1000, Training Loss (NLML): -958.3521\n",
      "convergence dfGPdfNN Run 4/10, Epoch 608/1000, Training Loss (NLML): -958.3606\n",
      "convergence dfGPdfNN Run 4/10, Epoch 609/1000, Training Loss (NLML): -958.3649\n",
      "convergence dfGPdfNN Run 4/10, Epoch 610/1000, Training Loss (NLML): -958.3756\n",
      "convergence dfGPdfNN Run 4/10, Epoch 611/1000, Training Loss (NLML): -958.3811\n",
      "convergence dfGPdfNN Run 4/10, Epoch 612/1000, Training Loss (NLML): -958.3885\n",
      "convergence dfGPdfNN Run 4/10, Epoch 613/1000, Training Loss (NLML): -958.3076\n",
      "convergence dfGPdfNN Run 4/10, Epoch 614/1000, Training Loss (NLML): -958.3140\n",
      "convergence dfGPdfNN Run 4/10, Epoch 615/1000, Training Loss (NLML): -958.2538\n",
      "convergence dfGPdfNN Run 4/10, Epoch 616/1000, Training Loss (NLML): -958.2627\n",
      "convergence dfGPdfNN Run 4/10, Epoch 617/1000, Training Loss (NLML): -958.2694\n",
      "convergence dfGPdfNN Run 4/10, Epoch 618/1000, Training Loss (NLML): -958.2799\n",
      "convergence dfGPdfNN Run 4/10, Epoch 619/1000, Training Loss (NLML): -958.3064\n",
      "convergence dfGPdfNN Run 4/10, Epoch 620/1000, Training Loss (NLML): -958.3143\n",
      "convergence dfGPdfNN Run 4/10, Epoch 621/1000, Training Loss (NLML): -958.3218\n",
      "convergence dfGPdfNN Run 4/10, Epoch 622/1000, Training Loss (NLML): -958.3086\n",
      "convergence dfGPdfNN Run 4/10, Epoch 623/1000, Training Loss (NLML): -958.2439\n",
      "convergence dfGPdfNN Run 4/10, Epoch 624/1000, Training Loss (NLML): -958.3218\n",
      "convergence dfGPdfNN Run 4/10, Epoch 625/1000, Training Loss (NLML): -958.3287\n",
      "convergence dfGPdfNN Run 4/10, Epoch 626/1000, Training Loss (NLML): -958.3364\n",
      "convergence dfGPdfNN Run 4/10, Epoch 627/1000, Training Loss (NLML): -958.3445\n",
      "convergence dfGPdfNN Run 4/10, Epoch 628/1000, Training Loss (NLML): -958.3523\n",
      "convergence dfGPdfNN Run 4/10, Epoch 629/1000, Training Loss (NLML): -958.3578\n",
      "convergence dfGPdfNN Run 4/10, Epoch 630/1000, Training Loss (NLML): -958.3860\n",
      "convergence dfGPdfNN Run 4/10, Epoch 631/1000, Training Loss (NLML): -958.3918\n",
      "convergence dfGPdfNN Run 4/10, Epoch 632/1000, Training Loss (NLML): -958.4006\n",
      "convergence dfGPdfNN Run 4/10, Epoch 633/1000, Training Loss (NLML): -958.3865\n",
      "convergence dfGPdfNN Run 4/10, Epoch 634/1000, Training Loss (NLML): -958.3948\n",
      "convergence dfGPdfNN Run 4/10, Epoch 635/1000, Training Loss (NLML): -958.4015\n",
      "convergence dfGPdfNN Run 4/10, Epoch 636/1000, Training Loss (NLML): -958.4071\n",
      "convergence dfGPdfNN Run 4/10, Epoch 637/1000, Training Loss (NLML): -958.4153\n",
      "convergence dfGPdfNN Run 4/10, Epoch 638/1000, Training Loss (NLML): -958.4214\n",
      "convergence dfGPdfNN Run 4/10, Epoch 639/1000, Training Loss (NLML): -958.4487\n",
      "convergence dfGPdfNN Run 4/10, Epoch 640/1000, Training Loss (NLML): -958.4547\n",
      "convergence dfGPdfNN Run 4/10, Epoch 641/1000, Training Loss (NLML): -958.4614\n",
      "convergence dfGPdfNN Run 4/10, Epoch 642/1000, Training Loss (NLML): -958.4475\n",
      "convergence dfGPdfNN Run 4/10, Epoch 643/1000, Training Loss (NLML): -958.4557\n",
      "convergence dfGPdfNN Run 4/10, Epoch 644/1000, Training Loss (NLML): -958.4619\n",
      "convergence dfGPdfNN Run 4/10, Epoch 645/1000, Training Loss (NLML): -958.4695\n",
      "convergence dfGPdfNN Run 4/10, Epoch 646/1000, Training Loss (NLML): -958.4771\n",
      "convergence dfGPdfNN Run 4/10, Epoch 647/1000, Training Loss (NLML): -958.5039\n",
      "convergence dfGPdfNN Run 4/10, Epoch 648/1000, Training Loss (NLML): -958.5099\n",
      "convergence dfGPdfNN Run 4/10, Epoch 649/1000, Training Loss (NLML): -958.5155\n",
      "convergence dfGPdfNN Run 4/10, Epoch 650/1000, Training Loss (NLML): -958.5038\n",
      "convergence dfGPdfNN Run 4/10, Epoch 651/1000, Training Loss (NLML): -958.5098\n",
      "convergence dfGPdfNN Run 4/10, Epoch 652/1000, Training Loss (NLML): -958.5157\n",
      "convergence dfGPdfNN Run 4/10, Epoch 653/1000, Training Loss (NLML): -958.5238\n",
      "convergence dfGPdfNN Run 4/10, Epoch 654/1000, Training Loss (NLML): -958.5487\n",
      "convergence dfGPdfNN Run 4/10, Epoch 655/1000, Training Loss (NLML): -958.5551\n",
      "convergence dfGPdfNN Run 4/10, Epoch 656/1000, Training Loss (NLML): -958.5428\n",
      "convergence dfGPdfNN Run 4/10, Epoch 657/1000, Training Loss (NLML): -958.5481\n",
      "convergence dfGPdfNN Run 4/10, Epoch 658/1000, Training Loss (NLML): -958.5549\n",
      "convergence dfGPdfNN Run 4/10, Epoch 659/1000, Training Loss (NLML): -958.5811\n",
      "convergence dfGPdfNN Run 4/10, Epoch 660/1000, Training Loss (NLML): -958.5685\n",
      "convergence dfGPdfNN Run 4/10, Epoch 661/1000, Training Loss (NLML): -958.5941\n",
      "convergence dfGPdfNN Run 4/10, Epoch 662/1000, Training Loss (NLML): -958.5822\n",
      "convergence dfGPdfNN Run 4/10, Epoch 663/1000, Training Loss (NLML): -958.6062\n",
      "convergence dfGPdfNN Run 4/10, Epoch 664/1000, Training Loss (NLML): -958.5950\n",
      "convergence dfGPdfNN Run 4/10, Epoch 665/1000, Training Loss (NLML): -958.6013\n",
      "convergence dfGPdfNN Run 4/10, Epoch 666/1000, Training Loss (NLML): -958.6266\n",
      "convergence dfGPdfNN Run 4/10, Epoch 667/1000, Training Loss (NLML): -958.6333\n",
      "convergence dfGPdfNN Run 4/10, Epoch 668/1000, Training Loss (NLML): -958.6198\n",
      "convergence dfGPdfNN Run 4/10, Epoch 669/1000, Training Loss (NLML): -958.6263\n",
      "convergence dfGPdfNN Run 4/10, Epoch 670/1000, Training Loss (NLML): -958.6511\n",
      "convergence dfGPdfNN Run 4/10, Epoch 671/1000, Training Loss (NLML): -958.6584\n",
      "convergence dfGPdfNN Run 4/10, Epoch 672/1000, Training Loss (NLML): -958.6638\n",
      "convergence dfGPdfNN Run 4/10, Epoch 673/1000, Training Loss (NLML): -958.6519\n",
      "convergence dfGPdfNN Run 4/10, Epoch 674/1000, Training Loss (NLML): -958.6582\n",
      "convergence dfGPdfNN Run 4/10, Epoch 675/1000, Training Loss (NLML): -958.6626\n",
      "convergence dfGPdfNN Run 4/10, Epoch 676/1000, Training Loss (NLML): -958.6887\n",
      "convergence dfGPdfNN Run 4/10, Epoch 677/1000, Training Loss (NLML): -958.6948\n",
      "convergence dfGPdfNN Run 4/10, Epoch 678/1000, Training Loss (NLML): -958.7001\n",
      "convergence dfGPdfNN Run 4/10, Epoch 679/1000, Training Loss (NLML): -958.7063\n",
      "convergence dfGPdfNN Run 4/10, Epoch 680/1000, Training Loss (NLML): -958.6926\n",
      "convergence dfGPdfNN Run 4/10, Epoch 681/1000, Training Loss (NLML): -958.7103\n",
      "convergence dfGPdfNN Run 4/10, Epoch 682/1000, Training Loss (NLML): -958.7152\n",
      "convergence dfGPdfNN Run 4/10, Epoch 683/1000, Training Loss (NLML): -958.7407\n",
      "convergence dfGPdfNN Run 4/10, Epoch 684/1000, Training Loss (NLML): -958.7375\n",
      "convergence dfGPdfNN Run 4/10, Epoch 685/1000, Training Loss (NLML): -958.7419\n",
      "convergence dfGPdfNN Run 4/10, Epoch 686/1000, Training Loss (NLML): -958.7498\n",
      "convergence dfGPdfNN Run 4/10, Epoch 687/1000, Training Loss (NLML): -958.7358\n",
      "convergence dfGPdfNN Run 4/10, Epoch 688/1000, Training Loss (NLML): -958.7422\n",
      "convergence dfGPdfNN Run 4/10, Epoch 689/1000, Training Loss (NLML): -958.7666\n",
      "convergence dfGPdfNN Run 4/10, Epoch 690/1000, Training Loss (NLML): -958.7729\n",
      "convergence dfGPdfNN Run 4/10, Epoch 691/1000, Training Loss (NLML): -958.7786\n",
      "convergence dfGPdfNN Run 4/10, Epoch 692/1000, Training Loss (NLML): -958.7821\n",
      "convergence dfGPdfNN Run 4/10, Epoch 693/1000, Training Loss (NLML): -958.7881\n",
      "convergence dfGPdfNN Run 4/10, Epoch 694/1000, Training Loss (NLML): -958.7754\n",
      "convergence dfGPdfNN Run 4/10, Epoch 695/1000, Training Loss (NLML): -958.7994\n",
      "convergence dfGPdfNN Run 4/10, Epoch 696/1000, Training Loss (NLML): -958.8059\n",
      "convergence dfGPdfNN Run 4/10, Epoch 697/1000, Training Loss (NLML): -958.8125\n",
      "convergence dfGPdfNN Run 4/10, Epoch 698/1000, Training Loss (NLML): -958.8190\n",
      "convergence dfGPdfNN Run 4/10, Epoch 699/1000, Training Loss (NLML): -958.8245\n",
      "convergence dfGPdfNN Run 4/10, Epoch 700/1000, Training Loss (NLML): -958.8296\n",
      "convergence dfGPdfNN Run 4/10, Epoch 701/1000, Training Loss (NLML): -958.8165\n",
      "convergence dfGPdfNN Run 4/10, Epoch 702/1000, Training Loss (NLML): -958.8407\n",
      "convergence dfGPdfNN Run 4/10, Epoch 703/1000, Training Loss (NLML): -958.8467\n",
      "convergence dfGPdfNN Run 4/10, Epoch 704/1000, Training Loss (NLML): -958.8521\n",
      "convergence dfGPdfNN Run 4/10, Epoch 705/1000, Training Loss (NLML): -958.8588\n",
      "convergence dfGPdfNN Run 4/10, Epoch 706/1000, Training Loss (NLML): -958.8132\n",
      "convergence dfGPdfNN Run 4/10, Epoch 707/1000, Training Loss (NLML): -958.8369\n",
      "convergence dfGPdfNN Run 4/10, Epoch 708/1000, Training Loss (NLML): -958.8741\n",
      "convergence dfGPdfNN Run 4/10, Epoch 709/1000, Training Loss (NLML): -958.8784\n",
      "convergence dfGPdfNN Run 4/10, Epoch 710/1000, Training Loss (NLML): -958.8849\n",
      "convergence dfGPdfNN Run 4/10, Epoch 711/1000, Training Loss (NLML): -958.8608\n",
      "convergence dfGPdfNN Run 4/10, Epoch 712/1000, Training Loss (NLML): -958.8663\n",
      "convergence dfGPdfNN Run 4/10, Epoch 713/1000, Training Loss (NLML): -958.8726\n",
      "convergence dfGPdfNN Run 4/10, Epoch 714/1000, Training Loss (NLML): -958.8772\n",
      "convergence dfGPdfNN Run 4/10, Epoch 715/1000, Training Loss (NLML): -958.8824\n",
      "convergence dfGPdfNN Run 4/10, Epoch 716/1000, Training Loss (NLML): -958.8879\n",
      "convergence dfGPdfNN Run 4/10, Epoch 717/1000, Training Loss (NLML): -958.8939\n",
      "convergence dfGPdfNN Run 4/10, Epoch 718/1000, Training Loss (NLML): -958.8995\n",
      "convergence dfGPdfNN Run 4/10, Epoch 719/1000, Training Loss (NLML): -958.9337\n",
      "convergence dfGPdfNN Run 4/10, Epoch 720/1000, Training Loss (NLML): -957.3345\n",
      "convergence dfGPdfNN Run 4/10, Epoch 721/1000, Training Loss (NLML): -958.9185\n",
      "convergence dfGPdfNN Run 4/10, Epoch 722/1000, Training Loss (NLML): -958.9143\n",
      "convergence dfGPdfNN Run 4/10, Epoch 723/1000, Training Loss (NLML): -958.9077\n",
      "convergence dfGPdfNN Run 4/10, Epoch 724/1000, Training Loss (NLML): -958.9021\n",
      "convergence dfGPdfNN Run 4/10, Epoch 725/1000, Training Loss (NLML): -958.9475\n",
      "convergence dfGPdfNN Run 4/10, Epoch 726/1000, Training Loss (NLML): -958.9424\n",
      "convergence dfGPdfNN Run 4/10, Epoch 727/1000, Training Loss (NLML): -958.9659\n",
      "convergence dfGPdfNN Run 4/10, Epoch 728/1000, Training Loss (NLML): -958.9646\n",
      "convergence dfGPdfNN Run 4/10, Epoch 729/1000, Training Loss (NLML): -958.9692\n",
      "convergence dfGPdfNN Run 4/10, Epoch 730/1000, Training Loss (NLML): -958.9199\n",
      "convergence dfGPdfNN Run 4/10, Epoch 731/1000, Training Loss (NLML): -958.9846\n",
      "convergence dfGPdfNN Run 4/10, Epoch 732/1000, Training Loss (NLML): -959.0466\n",
      "convergence dfGPdfNN Run 4/10, Epoch 733/1000, Training Loss (NLML): -959.0535\n",
      "convergence dfGPdfNN Run 4/10, Epoch 734/1000, Training Loss (NLML): -959.0706\n",
      "convergence dfGPdfNN Run 4/10, Epoch 735/1000, Training Loss (NLML): -959.0687\n",
      "convergence dfGPdfNN Run 4/10, Epoch 736/1000, Training Loss (NLML): -958.9872\n",
      "convergence dfGPdfNN Run 4/10, Epoch 737/1000, Training Loss (NLML): -958.9869\n",
      "convergence dfGPdfNN Run 4/10, Epoch 738/1000, Training Loss (NLML): -959.1877\n",
      "convergence dfGPdfNN Run 4/10, Epoch 739/1000, Training Loss (NLML): -959.1696\n",
      "convergence dfGPdfNN Run 4/10, Epoch 740/1000, Training Loss (NLML): -959.1753\n",
      "convergence dfGPdfNN Run 4/10, Epoch 741/1000, Training Loss (NLML): -959.1809\n",
      "convergence dfGPdfNN Run 4/10, Epoch 742/1000, Training Loss (NLML): -959.1842\n",
      "convergence dfGPdfNN Run 4/10, Epoch 743/1000, Training Loss (NLML): -959.1908\n",
      "convergence dfGPdfNN Run 4/10, Epoch 744/1000, Training Loss (NLML): -959.2355\n",
      "convergence dfGPdfNN Run 4/10, Epoch 745/1000, Training Loss (NLML): -959.2383\n",
      "convergence dfGPdfNN Run 4/10, Epoch 746/1000, Training Loss (NLML): -959.2740\n",
      "convergence dfGPdfNN Run 4/10, Epoch 747/1000, Training Loss (NLML): -959.3022\n",
      "convergence dfGPdfNN Run 4/10, Epoch 748/1000, Training Loss (NLML): -959.3080\n",
      "convergence dfGPdfNN Run 4/10, Epoch 749/1000, Training Loss (NLML): -959.3142\n",
      "convergence dfGPdfNN Run 4/10, Epoch 750/1000, Training Loss (NLML): -959.3184\n",
      "convergence dfGPdfNN Run 4/10, Epoch 751/1000, Training Loss (NLML): -959.3291\n",
      "convergence dfGPdfNN Run 4/10, Epoch 752/1000, Training Loss (NLML): -959.3358\n",
      "convergence dfGPdfNN Run 4/10, Epoch 753/1000, Training Loss (NLML): -959.3416\n",
      "convergence dfGPdfNN Run 4/10, Epoch 754/1000, Training Loss (NLML): -959.3345\n",
      "convergence dfGPdfNN Run 4/10, Epoch 755/1000, Training Loss (NLML): -959.3647\n",
      "convergence dfGPdfNN Run 4/10, Epoch 756/1000, Training Loss (NLML): -959.3927\n",
      "convergence dfGPdfNN Run 4/10, Epoch 757/1000, Training Loss (NLML): -959.3969\n",
      "convergence dfGPdfNN Run 4/10, Epoch 758/1000, Training Loss (NLML): -959.4016\n",
      "convergence dfGPdfNN Run 4/10, Epoch 759/1000, Training Loss (NLML): -959.4066\n",
      "convergence dfGPdfNN Run 4/10, Epoch 760/1000, Training Loss (NLML): -959.4072\n",
      "convergence dfGPdfNN Run 4/10, Epoch 761/1000, Training Loss (NLML): -959.4113\n",
      "convergence dfGPdfNN Run 4/10, Epoch 762/1000, Training Loss (NLML): -959.4161\n",
      "convergence dfGPdfNN Run 4/10, Epoch 763/1000, Training Loss (NLML): -959.4293\n",
      "convergence dfGPdfNN Run 4/10, Epoch 764/1000, Training Loss (NLML): -959.4346\n",
      "convergence dfGPdfNN Run 4/10, Epoch 765/1000, Training Loss (NLML): -959.4275\n",
      "convergence dfGPdfNN Run 4/10, Epoch 766/1000, Training Loss (NLML): -959.4082\n",
      "convergence dfGPdfNN Run 4/10, Epoch 767/1000, Training Loss (NLML): -959.4116\n",
      "convergence dfGPdfNN Run 4/10, Epoch 768/1000, Training Loss (NLML): -959.4174\n",
      "convergence dfGPdfNN Run 4/10, Epoch 769/1000, Training Loss (NLML): -959.4204\n",
      "convergence dfGPdfNN Run 4/10, Epoch 770/1000, Training Loss (NLML): -959.4248\n",
      "convergence dfGPdfNN Run 4/10, Epoch 771/1000, Training Loss (NLML): -959.4290\n",
      "convergence dfGPdfNN Run 4/10, Epoch 772/1000, Training Loss (NLML): -959.4330\n",
      "convergence dfGPdfNN Run 4/10, Epoch 773/1000, Training Loss (NLML): -959.4371\n",
      "convergence dfGPdfNN Run 4/10, Epoch 774/1000, Training Loss (NLML): -959.4622\n",
      "convergence dfGPdfNN Run 4/10, Epoch 775/1000, Training Loss (NLML): -959.4435\n",
      "convergence dfGPdfNN Run 4/10, Epoch 776/1000, Training Loss (NLML): -959.4225\n",
      "convergence dfGPdfNN Run 4/10, Epoch 777/1000, Training Loss (NLML): -959.4275\n",
      "convergence dfGPdfNN Run 4/10, Epoch 778/1000, Training Loss (NLML): -959.4313\n",
      "convergence dfGPdfNN Run 4/10, Epoch 779/1000, Training Loss (NLML): -959.4681\n",
      "convergence dfGPdfNN Run 4/10, Epoch 780/1000, Training Loss (NLML): -959.4739\n",
      "convergence dfGPdfNN Run 4/10, Epoch 781/1000, Training Loss (NLML): -959.4791\n",
      "convergence dfGPdfNN Run 4/10, Epoch 782/1000, Training Loss (NLML): -959.4493\n",
      "convergence dfGPdfNN Run 4/10, Epoch 783/1000, Training Loss (NLML): -959.4282\n",
      "convergence dfGPdfNN Run 4/10, Epoch 784/1000, Training Loss (NLML): -959.4286\n",
      "convergence dfGPdfNN Run 4/10, Epoch 785/1000, Training Loss (NLML): -959.4320\n",
      "convergence dfGPdfNN Run 4/10, Epoch 786/1000, Training Loss (NLML): -959.4351\n",
      "convergence dfGPdfNN Run 4/10, Epoch 787/1000, Training Loss (NLML): -959.4406\n",
      "convergence dfGPdfNN Run 4/10, Epoch 788/1000, Training Loss (NLML): -959.4501\n",
      "convergence dfGPdfNN Run 4/10, Epoch 789/1000, Training Loss (NLML): -959.4164\n",
      "convergence dfGPdfNN Run 4/10, Epoch 790/1000, Training Loss (NLML): -959.4459\n",
      "convergence dfGPdfNN Run 4/10, Epoch 791/1000, Training Loss (NLML): -959.4463\n",
      "convergence dfGPdfNN Run 4/10, Epoch 792/1000, Training Loss (NLML): -959.4528\n",
      "convergence dfGPdfNN Run 4/10, Epoch 793/1000, Training Loss (NLML): -959.4835\n",
      "convergence dfGPdfNN Run 4/10, Epoch 794/1000, Training Loss (NLML): -959.4844\n",
      "convergence dfGPdfNN Run 4/10, Epoch 795/1000, Training Loss (NLML): -959.4905\n",
      "convergence dfGPdfNN Run 4/10, Epoch 796/1000, Training Loss (NLML): -959.4938\n",
      "convergence dfGPdfNN Run 4/10, Epoch 797/1000, Training Loss (NLML): -959.5024\n",
      "convergence dfGPdfNN Run 4/10, Epoch 798/1000, Training Loss (NLML): -959.5187\n",
      "convergence dfGPdfNN Run 4/10, Epoch 799/1000, Training Loss (NLML): -959.5093\n",
      "convergence dfGPdfNN Run 4/10, Epoch 800/1000, Training Loss (NLML): -959.4889\n",
      "convergence dfGPdfNN Run 4/10, Epoch 801/1000, Training Loss (NLML): -959.4973\n",
      "convergence dfGPdfNN Run 4/10, Epoch 802/1000, Training Loss (NLML): -959.4919\n",
      "convergence dfGPdfNN Run 4/10, Epoch 803/1000, Training Loss (NLML): -959.4995\n",
      "convergence dfGPdfNN Run 4/10, Epoch 804/1000, Training Loss (NLML): -959.5020\n",
      "convergence dfGPdfNN Run 4/10, Epoch 805/1000, Training Loss (NLML): -959.5103\n",
      "convergence dfGPdfNN Run 4/10, Epoch 806/1000, Training Loss (NLML): -959.5164\n",
      "convergence dfGPdfNN Run 4/10, Epoch 807/1000, Training Loss (NLML): -959.5216\n",
      "convergence dfGPdfNN Run 4/10, Epoch 808/1000, Training Loss (NLML): -959.5225\n",
      "convergence dfGPdfNN Run 4/10, Epoch 809/1000, Training Loss (NLML): -959.5234\n",
      "convergence dfGPdfNN Run 4/10, Epoch 810/1000, Training Loss (NLML): -959.5481\n",
      "convergence dfGPdfNN Run 4/10, Epoch 811/1000, Training Loss (NLML): -959.5502\n",
      "convergence dfGPdfNN Run 4/10, Epoch 812/1000, Training Loss (NLML): -959.5535\n",
      "convergence dfGPdfNN Run 4/10, Epoch 813/1000, Training Loss (NLML): -959.5579\n",
      "convergence dfGPdfNN Run 4/10, Epoch 814/1000, Training Loss (NLML): -959.5641\n",
      "convergence dfGPdfNN Run 4/10, Epoch 815/1000, Training Loss (NLML): -959.5476\n",
      "convergence dfGPdfNN Run 4/10, Epoch 816/1000, Training Loss (NLML): -959.5552\n",
      "convergence dfGPdfNN Run 4/10, Epoch 817/1000, Training Loss (NLML): -959.5602\n",
      "convergence dfGPdfNN Run 4/10, Epoch 818/1000, Training Loss (NLML): -959.5627\n",
      "convergence dfGPdfNN Run 4/10, Epoch 819/1000, Training Loss (NLML): -959.5665\n",
      "convergence dfGPdfNN Run 4/10, Epoch 820/1000, Training Loss (NLML): -959.5710\n",
      "convergence dfGPdfNN Run 4/10, Epoch 821/1000, Training Loss (NLML): -959.5735\n",
      "convergence dfGPdfNN Run 4/10, Epoch 822/1000, Training Loss (NLML): -959.5790\n",
      "convergence dfGPdfNN Run 4/10, Epoch 823/1000, Training Loss (NLML): -959.5813\n",
      "convergence dfGPdfNN Run 4/10, Epoch 824/1000, Training Loss (NLML): -959.5862\n",
      "convergence dfGPdfNN Run 4/10, Epoch 825/1000, Training Loss (NLML): -959.5916\n",
      "convergence dfGPdfNN Run 4/10, Epoch 826/1000, Training Loss (NLML): -959.5938\n",
      "convergence dfGPdfNN Run 4/10, Epoch 827/1000, Training Loss (NLML): -959.5989\n",
      "convergence dfGPdfNN Run 4/10, Epoch 828/1000, Training Loss (NLML): -959.5934\n",
      "convergence dfGPdfNN Run 4/10, Epoch 829/1000, Training Loss (NLML): -959.5977\n",
      "convergence dfGPdfNN Run 4/10, Epoch 830/1000, Training Loss (NLML): -959.6012\n",
      "convergence dfGPdfNN Run 4/10, Epoch 831/1000, Training Loss (NLML): -959.6028\n",
      "convergence dfGPdfNN Run 4/10, Epoch 832/1000, Training Loss (NLML): -959.6077\n",
      "convergence dfGPdfNN Run 4/10, Epoch 833/1000, Training Loss (NLML): -959.6205\n",
      "convergence dfGPdfNN Run 4/10, Epoch 834/1000, Training Loss (NLML): -959.6228\n",
      "convergence dfGPdfNN Run 4/10, Epoch 835/1000, Training Loss (NLML): -959.6255\n",
      "convergence dfGPdfNN Run 4/10, Epoch 836/1000, Training Loss (NLML): -959.6307\n",
      "convergence dfGPdfNN Run 4/10, Epoch 837/1000, Training Loss (NLML): -959.6328\n",
      "convergence dfGPdfNN Run 4/10, Epoch 838/1000, Training Loss (NLML): -959.6376\n",
      "convergence dfGPdfNN Run 4/10, Epoch 839/1000, Training Loss (NLML): -959.6406\n",
      "convergence dfGPdfNN Run 4/10, Epoch 840/1000, Training Loss (NLML): -959.6438\n",
      "convergence dfGPdfNN Run 4/10, Epoch 841/1000, Training Loss (NLML): -959.6482\n",
      "convergence dfGPdfNN Run 4/10, Epoch 842/1000, Training Loss (NLML): -959.6497\n",
      "convergence dfGPdfNN Run 4/10, Epoch 843/1000, Training Loss (NLML): -959.6544\n",
      "convergence dfGPdfNN Run 4/10, Epoch 844/1000, Training Loss (NLML): -959.6572\n",
      "convergence dfGPdfNN Run 4/10, Epoch 845/1000, Training Loss (NLML): -959.6606\n",
      "convergence dfGPdfNN Run 4/10, Epoch 846/1000, Training Loss (NLML): -959.6632\n",
      "convergence dfGPdfNN Run 4/10, Epoch 847/1000, Training Loss (NLML): -959.6669\n",
      "convergence dfGPdfNN Run 4/10, Epoch 848/1000, Training Loss (NLML): -959.6720\n",
      "convergence dfGPdfNN Run 4/10, Epoch 849/1000, Training Loss (NLML): -959.6749\n",
      "convergence dfGPdfNN Run 4/10, Epoch 850/1000, Training Loss (NLML): -959.6782\n",
      "convergence dfGPdfNN Run 4/10, Epoch 851/1000, Training Loss (NLML): -959.6808\n",
      "convergence dfGPdfNN Run 4/10, Epoch 852/1000, Training Loss (NLML): -959.6859\n",
      "convergence dfGPdfNN Run 4/10, Epoch 853/1000, Training Loss (NLML): -959.6882\n",
      "convergence dfGPdfNN Run 4/10, Epoch 854/1000, Training Loss (NLML): -959.6921\n",
      "convergence dfGPdfNN Run 4/10, Epoch 855/1000, Training Loss (NLML): -959.6947\n",
      "convergence dfGPdfNN Run 4/10, Epoch 856/1000, Training Loss (NLML): -959.6989\n",
      "convergence dfGPdfNN Run 4/10, Epoch 857/1000, Training Loss (NLML): -959.7028\n",
      "convergence dfGPdfNN Run 4/10, Epoch 858/1000, Training Loss (NLML): -959.7043\n",
      "convergence dfGPdfNN Run 4/10, Epoch 859/1000, Training Loss (NLML): -959.7079\n",
      "convergence dfGPdfNN Run 4/10, Epoch 860/1000, Training Loss (NLML): -959.7125\n",
      "convergence dfGPdfNN Run 4/10, Epoch 861/1000, Training Loss (NLML): -959.7151\n",
      "convergence dfGPdfNN Run 4/10, Epoch 862/1000, Training Loss (NLML): -959.7192\n",
      "convergence dfGPdfNN Run 4/10, Epoch 863/1000, Training Loss (NLML): -959.7218\n",
      "convergence dfGPdfNN Run 4/10, Epoch 864/1000, Training Loss (NLML): -959.7251\n",
      "convergence dfGPdfNN Run 4/10, Epoch 865/1000, Training Loss (NLML): -959.7274\n",
      "convergence dfGPdfNN Run 4/10, Epoch 866/1000, Training Loss (NLML): -959.7316\n",
      "convergence dfGPdfNN Run 4/10, Epoch 867/1000, Training Loss (NLML): -959.7344\n",
      "convergence dfGPdfNN Run 4/10, Epoch 868/1000, Training Loss (NLML): -959.7371\n",
      "convergence dfGPdfNN Run 4/10, Epoch 869/1000, Training Loss (NLML): -959.7408\n",
      "convergence dfGPdfNN Run 4/10, Epoch 870/1000, Training Loss (NLML): -959.7554\n",
      "convergence dfGPdfNN Run 4/10, Epoch 871/1000, Training Loss (NLML): -959.7482\n",
      "convergence dfGPdfNN Run 4/10, Epoch 872/1000, Training Loss (NLML): -959.7501\n",
      "convergence dfGPdfNN Run 4/10, Epoch 873/1000, Training Loss (NLML): -959.7535\n",
      "convergence dfGPdfNN Run 4/10, Epoch 874/1000, Training Loss (NLML): -959.7562\n",
      "convergence dfGPdfNN Run 4/10, Epoch 875/1000, Training Loss (NLML): -959.7616\n",
      "convergence dfGPdfNN Run 4/10, Epoch 876/1000, Training Loss (NLML): -959.7653\n",
      "convergence dfGPdfNN Run 4/10, Epoch 877/1000, Training Loss (NLML): -959.7670\n",
      "convergence dfGPdfNN Run 4/10, Epoch 878/1000, Training Loss (NLML): -959.7699\n",
      "convergence dfGPdfNN Run 4/10, Epoch 879/1000, Training Loss (NLML): -959.7729\n",
      "convergence dfGPdfNN Run 4/10, Epoch 880/1000, Training Loss (NLML): -959.7766\n",
      "convergence dfGPdfNN Run 4/10, Epoch 881/1000, Training Loss (NLML): -959.7786\n",
      "convergence dfGPdfNN Run 4/10, Epoch 882/1000, Training Loss (NLML): -959.7825\n",
      "convergence dfGPdfNN Run 4/10, Epoch 883/1000, Training Loss (NLML): -959.7852\n",
      "convergence dfGPdfNN Run 4/10, Epoch 884/1000, Training Loss (NLML): -959.7891\n",
      "convergence dfGPdfNN Run 4/10, Epoch 885/1000, Training Loss (NLML): -959.7914\n",
      "convergence dfGPdfNN Run 4/10, Epoch 886/1000, Training Loss (NLML): -959.7948\n",
      "convergence dfGPdfNN Run 4/10, Epoch 887/1000, Training Loss (NLML): -959.7985\n",
      "convergence dfGPdfNN Run 4/10, Epoch 888/1000, Training Loss (NLML): -959.8020\n",
      "convergence dfGPdfNN Run 4/10, Epoch 889/1000, Training Loss (NLML): -959.8149\n",
      "convergence dfGPdfNN Run 4/10, Epoch 890/1000, Training Loss (NLML): -959.8087\n",
      "convergence dfGPdfNN Run 4/10, Epoch 891/1000, Training Loss (NLML): -959.8123\n",
      "convergence dfGPdfNN Run 4/10, Epoch 892/1000, Training Loss (NLML): -959.8147\n",
      "convergence dfGPdfNN Run 4/10, Epoch 893/1000, Training Loss (NLML): -959.8165\n",
      "convergence dfGPdfNN Run 4/10, Epoch 894/1000, Training Loss (NLML): -959.8198\n",
      "convergence dfGPdfNN Run 4/10, Epoch 895/1000, Training Loss (NLML): -959.8220\n",
      "convergence dfGPdfNN Run 4/10, Epoch 896/1000, Training Loss (NLML): -959.8251\n",
      "convergence dfGPdfNN Run 4/10, Epoch 897/1000, Training Loss (NLML): -959.8286\n",
      "convergence dfGPdfNN Run 4/10, Epoch 898/1000, Training Loss (NLML): -959.8414\n",
      "convergence dfGPdfNN Run 4/10, Epoch 899/1000, Training Loss (NLML): -959.8461\n",
      "convergence dfGPdfNN Run 4/10, Epoch 900/1000, Training Loss (NLML): -959.8389\n",
      "convergence dfGPdfNN Run 4/10, Epoch 901/1000, Training Loss (NLML): -959.8409\n",
      "convergence dfGPdfNN Run 4/10, Epoch 902/1000, Training Loss (NLML): -959.8448\n",
      "convergence dfGPdfNN Run 4/10, Epoch 903/1000, Training Loss (NLML): -959.8481\n",
      "convergence dfGPdfNN Run 4/10, Epoch 904/1000, Training Loss (NLML): -959.8499\n",
      "convergence dfGPdfNN Run 4/10, Epoch 905/1000, Training Loss (NLML): -959.8544\n",
      "convergence dfGPdfNN Run 4/10, Epoch 906/1000, Training Loss (NLML): -959.8563\n",
      "convergence dfGPdfNN Run 4/10, Epoch 907/1000, Training Loss (NLML): -959.8684\n",
      "convergence dfGPdfNN Run 4/10, Epoch 908/1000, Training Loss (NLML): -959.8718\n",
      "convergence dfGPdfNN Run 4/10, Epoch 909/1000, Training Loss (NLML): -959.8650\n",
      "convergence dfGPdfNN Run 4/10, Epoch 910/1000, Training Loss (NLML): -959.8671\n",
      "convergence dfGPdfNN Run 4/10, Epoch 911/1000, Training Loss (NLML): -959.8699\n",
      "convergence dfGPdfNN Run 4/10, Epoch 912/1000, Training Loss (NLML): -959.8746\n",
      "convergence dfGPdfNN Run 4/10, Epoch 913/1000, Training Loss (NLML): -959.8779\n",
      "convergence dfGPdfNN Run 4/10, Epoch 914/1000, Training Loss (NLML): -959.8914\n",
      "convergence dfGPdfNN Run 4/10, Epoch 915/1000, Training Loss (NLML): -959.8837\n",
      "convergence dfGPdfNN Run 4/10, Epoch 916/1000, Training Loss (NLML): -959.8860\n",
      "convergence dfGPdfNN Run 4/10, Epoch 917/1000, Training Loss (NLML): -959.9000\n",
      "convergence dfGPdfNN Run 4/10, Epoch 918/1000, Training Loss (NLML): -959.8928\n",
      "convergence dfGPdfNN Run 4/10, Epoch 919/1000, Training Loss (NLML): -959.8955\n",
      "convergence dfGPdfNN Run 4/10, Epoch 920/1000, Training Loss (NLML): -959.8972\n",
      "convergence dfGPdfNN Run 4/10, Epoch 921/1000, Training Loss (NLML): -959.9119\n",
      "convergence dfGPdfNN Run 4/10, Epoch 922/1000, Training Loss (NLML): -959.9042\n",
      "convergence dfGPdfNN Run 4/10, Epoch 923/1000, Training Loss (NLML): -959.9058\n",
      "convergence dfGPdfNN Run 4/10, Epoch 924/1000, Training Loss (NLML): -959.9200\n",
      "convergence dfGPdfNN Run 4/10, Epoch 925/1000, Training Loss (NLML): -959.9125\n",
      "convergence dfGPdfNN Run 4/10, Epoch 926/1000, Training Loss (NLML): -959.9147\n",
      "convergence dfGPdfNN Run 4/10, Epoch 927/1000, Training Loss (NLML): -959.9279\n",
      "convergence dfGPdfNN Run 4/10, Epoch 928/1000, Training Loss (NLML): -959.9196\n",
      "convergence dfGPdfNN Run 4/10, Epoch 929/1000, Training Loss (NLML): -959.9244\n",
      "convergence dfGPdfNN Run 4/10, Epoch 930/1000, Training Loss (NLML): -959.9373\n",
      "convergence dfGPdfNN Run 4/10, Epoch 931/1000, Training Loss (NLML): -959.9298\n",
      "convergence dfGPdfNN Run 4/10, Epoch 932/1000, Training Loss (NLML): -959.9314\n",
      "convergence dfGPdfNN Run 4/10, Epoch 933/1000, Training Loss (NLML): -959.9457\n",
      "convergence dfGPdfNN Run 4/10, Epoch 934/1000, Training Loss (NLML): -959.9478\n",
      "convergence dfGPdfNN Run 4/10, Epoch 935/1000, Training Loss (NLML): -959.9397\n",
      "convergence dfGPdfNN Run 4/10, Epoch 936/1000, Training Loss (NLML): -959.9421\n",
      "convergence dfGPdfNN Run 4/10, Epoch 937/1000, Training Loss (NLML): -959.9478\n",
      "convergence dfGPdfNN Run 4/10, Epoch 938/1000, Training Loss (NLML): -959.9596\n",
      "convergence dfGPdfNN Run 4/10, Epoch 939/1000, Training Loss (NLML): -959.9630\n",
      "convergence dfGPdfNN Run 4/10, Epoch 940/1000, Training Loss (NLML): -959.9546\n",
      "convergence dfGPdfNN Run 4/10, Epoch 941/1000, Training Loss (NLML): -959.9570\n",
      "convergence dfGPdfNN Run 4/10, Epoch 942/1000, Training Loss (NLML): -959.9607\n",
      "convergence dfGPdfNN Run 4/10, Epoch 943/1000, Training Loss (NLML): -959.9747\n",
      "convergence dfGPdfNN Run 4/10, Epoch 944/1000, Training Loss (NLML): -959.9763\n",
      "convergence dfGPdfNN Run 4/10, Epoch 945/1000, Training Loss (NLML): -959.9791\n",
      "convergence dfGPdfNN Run 4/10, Epoch 946/1000, Training Loss (NLML): -959.9705\n",
      "convergence dfGPdfNN Run 4/10, Epoch 947/1000, Training Loss (NLML): -959.9738\n",
      "convergence dfGPdfNN Run 4/10, Epoch 948/1000, Training Loss (NLML): -959.9774\n",
      "convergence dfGPdfNN Run 4/10, Epoch 949/1000, Training Loss (NLML): -959.9906\n",
      "convergence dfGPdfNN Run 4/10, Epoch 950/1000, Training Loss (NLML): -959.9929\n",
      "convergence dfGPdfNN Run 4/10, Epoch 951/1000, Training Loss (NLML): -959.9943\n",
      "convergence dfGPdfNN Run 4/10, Epoch 952/1000, Training Loss (NLML): -959.9896\n",
      "convergence dfGPdfNN Run 4/10, Epoch 953/1000, Training Loss (NLML): -959.9915\n",
      "convergence dfGPdfNN Run 4/10, Epoch 954/1000, Training Loss (NLML): -959.9949\n",
      "convergence dfGPdfNN Run 4/10, Epoch 955/1000, Training Loss (NLML): -960.0057\n",
      "convergence dfGPdfNN Run 4/10, Epoch 956/1000, Training Loss (NLML): -960.0092\n",
      "convergence dfGPdfNN Run 4/10, Epoch 957/1000, Training Loss (NLML): -960.0127\n",
      "convergence dfGPdfNN Run 4/10, Epoch 958/1000, Training Loss (NLML): -960.0142\n",
      "convergence dfGPdfNN Run 4/10, Epoch 959/1000, Training Loss (NLML): -960.0071\n",
      "convergence dfGPdfNN Run 4/10, Epoch 960/1000, Training Loss (NLML): -960.0098\n",
      "convergence dfGPdfNN Run 4/10, Epoch 961/1000, Training Loss (NLML): -960.0132\n",
      "convergence dfGPdfNN Run 4/10, Epoch 962/1000, Training Loss (NLML): -960.0260\n",
      "convergence dfGPdfNN Run 4/10, Epoch 963/1000, Training Loss (NLML): -960.0270\n",
      "convergence dfGPdfNN Run 4/10, Epoch 964/1000, Training Loss (NLML): -960.0311\n",
      "convergence dfGPdfNN Run 4/10, Epoch 965/1000, Training Loss (NLML): -960.0341\n",
      "convergence dfGPdfNN Run 4/10, Epoch 966/1000, Training Loss (NLML): -960.0238\n",
      "convergence dfGPdfNN Run 4/10, Epoch 967/1000, Training Loss (NLML): -960.0299\n",
      "convergence dfGPdfNN Run 4/10, Epoch 968/1000, Training Loss (NLML): -960.0426\n",
      "convergence dfGPdfNN Run 4/10, Epoch 969/1000, Training Loss (NLML): -960.0436\n",
      "convergence dfGPdfNN Run 4/10, Epoch 970/1000, Training Loss (NLML): -960.0466\n",
      "convergence dfGPdfNN Run 4/10, Epoch 971/1000, Training Loss (NLML): -960.0392\n",
      "convergence dfGPdfNN Run 4/10, Epoch 972/1000, Training Loss (NLML): -960.0431\n",
      "convergence dfGPdfNN Run 4/10, Epoch 973/1000, Training Loss (NLML): -960.0548\n",
      "convergence dfGPdfNN Run 4/10, Epoch 974/1000, Training Loss (NLML): -960.0582\n",
      "convergence dfGPdfNN Run 4/10, Epoch 975/1000, Training Loss (NLML): -960.0604\n",
      "convergence dfGPdfNN Run 4/10, Epoch 976/1000, Training Loss (NLML): -960.0619\n",
      "convergence dfGPdfNN Run 4/10, Epoch 977/1000, Training Loss (NLML): -960.0552\n",
      "convergence dfGPdfNN Run 4/10, Epoch 978/1000, Training Loss (NLML): -960.0576\n",
      "convergence dfGPdfNN Run 4/10, Epoch 979/1000, Training Loss (NLML): -960.0717\n",
      "convergence dfGPdfNN Run 4/10, Epoch 980/1000, Training Loss (NLML): -960.0717\n",
      "convergence dfGPdfNN Run 4/10, Epoch 981/1000, Training Loss (NLML): -960.0747\n",
      "convergence dfGPdfNN Run 4/10, Epoch 982/1000, Training Loss (NLML): -960.0780\n",
      "convergence dfGPdfNN Run 4/10, Epoch 983/1000, Training Loss (NLML): -960.0797\n",
      "convergence dfGPdfNN Run 4/10, Epoch 984/1000, Training Loss (NLML): -960.0739\n",
      "convergence dfGPdfNN Run 4/10, Epoch 985/1000, Training Loss (NLML): -960.0764\n",
      "convergence dfGPdfNN Run 4/10, Epoch 986/1000, Training Loss (NLML): -960.0898\n",
      "convergence dfGPdfNN Run 4/10, Epoch 987/1000, Training Loss (NLML): -960.0898\n",
      "convergence dfGPdfNN Run 4/10, Epoch 988/1000, Training Loss (NLML): -960.0936\n",
      "convergence dfGPdfNN Run 4/10, Epoch 989/1000, Training Loss (NLML): -960.0945\n",
      "convergence dfGPdfNN Run 4/10, Epoch 990/1000, Training Loss (NLML): -960.0996\n",
      "convergence dfGPdfNN Run 4/10, Epoch 991/1000, Training Loss (NLML): -960.1011\n",
      "convergence dfGPdfNN Run 4/10, Epoch 992/1000, Training Loss (NLML): -960.0934\n",
      "convergence dfGPdfNN Run 4/10, Epoch 993/1000, Training Loss (NLML): -960.0957\n",
      "convergence dfGPdfNN Run 4/10, Epoch 994/1000, Training Loss (NLML): -960.1088\n",
      "convergence dfGPdfNN Run 4/10, Epoch 995/1000, Training Loss (NLML): -960.1105\n",
      "convergence dfGPdfNN Run 4/10, Epoch 996/1000, Training Loss (NLML): -960.1128\n",
      "convergence dfGPdfNN Run 4/10, Epoch 997/1000, Training Loss (NLML): -960.1179\n",
      "convergence dfGPdfNN Run 4/10, Epoch 998/1000, Training Loss (NLML): -960.1199\n",
      "convergence dfGPdfNN Run 4/10, Epoch 999/1000, Training Loss (NLML): -960.1221\n",
      "convergence dfGPdfNN Run 4/10, Epoch 1000/1000, Training Loss (NLML): -960.1249\n",
      "\n",
      "--- Training Run 5/10 ---\n",
      "\n",
      "Start Training\n",
      "convergence dfGPdfNN Run 5/10, Epoch 1/1000, Training Loss (NLML): -896.1194\n",
      "convergence dfGPdfNN Run 5/10, Epoch 2/1000, Training Loss (NLML): -907.4653\n",
      "convergence dfGPdfNN Run 5/10, Epoch 3/1000, Training Loss (NLML): -910.5035\n",
      "convergence dfGPdfNN Run 5/10, Epoch 4/1000, Training Loss (NLML): -910.7133\n",
      "convergence dfGPdfNN Run 5/10, Epoch 5/1000, Training Loss (NLML): -912.4310\n",
      "convergence dfGPdfNN Run 5/10, Epoch 6/1000, Training Loss (NLML): -913.9253\n",
      "convergence dfGPdfNN Run 5/10, Epoch 7/1000, Training Loss (NLML): -914.5103\n",
      "convergence dfGPdfNN Run 5/10, Epoch 8/1000, Training Loss (NLML): -916.2389\n",
      "convergence dfGPdfNN Run 5/10, Epoch 9/1000, Training Loss (NLML): -918.1936\n",
      "convergence dfGPdfNN Run 5/10, Epoch 10/1000, Training Loss (NLML): -919.6748\n",
      "convergence dfGPdfNN Run 5/10, Epoch 11/1000, Training Loss (NLML): -921.0027\n",
      "convergence dfGPdfNN Run 5/10, Epoch 12/1000, Training Loss (NLML): -922.1072\n",
      "convergence dfGPdfNN Run 5/10, Epoch 13/1000, Training Loss (NLML): -923.0941\n",
      "convergence dfGPdfNN Run 5/10, Epoch 14/1000, Training Loss (NLML): -924.0535\n",
      "convergence dfGPdfNN Run 5/10, Epoch 15/1000, Training Loss (NLML): -925.0524\n",
      "convergence dfGPdfNN Run 5/10, Epoch 16/1000, Training Loss (NLML): -926.0107\n",
      "convergence dfGPdfNN Run 5/10, Epoch 17/1000, Training Loss (NLML): -926.9684\n",
      "convergence dfGPdfNN Run 5/10, Epoch 18/1000, Training Loss (NLML): -927.8149\n",
      "convergence dfGPdfNN Run 5/10, Epoch 19/1000, Training Loss (NLML): -928.3574\n",
      "convergence dfGPdfNN Run 5/10, Epoch 20/1000, Training Loss (NLML): -929.5773\n",
      "convergence dfGPdfNN Run 5/10, Epoch 21/1000, Training Loss (NLML): -930.4033\n",
      "convergence dfGPdfNN Run 5/10, Epoch 22/1000, Training Loss (NLML): -931.2063\n",
      "convergence dfGPdfNN Run 5/10, Epoch 23/1000, Training Loss (NLML): -931.9612\n",
      "convergence dfGPdfNN Run 5/10, Epoch 24/1000, Training Loss (NLML): -932.6714\n",
      "convergence dfGPdfNN Run 5/10, Epoch 25/1000, Training Loss (NLML): -933.3492\n",
      "convergence dfGPdfNN Run 5/10, Epoch 26/1000, Training Loss (NLML): -934.0059\n",
      "convergence dfGPdfNN Run 5/10, Epoch 27/1000, Training Loss (NLML): -934.6360\n",
      "convergence dfGPdfNN Run 5/10, Epoch 28/1000, Training Loss (NLML): -935.2482\n",
      "convergence dfGPdfNN Run 5/10, Epoch 29/1000, Training Loss (NLML): -935.8285\n",
      "convergence dfGPdfNN Run 5/10, Epoch 30/1000, Training Loss (NLML): -936.3923\n",
      "convergence dfGPdfNN Run 5/10, Epoch 31/1000, Training Loss (NLML): -936.9379\n",
      "convergence dfGPdfNN Run 5/10, Epoch 32/1000, Training Loss (NLML): -937.4814\n",
      "convergence dfGPdfNN Run 5/10, Epoch 33/1000, Training Loss (NLML): -937.7180\n",
      "convergence dfGPdfNN Run 5/10, Epoch 34/1000, Training Loss (NLML): -938.2628\n",
      "convergence dfGPdfNN Run 5/10, Epoch 35/1000, Training Loss (NLML): -939.0376\n",
      "convergence dfGPdfNN Run 5/10, Epoch 36/1000, Training Loss (NLML): -939.5325\n",
      "convergence dfGPdfNN Run 5/10, Epoch 37/1000, Training Loss (NLML): -940.0157\n",
      "convergence dfGPdfNN Run 5/10, Epoch 38/1000, Training Loss (NLML): -940.4784\n",
      "convergence dfGPdfNN Run 5/10, Epoch 39/1000, Training Loss (NLML): -940.9219\n",
      "convergence dfGPdfNN Run 5/10, Epoch 40/1000, Training Loss (NLML): -941.3639\n",
      "convergence dfGPdfNN Run 5/10, Epoch 41/1000, Training Loss (NLML): -941.7263\n",
      "convergence dfGPdfNN Run 5/10, Epoch 42/1000, Training Loss (NLML): -942.1208\n",
      "convergence dfGPdfNN Run 5/10, Epoch 43/1000, Training Loss (NLML): -942.5653\n",
      "convergence dfGPdfNN Run 5/10, Epoch 44/1000, Training Loss (NLML): -942.9247\n",
      "convergence dfGPdfNN Run 5/10, Epoch 45/1000, Training Loss (NLML): -943.2994\n",
      "convergence dfGPdfNN Run 5/10, Epoch 46/1000, Training Loss (NLML): -943.6251\n",
      "convergence dfGPdfNN Run 5/10, Epoch 47/1000, Training Loss (NLML): -943.9504\n",
      "convergence dfGPdfNN Run 5/10, Epoch 48/1000, Training Loss (NLML): -944.3472\n",
      "convergence dfGPdfNN Run 5/10, Epoch 49/1000, Training Loss (NLML): -944.6833\n",
      "convergence dfGPdfNN Run 5/10, Epoch 50/1000, Training Loss (NLML): -945.0117\n",
      "convergence dfGPdfNN Run 5/10, Epoch 51/1000, Training Loss (NLML): -945.3419\n",
      "convergence dfGPdfNN Run 5/10, Epoch 52/1000, Training Loss (NLML): -945.6523\n",
      "convergence dfGPdfNN Run 5/10, Epoch 53/1000, Training Loss (NLML): -945.9524\n",
      "convergence dfGPdfNN Run 5/10, Epoch 54/1000, Training Loss (NLML): -944.2607\n",
      "convergence dfGPdfNN Run 5/10, Epoch 55/1000, Training Loss (NLML): -946.5247\n",
      "convergence dfGPdfNN Run 5/10, Epoch 56/1000, Training Loss (NLML): -946.7891\n",
      "convergence dfGPdfNN Run 5/10, Epoch 57/1000, Training Loss (NLML): -947.0499\n",
      "convergence dfGPdfNN Run 5/10, Epoch 58/1000, Training Loss (NLML): -947.2921\n",
      "convergence dfGPdfNN Run 5/10, Epoch 59/1000, Training Loss (NLML): -947.5155\n",
      "convergence dfGPdfNN Run 5/10, Epoch 60/1000, Training Loss (NLML): -947.7739\n",
      "convergence dfGPdfNN Run 5/10, Epoch 61/1000, Training Loss (NLML): -948.0110\n",
      "convergence dfGPdfNN Run 5/10, Epoch 62/1000, Training Loss (NLML): -948.2939\n",
      "convergence dfGPdfNN Run 5/10, Epoch 63/1000, Training Loss (NLML): -948.5210\n",
      "convergence dfGPdfNN Run 5/10, Epoch 64/1000, Training Loss (NLML): -948.6049\n",
      "convergence dfGPdfNN Run 5/10, Epoch 65/1000, Training Loss (NLML): -948.8469\n",
      "convergence dfGPdfNN Run 5/10, Epoch 66/1000, Training Loss (NLML): -949.1223\n",
      "convergence dfGPdfNN Run 5/10, Epoch 67/1000, Training Loss (NLML): -949.3472\n",
      "convergence dfGPdfNN Run 5/10, Epoch 68/1000, Training Loss (NLML): -949.5100\n",
      "convergence dfGPdfNN Run 5/10, Epoch 69/1000, Training Loss (NLML): -949.7361\n",
      "convergence dfGPdfNN Run 5/10, Epoch 70/1000, Training Loss (NLML): -949.9701\n",
      "convergence dfGPdfNN Run 5/10, Epoch 71/1000, Training Loss (NLML): -950.1764\n",
      "convergence dfGPdfNN Run 5/10, Epoch 72/1000, Training Loss (NLML): -950.4067\n",
      "convergence dfGPdfNN Run 5/10, Epoch 73/1000, Training Loss (NLML): -950.6279\n",
      "convergence dfGPdfNN Run 5/10, Epoch 74/1000, Training Loss (NLML): -950.8643\n",
      "convergence dfGPdfNN Run 5/10, Epoch 75/1000, Training Loss (NLML): -951.0792\n",
      "convergence dfGPdfNN Run 5/10, Epoch 76/1000, Training Loss (NLML): -951.3206\n",
      "convergence dfGPdfNN Run 5/10, Epoch 77/1000, Training Loss (NLML): -951.4382\n",
      "convergence dfGPdfNN Run 5/10, Epoch 78/1000, Training Loss (NLML): -951.5848\n",
      "convergence dfGPdfNN Run 5/10, Epoch 79/1000, Training Loss (NLML): -951.8333\n",
      "convergence dfGPdfNN Run 5/10, Epoch 80/1000, Training Loss (NLML): -952.0480\n",
      "convergence dfGPdfNN Run 5/10, Epoch 81/1000, Training Loss (NLML): -952.2310\n",
      "convergence dfGPdfNN Run 5/10, Epoch 82/1000, Training Loss (NLML): -952.3269\n",
      "convergence dfGPdfNN Run 5/10, Epoch 83/1000, Training Loss (NLML): -952.4736\n",
      "convergence dfGPdfNN Run 5/10, Epoch 84/1000, Training Loss (NLML): -952.5975\n",
      "convergence dfGPdfNN Run 5/10, Epoch 85/1000, Training Loss (NLML): -952.8486\n",
      "convergence dfGPdfNN Run 5/10, Epoch 86/1000, Training Loss (NLML): -953.0935\n",
      "convergence dfGPdfNN Run 5/10, Epoch 87/1000, Training Loss (NLML): -953.2643\n",
      "convergence dfGPdfNN Run 5/10, Epoch 88/1000, Training Loss (NLML): -953.3534\n",
      "convergence dfGPdfNN Run 5/10, Epoch 89/1000, Training Loss (NLML): -953.4056\n",
      "convergence dfGPdfNN Run 5/10, Epoch 90/1000, Training Loss (NLML): -953.3477\n",
      "convergence dfGPdfNN Run 5/10, Epoch 91/1000, Training Loss (NLML): -953.5901\n",
      "convergence dfGPdfNN Run 5/10, Epoch 92/1000, Training Loss (NLML): -953.7435\n",
      "convergence dfGPdfNN Run 5/10, Epoch 93/1000, Training Loss (NLML): -953.8817\n",
      "convergence dfGPdfNN Run 5/10, Epoch 94/1000, Training Loss (NLML): -953.8671\n",
      "convergence dfGPdfNN Run 5/10, Epoch 95/1000, Training Loss (NLML): -953.9641\n",
      "convergence dfGPdfNN Run 5/10, Epoch 96/1000, Training Loss (NLML): -954.0474\n",
      "convergence dfGPdfNN Run 5/10, Epoch 97/1000, Training Loss (NLML): -954.2189\n",
      "convergence dfGPdfNN Run 5/10, Epoch 98/1000, Training Loss (NLML): -954.3962\n",
      "convergence dfGPdfNN Run 5/10, Epoch 99/1000, Training Loss (NLML): -954.7307\n",
      "convergence dfGPdfNN Run 5/10, Epoch 100/1000, Training Loss (NLML): -954.9735\n",
      "convergence dfGPdfNN Run 5/10, Epoch 101/1000, Training Loss (NLML): -955.1318\n",
      "convergence dfGPdfNN Run 5/10, Epoch 102/1000, Training Loss (NLML): -955.2322\n",
      "convergence dfGPdfNN Run 5/10, Epoch 103/1000, Training Loss (NLML): -955.1989\n",
      "convergence dfGPdfNN Run 5/10, Epoch 104/1000, Training Loss (NLML): -955.2740\n",
      "convergence dfGPdfNN Run 5/10, Epoch 105/1000, Training Loss (NLML): -955.5824\n",
      "convergence dfGPdfNN Run 5/10, Epoch 106/1000, Training Loss (NLML): -955.7581\n",
      "convergence dfGPdfNN Run 5/10, Epoch 107/1000, Training Loss (NLML): -955.8267\n",
      "convergence dfGPdfNN Run 5/10, Epoch 108/1000, Training Loss (NLML): -955.7673\n",
      "convergence dfGPdfNN Run 5/10, Epoch 109/1000, Training Loss (NLML): -955.8021\n",
      "convergence dfGPdfNN Run 5/10, Epoch 110/1000, Training Loss (NLML): -956.0759\n",
      "convergence dfGPdfNN Run 5/10, Epoch 111/1000, Training Loss (NLML): -956.0830\n",
      "convergence dfGPdfNN Run 5/10, Epoch 112/1000, Training Loss (NLML): -955.9377\n",
      "convergence dfGPdfNN Run 5/10, Epoch 113/1000, Training Loss (NLML): -956.1935\n",
      "convergence dfGPdfNN Run 5/10, Epoch 114/1000, Training Loss (NLML): -956.2366\n",
      "convergence dfGPdfNN Run 5/10, Epoch 115/1000, Training Loss (NLML): -956.2487\n",
      "convergence dfGPdfNN Run 5/10, Epoch 116/1000, Training Loss (NLML): -956.3490\n",
      "convergence dfGPdfNN Run 5/10, Epoch 117/1000, Training Loss (NLML): -956.3721\n",
      "convergence dfGPdfNN Run 5/10, Epoch 118/1000, Training Loss (NLML): -956.3162\n",
      "convergence dfGPdfNN Run 5/10, Epoch 119/1000, Training Loss (NLML): -956.4417\n",
      "convergence dfGPdfNN Run 5/10, Epoch 120/1000, Training Loss (NLML): -956.3099\n",
      "convergence dfGPdfNN Run 5/10, Epoch 121/1000, Training Loss (NLML): -956.1532\n",
      "convergence dfGPdfNN Run 5/10, Epoch 122/1000, Training Loss (NLML): -956.1688\n",
      "convergence dfGPdfNN Run 5/10, Epoch 123/1000, Training Loss (NLML): -956.2787\n",
      "convergence dfGPdfNN Run 5/10, Epoch 124/1000, Training Loss (NLML): -956.3773\n",
      "convergence dfGPdfNN Run 5/10, Epoch 125/1000, Training Loss (NLML): -956.4684\n",
      "convergence dfGPdfNN Run 5/10, Epoch 126/1000, Training Loss (NLML): -956.5574\n",
      "convergence dfGPdfNN Run 5/10, Epoch 127/1000, Training Loss (NLML): -956.6663\n",
      "convergence dfGPdfNN Run 5/10, Epoch 128/1000, Training Loss (NLML): -956.6853\n",
      "convergence dfGPdfNN Run 5/10, Epoch 129/1000, Training Loss (NLML): -956.5492\n",
      "convergence dfGPdfNN Run 5/10, Epoch 130/1000, Training Loss (NLML): -956.4489\n",
      "convergence dfGPdfNN Run 5/10, Epoch 131/1000, Training Loss (NLML): -956.2738\n",
      "convergence dfGPdfNN Run 5/10, Epoch 132/1000, Training Loss (NLML): -956.3865\n",
      "convergence dfGPdfNN Run 5/10, Epoch 133/1000, Training Loss (NLML): -956.4081\n",
      "convergence dfGPdfNN Run 5/10, Epoch 134/1000, Training Loss (NLML): -956.4841\n",
      "convergence dfGPdfNN Run 5/10, Epoch 135/1000, Training Loss (NLML): -956.5394\n",
      "convergence dfGPdfNN Run 5/10, Epoch 136/1000, Training Loss (NLML): -956.4454\n",
      "convergence dfGPdfNN Run 5/10, Epoch 137/1000, Training Loss (NLML): -956.5620\n",
      "convergence dfGPdfNN Run 5/10, Epoch 138/1000, Training Loss (NLML): -956.3861\n",
      "convergence dfGPdfNN Run 5/10, Epoch 139/1000, Training Loss (NLML): -956.5242\n",
      "convergence dfGPdfNN Run 5/10, Epoch 140/1000, Training Loss (NLML): -956.5841\n",
      "convergence dfGPdfNN Run 5/10, Epoch 141/1000, Training Loss (NLML): -956.6527\n",
      "convergence dfGPdfNN Run 5/10, Epoch 142/1000, Training Loss (NLML): -956.6897\n",
      "convergence dfGPdfNN Run 5/10, Epoch 143/1000, Training Loss (NLML): -956.6273\n",
      "convergence dfGPdfNN Run 5/10, Epoch 144/1000, Training Loss (NLML): -956.6200\n",
      "convergence dfGPdfNN Run 5/10, Epoch 145/1000, Training Loss (NLML): -956.7869\n",
      "convergence dfGPdfNN Run 5/10, Epoch 146/1000, Training Loss (NLML): -956.7795\n",
      "convergence dfGPdfNN Run 5/10, Epoch 147/1000, Training Loss (NLML): -956.6290\n",
      "convergence dfGPdfNN Run 5/10, Epoch 148/1000, Training Loss (NLML): -956.7031\n",
      "convergence dfGPdfNN Run 5/10, Epoch 149/1000, Training Loss (NLML): -956.7386\n",
      "convergence dfGPdfNN Run 5/10, Epoch 150/1000, Training Loss (NLML): -956.7134\n",
      "convergence dfGPdfNN Run 5/10, Epoch 151/1000, Training Loss (NLML): -956.7712\n",
      "convergence dfGPdfNN Run 5/10, Epoch 152/1000, Training Loss (NLML): -956.7158\n",
      "convergence dfGPdfNN Run 5/10, Epoch 153/1000, Training Loss (NLML): -956.7357\n",
      "convergence dfGPdfNN Run 5/10, Epoch 154/1000, Training Loss (NLML): -956.7574\n",
      "convergence dfGPdfNN Run 5/10, Epoch 155/1000, Training Loss (NLML): -956.8022\n",
      "convergence dfGPdfNN Run 5/10, Epoch 156/1000, Training Loss (NLML): -956.8304\n",
      "convergence dfGPdfNN Run 5/10, Epoch 157/1000, Training Loss (NLML): -956.8971\n",
      "convergence dfGPdfNN Run 5/10, Epoch 158/1000, Training Loss (NLML): -956.9111\n",
      "convergence dfGPdfNN Run 5/10, Epoch 159/1000, Training Loss (NLML): -956.9266\n",
      "convergence dfGPdfNN Run 5/10, Epoch 160/1000, Training Loss (NLML): -956.9669\n",
      "convergence dfGPdfNN Run 5/10, Epoch 161/1000, Training Loss (NLML): -956.9840\n",
      "convergence dfGPdfNN Run 5/10, Epoch 162/1000, Training Loss (NLML): -957.0176\n",
      "convergence dfGPdfNN Run 5/10, Epoch 163/1000, Training Loss (NLML): -956.9716\n",
      "convergence dfGPdfNN Run 5/10, Epoch 164/1000, Training Loss (NLML): -956.9988\n",
      "convergence dfGPdfNN Run 5/10, Epoch 165/1000, Training Loss (NLML): -956.9977\n",
      "convergence dfGPdfNN Run 5/10, Epoch 166/1000, Training Loss (NLML): -957.0372\n",
      "convergence dfGPdfNN Run 5/10, Epoch 167/1000, Training Loss (NLML): -957.0555\n",
      "convergence dfGPdfNN Run 5/10, Epoch 168/1000, Training Loss (NLML): -957.0742\n",
      "convergence dfGPdfNN Run 5/10, Epoch 169/1000, Training Loss (NLML): -957.0935\n",
      "convergence dfGPdfNN Run 5/10, Epoch 170/1000, Training Loss (NLML): -957.1147\n",
      "convergence dfGPdfNN Run 5/10, Epoch 171/1000, Training Loss (NLML): -957.1576\n",
      "convergence dfGPdfNN Run 5/10, Epoch 172/1000, Training Loss (NLML): -957.1886\n",
      "convergence dfGPdfNN Run 5/10, Epoch 173/1000, Training Loss (NLML): -957.2463\n",
      "convergence dfGPdfNN Run 5/10, Epoch 174/1000, Training Loss (NLML): -957.2758\n",
      "convergence dfGPdfNN Run 5/10, Epoch 175/1000, Training Loss (NLML): -957.2854\n",
      "convergence dfGPdfNN Run 5/10, Epoch 176/1000, Training Loss (NLML): -957.3821\n",
      "convergence dfGPdfNN Run 5/10, Epoch 177/1000, Training Loss (NLML): -957.3074\n",
      "convergence dfGPdfNN Run 5/10, Epoch 178/1000, Training Loss (NLML): -957.3304\n",
      "convergence dfGPdfNN Run 5/10, Epoch 179/1000, Training Loss (NLML): -957.3477\n",
      "convergence dfGPdfNN Run 5/10, Epoch 180/1000, Training Loss (NLML): -957.4611\n",
      "convergence dfGPdfNN Run 5/10, Epoch 181/1000, Training Loss (NLML): -957.3914\n",
      "convergence dfGPdfNN Run 5/10, Epoch 182/1000, Training Loss (NLML): -957.4324\n",
      "convergence dfGPdfNN Run 5/10, Epoch 183/1000, Training Loss (NLML): -957.4436\n",
      "convergence dfGPdfNN Run 5/10, Epoch 184/1000, Training Loss (NLML): -957.4640\n",
      "convergence dfGPdfNN Run 5/10, Epoch 185/1000, Training Loss (NLML): -957.4855\n",
      "convergence dfGPdfNN Run 5/10, Epoch 186/1000, Training Loss (NLML): -957.4928\n",
      "convergence dfGPdfNN Run 5/10, Epoch 187/1000, Training Loss (NLML): -957.5098\n",
      "convergence dfGPdfNN Run 5/10, Epoch 188/1000, Training Loss (NLML): -957.5138\n",
      "convergence dfGPdfNN Run 5/10, Epoch 189/1000, Training Loss (NLML): -957.6051\n",
      "convergence dfGPdfNN Run 5/10, Epoch 190/1000, Training Loss (NLML): -957.6365\n",
      "convergence dfGPdfNN Run 5/10, Epoch 191/1000, Training Loss (NLML): -957.5872\n",
      "convergence dfGPdfNN Run 5/10, Epoch 192/1000, Training Loss (NLML): -957.5848\n",
      "convergence dfGPdfNN Run 5/10, Epoch 193/1000, Training Loss (NLML): -957.5885\n",
      "convergence dfGPdfNN Run 5/10, Epoch 194/1000, Training Loss (NLML): -957.5944\n",
      "convergence dfGPdfNN Run 5/10, Epoch 195/1000, Training Loss (NLML): -957.5958\n",
      "convergence dfGPdfNN Run 5/10, Epoch 196/1000, Training Loss (NLML): -957.6892\n",
      "convergence dfGPdfNN Run 5/10, Epoch 197/1000, Training Loss (NLML): -957.7346\n",
      "convergence dfGPdfNN Run 5/10, Epoch 198/1000, Training Loss (NLML): -957.7581\n",
      "convergence dfGPdfNN Run 5/10, Epoch 199/1000, Training Loss (NLML): -957.6920\n",
      "convergence dfGPdfNN Run 5/10, Epoch 200/1000, Training Loss (NLML): -957.6968\n",
      "convergence dfGPdfNN Run 5/10, Epoch 201/1000, Training Loss (NLML): -957.7189\n",
      "convergence dfGPdfNN Run 5/10, Epoch 202/1000, Training Loss (NLML): -957.7219\n",
      "convergence dfGPdfNN Run 5/10, Epoch 203/1000, Training Loss (NLML): -957.7983\n",
      "convergence dfGPdfNN Run 5/10, Epoch 204/1000, Training Loss (NLML): -957.7977\n",
      "convergence dfGPdfNN Run 5/10, Epoch 205/1000, Training Loss (NLML): -957.8237\n",
      "convergence dfGPdfNN Run 5/10, Epoch 206/1000, Training Loss (NLML): -957.7756\n",
      "convergence dfGPdfNN Run 5/10, Epoch 207/1000, Training Loss (NLML): -957.7699\n",
      "convergence dfGPdfNN Run 5/10, Epoch 208/1000, Training Loss (NLML): -957.7742\n",
      "convergence dfGPdfNN Run 5/10, Epoch 209/1000, Training Loss (NLML): -957.7828\n",
      "convergence dfGPdfNN Run 5/10, Epoch 210/1000, Training Loss (NLML): -957.8616\n",
      "convergence dfGPdfNN Run 5/10, Epoch 211/1000, Training Loss (NLML): -957.8906\n",
      "convergence dfGPdfNN Run 5/10, Epoch 212/1000, Training Loss (NLML): -957.9102\n",
      "convergence dfGPdfNN Run 5/10, Epoch 213/1000, Training Loss (NLML): -957.9213\n",
      "convergence dfGPdfNN Run 5/10, Epoch 214/1000, Training Loss (NLML): -957.8528\n",
      "convergence dfGPdfNN Run 5/10, Epoch 215/1000, Training Loss (NLML): -957.9297\n",
      "convergence dfGPdfNN Run 5/10, Epoch 216/1000, Training Loss (NLML): -957.9580\n",
      "convergence dfGPdfNN Run 5/10, Epoch 217/1000, Training Loss (NLML): -957.9764\n",
      "convergence dfGPdfNN Run 5/10, Epoch 218/1000, Training Loss (NLML): -957.9983\n",
      "convergence dfGPdfNN Run 5/10, Epoch 219/1000, Training Loss (NLML): -958.0228\n",
      "convergence dfGPdfNN Run 5/10, Epoch 220/1000, Training Loss (NLML): -958.0358\n",
      "convergence dfGPdfNN Run 5/10, Epoch 221/1000, Training Loss (NLML): -958.0476\n",
      "convergence dfGPdfNN Run 5/10, Epoch 222/1000, Training Loss (NLML): -958.0663\n",
      "convergence dfGPdfNN Run 5/10, Epoch 223/1000, Training Loss (NLML): -958.1042\n",
      "convergence dfGPdfNN Run 5/10, Epoch 224/1000, Training Loss (NLML): -958.1292\n",
      "convergence dfGPdfNN Run 5/10, Epoch 225/1000, Training Loss (NLML): -958.1343\n",
      "convergence dfGPdfNN Run 5/10, Epoch 226/1000, Training Loss (NLML): -958.1213\n",
      "convergence dfGPdfNN Run 5/10, Epoch 227/1000, Training Loss (NLML): -958.1257\n",
      "convergence dfGPdfNN Run 5/10, Epoch 228/1000, Training Loss (NLML): -958.1295\n",
      "convergence dfGPdfNN Run 5/10, Epoch 229/1000, Training Loss (NLML): -958.1312\n",
      "convergence dfGPdfNN Run 5/10, Epoch 230/1000, Training Loss (NLML): -958.1477\n",
      "convergence dfGPdfNN Run 5/10, Epoch 231/1000, Training Loss (NLML): -958.1914\n",
      "convergence dfGPdfNN Run 5/10, Epoch 232/1000, Training Loss (NLML): -958.2035\n",
      "convergence dfGPdfNN Run 5/10, Epoch 233/1000, Training Loss (NLML): -958.2145\n",
      "convergence dfGPdfNN Run 5/10, Epoch 234/1000, Training Loss (NLML): -958.2217\n",
      "convergence dfGPdfNN Run 5/10, Epoch 235/1000, Training Loss (NLML): -958.2322\n",
      "convergence dfGPdfNN Run 5/10, Epoch 236/1000, Training Loss (NLML): -958.2471\n",
      "convergence dfGPdfNN Run 5/10, Epoch 237/1000, Training Loss (NLML): -958.2679\n",
      "convergence dfGPdfNN Run 5/10, Epoch 238/1000, Training Loss (NLML): -958.2842\n",
      "convergence dfGPdfNN Run 5/10, Epoch 239/1000, Training Loss (NLML): -958.2767\n",
      "convergence dfGPdfNN Run 5/10, Epoch 240/1000, Training Loss (NLML): -958.3118\n",
      "convergence dfGPdfNN Run 5/10, Epoch 241/1000, Training Loss (NLML): -958.3640\n",
      "convergence dfGPdfNN Run 5/10, Epoch 242/1000, Training Loss (NLML): -958.3635\n",
      "convergence dfGPdfNN Run 5/10, Epoch 243/1000, Training Loss (NLML): -958.3760\n",
      "convergence dfGPdfNN Run 5/10, Epoch 244/1000, Training Loss (NLML): -958.3793\n",
      "convergence dfGPdfNN Run 5/10, Epoch 245/1000, Training Loss (NLML): -958.4263\n",
      "convergence dfGPdfNN Run 5/10, Epoch 246/1000, Training Loss (NLML): -958.3976\n",
      "convergence dfGPdfNN Run 5/10, Epoch 247/1000, Training Loss (NLML): -958.4332\n",
      "convergence dfGPdfNN Run 5/10, Epoch 248/1000, Training Loss (NLML): -958.4828\n",
      "convergence dfGPdfNN Run 5/10, Epoch 249/1000, Training Loss (NLML): -958.5034\n",
      "convergence dfGPdfNN Run 5/10, Epoch 250/1000, Training Loss (NLML): -958.4733\n",
      "convergence dfGPdfNN Run 5/10, Epoch 251/1000, Training Loss (NLML): -958.5023\n",
      "convergence dfGPdfNN Run 5/10, Epoch 252/1000, Training Loss (NLML): -958.5347\n",
      "convergence dfGPdfNN Run 5/10, Epoch 253/1000, Training Loss (NLML): -958.5808\n",
      "convergence dfGPdfNN Run 5/10, Epoch 254/1000, Training Loss (NLML): -958.6030\n",
      "convergence dfGPdfNN Run 5/10, Epoch 255/1000, Training Loss (NLML): -958.6215\n",
      "convergence dfGPdfNN Run 5/10, Epoch 256/1000, Training Loss (NLML): -958.6395\n",
      "convergence dfGPdfNN Run 5/10, Epoch 257/1000, Training Loss (NLML): -958.6705\n",
      "convergence dfGPdfNN Run 5/10, Epoch 258/1000, Training Loss (NLML): -958.7015\n",
      "convergence dfGPdfNN Run 5/10, Epoch 259/1000, Training Loss (NLML): -958.7185\n",
      "convergence dfGPdfNN Run 5/10, Epoch 260/1000, Training Loss (NLML): -958.7627\n",
      "convergence dfGPdfNN Run 5/10, Epoch 261/1000, Training Loss (NLML): -958.7844\n",
      "convergence dfGPdfNN Run 5/10, Epoch 262/1000, Training Loss (NLML): -958.8085\n",
      "convergence dfGPdfNN Run 5/10, Epoch 263/1000, Training Loss (NLML): -958.7804\n",
      "convergence dfGPdfNN Run 5/10, Epoch 264/1000, Training Loss (NLML): -958.7809\n",
      "convergence dfGPdfNN Run 5/10, Epoch 265/1000, Training Loss (NLML): -958.7728\n",
      "convergence dfGPdfNN Run 5/10, Epoch 266/1000, Training Loss (NLML): -958.7760\n",
      "convergence dfGPdfNN Run 5/10, Epoch 267/1000, Training Loss (NLML): -958.7582\n",
      "convergence dfGPdfNN Run 5/10, Epoch 268/1000, Training Loss (NLML): -958.7715\n",
      "convergence dfGPdfNN Run 5/10, Epoch 269/1000, Training Loss (NLML): -958.8147\n",
      "convergence dfGPdfNN Run 5/10, Epoch 270/1000, Training Loss (NLML): -958.8099\n",
      "convergence dfGPdfNN Run 5/10, Epoch 271/1000, Training Loss (NLML): -958.8242\n",
      "convergence dfGPdfNN Run 5/10, Epoch 272/1000, Training Loss (NLML): -958.8636\n",
      "convergence dfGPdfNN Run 5/10, Epoch 273/1000, Training Loss (NLML): -958.8397\n",
      "convergence dfGPdfNN Run 5/10, Epoch 274/1000, Training Loss (NLML): -958.8488\n",
      "convergence dfGPdfNN Run 5/10, Epoch 275/1000, Training Loss (NLML): -958.8665\n",
      "convergence dfGPdfNN Run 5/10, Epoch 276/1000, Training Loss (NLML): -958.8413\n",
      "convergence dfGPdfNN Run 5/10, Epoch 277/1000, Training Loss (NLML): -958.8406\n",
      "convergence dfGPdfNN Run 5/10, Epoch 278/1000, Training Loss (NLML): -957.8207\n",
      "convergence dfGPdfNN Run 5/10, Epoch 279/1000, Training Loss (NLML): -958.6885\n",
      "convergence dfGPdfNN Run 5/10, Epoch 280/1000, Training Loss (NLML): -958.6262\n",
      "convergence dfGPdfNN Run 5/10, Epoch 281/1000, Training Loss (NLML): -958.5660\n",
      "convergence dfGPdfNN Run 5/10, Epoch 282/1000, Training Loss (NLML): -958.5237\n",
      "convergence dfGPdfNN Run 5/10, Epoch 283/1000, Training Loss (NLML): -958.3265\n",
      "convergence dfGPdfNN Run 5/10, Epoch 284/1000, Training Loss (NLML): -958.5168\n",
      "convergence dfGPdfNN Run 5/10, Epoch 285/1000, Training Loss (NLML): -958.6204\n",
      "convergence dfGPdfNN Run 5/10, Epoch 286/1000, Training Loss (NLML): -958.5359\n",
      "convergence dfGPdfNN Run 5/10, Epoch 287/1000, Training Loss (NLML): -958.5193\n",
      "convergence dfGPdfNN Run 5/10, Epoch 288/1000, Training Loss (NLML): -958.7605\n",
      "convergence dfGPdfNN Run 5/10, Epoch 289/1000, Training Loss (NLML): -958.7712\n",
      "convergence dfGPdfNN Run 5/10, Epoch 290/1000, Training Loss (NLML): -958.7607\n",
      "convergence dfGPdfNN Run 5/10, Epoch 291/1000, Training Loss (NLML): -958.7034\n",
      "convergence dfGPdfNN Run 5/10, Epoch 292/1000, Training Loss (NLML): -958.6405\n",
      "convergence dfGPdfNN Run 5/10, Epoch 293/1000, Training Loss (NLML): -958.5770\n",
      "convergence dfGPdfNN Run 5/10, Epoch 294/1000, Training Loss (NLML): -958.5367\n",
      "convergence dfGPdfNN Run 5/10, Epoch 295/1000, Training Loss (NLML): -958.5500\n",
      "convergence dfGPdfNN Run 5/10, Epoch 296/1000, Training Loss (NLML): -958.6578\n",
      "convergence dfGPdfNN Run 5/10, Epoch 297/1000, Training Loss (NLML): -958.7798\n",
      "convergence dfGPdfNN Run 5/10, Epoch 298/1000, Training Loss (NLML): -958.8361\n",
      "convergence dfGPdfNN Run 5/10, Epoch 299/1000, Training Loss (NLML): -958.7524\n",
      "convergence dfGPdfNN Run 5/10, Epoch 300/1000, Training Loss (NLML): -958.6957\n",
      "convergence dfGPdfNN Run 5/10, Epoch 301/1000, Training Loss (NLML): -958.7184\n",
      "convergence dfGPdfNN Run 5/10, Epoch 302/1000, Training Loss (NLML): -958.7090\n",
      "convergence dfGPdfNN Run 5/10, Epoch 303/1000, Training Loss (NLML): -958.7067\n",
      "convergence dfGPdfNN Run 5/10, Epoch 304/1000, Training Loss (NLML): -958.7429\n",
      "convergence dfGPdfNN Run 5/10, Epoch 305/1000, Training Loss (NLML): -958.8357\n",
      "convergence dfGPdfNN Run 5/10, Epoch 306/1000, Training Loss (NLML): -958.9480\n",
      "convergence dfGPdfNN Run 5/10, Epoch 307/1000, Training Loss (NLML): -959.0048\n",
      "convergence dfGPdfNN Run 5/10, Epoch 308/1000, Training Loss (NLML): -959.0300\n",
      "convergence dfGPdfNN Run 5/10, Epoch 309/1000, Training Loss (NLML): -959.0917\n",
      "convergence dfGPdfNN Run 5/10, Epoch 310/1000, Training Loss (NLML): -959.0210\n",
      "convergence dfGPdfNN Run 5/10, Epoch 311/1000, Training Loss (NLML): -959.0382\n",
      "convergence dfGPdfNN Run 5/10, Epoch 312/1000, Training Loss (NLML): -959.0258\n",
      "convergence dfGPdfNN Run 5/10, Epoch 313/1000, Training Loss (NLML): -959.0438\n",
      "convergence dfGPdfNN Run 5/10, Epoch 314/1000, Training Loss (NLML): -958.9595\n",
      "convergence dfGPdfNN Run 5/10, Epoch 315/1000, Training Loss (NLML): -958.9532\n",
      "convergence dfGPdfNN Run 5/10, Epoch 316/1000, Training Loss (NLML): -958.9135\n",
      "convergence dfGPdfNN Run 5/10, Epoch 317/1000, Training Loss (NLML): -958.9082\n",
      "convergence dfGPdfNN Run 5/10, Epoch 318/1000, Training Loss (NLML): -958.9360\n",
      "convergence dfGPdfNN Run 5/10, Epoch 319/1000, Training Loss (NLML): -958.9872\n",
      "convergence dfGPdfNN Run 5/10, Epoch 320/1000, Training Loss (NLML): -959.0331\n",
      "convergence dfGPdfNN Run 5/10, Epoch 321/1000, Training Loss (NLML): -959.0704\n",
      "convergence dfGPdfNN Run 5/10, Epoch 322/1000, Training Loss (NLML): -959.0916\n",
      "convergence dfGPdfNN Run 5/10, Epoch 323/1000, Training Loss (NLML): -959.0883\n",
      "convergence dfGPdfNN Run 5/10, Epoch 324/1000, Training Loss (NLML): -959.1067\n",
      "convergence dfGPdfNN Run 5/10, Epoch 325/1000, Training Loss (NLML): -959.1212\n",
      "convergence dfGPdfNN Run 5/10, Epoch 326/1000, Training Loss (NLML): -959.1210\n",
      "convergence dfGPdfNN Run 5/10, Epoch 327/1000, Training Loss (NLML): -959.1506\n",
      "convergence dfGPdfNN Run 5/10, Epoch 328/1000, Training Loss (NLML): -959.1577\n",
      "convergence dfGPdfNN Run 5/10, Epoch 329/1000, Training Loss (NLML): -959.1887\n",
      "convergence dfGPdfNN Run 5/10, Epoch 330/1000, Training Loss (NLML): -959.1881\n",
      "convergence dfGPdfNN Run 5/10, Epoch 331/1000, Training Loss (NLML): -959.2150\n",
      "convergence dfGPdfNN Run 5/10, Epoch 332/1000, Training Loss (NLML): -959.2162\n",
      "convergence dfGPdfNN Run 5/10, Epoch 333/1000, Training Loss (NLML): -959.2122\n",
      "convergence dfGPdfNN Run 5/10, Epoch 334/1000, Training Loss (NLML): -959.2170\n",
      "convergence dfGPdfNN Run 5/10, Epoch 335/1000, Training Loss (NLML): -959.1869\n",
      "convergence dfGPdfNN Run 5/10, Epoch 336/1000, Training Loss (NLML): -959.1602\n",
      "convergence dfGPdfNN Run 5/10, Epoch 337/1000, Training Loss (NLML): -959.1741\n",
      "convergence dfGPdfNN Run 5/10, Epoch 338/1000, Training Loss (NLML): -959.1891\n",
      "convergence dfGPdfNN Run 5/10, Epoch 339/1000, Training Loss (NLML): -959.1583\n",
      "convergence dfGPdfNN Run 5/10, Epoch 340/1000, Training Loss (NLML): -959.1703\n",
      "convergence dfGPdfNN Run 5/10, Epoch 341/1000, Training Loss (NLML): -959.1895\n",
      "convergence dfGPdfNN Run 5/10, Epoch 342/1000, Training Loss (NLML): -959.1593\n",
      "convergence dfGPdfNN Run 5/10, Epoch 343/1000, Training Loss (NLML): -959.1553\n",
      "convergence dfGPdfNN Run 5/10, Epoch 344/1000, Training Loss (NLML): -959.1702\n",
      "convergence dfGPdfNN Run 5/10, Epoch 345/1000, Training Loss (NLML): -959.2058\n",
      "convergence dfGPdfNN Run 5/10, Epoch 346/1000, Training Loss (NLML): -959.2549\n",
      "convergence dfGPdfNN Run 5/10, Epoch 347/1000, Training Loss (NLML): -959.2850\n",
      "convergence dfGPdfNN Run 5/10, Epoch 348/1000, Training Loss (NLML): -959.2971\n",
      "convergence dfGPdfNN Run 5/10, Epoch 349/1000, Training Loss (NLML): -959.3235\n",
      "convergence dfGPdfNN Run 5/10, Epoch 350/1000, Training Loss (NLML): -959.3243\n",
      "convergence dfGPdfNN Run 5/10, Epoch 351/1000, Training Loss (NLML): -959.3280\n",
      "convergence dfGPdfNN Run 5/10, Epoch 352/1000, Training Loss (NLML): -959.3522\n",
      "convergence dfGPdfNN Run 5/10, Epoch 353/1000, Training Loss (NLML): -959.3630\n",
      "convergence dfGPdfNN Run 5/10, Epoch 354/1000, Training Loss (NLML): -959.3728\n",
      "convergence dfGPdfNN Run 5/10, Epoch 355/1000, Training Loss (NLML): -959.3861\n",
      "convergence dfGPdfNN Run 5/10, Epoch 356/1000, Training Loss (NLML): -959.4329\n",
      "convergence dfGPdfNN Run 5/10, Epoch 357/1000, Training Loss (NLML): -959.4229\n",
      "convergence dfGPdfNN Run 5/10, Epoch 358/1000, Training Loss (NLML): -959.4303\n",
      "convergence dfGPdfNN Run 5/10, Epoch 359/1000, Training Loss (NLML): -959.4370\n",
      "convergence dfGPdfNN Run 5/10, Epoch 360/1000, Training Loss (NLML): -959.4465\n",
      "convergence dfGPdfNN Run 5/10, Epoch 361/1000, Training Loss (NLML): -959.4629\n",
      "convergence dfGPdfNN Run 5/10, Epoch 362/1000, Training Loss (NLML): -959.4810\n",
      "convergence dfGPdfNN Run 5/10, Epoch 363/1000, Training Loss (NLML): -959.4900\n",
      "convergence dfGPdfNN Run 5/10, Epoch 364/1000, Training Loss (NLML): -959.4994\n",
      "convergence dfGPdfNN Run 5/10, Epoch 365/1000, Training Loss (NLML): -959.5023\n",
      "convergence dfGPdfNN Run 5/10, Epoch 366/1000, Training Loss (NLML): -959.5052\n",
      "convergence dfGPdfNN Run 5/10, Epoch 367/1000, Training Loss (NLML): -959.5096\n",
      "convergence dfGPdfNN Run 5/10, Epoch 368/1000, Training Loss (NLML): -959.5178\n",
      "convergence dfGPdfNN Run 5/10, Epoch 369/1000, Training Loss (NLML): -959.5219\n",
      "convergence dfGPdfNN Run 5/10, Epoch 370/1000, Training Loss (NLML): -959.5310\n",
      "convergence dfGPdfNN Run 5/10, Epoch 371/1000, Training Loss (NLML): -959.5436\n",
      "convergence dfGPdfNN Run 5/10, Epoch 372/1000, Training Loss (NLML): -959.5283\n",
      "convergence dfGPdfNN Run 5/10, Epoch 373/1000, Training Loss (NLML): -959.5271\n",
      "convergence dfGPdfNN Run 5/10, Epoch 374/1000, Training Loss (NLML): -959.5262\n",
      "convergence dfGPdfNN Run 5/10, Epoch 375/1000, Training Loss (NLML): -959.5343\n",
      "convergence dfGPdfNN Run 5/10, Epoch 376/1000, Training Loss (NLML): -959.5376\n",
      "convergence dfGPdfNN Run 5/10, Epoch 377/1000, Training Loss (NLML): -959.5560\n",
      "convergence dfGPdfNN Run 5/10, Epoch 378/1000, Training Loss (NLML): -959.5614\n",
      "convergence dfGPdfNN Run 5/10, Epoch 379/1000, Training Loss (NLML): -959.5804\n",
      "convergence dfGPdfNN Run 5/10, Epoch 380/1000, Training Loss (NLML): -959.5718\n",
      "convergence dfGPdfNN Run 5/10, Epoch 381/1000, Training Loss (NLML): -959.5747\n",
      "convergence dfGPdfNN Run 5/10, Epoch 382/1000, Training Loss (NLML): -959.5807\n",
      "convergence dfGPdfNN Run 5/10, Epoch 383/1000, Training Loss (NLML): -959.5881\n",
      "convergence dfGPdfNN Run 5/10, Epoch 384/1000, Training Loss (NLML): -959.5513\n",
      "convergence dfGPdfNN Run 5/10, Epoch 385/1000, Training Loss (NLML): -959.6030\n",
      "convergence dfGPdfNN Run 5/10, Epoch 386/1000, Training Loss (NLML): -959.5892\n",
      "convergence dfGPdfNN Run 5/10, Epoch 387/1000, Training Loss (NLML): -959.5831\n",
      "convergence dfGPdfNN Run 5/10, Epoch 388/1000, Training Loss (NLML): -959.5891\n",
      "convergence dfGPdfNN Run 5/10, Epoch 389/1000, Training Loss (NLML): -959.5991\n",
      "convergence dfGPdfNN Run 5/10, Epoch 390/1000, Training Loss (NLML): -959.5961\n",
      "convergence dfGPdfNN Run 5/10, Epoch 391/1000, Training Loss (NLML): -959.5709\n",
      "convergence dfGPdfNN Run 5/10, Epoch 392/1000, Training Loss (NLML): -959.5752\n",
      "convergence dfGPdfNN Run 5/10, Epoch 393/1000, Training Loss (NLML): -959.5703\n",
      "convergence dfGPdfNN Run 5/10, Epoch 394/1000, Training Loss (NLML): -959.5836\n",
      "convergence dfGPdfNN Run 5/10, Epoch 395/1000, Training Loss (NLML): -959.5818\n",
      "convergence dfGPdfNN Run 5/10, Epoch 396/1000, Training Loss (NLML): -959.5798\n",
      "convergence dfGPdfNN Run 5/10, Epoch 397/1000, Training Loss (NLML): -959.5994\n",
      "convergence dfGPdfNN Run 5/10, Epoch 398/1000, Training Loss (NLML): -959.5942\n",
      "convergence dfGPdfNN Run 5/10, Epoch 399/1000, Training Loss (NLML): -959.5959\n",
      "convergence dfGPdfNN Run 5/10, Epoch 400/1000, Training Loss (NLML): -959.6104\n",
      "convergence dfGPdfNN Run 5/10, Epoch 401/1000, Training Loss (NLML): -959.6191\n",
      "convergence dfGPdfNN Run 5/10, Epoch 402/1000, Training Loss (NLML): -959.6324\n",
      "convergence dfGPdfNN Run 5/10, Epoch 403/1000, Training Loss (NLML): -959.6375\n",
      "convergence dfGPdfNN Run 5/10, Epoch 404/1000, Training Loss (NLML): -959.6394\n",
      "convergence dfGPdfNN Run 5/10, Epoch 405/1000, Training Loss (NLML): -959.6350\n",
      "convergence dfGPdfNN Run 5/10, Epoch 406/1000, Training Loss (NLML): -959.6179\n",
      "convergence dfGPdfNN Run 5/10, Epoch 407/1000, Training Loss (NLML): -959.6062\n",
      "convergence dfGPdfNN Run 5/10, Epoch 408/1000, Training Loss (NLML): -959.6057\n",
      "convergence dfGPdfNN Run 5/10, Epoch 409/1000, Training Loss (NLML): -959.5903\n",
      "convergence dfGPdfNN Run 5/10, Epoch 410/1000, Training Loss (NLML): -959.5868\n",
      "convergence dfGPdfNN Run 5/10, Epoch 411/1000, Training Loss (NLML): -959.5692\n",
      "convergence dfGPdfNN Run 5/10, Epoch 412/1000, Training Loss (NLML): -959.6672\n",
      "convergence dfGPdfNN Run 5/10, Epoch 413/1000, Training Loss (NLML): -959.6777\n",
      "convergence dfGPdfNN Run 5/10, Epoch 414/1000, Training Loss (NLML): -959.6101\n",
      "convergence dfGPdfNN Run 5/10, Epoch 415/1000, Training Loss (NLML): -959.6049\n",
      "convergence dfGPdfNN Run 5/10, Epoch 416/1000, Training Loss (NLML): -959.6525\n",
      "convergence dfGPdfNN Run 5/10, Epoch 417/1000, Training Loss (NLML): -959.6495\n",
      "convergence dfGPdfNN Run 5/10, Epoch 418/1000, Training Loss (NLML): -959.6492\n",
      "convergence dfGPdfNN Run 5/10, Epoch 419/1000, Training Loss (NLML): -959.6769\n",
      "convergence dfGPdfNN Run 5/10, Epoch 420/1000, Training Loss (NLML): -959.6746\n",
      "convergence dfGPdfNN Run 5/10, Epoch 421/1000, Training Loss (NLML): -959.6793\n",
      "convergence dfGPdfNN Run 5/10, Epoch 422/1000, Training Loss (NLML): -959.6932\n",
      "convergence dfGPdfNN Run 5/10, Epoch 423/1000, Training Loss (NLML): -959.7090\n",
      "convergence dfGPdfNN Run 5/10, Epoch 424/1000, Training Loss (NLML): -959.7251\n",
      "convergence dfGPdfNN Run 5/10, Epoch 425/1000, Training Loss (NLML): -959.7568\n",
      "convergence dfGPdfNN Run 5/10, Epoch 426/1000, Training Loss (NLML): -959.7555\n",
      "convergence dfGPdfNN Run 5/10, Epoch 427/1000, Training Loss (NLML): -959.7738\n",
      "convergence dfGPdfNN Run 5/10, Epoch 428/1000, Training Loss (NLML): -959.7817\n",
      "convergence dfGPdfNN Run 5/10, Epoch 429/1000, Training Loss (NLML): -959.7885\n",
      "convergence dfGPdfNN Run 5/10, Epoch 430/1000, Training Loss (NLML): -959.7563\n",
      "convergence dfGPdfNN Run 5/10, Epoch 431/1000, Training Loss (NLML): -959.7548\n",
      "convergence dfGPdfNN Run 5/10, Epoch 432/1000, Training Loss (NLML): -959.7592\n",
      "convergence dfGPdfNN Run 5/10, Epoch 433/1000, Training Loss (NLML): -959.7649\n",
      "convergence dfGPdfNN Run 5/10, Epoch 434/1000, Training Loss (NLML): -959.7858\n",
      "convergence dfGPdfNN Run 5/10, Epoch 435/1000, Training Loss (NLML): -959.7958\n",
      "convergence dfGPdfNN Run 5/10, Epoch 436/1000, Training Loss (NLML): -959.8015\n",
      "convergence dfGPdfNN Run 5/10, Epoch 437/1000, Training Loss (NLML): -959.7980\n",
      "convergence dfGPdfNN Run 5/10, Epoch 438/1000, Training Loss (NLML): -959.8026\n",
      "convergence dfGPdfNN Run 5/10, Epoch 439/1000, Training Loss (NLML): -959.8394\n",
      "convergence dfGPdfNN Run 5/10, Epoch 440/1000, Training Loss (NLML): -959.8459\n",
      "convergence dfGPdfNN Run 5/10, Epoch 441/1000, Training Loss (NLML): -959.8505\n",
      "convergence dfGPdfNN Run 5/10, Epoch 442/1000, Training Loss (NLML): -959.8536\n",
      "convergence dfGPdfNN Run 5/10, Epoch 443/1000, Training Loss (NLML): -959.8606\n",
      "convergence dfGPdfNN Run 5/10, Epoch 444/1000, Training Loss (NLML): -959.8690\n",
      "convergence dfGPdfNN Run 5/10, Epoch 445/1000, Training Loss (NLML): -959.8759\n",
      "convergence dfGPdfNN Run 5/10, Epoch 446/1000, Training Loss (NLML): -959.8792\n",
      "convergence dfGPdfNN Run 5/10, Epoch 447/1000, Training Loss (NLML): -959.8901\n",
      "convergence dfGPdfNN Run 5/10, Epoch 448/1000, Training Loss (NLML): -959.8928\n",
      "convergence dfGPdfNN Run 5/10, Epoch 449/1000, Training Loss (NLML): -959.8796\n",
      "convergence dfGPdfNN Run 5/10, Epoch 450/1000, Training Loss (NLML): -959.8862\n",
      "convergence dfGPdfNN Run 5/10, Epoch 451/1000, Training Loss (NLML): -959.8804\n",
      "convergence dfGPdfNN Run 5/10, Epoch 452/1000, Training Loss (NLML): -959.8877\n",
      "convergence dfGPdfNN Run 5/10, Epoch 453/1000, Training Loss (NLML): -959.8781\n",
      "convergence dfGPdfNN Run 5/10, Epoch 454/1000, Training Loss (NLML): -959.8846\n",
      "convergence dfGPdfNN Run 5/10, Epoch 455/1000, Training Loss (NLML): -959.8900\n",
      "convergence dfGPdfNN Run 5/10, Epoch 456/1000, Training Loss (NLML): -959.8958\n",
      "convergence dfGPdfNN Run 5/10, Epoch 457/1000, Training Loss (NLML): -959.9283\n",
      "convergence dfGPdfNN Run 5/10, Epoch 458/1000, Training Loss (NLML): -959.9301\n",
      "convergence dfGPdfNN Run 5/10, Epoch 459/1000, Training Loss (NLML): -959.9452\n",
      "convergence dfGPdfNN Run 5/10, Epoch 460/1000, Training Loss (NLML): -959.9509\n",
      "convergence dfGPdfNN Run 5/10, Epoch 461/1000, Training Loss (NLML): -959.9666\n",
      "convergence dfGPdfNN Run 5/10, Epoch 462/1000, Training Loss (NLML): -959.9719\n",
      "convergence dfGPdfNN Run 5/10, Epoch 463/1000, Training Loss (NLML): -959.9741\n",
      "convergence dfGPdfNN Run 5/10, Epoch 464/1000, Training Loss (NLML): -959.9803\n",
      "convergence dfGPdfNN Run 5/10, Epoch 465/1000, Training Loss (NLML): -959.9545\n",
      "convergence dfGPdfNN Run 5/10, Epoch 466/1000, Training Loss (NLML): -959.9589\n",
      "convergence dfGPdfNN Run 5/10, Epoch 467/1000, Training Loss (NLML): -959.9620\n",
      "convergence dfGPdfNN Run 5/10, Epoch 468/1000, Training Loss (NLML): -959.9562\n",
      "convergence dfGPdfNN Run 5/10, Epoch 469/1000, Training Loss (NLML): -959.9634\n",
      "convergence dfGPdfNN Run 5/10, Epoch 470/1000, Training Loss (NLML): -959.9630\n",
      "convergence dfGPdfNN Run 5/10, Epoch 471/1000, Training Loss (NLML): -959.9569\n",
      "convergence dfGPdfNN Run 5/10, Epoch 472/1000, Training Loss (NLML): -959.9642\n",
      "convergence dfGPdfNN Run 5/10, Epoch 473/1000, Training Loss (NLML): -959.9655\n",
      "convergence dfGPdfNN Run 5/10, Epoch 474/1000, Training Loss (NLML): -959.9672\n",
      "convergence dfGPdfNN Run 5/10, Epoch 475/1000, Training Loss (NLML): -959.9773\n",
      "convergence dfGPdfNN Run 5/10, Epoch 476/1000, Training Loss (NLML): -959.9913\n",
      "convergence dfGPdfNN Run 5/10, Epoch 477/1000, Training Loss (NLML): -959.9969\n",
      "convergence dfGPdfNN Run 5/10, Epoch 478/1000, Training Loss (NLML): -960.0176\n",
      "convergence dfGPdfNN Run 5/10, Epoch 479/1000, Training Loss (NLML): -960.0204\n",
      "convergence dfGPdfNN Run 5/10, Epoch 480/1000, Training Loss (NLML): -960.0243\n",
      "convergence dfGPdfNN Run 5/10, Epoch 481/1000, Training Loss (NLML): -960.0211\n",
      "convergence dfGPdfNN Run 5/10, Epoch 482/1000, Training Loss (NLML): -960.0258\n",
      "convergence dfGPdfNN Run 5/10, Epoch 483/1000, Training Loss (NLML): -960.0286\n",
      "convergence dfGPdfNN Run 5/10, Epoch 484/1000, Training Loss (NLML): -960.0358\n",
      "convergence dfGPdfNN Run 5/10, Epoch 485/1000, Training Loss (NLML): -960.0399\n",
      "convergence dfGPdfNN Run 5/10, Epoch 486/1000, Training Loss (NLML): -960.0363\n",
      "convergence dfGPdfNN Run 5/10, Epoch 487/1000, Training Loss (NLML): -960.0503\n",
      "convergence dfGPdfNN Run 5/10, Epoch 488/1000, Training Loss (NLML): -960.0503\n",
      "convergence dfGPdfNN Run 5/10, Epoch 489/1000, Training Loss (NLML): -960.0537\n",
      "convergence dfGPdfNN Run 5/10, Epoch 490/1000, Training Loss (NLML): -960.0580\n",
      "convergence dfGPdfNN Run 5/10, Epoch 491/1000, Training Loss (NLML): -960.0630\n",
      "convergence dfGPdfNN Run 5/10, Epoch 492/1000, Training Loss (NLML): -960.0653\n",
      "convergence dfGPdfNN Run 5/10, Epoch 493/1000, Training Loss (NLML): -960.0663\n",
      "convergence dfGPdfNN Run 5/10, Epoch 494/1000, Training Loss (NLML): -960.0715\n",
      "convergence dfGPdfNN Run 5/10, Epoch 495/1000, Training Loss (NLML): -960.0791\n",
      "convergence dfGPdfNN Run 5/10, Epoch 496/1000, Training Loss (NLML): -960.0815\n",
      "convergence dfGPdfNN Run 5/10, Epoch 497/1000, Training Loss (NLML): -960.1219\n",
      "convergence dfGPdfNN Run 5/10, Epoch 498/1000, Training Loss (NLML): -960.1210\n",
      "convergence dfGPdfNN Run 5/10, Epoch 499/1000, Training Loss (NLML): -960.1348\n",
      "convergence dfGPdfNN Run 5/10, Epoch 500/1000, Training Loss (NLML): -960.1422\n",
      "convergence dfGPdfNN Run 5/10, Epoch 501/1000, Training Loss (NLML): -960.1401\n",
      "convergence dfGPdfNN Run 5/10, Epoch 502/1000, Training Loss (NLML): -960.1454\n",
      "convergence dfGPdfNN Run 5/10, Epoch 503/1000, Training Loss (NLML): -960.1375\n",
      "convergence dfGPdfNN Run 5/10, Epoch 504/1000, Training Loss (NLML): -960.1412\n",
      "convergence dfGPdfNN Run 5/10, Epoch 505/1000, Training Loss (NLML): -960.1461\n",
      "convergence dfGPdfNN Run 5/10, Epoch 506/1000, Training Loss (NLML): -960.1520\n",
      "convergence dfGPdfNN Run 5/10, Epoch 507/1000, Training Loss (NLML): -960.1583\n",
      "convergence dfGPdfNN Run 5/10, Epoch 508/1000, Training Loss (NLML): -960.1794\n",
      "convergence dfGPdfNN Run 5/10, Epoch 509/1000, Training Loss (NLML): -960.1863\n",
      "convergence dfGPdfNN Run 5/10, Epoch 510/1000, Training Loss (NLML): -960.1942\n",
      "convergence dfGPdfNN Run 5/10, Epoch 511/1000, Training Loss (NLML): -960.2034\n",
      "convergence dfGPdfNN Run 5/10, Epoch 512/1000, Training Loss (NLML): -960.2118\n",
      "convergence dfGPdfNN Run 5/10, Epoch 513/1000, Training Loss (NLML): -960.2178\n",
      "convergence dfGPdfNN Run 5/10, Epoch 514/1000, Training Loss (NLML): -960.2034\n",
      "convergence dfGPdfNN Run 5/10, Epoch 515/1000, Training Loss (NLML): -960.1957\n",
      "convergence dfGPdfNN Run 5/10, Epoch 516/1000, Training Loss (NLML): -960.2095\n",
      "convergence dfGPdfNN Run 5/10, Epoch 517/1000, Training Loss (NLML): -960.2220\n",
      "convergence dfGPdfNN Run 5/10, Epoch 518/1000, Training Loss (NLML): -960.2194\n",
      "convergence dfGPdfNN Run 5/10, Epoch 519/1000, Training Loss (NLML): -960.2181\n",
      "convergence dfGPdfNN Run 5/10, Epoch 520/1000, Training Loss (NLML): -960.1936\n",
      "convergence dfGPdfNN Run 5/10, Epoch 521/1000, Training Loss (NLML): -960.2477\n",
      "convergence dfGPdfNN Run 5/10, Epoch 522/1000, Training Loss (NLML): -960.2520\n",
      "convergence dfGPdfNN Run 5/10, Epoch 523/1000, Training Loss (NLML): -960.2579\n",
      "convergence dfGPdfNN Run 5/10, Epoch 524/1000, Training Loss (NLML): -960.2644\n",
      "convergence dfGPdfNN Run 5/10, Epoch 525/1000, Training Loss (NLML): -960.2717\n",
      "convergence dfGPdfNN Run 5/10, Epoch 526/1000, Training Loss (NLML): -960.2765\n",
      "convergence dfGPdfNN Run 5/10, Epoch 527/1000, Training Loss (NLML): -960.2842\n",
      "convergence dfGPdfNN Run 5/10, Epoch 528/1000, Training Loss (NLML): -960.2911\n",
      "convergence dfGPdfNN Run 5/10, Epoch 529/1000, Training Loss (NLML): -960.2950\n",
      "convergence dfGPdfNN Run 5/10, Epoch 530/1000, Training Loss (NLML): -960.2987\n",
      "convergence dfGPdfNN Run 5/10, Epoch 531/1000, Training Loss (NLML): -960.3027\n",
      "convergence dfGPdfNN Run 5/10, Epoch 532/1000, Training Loss (NLML): -960.2983\n",
      "convergence dfGPdfNN Run 5/10, Epoch 533/1000, Training Loss (NLML): -960.3036\n",
      "convergence dfGPdfNN Run 5/10, Epoch 534/1000, Training Loss (NLML): -960.3081\n",
      "convergence dfGPdfNN Run 5/10, Epoch 535/1000, Training Loss (NLML): -960.3114\n",
      "convergence dfGPdfNN Run 5/10, Epoch 536/1000, Training Loss (NLML): -960.3156\n",
      "convergence dfGPdfNN Run 5/10, Epoch 537/1000, Training Loss (NLML): -960.3198\n",
      "convergence dfGPdfNN Run 5/10, Epoch 538/1000, Training Loss (NLML): -960.3232\n",
      "convergence dfGPdfNN Run 5/10, Epoch 539/1000, Training Loss (NLML): -960.3059\n",
      "convergence dfGPdfNN Run 5/10, Epoch 540/1000, Training Loss (NLML): -960.3104\n",
      "convergence dfGPdfNN Run 5/10, Epoch 541/1000, Training Loss (NLML): -960.3041\n",
      "convergence dfGPdfNN Run 5/10, Epoch 542/1000, Training Loss (NLML): -960.3083\n",
      "convergence dfGPdfNN Run 5/10, Epoch 543/1000, Training Loss (NLML): -960.3348\n",
      "convergence dfGPdfNN Run 5/10, Epoch 544/1000, Training Loss (NLML): -960.3339\n",
      "convergence dfGPdfNN Run 5/10, Epoch 545/1000, Training Loss (NLML): -960.3395\n",
      "convergence dfGPdfNN Run 5/10, Epoch 546/1000, Training Loss (NLML): -960.3409\n",
      "convergence dfGPdfNN Run 5/10, Epoch 547/1000, Training Loss (NLML): -960.3459\n",
      "convergence dfGPdfNN Run 5/10, Epoch 548/1000, Training Loss (NLML): -960.3458\n",
      "convergence dfGPdfNN Run 5/10, Epoch 549/1000, Training Loss (NLML): -960.3475\n",
      "convergence dfGPdfNN Run 5/10, Epoch 550/1000, Training Loss (NLML): -960.3534\n",
      "convergence dfGPdfNN Run 5/10, Epoch 551/1000, Training Loss (NLML): -960.3661\n",
      "convergence dfGPdfNN Run 5/10, Epoch 552/1000, Training Loss (NLML): -960.3682\n",
      "convergence dfGPdfNN Run 5/10, Epoch 553/1000, Training Loss (NLML): -960.3727\n",
      "convergence dfGPdfNN Run 5/10, Epoch 554/1000, Training Loss (NLML): -960.3805\n",
      "convergence dfGPdfNN Run 5/10, Epoch 555/1000, Training Loss (NLML): -960.3846\n",
      "convergence dfGPdfNN Run 5/10, Epoch 556/1000, Training Loss (NLML): -960.3875\n",
      "convergence dfGPdfNN Run 5/10, Epoch 557/1000, Training Loss (NLML): -960.3910\n",
      "convergence dfGPdfNN Run 5/10, Epoch 558/1000, Training Loss (NLML): -960.3940\n",
      "convergence dfGPdfNN Run 5/10, Epoch 559/1000, Training Loss (NLML): -960.3965\n",
      "convergence dfGPdfNN Run 5/10, Epoch 560/1000, Training Loss (NLML): -960.3995\n",
      "convergence dfGPdfNN Run 5/10, Epoch 561/1000, Training Loss (NLML): -960.4037\n",
      "convergence dfGPdfNN Run 5/10, Epoch 562/1000, Training Loss (NLML): -960.4073\n",
      "convergence dfGPdfNN Run 5/10, Epoch 563/1000, Training Loss (NLML): -960.4114\n",
      "convergence dfGPdfNN Run 5/10, Epoch 564/1000, Training Loss (NLML): -960.4137\n",
      "convergence dfGPdfNN Run 5/10, Epoch 565/1000, Training Loss (NLML): -960.4170\n",
      "convergence dfGPdfNN Run 5/10, Epoch 566/1000, Training Loss (NLML): -960.4192\n",
      "convergence dfGPdfNN Run 5/10, Epoch 567/1000, Training Loss (NLML): -960.4225\n",
      "convergence dfGPdfNN Run 5/10, Epoch 568/1000, Training Loss (NLML): -960.4250\n",
      "convergence dfGPdfNN Run 5/10, Epoch 569/1000, Training Loss (NLML): -960.4294\n",
      "convergence dfGPdfNN Run 5/10, Epoch 570/1000, Training Loss (NLML): -960.4310\n",
      "convergence dfGPdfNN Run 5/10, Epoch 571/1000, Training Loss (NLML): -960.4351\n",
      "convergence dfGPdfNN Run 5/10, Epoch 572/1000, Training Loss (NLML): -960.4392\n",
      "convergence dfGPdfNN Run 5/10, Epoch 573/1000, Training Loss (NLML): -960.4429\n",
      "convergence dfGPdfNN Run 5/10, Epoch 574/1000, Training Loss (NLML): -960.4447\n",
      "convergence dfGPdfNN Run 5/10, Epoch 575/1000, Training Loss (NLML): -960.4475\n",
      "convergence dfGPdfNN Run 5/10, Epoch 576/1000, Training Loss (NLML): -960.4510\n",
      "convergence dfGPdfNN Run 5/10, Epoch 577/1000, Training Loss (NLML): -960.4541\n",
      "convergence dfGPdfNN Run 5/10, Epoch 578/1000, Training Loss (NLML): -960.4574\n",
      "convergence dfGPdfNN Run 5/10, Epoch 579/1000, Training Loss (NLML): -960.4619\n",
      "convergence dfGPdfNN Run 5/10, Epoch 580/1000, Training Loss (NLML): -960.4633\n",
      "convergence dfGPdfNN Run 5/10, Epoch 581/1000, Training Loss (NLML): -960.4652\n",
      "convergence dfGPdfNN Run 5/10, Epoch 582/1000, Training Loss (NLML): -960.4694\n",
      "convergence dfGPdfNN Run 5/10, Epoch 583/1000, Training Loss (NLML): -960.4731\n",
      "convergence dfGPdfNN Run 5/10, Epoch 584/1000, Training Loss (NLML): -960.4761\n",
      "convergence dfGPdfNN Run 5/10, Epoch 585/1000, Training Loss (NLML): -960.4789\n",
      "convergence dfGPdfNN Run 5/10, Epoch 586/1000, Training Loss (NLML): -960.4808\n",
      "convergence dfGPdfNN Run 5/10, Epoch 587/1000, Training Loss (NLML): -960.4846\n",
      "convergence dfGPdfNN Run 5/10, Epoch 588/1000, Training Loss (NLML): -960.4878\n",
      "convergence dfGPdfNN Run 5/10, Epoch 589/1000, Training Loss (NLML): -960.4899\n",
      "convergence dfGPdfNN Run 5/10, Epoch 590/1000, Training Loss (NLML): -960.4927\n",
      "convergence dfGPdfNN Run 5/10, Epoch 591/1000, Training Loss (NLML): -960.4952\n",
      "convergence dfGPdfNN Run 5/10, Epoch 592/1000, Training Loss (NLML): -960.4983\n",
      "convergence dfGPdfNN Run 5/10, Epoch 593/1000, Training Loss (NLML): -960.5018\n",
      "convergence dfGPdfNN Run 5/10, Epoch 594/1000, Training Loss (NLML): -960.5055\n",
      "convergence dfGPdfNN Run 5/10, Epoch 595/1000, Training Loss (NLML): -960.5073\n",
      "convergence dfGPdfNN Run 5/10, Epoch 596/1000, Training Loss (NLML): -960.5100\n",
      "convergence dfGPdfNN Run 5/10, Epoch 597/1000, Training Loss (NLML): -960.5138\n",
      "convergence dfGPdfNN Run 5/10, Epoch 598/1000, Training Loss (NLML): -960.5155\n",
      "convergence dfGPdfNN Run 5/10, Epoch 599/1000, Training Loss (NLML): -960.5192\n",
      "convergence dfGPdfNN Run 5/10, Epoch 600/1000, Training Loss (NLML): -960.5208\n",
      "convergence dfGPdfNN Run 5/10, Epoch 601/1000, Training Loss (NLML): -960.5258\n",
      "convergence dfGPdfNN Run 5/10, Epoch 602/1000, Training Loss (NLML): -960.5280\n",
      "convergence dfGPdfNN Run 5/10, Epoch 603/1000, Training Loss (NLML): -960.5300\n",
      "convergence dfGPdfNN Run 5/10, Epoch 604/1000, Training Loss (NLML): -960.5337\n",
      "convergence dfGPdfNN Run 5/10, Epoch 605/1000, Training Loss (NLML): -960.5359\n",
      "convergence dfGPdfNN Run 5/10, Epoch 606/1000, Training Loss (NLML): -960.5380\n",
      "convergence dfGPdfNN Run 5/10, Epoch 607/1000, Training Loss (NLML): -960.5413\n",
      "convergence dfGPdfNN Run 5/10, Epoch 608/1000, Training Loss (NLML): -960.5436\n",
      "convergence dfGPdfNN Run 5/10, Epoch 609/1000, Training Loss (NLML): -960.5480\n",
      "convergence dfGPdfNN Run 5/10, Epoch 610/1000, Training Loss (NLML): -960.5505\n",
      "convergence dfGPdfNN Run 5/10, Epoch 611/1000, Training Loss (NLML): -960.5532\n",
      "convergence dfGPdfNN Run 5/10, Epoch 612/1000, Training Loss (NLML): -960.5551\n",
      "convergence dfGPdfNN Run 5/10, Epoch 613/1000, Training Loss (NLML): -960.5593\n",
      "convergence dfGPdfNN Run 5/10, Epoch 614/1000, Training Loss (NLML): -960.5630\n",
      "convergence dfGPdfNN Run 5/10, Epoch 615/1000, Training Loss (NLML): -960.5656\n",
      "convergence dfGPdfNN Run 5/10, Epoch 616/1000, Training Loss (NLML): -960.5687\n",
      "convergence dfGPdfNN Run 5/10, Epoch 617/1000, Training Loss (NLML): -960.5713\n",
      "convergence dfGPdfNN Run 5/10, Epoch 618/1000, Training Loss (NLML): -960.5751\n",
      "convergence dfGPdfNN Run 5/10, Epoch 619/1000, Training Loss (NLML): -960.5763\n",
      "convergence dfGPdfNN Run 5/10, Epoch 620/1000, Training Loss (NLML): -960.5785\n",
      "convergence dfGPdfNN Run 5/10, Epoch 621/1000, Training Loss (NLML): -960.5792\n",
      "convergence dfGPdfNN Run 5/10, Epoch 622/1000, Training Loss (NLML): -960.5835\n",
      "convergence dfGPdfNN Run 5/10, Epoch 623/1000, Training Loss (NLML): -960.5856\n",
      "convergence dfGPdfNN Run 5/10, Epoch 624/1000, Training Loss (NLML): -960.5980\n",
      "convergence dfGPdfNN Run 5/10, Epoch 625/1000, Training Loss (NLML): -960.6006\n",
      "convergence dfGPdfNN Run 5/10, Epoch 626/1000, Training Loss (NLML): -960.6041\n",
      "convergence dfGPdfNN Run 5/10, Epoch 627/1000, Training Loss (NLML): -960.5978\n",
      "convergence dfGPdfNN Run 5/10, Epoch 628/1000, Training Loss (NLML): -960.6005\n",
      "convergence dfGPdfNN Run 5/10, Epoch 629/1000, Training Loss (NLML): -960.6041\n",
      "convergence dfGPdfNN Run 5/10, Epoch 630/1000, Training Loss (NLML): -960.6052\n",
      "convergence dfGPdfNN Run 5/10, Epoch 631/1000, Training Loss (NLML): -960.6063\n",
      "convergence dfGPdfNN Run 5/10, Epoch 632/1000, Training Loss (NLML): -960.6093\n",
      "convergence dfGPdfNN Run 5/10, Epoch 633/1000, Training Loss (NLML): -960.6124\n",
      "convergence dfGPdfNN Run 5/10, Epoch 634/1000, Training Loss (NLML): -960.6160\n",
      "convergence dfGPdfNN Run 5/10, Epoch 635/1000, Training Loss (NLML): -960.6195\n",
      "convergence dfGPdfNN Run 5/10, Epoch 636/1000, Training Loss (NLML): -960.6230\n",
      "convergence dfGPdfNN Run 5/10, Epoch 637/1000, Training Loss (NLML): -960.6255\n",
      "convergence dfGPdfNN Run 5/10, Epoch 638/1000, Training Loss (NLML): -960.6290\n",
      "convergence dfGPdfNN Run 5/10, Epoch 639/1000, Training Loss (NLML): -960.6302\n",
      "convergence dfGPdfNN Run 5/10, Epoch 640/1000, Training Loss (NLML): -960.6310\n",
      "convergence dfGPdfNN Run 5/10, Epoch 641/1000, Training Loss (NLML): -960.6346\n",
      "convergence dfGPdfNN Run 5/10, Epoch 642/1000, Training Loss (NLML): -960.6389\n",
      "convergence dfGPdfNN Run 5/10, Epoch 643/1000, Training Loss (NLML): -960.6409\n",
      "convergence dfGPdfNN Run 5/10, Epoch 644/1000, Training Loss (NLML): -960.6426\n",
      "convergence dfGPdfNN Run 5/10, Epoch 645/1000, Training Loss (NLML): -960.6448\n",
      "convergence dfGPdfNN Run 5/10, Epoch 646/1000, Training Loss (NLML): -960.6471\n",
      "convergence dfGPdfNN Run 5/10, Epoch 647/1000, Training Loss (NLML): -960.6498\n",
      "convergence dfGPdfNN Run 5/10, Epoch 648/1000, Training Loss (NLML): -960.6547\n",
      "convergence dfGPdfNN Run 5/10, Epoch 649/1000, Training Loss (NLML): -960.6564\n",
      "convergence dfGPdfNN Run 5/10, Epoch 650/1000, Training Loss (NLML): -960.6520\n",
      "convergence dfGPdfNN Run 5/10, Epoch 651/1000, Training Loss (NLML): -960.6573\n",
      "convergence dfGPdfNN Run 5/10, Epoch 652/1000, Training Loss (NLML): -960.6603\n",
      "convergence dfGPdfNN Run 5/10, Epoch 653/1000, Training Loss (NLML): -960.6606\n",
      "convergence dfGPdfNN Run 5/10, Epoch 654/1000, Training Loss (NLML): -960.6725\n",
      "convergence dfGPdfNN Run 5/10, Epoch 655/1000, Training Loss (NLML): -960.6765\n",
      "convergence dfGPdfNN Run 5/10, Epoch 656/1000, Training Loss (NLML): -960.6692\n",
      "convergence dfGPdfNN Run 5/10, Epoch 657/1000, Training Loss (NLML): -960.6731\n",
      "convergence dfGPdfNN Run 5/10, Epoch 658/1000, Training Loss (NLML): -960.6747\n",
      "convergence dfGPdfNN Run 5/10, Epoch 659/1000, Training Loss (NLML): -960.6775\n",
      "convergence dfGPdfNN Run 5/10, Epoch 660/1000, Training Loss (NLML): -960.6815\n",
      "convergence dfGPdfNN Run 5/10, Epoch 661/1000, Training Loss (NLML): -960.6833\n",
      "convergence dfGPdfNN Run 5/10, Epoch 662/1000, Training Loss (NLML): -960.6855\n",
      "convergence dfGPdfNN Run 5/10, Epoch 663/1000, Training Loss (NLML): -960.6852\n",
      "convergence dfGPdfNN Run 5/10, Epoch 664/1000, Training Loss (NLML): -960.6887\n",
      "convergence dfGPdfNN Run 5/10, Epoch 665/1000, Training Loss (NLML): -960.6907\n",
      "convergence dfGPdfNN Run 5/10, Epoch 666/1000, Training Loss (NLML): -960.6932\n",
      "convergence dfGPdfNN Run 5/10, Epoch 667/1000, Training Loss (NLML): -960.6964\n",
      "convergence dfGPdfNN Run 5/10, Epoch 668/1000, Training Loss (NLML): -960.6992\n",
      "convergence dfGPdfNN Run 5/10, Epoch 669/1000, Training Loss (NLML): -960.7017\n",
      "convergence dfGPdfNN Run 5/10, Epoch 670/1000, Training Loss (NLML): -960.7034\n",
      "convergence dfGPdfNN Run 5/10, Epoch 671/1000, Training Loss (NLML): -960.7152\n",
      "convergence dfGPdfNN Run 5/10, Epoch 672/1000, Training Loss (NLML): -960.7188\n",
      "convergence dfGPdfNN Run 5/10, Epoch 673/1000, Training Loss (NLML): -960.7104\n",
      "convergence dfGPdfNN Run 5/10, Epoch 674/1000, Training Loss (NLML): -960.7147\n",
      "convergence dfGPdfNN Run 5/10, Epoch 675/1000, Training Loss (NLML): -960.7150\n",
      "convergence dfGPdfNN Run 5/10, Epoch 676/1000, Training Loss (NLML): -960.7186\n",
      "convergence dfGPdfNN Run 5/10, Epoch 677/1000, Training Loss (NLML): -960.7201\n",
      "convergence dfGPdfNN Run 5/10, Epoch 678/1000, Training Loss (NLML): -960.7250\n",
      "convergence dfGPdfNN Run 5/10, Epoch 679/1000, Training Loss (NLML): -960.7251\n",
      "convergence dfGPdfNN Run 5/10, Epoch 680/1000, Training Loss (NLML): -960.7294\n",
      "convergence dfGPdfNN Run 5/10, Epoch 681/1000, Training Loss (NLML): -960.7301\n",
      "convergence dfGPdfNN Run 5/10, Epoch 682/1000, Training Loss (NLML): -960.7419\n",
      "convergence dfGPdfNN Run 5/10, Epoch 683/1000, Training Loss (NLML): -960.7357\n",
      "convergence dfGPdfNN Run 5/10, Epoch 684/1000, Training Loss (NLML): -960.7369\n",
      "convergence dfGPdfNN Run 5/10, Epoch 685/1000, Training Loss (NLML): -960.7408\n",
      "convergence dfGPdfNN Run 5/10, Epoch 686/1000, Training Loss (NLML): -960.7434\n",
      "convergence dfGPdfNN Run 5/10, Epoch 687/1000, Training Loss (NLML): -960.7462\n",
      "convergence dfGPdfNN Run 5/10, Epoch 688/1000, Training Loss (NLML): -960.7567\n",
      "convergence dfGPdfNN Run 5/10, Epoch 689/1000, Training Loss (NLML): -960.7494\n",
      "convergence dfGPdfNN Run 5/10, Epoch 690/1000, Training Loss (NLML): -960.7522\n",
      "convergence dfGPdfNN Run 5/10, Epoch 691/1000, Training Loss (NLML): -960.7554\n",
      "convergence dfGPdfNN Run 5/10, Epoch 692/1000, Training Loss (NLML): -960.7571\n",
      "convergence dfGPdfNN Run 5/10, Epoch 693/1000, Training Loss (NLML): -960.7573\n",
      "convergence dfGPdfNN Run 5/10, Epoch 694/1000, Training Loss (NLML): -960.7622\n",
      "convergence dfGPdfNN Run 5/10, Epoch 695/1000, Training Loss (NLML): -960.7721\n",
      "convergence dfGPdfNN Run 5/10, Epoch 696/1000, Training Loss (NLML): -960.7740\n",
      "convergence dfGPdfNN Run 5/10, Epoch 697/1000, Training Loss (NLML): -960.7694\n",
      "convergence dfGPdfNN Run 5/10, Epoch 698/1000, Training Loss (NLML): -960.7708\n",
      "convergence dfGPdfNN Run 5/10, Epoch 699/1000, Training Loss (NLML): -960.7727\n",
      "convergence dfGPdfNN Run 5/10, Epoch 700/1000, Training Loss (NLML): -960.7764\n",
      "convergence dfGPdfNN Run 5/10, Epoch 701/1000, Training Loss (NLML): -960.7794\n",
      "convergence dfGPdfNN Run 5/10, Epoch 702/1000, Training Loss (NLML): -960.7802\n",
      "convergence dfGPdfNN Run 5/10, Epoch 703/1000, Training Loss (NLML): -960.7897\n",
      "convergence dfGPdfNN Run 5/10, Epoch 704/1000, Training Loss (NLML): -960.7914\n",
      "convergence dfGPdfNN Run 5/10, Epoch 705/1000, Training Loss (NLML): -960.7850\n",
      "convergence dfGPdfNN Run 5/10, Epoch 706/1000, Training Loss (NLML): -960.7889\n",
      "convergence dfGPdfNN Run 5/10, Epoch 707/1000, Training Loss (NLML): -960.7894\n",
      "convergence dfGPdfNN Run 5/10, Epoch 708/1000, Training Loss (NLML): -960.7933\n",
      "convergence dfGPdfNN Run 5/10, Epoch 709/1000, Training Loss (NLML): -960.7952\n",
      "convergence dfGPdfNN Run 5/10, Epoch 710/1000, Training Loss (NLML): -960.7966\n",
      "convergence dfGPdfNN Run 5/10, Epoch 711/1000, Training Loss (NLML): -960.8063\n",
      "convergence dfGPdfNN Run 5/10, Epoch 712/1000, Training Loss (NLML): -960.8102\n",
      "convergence dfGPdfNN Run 5/10, Epoch 713/1000, Training Loss (NLML): -960.8044\n",
      "convergence dfGPdfNN Run 5/10, Epoch 714/1000, Training Loss (NLML): -960.8065\n",
      "convergence dfGPdfNN Run 5/10, Epoch 715/1000, Training Loss (NLML): -960.8094\n",
      "convergence dfGPdfNN Run 5/10, Epoch 716/1000, Training Loss (NLML): -960.8113\n",
      "convergence dfGPdfNN Run 5/10, Epoch 717/1000, Training Loss (NLML): -960.8118\n",
      "convergence dfGPdfNN Run 5/10, Epoch 718/1000, Training Loss (NLML): -960.8237\n",
      "convergence dfGPdfNN Run 5/10, Epoch 719/1000, Training Loss (NLML): -960.8253\n",
      "convergence dfGPdfNN Run 5/10, Epoch 720/1000, Training Loss (NLML): -960.8171\n",
      "convergence dfGPdfNN Run 5/10, Epoch 721/1000, Training Loss (NLML): -960.8234\n",
      "convergence dfGPdfNN Run 5/10, Epoch 722/1000, Training Loss (NLML): -960.8239\n",
      "convergence dfGPdfNN Run 5/10, Epoch 723/1000, Training Loss (NLML): -960.8262\n",
      "convergence dfGPdfNN Run 5/10, Epoch 724/1000, Training Loss (NLML): -960.8369\n",
      "convergence dfGPdfNN Run 5/10, Epoch 725/1000, Training Loss (NLML): -960.8392\n",
      "convergence dfGPdfNN Run 5/10, Epoch 726/1000, Training Loss (NLML): -960.8336\n",
      "convergence dfGPdfNN Run 5/10, Epoch 727/1000, Training Loss (NLML): -960.8353\n",
      "convergence dfGPdfNN Run 5/10, Epoch 728/1000, Training Loss (NLML): -960.8390\n",
      "convergence dfGPdfNN Run 5/10, Epoch 729/1000, Training Loss (NLML): -960.8390\n",
      "convergence dfGPdfNN Run 5/10, Epoch 730/1000, Training Loss (NLML): -960.8505\n",
      "convergence dfGPdfNN Run 5/10, Epoch 731/1000, Training Loss (NLML): -960.8513\n",
      "convergence dfGPdfNN Run 5/10, Epoch 732/1000, Training Loss (NLML): -960.8481\n",
      "convergence dfGPdfNN Run 5/10, Epoch 733/1000, Training Loss (NLML): -960.8474\n",
      "convergence dfGPdfNN Run 5/10, Epoch 734/1000, Training Loss (NLML): -960.8446\n",
      "convergence dfGPdfNN Run 5/10, Epoch 735/1000, Training Loss (NLML): -960.8544\n",
      "convergence dfGPdfNN Run 5/10, Epoch 736/1000, Training Loss (NLML): -960.8510\n",
      "convergence dfGPdfNN Run 5/10, Epoch 737/1000, Training Loss (NLML): -960.8538\n",
      "convergence dfGPdfNN Run 5/10, Epoch 738/1000, Training Loss (NLML): -960.8654\n",
      "convergence dfGPdfNN Run 5/10, Epoch 739/1000, Training Loss (NLML): -960.8666\n",
      "convergence dfGPdfNN Run 5/10, Epoch 740/1000, Training Loss (NLML): -960.8682\n",
      "convergence dfGPdfNN Run 5/10, Epoch 741/1000, Training Loss (NLML): -960.8656\n",
      "convergence dfGPdfNN Run 5/10, Epoch 742/1000, Training Loss (NLML): -960.8723\n",
      "convergence dfGPdfNN Run 5/10, Epoch 743/1000, Training Loss (NLML): -960.8724\n",
      "convergence dfGPdfNN Run 5/10, Epoch 744/1000, Training Loss (NLML): -960.8732\n",
      "convergence dfGPdfNN Run 5/10, Epoch 745/1000, Training Loss (NLML): -960.8749\n",
      "convergence dfGPdfNN Run 5/10, Epoch 746/1000, Training Loss (NLML): -960.8765\n",
      "convergence dfGPdfNN Run 5/10, Epoch 747/1000, Training Loss (NLML): -960.8790\n",
      "convergence dfGPdfNN Run 5/10, Epoch 748/1000, Training Loss (NLML): -960.8827\n",
      "convergence dfGPdfNN Run 5/10, Epoch 749/1000, Training Loss (NLML): -960.8843\n",
      "convergence dfGPdfNN Run 5/10, Epoch 750/1000, Training Loss (NLML): -960.8859\n",
      "convergence dfGPdfNN Run 5/10, Epoch 751/1000, Training Loss (NLML): -960.8940\n",
      "convergence dfGPdfNN Run 5/10, Epoch 752/1000, Training Loss (NLML): -960.8993\n",
      "convergence dfGPdfNN Run 5/10, Epoch 753/1000, Training Loss (NLML): -960.9041\n",
      "convergence dfGPdfNN Run 5/10, Epoch 754/1000, Training Loss (NLML): -960.8934\n",
      "convergence dfGPdfNN Run 5/10, Epoch 755/1000, Training Loss (NLML): -960.8938\n",
      "convergence dfGPdfNN Run 5/10, Epoch 756/1000, Training Loss (NLML): -960.8983\n",
      "convergence dfGPdfNN Run 5/10, Epoch 757/1000, Training Loss (NLML): -960.9021\n",
      "convergence dfGPdfNN Run 5/10, Epoch 758/1000, Training Loss (NLML): -960.9058\n",
      "convergence dfGPdfNN Run 5/10, Epoch 759/1000, Training Loss (NLML): -960.9039\n",
      "convergence dfGPdfNN Run 5/10, Epoch 760/1000, Training Loss (NLML): -960.9067\n",
      "convergence dfGPdfNN Run 5/10, Epoch 761/1000, Training Loss (NLML): -960.9095\n",
      "convergence dfGPdfNN Run 5/10, Epoch 762/1000, Training Loss (NLML): -960.9087\n",
      "convergence dfGPdfNN Run 5/10, Epoch 763/1000, Training Loss (NLML): -960.9193\n",
      "convergence dfGPdfNN Run 5/10, Epoch 764/1000, Training Loss (NLML): -960.9222\n",
      "convergence dfGPdfNN Run 5/10, Epoch 765/1000, Training Loss (NLML): -960.9238\n",
      "convergence dfGPdfNN Run 5/10, Epoch 766/1000, Training Loss (NLML): -960.9170\n",
      "convergence dfGPdfNN Run 5/10, Epoch 767/1000, Training Loss (NLML): -960.9143\n",
      "convergence dfGPdfNN Run 5/10, Epoch 768/1000, Training Loss (NLML): -960.9207\n",
      "convergence dfGPdfNN Run 5/10, Epoch 769/1000, Training Loss (NLML): -960.9307\n",
      "convergence dfGPdfNN Run 5/10, Epoch 770/1000, Training Loss (NLML): -960.9353\n",
      "convergence dfGPdfNN Run 5/10, Epoch 771/1000, Training Loss (NLML): -960.9292\n",
      "convergence dfGPdfNN Run 5/10, Epoch 772/1000, Training Loss (NLML): -960.9309\n",
      "convergence dfGPdfNN Run 5/10, Epoch 773/1000, Training Loss (NLML): -960.9315\n",
      "convergence dfGPdfNN Run 5/10, Epoch 774/1000, Training Loss (NLML): -960.9323\n",
      "convergence dfGPdfNN Run 5/10, Epoch 775/1000, Training Loss (NLML): -960.9468\n",
      "convergence dfGPdfNN Run 5/10, Epoch 776/1000, Training Loss (NLML): -960.9373\n",
      "convergence dfGPdfNN Run 5/10, Epoch 777/1000, Training Loss (NLML): -960.9473\n",
      "convergence dfGPdfNN Run 5/10, Epoch 778/1000, Training Loss (NLML): -960.9410\n",
      "convergence dfGPdfNN Run 5/10, Epoch 779/1000, Training Loss (NLML): -960.9408\n",
      "convergence dfGPdfNN Run 5/10, Epoch 780/1000, Training Loss (NLML): -960.9528\n",
      "convergence dfGPdfNN Run 5/10, Epoch 781/1000, Training Loss (NLML): -960.9540\n",
      "convergence dfGPdfNN Run 5/10, Epoch 782/1000, Training Loss (NLML): -960.9496\n",
      "convergence dfGPdfNN Run 5/10, Epoch 783/1000, Training Loss (NLML): -960.9513\n",
      "convergence dfGPdfNN Run 5/10, Epoch 784/1000, Training Loss (NLML): -960.9532\n",
      "convergence dfGPdfNN Run 5/10, Epoch 785/1000, Training Loss (NLML): -960.9543\n",
      "convergence dfGPdfNN Run 5/10, Epoch 786/1000, Training Loss (NLML): -960.9553\n",
      "convergence dfGPdfNN Run 5/10, Epoch 787/1000, Training Loss (NLML): -960.9663\n",
      "convergence dfGPdfNN Run 5/10, Epoch 788/1000, Training Loss (NLML): -960.9673\n",
      "convergence dfGPdfNN Run 5/10, Epoch 789/1000, Training Loss (NLML): -960.9681\n",
      "convergence dfGPdfNN Run 5/10, Epoch 790/1000, Training Loss (NLML): -960.9656\n",
      "convergence dfGPdfNN Run 5/10, Epoch 791/1000, Training Loss (NLML): -960.9657\n",
      "convergence dfGPdfNN Run 5/10, Epoch 792/1000, Training Loss (NLML): -960.9673\n",
      "convergence dfGPdfNN Run 5/10, Epoch 793/1000, Training Loss (NLML): -960.9661\n",
      "convergence dfGPdfNN Run 5/10, Epoch 794/1000, Training Loss (NLML): -960.9761\n",
      "convergence dfGPdfNN Run 5/10, Epoch 795/1000, Training Loss (NLML): -960.9801\n",
      "convergence dfGPdfNN Run 5/10, Epoch 796/1000, Training Loss (NLML): -960.9835\n",
      "convergence dfGPdfNN Run 5/10, Epoch 797/1000, Training Loss (NLML): -960.9857\n",
      "convergence dfGPdfNN Run 5/10, Epoch 798/1000, Training Loss (NLML): -960.9778\n",
      "convergence dfGPdfNN Run 5/10, Epoch 799/1000, Training Loss (NLML): -960.9823\n",
      "convergence dfGPdfNN Run 5/10, Epoch 800/1000, Training Loss (NLML): -960.9825\n",
      "convergence dfGPdfNN Run 5/10, Epoch 801/1000, Training Loss (NLML): -960.9838\n",
      "convergence dfGPdfNN Run 5/10, Epoch 802/1000, Training Loss (NLML): -960.9950\n",
      "convergence dfGPdfNN Run 5/10, Epoch 803/1000, Training Loss (NLML): -960.9963\n",
      "convergence dfGPdfNN Run 5/10, Epoch 804/1000, Training Loss (NLML): -960.9985\n",
      "convergence dfGPdfNN Run 5/10, Epoch 805/1000, Training Loss (NLML): -960.9974\n",
      "convergence dfGPdfNN Run 5/10, Epoch 806/1000, Training Loss (NLML): -960.9940\n",
      "convergence dfGPdfNN Run 5/10, Epoch 807/1000, Training Loss (NLML): -961.0021\n",
      "convergence dfGPdfNN Run 5/10, Epoch 808/1000, Training Loss (NLML): -960.9985\n",
      "convergence dfGPdfNN Run 5/10, Epoch 809/1000, Training Loss (NLML): -961.0071\n",
      "convergence dfGPdfNN Run 5/10, Epoch 810/1000, Training Loss (NLML): -960.9993\n",
      "convergence dfGPdfNN Run 5/10, Epoch 811/1000, Training Loss (NLML): -961.0103\n",
      "convergence dfGPdfNN Run 5/10, Epoch 812/1000, Training Loss (NLML): -961.0121\n",
      "convergence dfGPdfNN Run 5/10, Epoch 813/1000, Training Loss (NLML): -961.0140\n",
      "convergence dfGPdfNN Run 5/10, Epoch 814/1000, Training Loss (NLML): -961.0164\n",
      "convergence dfGPdfNN Run 5/10, Epoch 815/1000, Training Loss (NLML): -961.0115\n",
      "convergence dfGPdfNN Run 5/10, Epoch 816/1000, Training Loss (NLML): -961.0192\n",
      "convergence dfGPdfNN Run 5/10, Epoch 817/1000, Training Loss (NLML): -961.0154\n",
      "convergence dfGPdfNN Run 5/10, Epoch 818/1000, Training Loss (NLML): -961.0146\n",
      "convergence dfGPdfNN Run 5/10, Epoch 819/1000, Training Loss (NLML): -961.0250\n",
      "convergence dfGPdfNN Run 5/10, Epoch 820/1000, Training Loss (NLML): -961.0256\n",
      "convergence dfGPdfNN Run 5/10, Epoch 821/1000, Training Loss (NLML): -961.0323\n",
      "convergence dfGPdfNN Run 5/10, Epoch 822/1000, Training Loss (NLML): -961.0204\n",
      "convergence dfGPdfNN Run 5/10, Epoch 823/1000, Training Loss (NLML): -961.0232\n",
      "convergence dfGPdfNN Run 5/10, Epoch 824/1000, Training Loss (NLML): -961.0333\n",
      "convergence dfGPdfNN Run 5/10, Epoch 825/1000, Training Loss (NLML): -961.0355\n",
      "convergence dfGPdfNN Run 5/10, Epoch 826/1000, Training Loss (NLML): -961.0293\n",
      "convergence dfGPdfNN Run 5/10, Epoch 827/1000, Training Loss (NLML): -961.0298\n",
      "convergence dfGPdfNN Run 5/10, Epoch 828/1000, Training Loss (NLML): -961.0414\n",
      "convergence dfGPdfNN Run 5/10, Epoch 829/1000, Training Loss (NLML): -961.0441\n",
      "convergence dfGPdfNN Run 5/10, Epoch 830/1000, Training Loss (NLML): -961.0435\n",
      "convergence dfGPdfNN Run 5/10, Epoch 831/1000, Training Loss (NLML): -961.0404\n",
      "convergence dfGPdfNN Run 5/10, Epoch 832/1000, Training Loss (NLML): -961.0471\n",
      "convergence dfGPdfNN Run 5/10, Epoch 833/1000, Training Loss (NLML): -961.0432\n",
      "convergence dfGPdfNN Run 5/10, Epoch 834/1000, Training Loss (NLML): -961.0508\n",
      "convergence dfGPdfNN Run 5/10, Epoch 835/1000, Training Loss (NLML): -961.0516\n",
      "convergence dfGPdfNN Run 5/10, Epoch 836/1000, Training Loss (NLML): -961.0483\n",
      "convergence dfGPdfNN Run 5/10, Epoch 837/1000, Training Loss (NLML): -961.0522\n",
      "convergence dfGPdfNN Run 5/10, Epoch 838/1000, Training Loss (NLML): -961.0631\n",
      "convergence dfGPdfNN Run 5/10, Epoch 839/1000, Training Loss (NLML): -961.0592\n",
      "convergence dfGPdfNN Run 5/10, Epoch 840/1000, Training Loss (NLML): -961.0605\n",
      "convergence dfGPdfNN Run 5/10, Epoch 841/1000, Training Loss (NLML): -961.0648\n",
      "convergence dfGPdfNN Run 5/10, Epoch 842/1000, Training Loss (NLML): -961.0658\n",
      "convergence dfGPdfNN Run 5/10, Epoch 843/1000, Training Loss (NLML): -961.0663\n",
      "convergence dfGPdfNN Run 5/10, Epoch 844/1000, Training Loss (NLML): -961.0607\n",
      "convergence dfGPdfNN Run 5/10, Epoch 845/1000, Training Loss (NLML): -961.0621\n",
      "convergence dfGPdfNN Run 5/10, Epoch 846/1000, Training Loss (NLML): -961.0709\n",
      "convergence dfGPdfNN Run 5/10, Epoch 847/1000, Training Loss (NLML): -961.0737\n",
      "convergence dfGPdfNN Run 5/10, Epoch 848/1000, Training Loss (NLML): -961.0819\n",
      "convergence dfGPdfNN Run 5/10, Epoch 849/1000, Training Loss (NLML): -961.0780\n",
      "convergence dfGPdfNN Run 5/10, Epoch 850/1000, Training Loss (NLML): -961.0806\n",
      "convergence dfGPdfNN Run 5/10, Epoch 851/1000, Training Loss (NLML): -961.0780\n",
      "convergence dfGPdfNN Run 5/10, Epoch 852/1000, Training Loss (NLML): -961.0730\n",
      "convergence dfGPdfNN Run 5/10, Epoch 853/1000, Training Loss (NLML): -961.0774\n",
      "convergence dfGPdfNN Run 5/10, Epoch 854/1000, Training Loss (NLML): -961.0830\n",
      "convergence dfGPdfNN Run 5/10, Epoch 855/1000, Training Loss (NLML): -961.0891\n",
      "convergence dfGPdfNN Run 5/10, Epoch 856/1000, Training Loss (NLML): -961.0878\n",
      "convergence dfGPdfNN Run 5/10, Epoch 857/1000, Training Loss (NLML): -961.0906\n",
      "convergence dfGPdfNN Run 5/10, Epoch 858/1000, Training Loss (NLML): -961.0930\n",
      "convergence dfGPdfNN Run 5/10, Epoch 859/1000, Training Loss (NLML): -961.1002\n",
      "convergence dfGPdfNN Run 5/10, Epoch 860/1000, Training Loss (NLML): -961.0975\n",
      "convergence dfGPdfNN Run 5/10, Epoch 861/1000, Training Loss (NLML): -961.0895\n",
      "convergence dfGPdfNN Run 5/10, Epoch 862/1000, Training Loss (NLML): -961.0911\n",
      "convergence dfGPdfNN Run 5/10, Epoch 863/1000, Training Loss (NLML): -961.1069\n",
      "convergence dfGPdfNN Run 5/10, Epoch 864/1000, Training Loss (NLML): -961.1021\n",
      "convergence dfGPdfNN Run 5/10, Epoch 865/1000, Training Loss (NLML): -961.1047\n",
      "convergence dfGPdfNN Run 5/10, Epoch 866/1000, Training Loss (NLML): -961.1062\n",
      "convergence dfGPdfNN Run 5/10, Epoch 867/1000, Training Loss (NLML): -961.1136\n",
      "convergence dfGPdfNN Run 5/10, Epoch 868/1000, Training Loss (NLML): -961.1024\n",
      "convergence dfGPdfNN Run 5/10, Epoch 869/1000, Training Loss (NLML): -961.1095\n",
      "convergence dfGPdfNN Run 5/10, Epoch 870/1000, Training Loss (NLML): -961.1125\n",
      "convergence dfGPdfNN Run 5/10, Epoch 871/1000, Training Loss (NLML): -961.1196\n",
      "convergence dfGPdfNN Run 5/10, Epoch 872/1000, Training Loss (NLML): -961.1061\n",
      "convergence dfGPdfNN Run 5/10, Epoch 873/1000, Training Loss (NLML): -961.1180\n",
      "convergence dfGPdfNN Run 5/10, Epoch 874/1000, Training Loss (NLML): -961.1187\n",
      "convergence dfGPdfNN Run 5/10, Epoch 875/1000, Training Loss (NLML): -961.1201\n",
      "convergence dfGPdfNN Run 5/10, Epoch 876/1000, Training Loss (NLML): -961.1224\n",
      "convergence dfGPdfNN Run 5/10, Epoch 877/1000, Training Loss (NLML): -961.1268\n",
      "convergence dfGPdfNN Run 5/10, Epoch 878/1000, Training Loss (NLML): -961.1259\n",
      "convergence dfGPdfNN Run 5/10, Epoch 879/1000, Training Loss (NLML): -961.1283\n",
      "convergence dfGPdfNN Run 5/10, Epoch 880/1000, Training Loss (NLML): -961.1287\n",
      "convergence dfGPdfNN Run 5/10, Epoch 881/1000, Training Loss (NLML): -961.1226\n",
      "convergence dfGPdfNN Run 5/10, Epoch 882/1000, Training Loss (NLML): -961.1250\n",
      "convergence dfGPdfNN Run 5/10, Epoch 883/1000, Training Loss (NLML): -961.1312\n",
      "convergence dfGPdfNN Run 5/10, Epoch 884/1000, Training Loss (NLML): -961.1338\n",
      "convergence dfGPdfNN Run 5/10, Epoch 885/1000, Training Loss (NLML): -961.1379\n",
      "convergence dfGPdfNN Run 5/10, Epoch 886/1000, Training Loss (NLML): -961.1360\n",
      "convergence dfGPdfNN Run 5/10, Epoch 887/1000, Training Loss (NLML): -961.1445\n",
      "convergence dfGPdfNN Run 5/10, Epoch 888/1000, Training Loss (NLML): -961.1416\n",
      "convergence dfGPdfNN Run 5/10, Epoch 889/1000, Training Loss (NLML): -961.1422\n",
      "convergence dfGPdfNN Run 5/10, Epoch 890/1000, Training Loss (NLML): -961.1515\n",
      "convergence dfGPdfNN Run 5/10, Epoch 891/1000, Training Loss (NLML): -961.1445\n",
      "convergence dfGPdfNN Run 5/10, Epoch 892/1000, Training Loss (NLML): -961.1519\n",
      "convergence dfGPdfNN Run 5/10, Epoch 893/1000, Training Loss (NLML): -961.1499\n",
      "convergence dfGPdfNN Run 5/10, Epoch 894/1000, Training Loss (NLML): -961.1425\n",
      "convergence dfGPdfNN Run 5/10, Epoch 895/1000, Training Loss (NLML): -961.1521\n",
      "convergence dfGPdfNN Run 5/10, Epoch 896/1000, Training Loss (NLML): -961.1578\n",
      "convergence dfGPdfNN Run 5/10, Epoch 897/1000, Training Loss (NLML): -961.1550\n",
      "convergence dfGPdfNN Run 5/10, Epoch 898/1000, Training Loss (NLML): -961.1584\n",
      "convergence dfGPdfNN Run 5/10, Epoch 899/1000, Training Loss (NLML): -961.1637\n",
      "convergence dfGPdfNN Run 5/10, Epoch 900/1000, Training Loss (NLML): -961.1636\n",
      "convergence dfGPdfNN Run 5/10, Epoch 901/1000, Training Loss (NLML): -961.1636\n",
      "convergence dfGPdfNN Run 5/10, Epoch 902/1000, Training Loss (NLML): -961.1658\n",
      "convergence dfGPdfNN Run 5/10, Epoch 903/1000, Training Loss (NLML): -961.1648\n",
      "convergence dfGPdfNN Run 5/10, Epoch 904/1000, Training Loss (NLML): -961.1711\n",
      "convergence dfGPdfNN Run 5/10, Epoch 905/1000, Training Loss (NLML): -961.1725\n",
      "convergence dfGPdfNN Run 5/10, Epoch 906/1000, Training Loss (NLML): -961.1672\n",
      "convergence dfGPdfNN Run 5/10, Epoch 907/1000, Training Loss (NLML): -961.1703\n",
      "convergence dfGPdfNN Run 5/10, Epoch 908/1000, Training Loss (NLML): -961.1617\n",
      "convergence dfGPdfNN Run 5/10, Epoch 909/1000, Training Loss (NLML): -961.1793\n",
      "convergence dfGPdfNN Run 5/10, Epoch 910/1000, Training Loss (NLML): -961.1758\n",
      "convergence dfGPdfNN Run 5/10, Epoch 911/1000, Training Loss (NLML): -961.1812\n",
      "convergence dfGPdfNN Run 5/10, Epoch 912/1000, Training Loss (NLML): -961.1782\n",
      "convergence dfGPdfNN Run 5/10, Epoch 913/1000, Training Loss (NLML): -961.1792\n",
      "convergence dfGPdfNN Run 5/10, Epoch 914/1000, Training Loss (NLML): -961.1864\n",
      "convergence dfGPdfNN Run 5/10, Epoch 915/1000, Training Loss (NLML): -961.1880\n",
      "convergence dfGPdfNN Run 5/10, Epoch 916/1000, Training Loss (NLML): -961.1823\n",
      "convergence dfGPdfNN Run 5/10, Epoch 917/1000, Training Loss (NLML): -961.1835\n",
      "convergence dfGPdfNN Run 5/10, Epoch 918/1000, Training Loss (NLML): -961.1879\n",
      "convergence dfGPdfNN Run 5/10, Epoch 919/1000, Training Loss (NLML): -961.1924\n",
      "convergence dfGPdfNN Run 5/10, Epoch 920/1000, Training Loss (NLML): -961.1935\n",
      "convergence dfGPdfNN Run 5/10, Epoch 921/1000, Training Loss (NLML): -961.1970\n",
      "convergence dfGPdfNN Run 5/10, Epoch 922/1000, Training Loss (NLML): -961.1917\n",
      "convergence dfGPdfNN Run 5/10, Epoch 923/1000, Training Loss (NLML): -961.1946\n",
      "convergence dfGPdfNN Run 5/10, Epoch 924/1000, Training Loss (NLML): -961.1957\n",
      "convergence dfGPdfNN Run 5/10, Epoch 925/1000, Training Loss (NLML): -961.2041\n",
      "convergence dfGPdfNN Run 5/10, Epoch 926/1000, Training Loss (NLML): -961.2029\n",
      "convergence dfGPdfNN Run 5/10, Epoch 927/1000, Training Loss (NLML): -961.2069\n",
      "convergence dfGPdfNN Run 5/10, Epoch 928/1000, Training Loss (NLML): -961.1951\n",
      "convergence dfGPdfNN Run 5/10, Epoch 929/1000, Training Loss (NLML): -961.2023\n",
      "convergence dfGPdfNN Run 5/10, Epoch 930/1000, Training Loss (NLML): -961.2024\n",
      "convergence dfGPdfNN Run 5/10, Epoch 931/1000, Training Loss (NLML): -961.2098\n",
      "convergence dfGPdfNN Run 5/10, Epoch 932/1000, Training Loss (NLML): -961.2113\n",
      "convergence dfGPdfNN Run 5/10, Epoch 933/1000, Training Loss (NLML): -961.2100\n",
      "convergence dfGPdfNN Run 5/10, Epoch 934/1000, Training Loss (NLML): -961.2101\n",
      "convergence dfGPdfNN Run 5/10, Epoch 935/1000, Training Loss (NLML): -961.2112\n",
      "convergence dfGPdfNN Run 5/10, Epoch 936/1000, Training Loss (NLML): -961.2137\n",
      "convergence dfGPdfNN Run 5/10, Epoch 937/1000, Training Loss (NLML): -961.2180\n",
      "convergence dfGPdfNN Run 5/10, Epoch 938/1000, Training Loss (NLML): -961.2214\n",
      "convergence dfGPdfNN Run 5/10, Epoch 939/1000, Training Loss (NLML): -961.2181\n",
      "convergence dfGPdfNN Run 5/10, Epoch 940/1000, Training Loss (NLML): -961.2206\n",
      "convergence dfGPdfNN Run 5/10, Epoch 941/1000, Training Loss (NLML): -961.2261\n",
      "convergence dfGPdfNN Run 5/10, Epoch 942/1000, Training Loss (NLML): -961.2284\n",
      "convergence dfGPdfNN Run 5/10, Epoch 943/1000, Training Loss (NLML): -961.2283\n",
      "convergence dfGPdfNN Run 5/10, Epoch 944/1000, Training Loss (NLML): -961.2214\n",
      "convergence dfGPdfNN Run 5/10, Epoch 945/1000, Training Loss (NLML): -961.2266\n",
      "convergence dfGPdfNN Run 5/10, Epoch 946/1000, Training Loss (NLML): -961.2301\n",
      "convergence dfGPdfNN Run 5/10, Epoch 947/1000, Training Loss (NLML): -961.2365\n",
      "convergence dfGPdfNN Run 5/10, Epoch 948/1000, Training Loss (NLML): -961.2354\n",
      "convergence dfGPdfNN Run 5/10, Epoch 949/1000, Training Loss (NLML): -961.2397\n",
      "convergence dfGPdfNN Run 5/10, Epoch 950/1000, Training Loss (NLML): -961.2311\n",
      "convergence dfGPdfNN Run 5/10, Epoch 951/1000, Training Loss (NLML): -961.2343\n",
      "convergence dfGPdfNN Run 5/10, Epoch 952/1000, Training Loss (NLML): -961.2335\n",
      "convergence dfGPdfNN Run 5/10, Epoch 953/1000, Training Loss (NLML): -961.2422\n",
      "convergence dfGPdfNN Run 5/10, Epoch 954/1000, Training Loss (NLML): -961.2441\n",
      "convergence dfGPdfNN Run 5/10, Epoch 955/1000, Training Loss (NLML): -961.2439\n",
      "convergence dfGPdfNN Run 5/10, Epoch 956/1000, Training Loss (NLML): -961.2479\n",
      "convergence dfGPdfNN Run 5/10, Epoch 957/1000, Training Loss (NLML): -961.2429\n",
      "convergence dfGPdfNN Run 5/10, Epoch 958/1000, Training Loss (NLML): -961.2458\n",
      "convergence dfGPdfNN Run 5/10, Epoch 959/1000, Training Loss (NLML): -961.2444\n",
      "convergence dfGPdfNN Run 5/10, Epoch 960/1000, Training Loss (NLML): -961.2499\n",
      "convergence dfGPdfNN Run 5/10, Epoch 961/1000, Training Loss (NLML): -961.2540\n",
      "convergence dfGPdfNN Run 5/10, Epoch 962/1000, Training Loss (NLML): -961.2560\n",
      "convergence dfGPdfNN Run 5/10, Epoch 963/1000, Training Loss (NLML): -961.2590\n",
      "convergence dfGPdfNN Run 5/10, Epoch 964/1000, Training Loss (NLML): -961.2584\n",
      "convergence dfGPdfNN Run 5/10, Epoch 965/1000, Training Loss (NLML): -961.2606\n",
      "convergence dfGPdfNN Run 5/10, Epoch 966/1000, Training Loss (NLML): -961.2532\n",
      "convergence dfGPdfNN Run 5/10, Epoch 967/1000, Training Loss (NLML): -961.2612\n",
      "convergence dfGPdfNN Run 5/10, Epoch 968/1000, Training Loss (NLML): -961.2646\n",
      "convergence dfGPdfNN Run 5/10, Epoch 969/1000, Training Loss (NLML): -961.2670\n",
      "convergence dfGPdfNN Run 5/10, Epoch 970/1000, Training Loss (NLML): -961.2662\n",
      "convergence dfGPdfNN Run 5/10, Epoch 971/1000, Training Loss (NLML): -961.2623\n",
      "convergence dfGPdfNN Run 5/10, Epoch 972/1000, Training Loss (NLML): -961.2665\n",
      "convergence dfGPdfNN Run 5/10, Epoch 973/1000, Training Loss (NLML): -961.2681\n",
      "convergence dfGPdfNN Run 5/10, Epoch 974/1000, Training Loss (NLML): -961.2698\n",
      "convergence dfGPdfNN Run 5/10, Epoch 975/1000, Training Loss (NLML): -961.2744\n",
      "convergence dfGPdfNN Run 5/10, Epoch 976/1000, Training Loss (NLML): -961.2682\n",
      "convergence dfGPdfNN Run 5/10, Epoch 977/1000, Training Loss (NLML): -961.2727\n",
      "convergence dfGPdfNN Run 5/10, Epoch 978/1000, Training Loss (NLML): -961.2732\n",
      "convergence dfGPdfNN Run 5/10, Epoch 979/1000, Training Loss (NLML): -961.2772\n",
      "convergence dfGPdfNN Run 5/10, Epoch 980/1000, Training Loss (NLML): -961.2798\n",
      "convergence dfGPdfNN Run 5/10, Epoch 981/1000, Training Loss (NLML): -961.2781\n",
      "convergence dfGPdfNN Run 5/10, Epoch 982/1000, Training Loss (NLML): -961.2816\n",
      "convergence dfGPdfNN Run 5/10, Epoch 983/1000, Training Loss (NLML): -961.2784\n",
      "convergence dfGPdfNN Run 5/10, Epoch 984/1000, Training Loss (NLML): -961.2826\n",
      "convergence dfGPdfNN Run 5/10, Epoch 985/1000, Training Loss (NLML): -961.2877\n",
      "convergence dfGPdfNN Run 5/10, Epoch 986/1000, Training Loss (NLML): -961.2902\n",
      "convergence dfGPdfNN Run 5/10, Epoch 987/1000, Training Loss (NLML): -961.2898\n",
      "convergence dfGPdfNN Run 5/10, Epoch 988/1000, Training Loss (NLML): -961.2910\n",
      "convergence dfGPdfNN Run 5/10, Epoch 989/1000, Training Loss (NLML): -961.2856\n",
      "convergence dfGPdfNN Run 5/10, Epoch 990/1000, Training Loss (NLML): -961.2891\n",
      "convergence dfGPdfNN Run 5/10, Epoch 991/1000, Training Loss (NLML): -961.2938\n",
      "convergence dfGPdfNN Run 5/10, Epoch 992/1000, Training Loss (NLML): -961.2976\n",
      "convergence dfGPdfNN Run 5/10, Epoch 993/1000, Training Loss (NLML): -961.2960\n",
      "convergence dfGPdfNN Run 5/10, Epoch 994/1000, Training Loss (NLML): -961.3010\n",
      "convergence dfGPdfNN Run 5/10, Epoch 995/1000, Training Loss (NLML): -961.3013\n",
      "convergence dfGPdfNN Run 5/10, Epoch 996/1000, Training Loss (NLML): -961.2970\n",
      "convergence dfGPdfNN Run 5/10, Epoch 997/1000, Training Loss (NLML): -961.2976\n",
      "convergence dfGPdfNN Run 5/10, Epoch 998/1000, Training Loss (NLML): -961.3031\n",
      "convergence dfGPdfNN Run 5/10, Epoch 999/1000, Training Loss (NLML): -961.3043\n",
      "convergence dfGPdfNN Run 5/10, Epoch 1000/1000, Training Loss (NLML): -961.3062\n",
      "\n",
      "--- Training Run 6/10 ---\n",
      "\n",
      "Start Training\n",
      "convergence dfGPdfNN Run 6/10, Epoch 1/1000, Training Loss (NLML): -886.5521\n",
      "convergence dfGPdfNN Run 6/10, Epoch 2/1000, Training Loss (NLML): -893.1039\n",
      "convergence dfGPdfNN Run 6/10, Epoch 3/1000, Training Loss (NLML): -897.0354\n",
      "convergence dfGPdfNN Run 6/10, Epoch 4/1000, Training Loss (NLML): -900.4486\n",
      "convergence dfGPdfNN Run 6/10, Epoch 5/1000, Training Loss (NLML): -902.7788\n",
      "convergence dfGPdfNN Run 6/10, Epoch 6/1000, Training Loss (NLML): -905.1437\n",
      "convergence dfGPdfNN Run 6/10, Epoch 7/1000, Training Loss (NLML): -906.4940\n",
      "convergence dfGPdfNN Run 6/10, Epoch 8/1000, Training Loss (NLML): -907.6323\n",
      "convergence dfGPdfNN Run 6/10, Epoch 9/1000, Training Loss (NLML): -909.5288\n",
      "convergence dfGPdfNN Run 6/10, Epoch 10/1000, Training Loss (NLML): -911.1191\n",
      "convergence dfGPdfNN Run 6/10, Epoch 11/1000, Training Loss (NLML): -912.7743\n",
      "convergence dfGPdfNN Run 6/10, Epoch 12/1000, Training Loss (NLML): -914.2386\n",
      "convergence dfGPdfNN Run 6/10, Epoch 13/1000, Training Loss (NLML): -915.5397\n",
      "convergence dfGPdfNN Run 6/10, Epoch 14/1000, Training Loss (NLML): -916.8224\n",
      "convergence dfGPdfNN Run 6/10, Epoch 15/1000, Training Loss (NLML): -918.0403\n",
      "convergence dfGPdfNN Run 6/10, Epoch 16/1000, Training Loss (NLML): -919.2117\n",
      "convergence dfGPdfNN Run 6/10, Epoch 17/1000, Training Loss (NLML): -920.0729\n",
      "convergence dfGPdfNN Run 6/10, Epoch 18/1000, Training Loss (NLML): -920.9659\n",
      "convergence dfGPdfNN Run 6/10, Epoch 19/1000, Training Loss (NLML): -921.9161\n",
      "convergence dfGPdfNN Run 6/10, Epoch 20/1000, Training Loss (NLML): -922.7164\n",
      "convergence dfGPdfNN Run 6/10, Epoch 21/1000, Training Loss (NLML): -923.8347\n",
      "convergence dfGPdfNN Run 6/10, Epoch 22/1000, Training Loss (NLML): -924.9926\n",
      "convergence dfGPdfNN Run 6/10, Epoch 23/1000, Training Loss (NLML): -925.8722\n",
      "convergence dfGPdfNN Run 6/10, Epoch 24/1000, Training Loss (NLML): -926.7081\n",
      "convergence dfGPdfNN Run 6/10, Epoch 25/1000, Training Loss (NLML): -927.5498\n",
      "convergence dfGPdfNN Run 6/10, Epoch 26/1000, Training Loss (NLML): -928.3245\n",
      "convergence dfGPdfNN Run 6/10, Epoch 27/1000, Training Loss (NLML): -929.0441\n",
      "convergence dfGPdfNN Run 6/10, Epoch 28/1000, Training Loss (NLML): -929.7520\n",
      "convergence dfGPdfNN Run 6/10, Epoch 29/1000, Training Loss (NLML): -930.4374\n",
      "convergence dfGPdfNN Run 6/10, Epoch 30/1000, Training Loss (NLML): -931.0737\n",
      "convergence dfGPdfNN Run 6/10, Epoch 31/1000, Training Loss (NLML): -931.7380\n",
      "convergence dfGPdfNN Run 6/10, Epoch 32/1000, Training Loss (NLML): -932.3843\n",
      "convergence dfGPdfNN Run 6/10, Epoch 33/1000, Training Loss (NLML): -933.0015\n",
      "convergence dfGPdfNN Run 6/10, Epoch 34/1000, Training Loss (NLML): -933.6073\n",
      "convergence dfGPdfNN Run 6/10, Epoch 35/1000, Training Loss (NLML): -934.1870\n",
      "convergence dfGPdfNN Run 6/10, Epoch 36/1000, Training Loss (NLML): -934.7412\n",
      "convergence dfGPdfNN Run 6/10, Epoch 37/1000, Training Loss (NLML): -935.2888\n",
      "convergence dfGPdfNN Run 6/10, Epoch 38/1000, Training Loss (NLML): -935.8069\n",
      "convergence dfGPdfNN Run 6/10, Epoch 39/1000, Training Loss (NLML): -936.3196\n",
      "convergence dfGPdfNN Run 6/10, Epoch 40/1000, Training Loss (NLML): -936.8080\n",
      "convergence dfGPdfNN Run 6/10, Epoch 41/1000, Training Loss (NLML): -937.2731\n",
      "convergence dfGPdfNN Run 6/10, Epoch 42/1000, Training Loss (NLML): -937.7363\n",
      "convergence dfGPdfNN Run 6/10, Epoch 43/1000, Training Loss (NLML): -938.1757\n",
      "convergence dfGPdfNN Run 6/10, Epoch 44/1000, Training Loss (NLML): -938.6096\n",
      "convergence dfGPdfNN Run 6/10, Epoch 45/1000, Training Loss (NLML): -939.0323\n",
      "convergence dfGPdfNN Run 6/10, Epoch 46/1000, Training Loss (NLML): -939.4038\n",
      "convergence dfGPdfNN Run 6/10, Epoch 47/1000, Training Loss (NLML): -939.8116\n",
      "convergence dfGPdfNN Run 6/10, Epoch 48/1000, Training Loss (NLML): -940.2075\n",
      "convergence dfGPdfNN Run 6/10, Epoch 49/1000, Training Loss (NLML): -940.5901\n",
      "convergence dfGPdfNN Run 6/10, Epoch 50/1000, Training Loss (NLML): -940.9669\n",
      "convergence dfGPdfNN Run 6/10, Epoch 51/1000, Training Loss (NLML): -941.3318\n",
      "convergence dfGPdfNN Run 6/10, Epoch 52/1000, Training Loss (NLML): -941.6841\n",
      "convergence dfGPdfNN Run 6/10, Epoch 53/1000, Training Loss (NLML): -942.0297\n",
      "convergence dfGPdfNN Run 6/10, Epoch 54/1000, Training Loss (NLML): -942.3584\n",
      "convergence dfGPdfNN Run 6/10, Epoch 55/1000, Training Loss (NLML): -942.6840\n",
      "convergence dfGPdfNN Run 6/10, Epoch 56/1000, Training Loss (NLML): -943.0020\n",
      "convergence dfGPdfNN Run 6/10, Epoch 57/1000, Training Loss (NLML): -942.0930\n",
      "convergence dfGPdfNN Run 6/10, Epoch 58/1000, Training Loss (NLML): -940.0685\n",
      "convergence dfGPdfNN Run 6/10, Epoch 59/1000, Training Loss (NLML): -939.9238\n",
      "convergence dfGPdfNN Run 6/10, Epoch 60/1000, Training Loss (NLML): -940.7235\n",
      "convergence dfGPdfNN Run 6/10, Epoch 61/1000, Training Loss (NLML): -942.3132\n",
      "convergence dfGPdfNN Run 6/10, Epoch 62/1000, Training Loss (NLML): -943.6466\n",
      "convergence dfGPdfNN Run 6/10, Epoch 63/1000, Training Loss (NLML): -944.2101\n",
      "convergence dfGPdfNN Run 6/10, Epoch 64/1000, Training Loss (NLML): -944.5939\n",
      "convergence dfGPdfNN Run 6/10, Epoch 65/1000, Training Loss (NLML): -944.8420\n",
      "convergence dfGPdfNN Run 6/10, Epoch 66/1000, Training Loss (NLML): -945.0764\n",
      "convergence dfGPdfNN Run 6/10, Epoch 67/1000, Training Loss (NLML): -945.3351\n",
      "convergence dfGPdfNN Run 6/10, Epoch 68/1000, Training Loss (NLML): -945.5952\n",
      "convergence dfGPdfNN Run 6/10, Epoch 69/1000, Training Loss (NLML): -945.8094\n",
      "convergence dfGPdfNN Run 6/10, Epoch 70/1000, Training Loss (NLML): -946.0253\n",
      "convergence dfGPdfNN Run 6/10, Epoch 71/1000, Training Loss (NLML): -946.1680\n",
      "convergence dfGPdfNN Run 6/10, Epoch 72/1000, Training Loss (NLML): -946.3292\n",
      "convergence dfGPdfNN Run 6/10, Epoch 73/1000, Training Loss (NLML): -946.4852\n",
      "convergence dfGPdfNN Run 6/10, Epoch 74/1000, Training Loss (NLML): -946.7643\n",
      "convergence dfGPdfNN Run 6/10, Epoch 75/1000, Training Loss (NLML): -946.9508\n",
      "convergence dfGPdfNN Run 6/10, Epoch 76/1000, Training Loss (NLML): -947.1512\n",
      "convergence dfGPdfNN Run 6/10, Epoch 77/1000, Training Loss (NLML): -947.3224\n",
      "convergence dfGPdfNN Run 6/10, Epoch 78/1000, Training Loss (NLML): -947.3835\n",
      "convergence dfGPdfNN Run 6/10, Epoch 79/1000, Training Loss (NLML): -947.5449\n",
      "convergence dfGPdfNN Run 6/10, Epoch 80/1000, Training Loss (NLML): -947.7408\n",
      "convergence dfGPdfNN Run 6/10, Epoch 81/1000, Training Loss (NLML): -948.0293\n",
      "convergence dfGPdfNN Run 6/10, Epoch 82/1000, Training Loss (NLML): -948.3416\n",
      "convergence dfGPdfNN Run 6/10, Epoch 83/1000, Training Loss (NLML): -948.4937\n",
      "convergence dfGPdfNN Run 6/10, Epoch 84/1000, Training Loss (NLML): -948.5740\n",
      "convergence dfGPdfNN Run 6/10, Epoch 85/1000, Training Loss (NLML): -948.6726\n",
      "convergence dfGPdfNN Run 6/10, Epoch 86/1000, Training Loss (NLML): -948.9346\n",
      "convergence dfGPdfNN Run 6/10, Epoch 87/1000, Training Loss (NLML): -949.1075\n",
      "convergence dfGPdfNN Run 6/10, Epoch 88/1000, Training Loss (NLML): -949.2844\n",
      "convergence dfGPdfNN Run 6/10, Epoch 89/1000, Training Loss (NLML): -949.4525\n",
      "convergence dfGPdfNN Run 6/10, Epoch 90/1000, Training Loss (NLML): -949.5656\n",
      "convergence dfGPdfNN Run 6/10, Epoch 91/1000, Training Loss (NLML): -949.6721\n",
      "convergence dfGPdfNN Run 6/10, Epoch 92/1000, Training Loss (NLML): -949.7738\n",
      "convergence dfGPdfNN Run 6/10, Epoch 93/1000, Training Loss (NLML): -949.9316\n",
      "convergence dfGPdfNN Run 6/10, Epoch 94/1000, Training Loss (NLML): -950.1552\n",
      "convergence dfGPdfNN Run 6/10, Epoch 95/1000, Training Loss (NLML): -950.3571\n",
      "convergence dfGPdfNN Run 6/10, Epoch 96/1000, Training Loss (NLML): -950.2026\n",
      "convergence dfGPdfNN Run 6/10, Epoch 97/1000, Training Loss (NLML): -950.3254\n",
      "convergence dfGPdfNN Run 6/10, Epoch 98/1000, Training Loss (NLML): -950.5126\n",
      "convergence dfGPdfNN Run 6/10, Epoch 99/1000, Training Loss (NLML): -950.6354\n",
      "convergence dfGPdfNN Run 6/10, Epoch 100/1000, Training Loss (NLML): -950.7668\n",
      "convergence dfGPdfNN Run 6/10, Epoch 101/1000, Training Loss (NLML): -951.1334\n",
      "convergence dfGPdfNN Run 6/10, Epoch 102/1000, Training Loss (NLML): -951.1758\n",
      "convergence dfGPdfNN Run 6/10, Epoch 103/1000, Training Loss (NLML): -951.3054\n",
      "convergence dfGPdfNN Run 6/10, Epoch 104/1000, Training Loss (NLML): -951.5321\n",
      "convergence dfGPdfNN Run 6/10, Epoch 105/1000, Training Loss (NLML): -951.5957\n",
      "convergence dfGPdfNN Run 6/10, Epoch 106/1000, Training Loss (NLML): -951.6183\n",
      "convergence dfGPdfNN Run 6/10, Epoch 107/1000, Training Loss (NLML): -951.6056\n",
      "convergence dfGPdfNN Run 6/10, Epoch 108/1000, Training Loss (NLML): -951.8701\n",
      "convergence dfGPdfNN Run 6/10, Epoch 109/1000, Training Loss (NLML): -952.0532\n",
      "convergence dfGPdfNN Run 6/10, Epoch 110/1000, Training Loss (NLML): -952.1696\n",
      "convergence dfGPdfNN Run 6/10, Epoch 111/1000, Training Loss (NLML): -952.2766\n",
      "convergence dfGPdfNN Run 6/10, Epoch 112/1000, Training Loss (NLML): -952.6495\n",
      "convergence dfGPdfNN Run 6/10, Epoch 113/1000, Training Loss (NLML): -952.8020\n",
      "convergence dfGPdfNN Run 6/10, Epoch 114/1000, Training Loss (NLML): -952.9979\n",
      "convergence dfGPdfNN Run 6/10, Epoch 115/1000, Training Loss (NLML): -952.9546\n",
      "convergence dfGPdfNN Run 6/10, Epoch 116/1000, Training Loss (NLML): -953.1084\n",
      "convergence dfGPdfNN Run 6/10, Epoch 117/1000, Training Loss (NLML): -953.2308\n",
      "convergence dfGPdfNN Run 6/10, Epoch 118/1000, Training Loss (NLML): -953.2410\n",
      "convergence dfGPdfNN Run 6/10, Epoch 119/1000, Training Loss (NLML): -953.0344\n",
      "convergence dfGPdfNN Run 6/10, Epoch 120/1000, Training Loss (NLML): -952.9318\n",
      "convergence dfGPdfNN Run 6/10, Epoch 121/1000, Training Loss (NLML): -952.9327\n",
      "convergence dfGPdfNN Run 6/10, Epoch 122/1000, Training Loss (NLML): -953.1251\n",
      "convergence dfGPdfNN Run 6/10, Epoch 123/1000, Training Loss (NLML): -953.2653\n",
      "convergence dfGPdfNN Run 6/10, Epoch 124/1000, Training Loss (NLML): -953.3705\n",
      "convergence dfGPdfNN Run 6/10, Epoch 125/1000, Training Loss (NLML): -953.5143\n",
      "convergence dfGPdfNN Run 6/10, Epoch 126/1000, Training Loss (NLML): -953.6178\n",
      "convergence dfGPdfNN Run 6/10, Epoch 127/1000, Training Loss (NLML): -953.7491\n",
      "convergence dfGPdfNN Run 6/10, Epoch 128/1000, Training Loss (NLML): -953.8612\n",
      "convergence dfGPdfNN Run 6/10, Epoch 129/1000, Training Loss (NLML): -953.9785\n",
      "convergence dfGPdfNN Run 6/10, Epoch 130/1000, Training Loss (NLML): -954.0299\n",
      "convergence dfGPdfNN Run 6/10, Epoch 131/1000, Training Loss (NLML): -954.1982\n",
      "convergence dfGPdfNN Run 6/10, Epoch 132/1000, Training Loss (NLML): -954.2194\n",
      "convergence dfGPdfNN Run 6/10, Epoch 133/1000, Training Loss (NLML): -954.3005\n",
      "convergence dfGPdfNN Run 6/10, Epoch 134/1000, Training Loss (NLML): -953.8351\n",
      "convergence dfGPdfNN Run 6/10, Epoch 135/1000, Training Loss (NLML): -954.3024\n",
      "convergence dfGPdfNN Run 6/10, Epoch 136/1000, Training Loss (NLML): -954.5627\n",
      "convergence dfGPdfNN Run 6/10, Epoch 137/1000, Training Loss (NLML): -954.7109\n",
      "convergence dfGPdfNN Run 6/10, Epoch 138/1000, Training Loss (NLML): -954.8013\n",
      "convergence dfGPdfNN Run 6/10, Epoch 139/1000, Training Loss (NLML): -954.9401\n",
      "convergence dfGPdfNN Run 6/10, Epoch 140/1000, Training Loss (NLML): -955.0667\n",
      "convergence dfGPdfNN Run 6/10, Epoch 141/1000, Training Loss (NLML): -955.1240\n",
      "convergence dfGPdfNN Run 6/10, Epoch 142/1000, Training Loss (NLML): -955.0605\n",
      "convergence dfGPdfNN Run 6/10, Epoch 143/1000, Training Loss (NLML): -955.2080\n",
      "convergence dfGPdfNN Run 6/10, Epoch 144/1000, Training Loss (NLML): -955.2959\n",
      "convergence dfGPdfNN Run 6/10, Epoch 145/1000, Training Loss (NLML): -955.2617\n",
      "convergence dfGPdfNN Run 6/10, Epoch 146/1000, Training Loss (NLML): -955.1927\n",
      "convergence dfGPdfNN Run 6/10, Epoch 147/1000, Training Loss (NLML): -955.4678\n",
      "convergence dfGPdfNN Run 6/10, Epoch 148/1000, Training Loss (NLML): -955.7202\n",
      "convergence dfGPdfNN Run 6/10, Epoch 149/1000, Training Loss (NLML): -955.8342\n",
      "convergence dfGPdfNN Run 6/10, Epoch 150/1000, Training Loss (NLML): -955.7216\n",
      "convergence dfGPdfNN Run 6/10, Epoch 151/1000, Training Loss (NLML): -955.6488\n",
      "convergence dfGPdfNN Run 6/10, Epoch 152/1000, Training Loss (NLML): -955.5844\n",
      "convergence dfGPdfNN Run 6/10, Epoch 153/1000, Training Loss (NLML): -955.8110\n",
      "convergence dfGPdfNN Run 6/10, Epoch 154/1000, Training Loss (NLML): -956.0037\n",
      "convergence dfGPdfNN Run 6/10, Epoch 155/1000, Training Loss (NLML): -956.1969\n",
      "convergence dfGPdfNN Run 6/10, Epoch 156/1000, Training Loss (NLML): -956.1146\n",
      "convergence dfGPdfNN Run 6/10, Epoch 157/1000, Training Loss (NLML): -956.1116\n",
      "convergence dfGPdfNN Run 6/10, Epoch 158/1000, Training Loss (NLML): -956.1385\n",
      "convergence dfGPdfNN Run 6/10, Epoch 159/1000, Training Loss (NLML): -956.3401\n",
      "convergence dfGPdfNN Run 6/10, Epoch 160/1000, Training Loss (NLML): -908.4000\n",
      "convergence dfGPdfNN Run 6/10, Epoch 161/1000, Training Loss (NLML): -956.0559\n",
      "convergence dfGPdfNN Run 6/10, Epoch 162/1000, Training Loss (NLML): -955.5271\n",
      "convergence dfGPdfNN Run 6/10, Epoch 163/1000, Training Loss (NLML): -955.1891\n",
      "convergence dfGPdfNN Run 6/10, Epoch 164/1000, Training Loss (NLML): -954.8281\n",
      "convergence dfGPdfNN Run 6/10, Epoch 165/1000, Training Loss (NLML): -954.4038\n",
      "convergence dfGPdfNN Run 6/10, Epoch 166/1000, Training Loss (NLML): -954.4048\n",
      "convergence dfGPdfNN Run 6/10, Epoch 167/1000, Training Loss (NLML): -954.4911\n",
      "convergence dfGPdfNN Run 6/10, Epoch 168/1000, Training Loss (NLML): -954.5942\n",
      "convergence dfGPdfNN Run 6/10, Epoch 169/1000, Training Loss (NLML): -954.8314\n",
      "convergence dfGPdfNN Run 6/10, Epoch 170/1000, Training Loss (NLML): -954.9738\n",
      "convergence dfGPdfNN Run 6/10, Epoch 171/1000, Training Loss (NLML): -954.8339\n",
      "convergence dfGPdfNN Run 6/10, Epoch 172/1000, Training Loss (NLML): -954.7374\n",
      "convergence dfGPdfNN Run 6/10, Epoch 173/1000, Training Loss (NLML): -954.9583\n",
      "convergence dfGPdfNN Run 6/10, Epoch 174/1000, Training Loss (NLML): -955.1785\n",
      "convergence dfGPdfNN Run 6/10, Epoch 175/1000, Training Loss (NLML): -955.3217\n",
      "convergence dfGPdfNN Run 6/10, Epoch 176/1000, Training Loss (NLML): -955.4812\n",
      "convergence dfGPdfNN Run 6/10, Epoch 177/1000, Training Loss (NLML): -955.6036\n",
      "convergence dfGPdfNN Run 6/10, Epoch 178/1000, Training Loss (NLML): -955.7952\n",
      "convergence dfGPdfNN Run 6/10, Epoch 179/1000, Training Loss (NLML): -956.0459\n",
      "convergence dfGPdfNN Run 6/10, Epoch 180/1000, Training Loss (NLML): -956.2217\n",
      "convergence dfGPdfNN Run 6/10, Epoch 181/1000, Training Loss (NLML): -956.2623\n",
      "convergence dfGPdfNN Run 6/10, Epoch 182/1000, Training Loss (NLML): -956.1759\n",
      "convergence dfGPdfNN Run 6/10, Epoch 183/1000, Training Loss (NLML): -956.2245\n",
      "convergence dfGPdfNN Run 6/10, Epoch 184/1000, Training Loss (NLML): -956.4178\n",
      "convergence dfGPdfNN Run 6/10, Epoch 185/1000, Training Loss (NLML): -956.6172\n",
      "convergence dfGPdfNN Run 6/10, Epoch 186/1000, Training Loss (NLML): -956.7052\n",
      "convergence dfGPdfNN Run 6/10, Epoch 187/1000, Training Loss (NLML): -956.6124\n",
      "convergence dfGPdfNN Run 6/10, Epoch 188/1000, Training Loss (NLML): -956.6376\n",
      "convergence dfGPdfNN Run 6/10, Epoch 189/1000, Training Loss (NLML): -956.7090\n",
      "convergence dfGPdfNN Run 6/10, Epoch 190/1000, Training Loss (NLML): -956.7894\n",
      "convergence dfGPdfNN Run 6/10, Epoch 191/1000, Training Loss (NLML): -956.9714\n",
      "convergence dfGPdfNN Run 6/10, Epoch 192/1000, Training Loss (NLML): -957.0586\n",
      "convergence dfGPdfNN Run 6/10, Epoch 193/1000, Training Loss (NLML): -957.1113\n",
      "convergence dfGPdfNN Run 6/10, Epoch 194/1000, Training Loss (NLML): -957.0969\n",
      "convergence dfGPdfNN Run 6/10, Epoch 195/1000, Training Loss (NLML): -957.1074\n",
      "convergence dfGPdfNN Run 6/10, Epoch 196/1000, Training Loss (NLML): -957.1021\n",
      "convergence dfGPdfNN Run 6/10, Epoch 197/1000, Training Loss (NLML): -955.0293\n",
      "convergence dfGPdfNN Run 6/10, Epoch 198/1000, Training Loss (NLML): -955.4093\n",
      "convergence dfGPdfNN Run 6/10, Epoch 199/1000, Training Loss (NLML): -956.6554\n",
      "convergence dfGPdfNN Run 6/10, Epoch 200/1000, Training Loss (NLML): -956.9806\n",
      "convergence dfGPdfNN Run 6/10, Epoch 201/1000, Training Loss (NLML): -957.1494\n",
      "convergence dfGPdfNN Run 6/10, Epoch 202/1000, Training Loss (NLML): -957.1404\n",
      "convergence dfGPdfNN Run 6/10, Epoch 203/1000, Training Loss (NLML): -957.1794\n",
      "convergence dfGPdfNN Run 6/10, Epoch 204/1000, Training Loss (NLML): -957.2871\n",
      "convergence dfGPdfNN Run 6/10, Epoch 205/1000, Training Loss (NLML): -957.3473\n",
      "convergence dfGPdfNN Run 6/10, Epoch 206/1000, Training Loss (NLML): -957.3353\n",
      "convergence dfGPdfNN Run 6/10, Epoch 207/1000, Training Loss (NLML): -957.3536\n",
      "convergence dfGPdfNN Run 6/10, Epoch 208/1000, Training Loss (NLML): -957.3857\n",
      "convergence dfGPdfNN Run 6/10, Epoch 209/1000, Training Loss (NLML): -957.3285\n",
      "convergence dfGPdfNN Run 6/10, Epoch 210/1000, Training Loss (NLML): -957.3770\n",
      "convergence dfGPdfNN Run 6/10, Epoch 211/1000, Training Loss (NLML): -957.4530\n",
      "convergence dfGPdfNN Run 6/10, Epoch 212/1000, Training Loss (NLML): -957.4199\n",
      "convergence dfGPdfNN Run 6/10, Epoch 213/1000, Training Loss (NLML): -957.2410\n",
      "convergence dfGPdfNN Run 6/10, Epoch 214/1000, Training Loss (NLML): -957.2614\n",
      "convergence dfGPdfNN Run 6/10, Epoch 215/1000, Training Loss (NLML): -957.3207\n",
      "convergence dfGPdfNN Run 6/10, Epoch 216/1000, Training Loss (NLML): -957.2927\n",
      "convergence dfGPdfNN Run 6/10, Epoch 217/1000, Training Loss (NLML): -957.2987\n",
      "convergence dfGPdfNN Run 6/10, Epoch 218/1000, Training Loss (NLML): -945.6627\n",
      "convergence dfGPdfNN Run 6/10, Epoch 219/1000, Training Loss (NLML): -957.3300\n",
      "convergence dfGPdfNN Run 6/10, Epoch 220/1000, Training Loss (NLML): -957.0922\n",
      "convergence dfGPdfNN Run 6/10, Epoch 221/1000, Training Loss (NLML): -957.0800\n",
      "convergence dfGPdfNN Run 6/10, Epoch 222/1000, Training Loss (NLML): -957.0531\n",
      "convergence dfGPdfNN Run 6/10, Epoch 223/1000, Training Loss (NLML): -956.9471\n",
      "convergence dfGPdfNN Run 6/10, Epoch 224/1000, Training Loss (NLML): -956.9775\n",
      "convergence dfGPdfNN Run 6/10, Epoch 225/1000, Training Loss (NLML): -956.9618\n",
      "convergence dfGPdfNN Run 6/10, Epoch 226/1000, Training Loss (NLML): -957.0527\n",
      "convergence dfGPdfNN Run 6/10, Epoch 227/1000, Training Loss (NLML): -957.1593\n",
      "convergence dfGPdfNN Run 6/10, Epoch 228/1000, Training Loss (NLML): -957.1185\n",
      "convergence dfGPdfNN Run 6/10, Epoch 229/1000, Training Loss (NLML): -957.1249\n",
      "convergence dfGPdfNN Run 6/10, Epoch 230/1000, Training Loss (NLML): -957.1721\n",
      "convergence dfGPdfNN Run 6/10, Epoch 231/1000, Training Loss (NLML): -957.2001\n",
      "convergence dfGPdfNN Run 6/10, Epoch 232/1000, Training Loss (NLML): -957.2708\n",
      "convergence dfGPdfNN Run 6/10, Epoch 233/1000, Training Loss (NLML): -957.4781\n",
      "convergence dfGPdfNN Run 6/10, Epoch 234/1000, Training Loss (NLML): -957.5170\n",
      "convergence dfGPdfNN Run 6/10, Epoch 235/1000, Training Loss (NLML): -957.5052\n",
      "convergence dfGPdfNN Run 6/10, Epoch 236/1000, Training Loss (NLML): -957.3778\n",
      "convergence dfGPdfNN Run 6/10, Epoch 237/1000, Training Loss (NLML): -957.4056\n",
      "convergence dfGPdfNN Run 6/10, Epoch 238/1000, Training Loss (NLML): -957.4320\n",
      "convergence dfGPdfNN Run 6/10, Epoch 239/1000, Training Loss (NLML): -957.4480\n",
      "convergence dfGPdfNN Run 6/10, Epoch 240/1000, Training Loss (NLML): -957.4487\n",
      "convergence dfGPdfNN Run 6/10, Epoch 241/1000, Training Loss (NLML): -956.7217\n",
      "convergence dfGPdfNN Run 6/10, Epoch 242/1000, Training Loss (NLML): -956.7589\n",
      "convergence dfGPdfNN Run 6/10, Epoch 243/1000, Training Loss (NLML): -957.3180\n",
      "convergence dfGPdfNN Run 6/10, Epoch 244/1000, Training Loss (NLML): -957.5452\n",
      "convergence dfGPdfNN Run 6/10, Epoch 245/1000, Training Loss (NLML): -957.6544\n",
      "convergence dfGPdfNN Run 6/10, Epoch 246/1000, Training Loss (NLML): -957.7135\n",
      "convergence dfGPdfNN Run 6/10, Epoch 247/1000, Training Loss (NLML): -957.6746\n",
      "convergence dfGPdfNN Run 6/10, Epoch 248/1000, Training Loss (NLML): -957.5222\n",
      "convergence dfGPdfNN Run 6/10, Epoch 249/1000, Training Loss (NLML): -957.4978\n",
      "convergence dfGPdfNN Run 6/10, Epoch 250/1000, Training Loss (NLML): -957.4253\n",
      "convergence dfGPdfNN Run 6/10, Epoch 251/1000, Training Loss (NLML): -957.4976\n",
      "convergence dfGPdfNN Run 6/10, Epoch 252/1000, Training Loss (NLML): -957.6036\n",
      "convergence dfGPdfNN Run 6/10, Epoch 253/1000, Training Loss (NLML): -957.6021\n",
      "convergence dfGPdfNN Run 6/10, Epoch 254/1000, Training Loss (NLML): -957.8612\n",
      "convergence dfGPdfNN Run 6/10, Epoch 255/1000, Training Loss (NLML): -958.0051\n",
      "convergence dfGPdfNN Run 6/10, Epoch 256/1000, Training Loss (NLML): -958.0282\n",
      "convergence dfGPdfNN Run 6/10, Epoch 257/1000, Training Loss (NLML): -957.9915\n",
      "convergence dfGPdfNN Run 6/10, Epoch 258/1000, Training Loss (NLML): -957.8252\n",
      "convergence dfGPdfNN Run 6/10, Epoch 259/1000, Training Loss (NLML): -957.8026\n",
      "convergence dfGPdfNN Run 6/10, Epoch 260/1000, Training Loss (NLML): -957.8198\n",
      "convergence dfGPdfNN Run 6/10, Epoch 261/1000, Training Loss (NLML): -957.8561\n",
      "convergence dfGPdfNN Run 6/10, Epoch 262/1000, Training Loss (NLML): -957.9482\n",
      "convergence dfGPdfNN Run 6/10, Epoch 263/1000, Training Loss (NLML): -958.0349\n",
      "convergence dfGPdfNN Run 6/10, Epoch 264/1000, Training Loss (NLML): -957.9910\n",
      "convergence dfGPdfNN Run 6/10, Epoch 265/1000, Training Loss (NLML): -957.9597\n",
      "convergence dfGPdfNN Run 6/10, Epoch 266/1000, Training Loss (NLML): -957.9083\n",
      "convergence dfGPdfNN Run 6/10, Epoch 267/1000, Training Loss (NLML): -957.8695\n",
      "convergence dfGPdfNN Run 6/10, Epoch 268/1000, Training Loss (NLML): -957.9639\n",
      "convergence dfGPdfNN Run 6/10, Epoch 269/1000, Training Loss (NLML): -958.0033\n",
      "convergence dfGPdfNN Run 6/10, Epoch 270/1000, Training Loss (NLML): -958.0734\n",
      "convergence dfGPdfNN Run 6/10, Epoch 271/1000, Training Loss (NLML): -958.0679\n",
      "convergence dfGPdfNN Run 6/10, Epoch 272/1000, Training Loss (NLML): -958.1697\n",
      "convergence dfGPdfNN Run 6/10, Epoch 273/1000, Training Loss (NLML): -958.2023\n",
      "convergence dfGPdfNN Run 6/10, Epoch 274/1000, Training Loss (NLML): -958.2216\n",
      "convergence dfGPdfNN Run 6/10, Epoch 275/1000, Training Loss (NLML): -958.2231\n",
      "convergence dfGPdfNN Run 6/10, Epoch 276/1000, Training Loss (NLML): -958.1960\n",
      "convergence dfGPdfNN Run 6/10, Epoch 277/1000, Training Loss (NLML): -958.2225\n",
      "convergence dfGPdfNN Run 6/10, Epoch 278/1000, Training Loss (NLML): -958.1880\n",
      "convergence dfGPdfNN Run 6/10, Epoch 279/1000, Training Loss (NLML): -958.0967\n",
      "convergence dfGPdfNN Run 6/10, Epoch 280/1000, Training Loss (NLML): -958.2412\n",
      "convergence dfGPdfNN Run 6/10, Epoch 281/1000, Training Loss (NLML): -958.2216\n",
      "convergence dfGPdfNN Run 6/10, Epoch 282/1000, Training Loss (NLML): -958.2311\n",
      "convergence dfGPdfNN Run 6/10, Epoch 283/1000, Training Loss (NLML): -958.1781\n",
      "convergence dfGPdfNN Run 6/10, Epoch 284/1000, Training Loss (NLML): -958.1769\n",
      "convergence dfGPdfNN Run 6/10, Epoch 285/1000, Training Loss (NLML): -958.2346\n",
      "convergence dfGPdfNN Run 6/10, Epoch 286/1000, Training Loss (NLML): -958.2747\n",
      "convergence dfGPdfNN Run 6/10, Epoch 287/1000, Training Loss (NLML): -958.2640\n",
      "convergence dfGPdfNN Run 6/10, Epoch 288/1000, Training Loss (NLML): -958.3470\n",
      "convergence dfGPdfNN Run 6/10, Epoch 289/1000, Training Loss (NLML): -958.3660\n",
      "convergence dfGPdfNN Run 6/10, Epoch 290/1000, Training Loss (NLML): -958.4086\n",
      "convergence dfGPdfNN Run 6/10, Epoch 291/1000, Training Loss (NLML): -958.4338\n",
      "convergence dfGPdfNN Run 6/10, Epoch 292/1000, Training Loss (NLML): -958.4872\n",
      "convergence dfGPdfNN Run 6/10, Epoch 293/1000, Training Loss (NLML): -958.5100\n",
      "convergence dfGPdfNN Run 6/10, Epoch 294/1000, Training Loss (NLML): -958.5142\n",
      "convergence dfGPdfNN Run 6/10, Epoch 295/1000, Training Loss (NLML): -958.4769\n",
      "convergence dfGPdfNN Run 6/10, Epoch 296/1000, Training Loss (NLML): -958.4951\n",
      "convergence dfGPdfNN Run 6/10, Epoch 297/1000, Training Loss (NLML): -958.5140\n",
      "convergence dfGPdfNN Run 6/10, Epoch 298/1000, Training Loss (NLML): -958.5300\n",
      "convergence dfGPdfNN Run 6/10, Epoch 299/1000, Training Loss (NLML): -958.5182\n",
      "convergence dfGPdfNN Run 6/10, Epoch 300/1000, Training Loss (NLML): -958.5883\n",
      "convergence dfGPdfNN Run 6/10, Epoch 301/1000, Training Loss (NLML): -958.6029\n",
      "convergence dfGPdfNN Run 6/10, Epoch 302/1000, Training Loss (NLML): -958.5977\n",
      "convergence dfGPdfNN Run 6/10, Epoch 303/1000, Training Loss (NLML): -958.5966\n",
      "convergence dfGPdfNN Run 6/10, Epoch 304/1000, Training Loss (NLML): -958.5608\n",
      "convergence dfGPdfNN Run 6/10, Epoch 305/1000, Training Loss (NLML): -958.5637\n",
      "convergence dfGPdfNN Run 6/10, Epoch 306/1000, Training Loss (NLML): -958.5876\n",
      "convergence dfGPdfNN Run 6/10, Epoch 307/1000, Training Loss (NLML): -958.5699\n",
      "convergence dfGPdfNN Run 6/10, Epoch 308/1000, Training Loss (NLML): -958.5142\n",
      "convergence dfGPdfNN Run 6/10, Epoch 309/1000, Training Loss (NLML): -958.4132\n",
      "convergence dfGPdfNN Run 6/10, Epoch 310/1000, Training Loss (NLML): -958.3698\n",
      "convergence dfGPdfNN Run 6/10, Epoch 311/1000, Training Loss (NLML): -958.3140\n",
      "convergence dfGPdfNN Run 6/10, Epoch 312/1000, Training Loss (NLML): -958.3044\n",
      "convergence dfGPdfNN Run 6/10, Epoch 313/1000, Training Loss (NLML): -958.3138\n",
      "convergence dfGPdfNN Run 6/10, Epoch 314/1000, Training Loss (NLML): -958.3479\n",
      "convergence dfGPdfNN Run 6/10, Epoch 315/1000, Training Loss (NLML): -958.3955\n",
      "convergence dfGPdfNN Run 6/10, Epoch 316/1000, Training Loss (NLML): -958.3767\n",
      "convergence dfGPdfNN Run 6/10, Epoch 317/1000, Training Loss (NLML): -958.4298\n",
      "convergence dfGPdfNN Run 6/10, Epoch 318/1000, Training Loss (NLML): -958.4714\n",
      "convergence dfGPdfNN Run 6/10, Epoch 319/1000, Training Loss (NLML): -958.4719\n",
      "convergence dfGPdfNN Run 6/10, Epoch 320/1000, Training Loss (NLML): -958.5591\n",
      "convergence dfGPdfNN Run 6/10, Epoch 321/1000, Training Loss (NLML): -958.5552\n",
      "convergence dfGPdfNN Run 6/10, Epoch 322/1000, Training Loss (NLML): -958.5824\n",
      "convergence dfGPdfNN Run 6/10, Epoch 323/1000, Training Loss (NLML): -958.5662\n",
      "convergence dfGPdfNN Run 6/10, Epoch 324/1000, Training Loss (NLML): -958.5695\n",
      "convergence dfGPdfNN Run 6/10, Epoch 325/1000, Training Loss (NLML): -958.6029\n",
      "convergence dfGPdfNN Run 6/10, Epoch 326/1000, Training Loss (NLML): -958.6061\n",
      "convergence dfGPdfNN Run 6/10, Epoch 327/1000, Training Loss (NLML): -958.6212\n",
      "convergence dfGPdfNN Run 6/10, Epoch 328/1000, Training Loss (NLML): -958.5801\n",
      "convergence dfGPdfNN Run 6/10, Epoch 329/1000, Training Loss (NLML): -958.5980\n",
      "convergence dfGPdfNN Run 6/10, Epoch 330/1000, Training Loss (NLML): -958.6010\n",
      "convergence dfGPdfNN Run 6/10, Epoch 331/1000, Training Loss (NLML): -958.5709\n",
      "convergence dfGPdfNN Run 6/10, Epoch 332/1000, Training Loss (NLML): -958.5181\n",
      "convergence dfGPdfNN Run 6/10, Epoch 333/1000, Training Loss (NLML): -958.5125\n",
      "convergence dfGPdfNN Run 6/10, Epoch 334/1000, Training Loss (NLML): -958.5442\n",
      "convergence dfGPdfNN Run 6/10, Epoch 335/1000, Training Loss (NLML): -958.5963\n",
      "convergence dfGPdfNN Run 6/10, Epoch 336/1000, Training Loss (NLML): -958.6584\n",
      "convergence dfGPdfNN Run 6/10, Epoch 337/1000, Training Loss (NLML): -958.7312\n",
      "convergence dfGPdfNN Run 6/10, Epoch 338/1000, Training Loss (NLML): -958.7520\n",
      "convergence dfGPdfNN Run 6/10, Epoch 339/1000, Training Loss (NLML): -958.7062\n",
      "convergence dfGPdfNN Run 6/10, Epoch 340/1000, Training Loss (NLML): -958.6851\n",
      "convergence dfGPdfNN Run 6/10, Epoch 341/1000, Training Loss (NLML): -958.7192\n",
      "convergence dfGPdfNN Run 6/10, Epoch 342/1000, Training Loss (NLML): -958.7213\n",
      "convergence dfGPdfNN Run 6/10, Epoch 343/1000, Training Loss (NLML): -958.7117\n",
      "convergence dfGPdfNN Run 6/10, Epoch 344/1000, Training Loss (NLML): -958.7224\n",
      "convergence dfGPdfNN Run 6/10, Epoch 345/1000, Training Loss (NLML): -958.7932\n",
      "convergence dfGPdfNN Run 6/10, Epoch 346/1000, Training Loss (NLML): -958.8699\n",
      "convergence dfGPdfNN Run 6/10, Epoch 347/1000, Training Loss (NLML): -958.9238\n",
      "convergence dfGPdfNN Run 6/10, Epoch 348/1000, Training Loss (NLML): -958.9806\n",
      "convergence dfGPdfNN Run 6/10, Epoch 349/1000, Training Loss (NLML): -958.9948\n",
      "convergence dfGPdfNN Run 6/10, Epoch 350/1000, Training Loss (NLML): -958.9950\n",
      "convergence dfGPdfNN Run 6/10, Epoch 351/1000, Training Loss (NLML): -959.0150\n",
      "convergence dfGPdfNN Run 6/10, Epoch 352/1000, Training Loss (NLML): -959.0240\n",
      "convergence dfGPdfNN Run 6/10, Epoch 353/1000, Training Loss (NLML): -959.0251\n",
      "convergence dfGPdfNN Run 6/10, Epoch 354/1000, Training Loss (NLML): -959.0133\n",
      "convergence dfGPdfNN Run 6/10, Epoch 355/1000, Training Loss (NLML): -959.0396\n",
      "convergence dfGPdfNN Run 6/10, Epoch 356/1000, Training Loss (NLML): -959.0918\n",
      "convergence dfGPdfNN Run 6/10, Epoch 357/1000, Training Loss (NLML): -959.1260\n",
      "convergence dfGPdfNN Run 6/10, Epoch 358/1000, Training Loss (NLML): -959.1106\n",
      "convergence dfGPdfNN Run 6/10, Epoch 359/1000, Training Loss (NLML): -959.0271\n",
      "convergence dfGPdfNN Run 6/10, Epoch 360/1000, Training Loss (NLML): -958.9871\n",
      "convergence dfGPdfNN Run 6/10, Epoch 361/1000, Training Loss (NLML): -958.9640\n",
      "convergence dfGPdfNN Run 6/10, Epoch 362/1000, Training Loss (NLML): -958.9910\n",
      "convergence dfGPdfNN Run 6/10, Epoch 363/1000, Training Loss (NLML): -959.0049\n",
      "convergence dfGPdfNN Run 6/10, Epoch 364/1000, Training Loss (NLML): -959.0369\n",
      "convergence dfGPdfNN Run 6/10, Epoch 365/1000, Training Loss (NLML): -959.0684\n",
      "convergence dfGPdfNN Run 6/10, Epoch 366/1000, Training Loss (NLML): -959.0541\n",
      "convergence dfGPdfNN Run 6/10, Epoch 367/1000, Training Loss (NLML): -959.1083\n",
      "convergence dfGPdfNN Run 6/10, Epoch 368/1000, Training Loss (NLML): -959.0824\n",
      "convergence dfGPdfNN Run 6/10, Epoch 369/1000, Training Loss (NLML): -959.1257\n",
      "convergence dfGPdfNN Run 6/10, Epoch 370/1000, Training Loss (NLML): -959.1038\n",
      "convergence dfGPdfNN Run 6/10, Epoch 371/1000, Training Loss (NLML): -959.0841\n",
      "convergence dfGPdfNN Run 6/10, Epoch 372/1000, Training Loss (NLML): -959.0461\n",
      "convergence dfGPdfNN Run 6/10, Epoch 373/1000, Training Loss (NLML): -959.0189\n",
      "convergence dfGPdfNN Run 6/10, Epoch 374/1000, Training Loss (NLML): -959.0918\n",
      "convergence dfGPdfNN Run 6/10, Epoch 375/1000, Training Loss (NLML): -959.0911\n",
      "convergence dfGPdfNN Run 6/10, Epoch 376/1000, Training Loss (NLML): -959.1433\n",
      "convergence dfGPdfNN Run 6/10, Epoch 377/1000, Training Loss (NLML): -959.1570\n",
      "convergence dfGPdfNN Run 6/10, Epoch 378/1000, Training Loss (NLML): -959.1880\n",
      "convergence dfGPdfNN Run 6/10, Epoch 379/1000, Training Loss (NLML): -959.1577\n",
      "convergence dfGPdfNN Run 6/10, Epoch 380/1000, Training Loss (NLML): -959.2114\n",
      "convergence dfGPdfNN Run 6/10, Epoch 381/1000, Training Loss (NLML): -959.2096\n",
      "convergence dfGPdfNN Run 6/10, Epoch 382/1000, Training Loss (NLML): -959.2332\n",
      "convergence dfGPdfNN Run 6/10, Epoch 383/1000, Training Loss (NLML): -959.2672\n",
      "convergence dfGPdfNN Run 6/10, Epoch 384/1000, Training Loss (NLML): -959.2435\n",
      "convergence dfGPdfNN Run 6/10, Epoch 385/1000, Training Loss (NLML): -959.2394\n",
      "convergence dfGPdfNN Run 6/10, Epoch 386/1000, Training Loss (NLML): -959.2365\n",
      "convergence dfGPdfNN Run 6/10, Epoch 387/1000, Training Loss (NLML): -959.2065\n",
      "convergence dfGPdfNN Run 6/10, Epoch 388/1000, Training Loss (NLML): -959.2123\n",
      "convergence dfGPdfNN Run 6/10, Epoch 389/1000, Training Loss (NLML): -959.1847\n",
      "convergence dfGPdfNN Run 6/10, Epoch 390/1000, Training Loss (NLML): -959.1937\n",
      "convergence dfGPdfNN Run 6/10, Epoch 391/1000, Training Loss (NLML): -959.1774\n",
      "convergence dfGPdfNN Run 6/10, Epoch 392/1000, Training Loss (NLML): -959.2091\n",
      "convergence dfGPdfNN Run 6/10, Epoch 393/1000, Training Loss (NLML): -959.2124\n",
      "convergence dfGPdfNN Run 6/10, Epoch 394/1000, Training Loss (NLML): -959.2540\n",
      "convergence dfGPdfNN Run 6/10, Epoch 395/1000, Training Loss (NLML): -959.3063\n",
      "convergence dfGPdfNN Run 6/10, Epoch 396/1000, Training Loss (NLML): -959.2252\n",
      "convergence dfGPdfNN Run 6/10, Epoch 397/1000, Training Loss (NLML): -959.3013\n",
      "convergence dfGPdfNN Run 6/10, Epoch 398/1000, Training Loss (NLML): -959.2819\n",
      "convergence dfGPdfNN Run 6/10, Epoch 399/1000, Training Loss (NLML): -959.1710\n",
      "convergence dfGPdfNN Run 6/10, Epoch 400/1000, Training Loss (NLML): -959.0802\n",
      "convergence dfGPdfNN Run 6/10, Epoch 401/1000, Training Loss (NLML): -959.1060\n",
      "convergence dfGPdfNN Run 6/10, Epoch 402/1000, Training Loss (NLML): -959.1503\n",
      "convergence dfGPdfNN Run 6/10, Epoch 403/1000, Training Loss (NLML): -959.2000\n",
      "convergence dfGPdfNN Run 6/10, Epoch 404/1000, Training Loss (NLML): -959.2031\n",
      "convergence dfGPdfNN Run 6/10, Epoch 405/1000, Training Loss (NLML): -959.2769\n",
      "convergence dfGPdfNN Run 6/10, Epoch 406/1000, Training Loss (NLML): -959.2942\n",
      "convergence dfGPdfNN Run 6/10, Epoch 407/1000, Training Loss (NLML): -959.2639\n",
      "convergence dfGPdfNN Run 6/10, Epoch 408/1000, Training Loss (NLML): -959.2822\n",
      "convergence dfGPdfNN Run 6/10, Epoch 409/1000, Training Loss (NLML): -959.3071\n",
      "convergence dfGPdfNN Run 6/10, Epoch 410/1000, Training Loss (NLML): -959.3097\n",
      "convergence dfGPdfNN Run 6/10, Epoch 411/1000, Training Loss (NLML): -959.2828\n",
      "convergence dfGPdfNN Run 6/10, Epoch 412/1000, Training Loss (NLML): -959.2496\n",
      "convergence dfGPdfNN Run 6/10, Epoch 413/1000, Training Loss (NLML): -959.2982\n",
      "convergence dfGPdfNN Run 6/10, Epoch 414/1000, Training Loss (NLML): -959.3649\n",
      "convergence dfGPdfNN Run 6/10, Epoch 415/1000, Training Loss (NLML): -959.2777\n",
      "convergence dfGPdfNN Run 6/10, Epoch 416/1000, Training Loss (NLML): -959.2081\n",
      "convergence dfGPdfNN Run 6/10, Epoch 417/1000, Training Loss (NLML): -959.1428\n",
      "convergence dfGPdfNN Run 6/10, Epoch 418/1000, Training Loss (NLML): -959.2555\n",
      "convergence dfGPdfNN Run 6/10, Epoch 419/1000, Training Loss (NLML): -959.2848\n",
      "convergence dfGPdfNN Run 6/10, Epoch 420/1000, Training Loss (NLML): -959.3062\n",
      "convergence dfGPdfNN Run 6/10, Epoch 421/1000, Training Loss (NLML): -959.3260\n",
      "convergence dfGPdfNN Run 6/10, Epoch 422/1000, Training Loss (NLML): -959.3436\n",
      "convergence dfGPdfNN Run 6/10, Epoch 423/1000, Training Loss (NLML): -959.3997\n",
      "convergence dfGPdfNN Run 6/10, Epoch 424/1000, Training Loss (NLML): -959.4222\n",
      "convergence dfGPdfNN Run 6/10, Epoch 425/1000, Training Loss (NLML): -959.4354\n",
      "convergence dfGPdfNN Run 6/10, Epoch 426/1000, Training Loss (NLML): -959.4746\n",
      "convergence dfGPdfNN Run 6/10, Epoch 427/1000, Training Loss (NLML): -959.4464\n",
      "convergence dfGPdfNN Run 6/10, Epoch 428/1000, Training Loss (NLML): -959.4850\n",
      "convergence dfGPdfNN Run 6/10, Epoch 429/1000, Training Loss (NLML): -959.4941\n",
      "convergence dfGPdfNN Run 6/10, Epoch 430/1000, Training Loss (NLML): -959.5808\n",
      "convergence dfGPdfNN Run 6/10, Epoch 431/1000, Training Loss (NLML): -959.6219\n",
      "convergence dfGPdfNN Run 6/10, Epoch 432/1000, Training Loss (NLML): -959.6506\n",
      "convergence dfGPdfNN Run 6/10, Epoch 433/1000, Training Loss (NLML): -959.6670\n",
      "convergence dfGPdfNN Run 6/10, Epoch 434/1000, Training Loss (NLML): -959.6648\n",
      "convergence dfGPdfNN Run 6/10, Epoch 435/1000, Training Loss (NLML): -959.6774\n",
      "convergence dfGPdfNN Run 6/10, Epoch 436/1000, Training Loss (NLML): -959.6537\n",
      "convergence dfGPdfNN Run 6/10, Epoch 437/1000, Training Loss (NLML): -959.6072\n",
      "convergence dfGPdfNN Run 6/10, Epoch 438/1000, Training Loss (NLML): -959.5005\n",
      "convergence dfGPdfNN Run 6/10, Epoch 439/1000, Training Loss (NLML): -959.4873\n",
      "convergence dfGPdfNN Run 6/10, Epoch 440/1000, Training Loss (NLML): -959.5168\n",
      "convergence dfGPdfNN Run 6/10, Epoch 441/1000, Training Loss (NLML): -959.5383\n",
      "convergence dfGPdfNN Run 6/10, Epoch 442/1000, Training Loss (NLML): -959.4885\n",
      "convergence dfGPdfNN Run 6/10, Epoch 443/1000, Training Loss (NLML): -959.4509\n",
      "convergence dfGPdfNN Run 6/10, Epoch 444/1000, Training Loss (NLML): -959.4003\n",
      "convergence dfGPdfNN Run 6/10, Epoch 445/1000, Training Loss (NLML): -959.4110\n",
      "convergence dfGPdfNN Run 6/10, Epoch 446/1000, Training Loss (NLML): -959.4315\n",
      "convergence dfGPdfNN Run 6/10, Epoch 447/1000, Training Loss (NLML): -959.4503\n",
      "convergence dfGPdfNN Run 6/10, Epoch 448/1000, Training Loss (NLML): -959.4657\n",
      "convergence dfGPdfNN Run 6/10, Epoch 449/1000, Training Loss (NLML): -959.4408\n",
      "convergence dfGPdfNN Run 6/10, Epoch 450/1000, Training Loss (NLML): -959.4200\n",
      "convergence dfGPdfNN Run 6/10, Epoch 451/1000, Training Loss (NLML): -959.4453\n",
      "convergence dfGPdfNN Run 6/10, Epoch 452/1000, Training Loss (NLML): -959.4794\n",
      "convergence dfGPdfNN Run 6/10, Epoch 453/1000, Training Loss (NLML): -959.5245\n",
      "convergence dfGPdfNN Run 6/10, Epoch 454/1000, Training Loss (NLML): -959.5430\n",
      "convergence dfGPdfNN Run 6/10, Epoch 455/1000, Training Loss (NLML): -959.5513\n",
      "convergence dfGPdfNN Run 6/10, Epoch 456/1000, Training Loss (NLML): -959.5525\n",
      "convergence dfGPdfNN Run 6/10, Epoch 457/1000, Training Loss (NLML): -959.5419\n",
      "convergence dfGPdfNN Run 6/10, Epoch 458/1000, Training Loss (NLML): -959.5740\n",
      "convergence dfGPdfNN Run 6/10, Epoch 459/1000, Training Loss (NLML): -959.6080\n",
      "convergence dfGPdfNN Run 6/10, Epoch 460/1000, Training Loss (NLML): -959.6533\n",
      "convergence dfGPdfNN Run 6/10, Epoch 461/1000, Training Loss (NLML): -959.6926\n",
      "convergence dfGPdfNN Run 6/10, Epoch 462/1000, Training Loss (NLML): -959.6875\n",
      "convergence dfGPdfNN Run 6/10, Epoch 463/1000, Training Loss (NLML): -959.6976\n",
      "convergence dfGPdfNN Run 6/10, Epoch 464/1000, Training Loss (NLML): -959.7045\n",
      "convergence dfGPdfNN Run 6/10, Epoch 465/1000, Training Loss (NLML): -959.7629\n",
      "convergence dfGPdfNN Run 6/10, Epoch 466/1000, Training Loss (NLML): -959.8059\n",
      "convergence dfGPdfNN Run 6/10, Epoch 467/1000, Training Loss (NLML): -959.8125\n",
      "convergence dfGPdfNN Run 6/10, Epoch 468/1000, Training Loss (NLML): -959.8107\n",
      "convergence dfGPdfNN Run 6/10, Epoch 469/1000, Training Loss (NLML): -959.7933\n",
      "convergence dfGPdfNN Run 6/10, Epoch 470/1000, Training Loss (NLML): -959.7625\n",
      "convergence dfGPdfNN Run 6/10, Epoch 471/1000, Training Loss (NLML): -959.7465\n",
      "convergence dfGPdfNN Run 6/10, Epoch 472/1000, Training Loss (NLML): -959.7820\n",
      "convergence dfGPdfNN Run 6/10, Epoch 473/1000, Training Loss (NLML): -959.7640\n",
      "convergence dfGPdfNN Run 6/10, Epoch 474/1000, Training Loss (NLML): -959.6923\n",
      "convergence dfGPdfNN Run 6/10, Epoch 475/1000, Training Loss (NLML): -959.6171\n",
      "convergence dfGPdfNN Run 6/10, Epoch 476/1000, Training Loss (NLML): -959.6667\n",
      "convergence dfGPdfNN Run 6/10, Epoch 477/1000, Training Loss (NLML): -959.6815\n",
      "convergence dfGPdfNN Run 6/10, Epoch 478/1000, Training Loss (NLML): -959.7180\n",
      "convergence dfGPdfNN Run 6/10, Epoch 479/1000, Training Loss (NLML): -959.7047\n",
      "convergence dfGPdfNN Run 6/10, Epoch 480/1000, Training Loss (NLML): -959.6805\n",
      "convergence dfGPdfNN Run 6/10, Epoch 481/1000, Training Loss (NLML): -959.6377\n",
      "convergence dfGPdfNN Run 6/10, Epoch 482/1000, Training Loss (NLML): -959.6489\n",
      "convergence dfGPdfNN Run 6/10, Epoch 483/1000, Training Loss (NLML): -959.6235\n",
      "convergence dfGPdfNN Run 6/10, Epoch 484/1000, Training Loss (NLML): -959.6593\n",
      "convergence dfGPdfNN Run 6/10, Epoch 485/1000, Training Loss (NLML): -959.6714\n",
      "convergence dfGPdfNN Run 6/10, Epoch 486/1000, Training Loss (NLML): -959.6720\n",
      "convergence dfGPdfNN Run 6/10, Epoch 487/1000, Training Loss (NLML): -959.6801\n",
      "convergence dfGPdfNN Run 6/10, Epoch 488/1000, Training Loss (NLML): -959.6779\n",
      "convergence dfGPdfNN Run 6/10, Epoch 489/1000, Training Loss (NLML): -959.7283\n",
      "convergence dfGPdfNN Run 6/10, Epoch 490/1000, Training Loss (NLML): -959.7283\n",
      "convergence dfGPdfNN Run 6/10, Epoch 491/1000, Training Loss (NLML): -959.7520\n",
      "convergence dfGPdfNN Run 6/10, Epoch 492/1000, Training Loss (NLML): -959.7499\n",
      "convergence dfGPdfNN Run 6/10, Epoch 493/1000, Training Loss (NLML): -959.7719\n",
      "convergence dfGPdfNN Run 6/10, Epoch 494/1000, Training Loss (NLML): -959.7736\n",
      "convergence dfGPdfNN Run 6/10, Epoch 495/1000, Training Loss (NLML): -959.7726\n",
      "convergence dfGPdfNN Run 6/10, Epoch 496/1000, Training Loss (NLML): -959.7780\n",
      "convergence dfGPdfNN Run 6/10, Epoch 497/1000, Training Loss (NLML): -959.8423\n",
      "convergence dfGPdfNN Run 6/10, Epoch 498/1000, Training Loss (NLML): -959.8971\n",
      "convergence dfGPdfNN Run 6/10, Epoch 499/1000, Training Loss (NLML): -959.9044\n",
      "convergence dfGPdfNN Run 6/10, Epoch 500/1000, Training Loss (NLML): -959.9202\n",
      "convergence dfGPdfNN Run 6/10, Epoch 501/1000, Training Loss (NLML): -959.9288\n",
      "convergence dfGPdfNN Run 6/10, Epoch 502/1000, Training Loss (NLML): -959.8837\n",
      "convergence dfGPdfNN Run 6/10, Epoch 503/1000, Training Loss (NLML): -959.8757\n",
      "convergence dfGPdfNN Run 6/10, Epoch 504/1000, Training Loss (NLML): -959.8429\n",
      "convergence dfGPdfNN Run 6/10, Epoch 505/1000, Training Loss (NLML): -959.8505\n",
      "convergence dfGPdfNN Run 6/10, Epoch 506/1000, Training Loss (NLML): -959.8669\n",
      "convergence dfGPdfNN Run 6/10, Epoch 507/1000, Training Loss (NLML): -959.9163\n",
      "convergence dfGPdfNN Run 6/10, Epoch 508/1000, Training Loss (NLML): -959.9465\n",
      "convergence dfGPdfNN Run 6/10, Epoch 509/1000, Training Loss (NLML): -959.9890\n",
      "convergence dfGPdfNN Run 6/10, Epoch 510/1000, Training Loss (NLML): -960.0029\n",
      "convergence dfGPdfNN Run 6/10, Epoch 511/1000, Training Loss (NLML): -960.0129\n",
      "convergence dfGPdfNN Run 6/10, Epoch 512/1000, Training Loss (NLML): -959.9973\n",
      "convergence dfGPdfNN Run 6/10, Epoch 513/1000, Training Loss (NLML): -959.9531\n",
      "convergence dfGPdfNN Run 6/10, Epoch 514/1000, Training Loss (NLML): -959.8690\n",
      "convergence dfGPdfNN Run 6/10, Epoch 515/1000, Training Loss (NLML): -959.8525\n",
      "convergence dfGPdfNN Run 6/10, Epoch 516/1000, Training Loss (NLML): -959.8610\n",
      "convergence dfGPdfNN Run 6/10, Epoch 517/1000, Training Loss (NLML): -959.8820\n",
      "convergence dfGPdfNN Run 6/10, Epoch 518/1000, Training Loss (NLML): -959.8888\n",
      "convergence dfGPdfNN Run 6/10, Epoch 519/1000, Training Loss (NLML): -959.8778\n",
      "convergence dfGPdfNN Run 6/10, Epoch 520/1000, Training Loss (NLML): -959.8783\n",
      "convergence dfGPdfNN Run 6/10, Epoch 521/1000, Training Loss (NLML): -959.8726\n",
      "convergence dfGPdfNN Run 6/10, Epoch 522/1000, Training Loss (NLML): -959.8718\n",
      "convergence dfGPdfNN Run 6/10, Epoch 523/1000, Training Loss (NLML): -959.8888\n",
      "convergence dfGPdfNN Run 6/10, Epoch 524/1000, Training Loss (NLML): -959.9045\n",
      "convergence dfGPdfNN Run 6/10, Epoch 525/1000, Training Loss (NLML): -959.9150\n",
      "convergence dfGPdfNN Run 6/10, Epoch 526/1000, Training Loss (NLML): -959.9266\n",
      "convergence dfGPdfNN Run 6/10, Epoch 527/1000, Training Loss (NLML): -959.9360\n",
      "convergence dfGPdfNN Run 6/10, Epoch 528/1000, Training Loss (NLML): -959.9404\n",
      "convergence dfGPdfNN Run 6/10, Epoch 529/1000, Training Loss (NLML): -959.9312\n",
      "convergence dfGPdfNN Run 6/10, Epoch 530/1000, Training Loss (NLML): -959.9420\n",
      "convergence dfGPdfNN Run 6/10, Epoch 531/1000, Training Loss (NLML): -959.9398\n",
      "convergence dfGPdfNN Run 6/10, Epoch 532/1000, Training Loss (NLML): -959.9098\n",
      "convergence dfGPdfNN Run 6/10, Epoch 533/1000, Training Loss (NLML): -959.9016\n",
      "convergence dfGPdfNN Run 6/10, Epoch 534/1000, Training Loss (NLML): -959.9119\n",
      "convergence dfGPdfNN Run 6/10, Epoch 535/1000, Training Loss (NLML): -959.8953\n",
      "convergence dfGPdfNN Run 6/10, Epoch 536/1000, Training Loss (NLML): -959.9263\n",
      "convergence dfGPdfNN Run 6/10, Epoch 537/1000, Training Loss (NLML): -959.9387\n",
      "convergence dfGPdfNN Run 6/10, Epoch 538/1000, Training Loss (NLML): -959.9473\n",
      "convergence dfGPdfNN Run 6/10, Epoch 539/1000, Training Loss (NLML): -959.9404\n",
      "convergence dfGPdfNN Run 6/10, Epoch 540/1000, Training Loss (NLML): -959.9381\n",
      "convergence dfGPdfNN Run 6/10, Epoch 541/1000, Training Loss (NLML): -959.9435\n",
      "convergence dfGPdfNN Run 6/10, Epoch 542/1000, Training Loss (NLML): -959.9556\n",
      "convergence dfGPdfNN Run 6/10, Epoch 543/1000, Training Loss (NLML): -959.9673\n",
      "convergence dfGPdfNN Run 6/10, Epoch 544/1000, Training Loss (NLML): -959.9790\n",
      "convergence dfGPdfNN Run 6/10, Epoch 545/1000, Training Loss (NLML): -959.9662\n",
      "convergence dfGPdfNN Run 6/10, Epoch 546/1000, Training Loss (NLML): -959.9685\n",
      "convergence dfGPdfNN Run 6/10, Epoch 547/1000, Training Loss (NLML): -959.9785\n",
      "convergence dfGPdfNN Run 6/10, Epoch 548/1000, Training Loss (NLML): -959.9851\n",
      "convergence dfGPdfNN Run 6/10, Epoch 549/1000, Training Loss (NLML): -959.9797\n",
      "convergence dfGPdfNN Run 6/10, Epoch 550/1000, Training Loss (NLML): -959.9850\n",
      "convergence dfGPdfNN Run 6/10, Epoch 551/1000, Training Loss (NLML): -959.9868\n",
      "convergence dfGPdfNN Run 6/10, Epoch 552/1000, Training Loss (NLML): -959.9922\n",
      "convergence dfGPdfNN Run 6/10, Epoch 553/1000, Training Loss (NLML): -959.9930\n",
      "convergence dfGPdfNN Run 6/10, Epoch 554/1000, Training Loss (NLML): -960.0015\n",
      "convergence dfGPdfNN Run 6/10, Epoch 555/1000, Training Loss (NLML): -960.0128\n",
      "convergence dfGPdfNN Run 6/10, Epoch 556/1000, Training Loss (NLML): -960.0234\n",
      "convergence dfGPdfNN Run 6/10, Epoch 557/1000, Training Loss (NLML): -960.0280\n",
      "convergence dfGPdfNN Run 6/10, Epoch 558/1000, Training Loss (NLML): -960.0283\n",
      "convergence dfGPdfNN Run 6/10, Epoch 559/1000, Training Loss (NLML): -960.0331\n",
      "convergence dfGPdfNN Run 6/10, Epoch 560/1000, Training Loss (NLML): -960.0410\n",
      "convergence dfGPdfNN Run 6/10, Epoch 561/1000, Training Loss (NLML): -960.0507\n",
      "convergence dfGPdfNN Run 6/10, Epoch 562/1000, Training Loss (NLML): -960.0560\n",
      "convergence dfGPdfNN Run 6/10, Epoch 563/1000, Training Loss (NLML): -960.0577\n",
      "convergence dfGPdfNN Run 6/10, Epoch 564/1000, Training Loss (NLML): -960.0601\n",
      "convergence dfGPdfNN Run 6/10, Epoch 565/1000, Training Loss (NLML): -960.0629\n",
      "convergence dfGPdfNN Run 6/10, Epoch 566/1000, Training Loss (NLML): -960.0652\n",
      "convergence dfGPdfNN Run 6/10, Epoch 567/1000, Training Loss (NLML): -960.0635\n",
      "convergence dfGPdfNN Run 6/10, Epoch 568/1000, Training Loss (NLML): -960.0631\n",
      "convergence dfGPdfNN Run 6/10, Epoch 569/1000, Training Loss (NLML): -960.0652\n",
      "convergence dfGPdfNN Run 6/10, Epoch 570/1000, Training Loss (NLML): -960.0699\n",
      "convergence dfGPdfNN Run 6/10, Epoch 571/1000, Training Loss (NLML): -960.0691\n",
      "convergence dfGPdfNN Run 6/10, Epoch 572/1000, Training Loss (NLML): -960.0741\n",
      "convergence dfGPdfNN Run 6/10, Epoch 573/1000, Training Loss (NLML): -960.0757\n",
      "convergence dfGPdfNN Run 6/10, Epoch 574/1000, Training Loss (NLML): -960.0792\n",
      "convergence dfGPdfNN Run 6/10, Epoch 575/1000, Training Loss (NLML): -960.0847\n",
      "convergence dfGPdfNN Run 6/10, Epoch 576/1000, Training Loss (NLML): -960.0961\n",
      "convergence dfGPdfNN Run 6/10, Epoch 577/1000, Training Loss (NLML): -960.1002\n",
      "convergence dfGPdfNN Run 6/10, Epoch 578/1000, Training Loss (NLML): -960.1040\n",
      "convergence dfGPdfNN Run 6/10, Epoch 579/1000, Training Loss (NLML): -960.0934\n",
      "convergence dfGPdfNN Run 6/10, Epoch 580/1000, Training Loss (NLML): -960.0853\n",
      "convergence dfGPdfNN Run 6/10, Epoch 581/1000, Training Loss (NLML): -960.1000\n",
      "convergence dfGPdfNN Run 6/10, Epoch 582/1000, Training Loss (NLML): -960.0938\n",
      "convergence dfGPdfNN Run 6/10, Epoch 583/1000, Training Loss (NLML): -960.0991\n",
      "convergence dfGPdfNN Run 6/10, Epoch 584/1000, Training Loss (NLML): -960.1030\n",
      "convergence dfGPdfNN Run 6/10, Epoch 585/1000, Training Loss (NLML): -960.1050\n",
      "convergence dfGPdfNN Run 6/10, Epoch 586/1000, Training Loss (NLML): -960.1116\n",
      "convergence dfGPdfNN Run 6/10, Epoch 587/1000, Training Loss (NLML): -960.1140\n",
      "convergence dfGPdfNN Run 6/10, Epoch 588/1000, Training Loss (NLML): -960.1176\n",
      "convergence dfGPdfNN Run 6/10, Epoch 589/1000, Training Loss (NLML): -960.1239\n",
      "convergence dfGPdfNN Run 6/10, Epoch 590/1000, Training Loss (NLML): -960.1436\n",
      "convergence dfGPdfNN Run 6/10, Epoch 591/1000, Training Loss (NLML): -960.1545\n",
      "convergence dfGPdfNN Run 6/10, Epoch 592/1000, Training Loss (NLML): -960.1478\n",
      "convergence dfGPdfNN Run 6/10, Epoch 593/1000, Training Loss (NLML): -960.1494\n",
      "convergence dfGPdfNN Run 6/10, Epoch 594/1000, Training Loss (NLML): -960.1580\n",
      "convergence dfGPdfNN Run 6/10, Epoch 595/1000, Training Loss (NLML): -960.1641\n",
      "convergence dfGPdfNN Run 6/10, Epoch 596/1000, Training Loss (NLML): -960.1705\n",
      "convergence dfGPdfNN Run 6/10, Epoch 597/1000, Training Loss (NLML): -960.1729\n",
      "convergence dfGPdfNN Run 6/10, Epoch 598/1000, Training Loss (NLML): -960.1750\n",
      "convergence dfGPdfNN Run 6/10, Epoch 599/1000, Training Loss (NLML): -960.1792\n",
      "convergence dfGPdfNN Run 6/10, Epoch 600/1000, Training Loss (NLML): -960.1848\n",
      "convergence dfGPdfNN Run 6/10, Epoch 601/1000, Training Loss (NLML): -960.1863\n",
      "convergence dfGPdfNN Run 6/10, Epoch 602/1000, Training Loss (NLML): -960.1897\n",
      "convergence dfGPdfNN Run 6/10, Epoch 603/1000, Training Loss (NLML): -960.1935\n",
      "convergence dfGPdfNN Run 6/10, Epoch 604/1000, Training Loss (NLML): -960.1958\n",
      "convergence dfGPdfNN Run 6/10, Epoch 605/1000, Training Loss (NLML): -960.2021\n",
      "convergence dfGPdfNN Run 6/10, Epoch 606/1000, Training Loss (NLML): -960.2109\n",
      "convergence dfGPdfNN Run 6/10, Epoch 607/1000, Training Loss (NLML): -960.2157\n",
      "convergence dfGPdfNN Run 6/10, Epoch 608/1000, Training Loss (NLML): -960.2179\n",
      "convergence dfGPdfNN Run 6/10, Epoch 609/1000, Training Loss (NLML): -960.2207\n",
      "convergence dfGPdfNN Run 6/10, Epoch 610/1000, Training Loss (NLML): -960.2260\n",
      "convergence dfGPdfNN Run 6/10, Epoch 611/1000, Training Loss (NLML): -960.2297\n",
      "convergence dfGPdfNN Run 6/10, Epoch 612/1000, Training Loss (NLML): -960.2313\n",
      "convergence dfGPdfNN Run 6/10, Epoch 613/1000, Training Loss (NLML): -960.2356\n",
      "convergence dfGPdfNN Run 6/10, Epoch 614/1000, Training Loss (NLML): -960.2445\n",
      "convergence dfGPdfNN Run 6/10, Epoch 615/1000, Training Loss (NLML): -960.2462\n",
      "convergence dfGPdfNN Run 6/10, Epoch 616/1000, Training Loss (NLML): -960.2473\n",
      "convergence dfGPdfNN Run 6/10, Epoch 617/1000, Training Loss (NLML): -960.2484\n",
      "convergence dfGPdfNN Run 6/10, Epoch 618/1000, Training Loss (NLML): -960.2372\n",
      "convergence dfGPdfNN Run 6/10, Epoch 619/1000, Training Loss (NLML): -960.2582\n",
      "convergence dfGPdfNN Run 6/10, Epoch 620/1000, Training Loss (NLML): -960.2599\n",
      "convergence dfGPdfNN Run 6/10, Epoch 621/1000, Training Loss (NLML): -960.2670\n",
      "convergence dfGPdfNN Run 6/10, Epoch 622/1000, Training Loss (NLML): -960.2689\n",
      "convergence dfGPdfNN Run 6/10, Epoch 623/1000, Training Loss (NLML): -960.2726\n",
      "convergence dfGPdfNN Run 6/10, Epoch 624/1000, Training Loss (NLML): -960.2775\n",
      "convergence dfGPdfNN Run 6/10, Epoch 625/1000, Training Loss (NLML): -960.2804\n",
      "convergence dfGPdfNN Run 6/10, Epoch 626/1000, Training Loss (NLML): -960.2834\n",
      "convergence dfGPdfNN Run 6/10, Epoch 627/1000, Training Loss (NLML): -960.2842\n",
      "convergence dfGPdfNN Run 6/10, Epoch 628/1000, Training Loss (NLML): -960.2872\n",
      "convergence dfGPdfNN Run 6/10, Epoch 629/1000, Training Loss (NLML): -960.2859\n",
      "convergence dfGPdfNN Run 6/10, Epoch 630/1000, Training Loss (NLML): -960.2881\n",
      "convergence dfGPdfNN Run 6/10, Epoch 631/1000, Training Loss (NLML): -960.2915\n",
      "convergence dfGPdfNN Run 6/10, Epoch 632/1000, Training Loss (NLML): -960.2961\n",
      "convergence dfGPdfNN Run 6/10, Epoch 633/1000, Training Loss (NLML): -960.2986\n",
      "convergence dfGPdfNN Run 6/10, Epoch 634/1000, Training Loss (NLML): -960.2897\n",
      "convergence dfGPdfNN Run 6/10, Epoch 635/1000, Training Loss (NLML): -960.2941\n",
      "convergence dfGPdfNN Run 6/10, Epoch 636/1000, Training Loss (NLML): -960.2983\n",
      "convergence dfGPdfNN Run 6/10, Epoch 637/1000, Training Loss (NLML): -960.3029\n",
      "convergence dfGPdfNN Run 6/10, Epoch 638/1000, Training Loss (NLML): -960.3055\n",
      "convergence dfGPdfNN Run 6/10, Epoch 639/1000, Training Loss (NLML): -960.3224\n",
      "convergence dfGPdfNN Run 6/10, Epoch 640/1000, Training Loss (NLML): -960.3247\n",
      "convergence dfGPdfNN Run 6/10, Epoch 641/1000, Training Loss (NLML): -960.3297\n",
      "convergence dfGPdfNN Run 6/10, Epoch 642/1000, Training Loss (NLML): -960.3245\n",
      "convergence dfGPdfNN Run 6/10, Epoch 643/1000, Training Loss (NLML): -960.3286\n",
      "convergence dfGPdfNN Run 6/10, Epoch 644/1000, Training Loss (NLML): -960.3378\n",
      "convergence dfGPdfNN Run 6/10, Epoch 645/1000, Training Loss (NLML): -960.3326\n",
      "convergence dfGPdfNN Run 6/10, Epoch 646/1000, Training Loss (NLML): -960.3379\n",
      "convergence dfGPdfNN Run 6/10, Epoch 647/1000, Training Loss (NLML): -960.3430\n",
      "convergence dfGPdfNN Run 6/10, Epoch 648/1000, Training Loss (NLML): -960.3475\n",
      "convergence dfGPdfNN Run 6/10, Epoch 649/1000, Training Loss (NLML): -960.3510\n",
      "convergence dfGPdfNN Run 6/10, Epoch 650/1000, Training Loss (NLML): -960.3511\n",
      "convergence dfGPdfNN Run 6/10, Epoch 651/1000, Training Loss (NLML): -960.3539\n",
      "convergence dfGPdfNN Run 6/10, Epoch 652/1000, Training Loss (NLML): -960.3527\n",
      "convergence dfGPdfNN Run 6/10, Epoch 653/1000, Training Loss (NLML): -960.3550\n",
      "convergence dfGPdfNN Run 6/10, Epoch 654/1000, Training Loss (NLML): -960.3586\n",
      "convergence dfGPdfNN Run 6/10, Epoch 655/1000, Training Loss (NLML): -960.3607\n",
      "convergence dfGPdfNN Run 6/10, Epoch 656/1000, Training Loss (NLML): -960.3698\n",
      "convergence dfGPdfNN Run 6/10, Epoch 657/1000, Training Loss (NLML): -960.3724\n",
      "convergence dfGPdfNN Run 6/10, Epoch 658/1000, Training Loss (NLML): -960.3763\n",
      "convergence dfGPdfNN Run 6/10, Epoch 659/1000, Training Loss (NLML): -960.3793\n",
      "convergence dfGPdfNN Run 6/10, Epoch 660/1000, Training Loss (NLML): -960.3817\n",
      "convergence dfGPdfNN Run 6/10, Epoch 661/1000, Training Loss (NLML): -960.3875\n",
      "convergence dfGPdfNN Run 6/10, Epoch 662/1000, Training Loss (NLML): -960.3844\n",
      "convergence dfGPdfNN Run 6/10, Epoch 663/1000, Training Loss (NLML): -960.3765\n",
      "convergence dfGPdfNN Run 6/10, Epoch 664/1000, Training Loss (NLML): -960.3694\n",
      "convergence dfGPdfNN Run 6/10, Epoch 665/1000, Training Loss (NLML): -960.3857\n",
      "convergence dfGPdfNN Run 6/10, Epoch 666/1000, Training Loss (NLML): -960.3954\n",
      "convergence dfGPdfNN Run 6/10, Epoch 667/1000, Training Loss (NLML): -960.3969\n",
      "convergence dfGPdfNN Run 6/10, Epoch 668/1000, Training Loss (NLML): -960.4001\n",
      "convergence dfGPdfNN Run 6/10, Epoch 669/1000, Training Loss (NLML): -960.4030\n",
      "convergence dfGPdfNN Run 6/10, Epoch 670/1000, Training Loss (NLML): -960.3901\n",
      "convergence dfGPdfNN Run 6/10, Epoch 671/1000, Training Loss (NLML): -960.3921\n",
      "convergence dfGPdfNN Run 6/10, Epoch 672/1000, Training Loss (NLML): -960.3959\n",
      "convergence dfGPdfNN Run 6/10, Epoch 673/1000, Training Loss (NLML): -960.4058\n",
      "convergence dfGPdfNN Run 6/10, Epoch 674/1000, Training Loss (NLML): -960.4098\n",
      "convergence dfGPdfNN Run 6/10, Epoch 675/1000, Training Loss (NLML): -960.4119\n",
      "convergence dfGPdfNN Run 6/10, Epoch 676/1000, Training Loss (NLML): -960.4172\n",
      "convergence dfGPdfNN Run 6/10, Epoch 677/1000, Training Loss (NLML): -960.4230\n",
      "convergence dfGPdfNN Run 6/10, Epoch 678/1000, Training Loss (NLML): -960.4279\n",
      "convergence dfGPdfNN Run 6/10, Epoch 679/1000, Training Loss (NLML): -960.4288\n",
      "convergence dfGPdfNN Run 6/10, Epoch 680/1000, Training Loss (NLML): -960.4337\n",
      "convergence dfGPdfNN Run 6/10, Epoch 681/1000, Training Loss (NLML): -960.4382\n",
      "convergence dfGPdfNN Run 6/10, Epoch 682/1000, Training Loss (NLML): -960.4410\n",
      "convergence dfGPdfNN Run 6/10, Epoch 683/1000, Training Loss (NLML): -960.4434\n",
      "convergence dfGPdfNN Run 6/10, Epoch 684/1000, Training Loss (NLML): -960.4404\n",
      "convergence dfGPdfNN Run 6/10, Epoch 685/1000, Training Loss (NLML): -960.4404\n",
      "convergence dfGPdfNN Run 6/10, Epoch 686/1000, Training Loss (NLML): -960.4456\n",
      "convergence dfGPdfNN Run 6/10, Epoch 687/1000, Training Loss (NLML): -960.4517\n",
      "convergence dfGPdfNN Run 6/10, Epoch 688/1000, Training Loss (NLML): -960.4546\n",
      "convergence dfGPdfNN Run 6/10, Epoch 689/1000, Training Loss (NLML): -960.4584\n",
      "convergence dfGPdfNN Run 6/10, Epoch 690/1000, Training Loss (NLML): -960.4624\n",
      "convergence dfGPdfNN Run 6/10, Epoch 691/1000, Training Loss (NLML): -960.4667\n",
      "convergence dfGPdfNN Run 6/10, Epoch 692/1000, Training Loss (NLML): -960.4695\n",
      "convergence dfGPdfNN Run 6/10, Epoch 693/1000, Training Loss (NLML): -960.4512\n",
      "convergence dfGPdfNN Run 6/10, Epoch 694/1000, Training Loss (NLML): -960.4561\n",
      "convergence dfGPdfNN Run 6/10, Epoch 695/1000, Training Loss (NLML): -960.4557\n",
      "convergence dfGPdfNN Run 6/10, Epoch 696/1000, Training Loss (NLML): -960.4569\n",
      "convergence dfGPdfNN Run 6/10, Epoch 697/1000, Training Loss (NLML): -960.4601\n",
      "convergence dfGPdfNN Run 6/10, Epoch 698/1000, Training Loss (NLML): -960.4340\n",
      "convergence dfGPdfNN Run 6/10, Epoch 699/1000, Training Loss (NLML): -960.4235\n",
      "convergence dfGPdfNN Run 6/10, Epoch 700/1000, Training Loss (NLML): -960.4288\n",
      "convergence dfGPdfNN Run 6/10, Epoch 701/1000, Training Loss (NLML): -960.4623\n",
      "convergence dfGPdfNN Run 6/10, Epoch 702/1000, Training Loss (NLML): -960.4346\n",
      "convergence dfGPdfNN Run 6/10, Epoch 703/1000, Training Loss (NLML): -960.4384\n",
      "convergence dfGPdfNN Run 6/10, Epoch 704/1000, Training Loss (NLML): -960.4412\n",
      "convergence dfGPdfNN Run 6/10, Epoch 705/1000, Training Loss (NLML): -960.4200\n",
      "convergence dfGPdfNN Run 6/10, Epoch 706/1000, Training Loss (NLML): -960.4231\n",
      "convergence dfGPdfNN Run 6/10, Epoch 707/1000, Training Loss (NLML): -960.4260\n",
      "convergence dfGPdfNN Run 6/10, Epoch 708/1000, Training Loss (NLML): -960.4137\n",
      "convergence dfGPdfNN Run 6/10, Epoch 709/1000, Training Loss (NLML): -960.4353\n",
      "convergence dfGPdfNN Run 6/10, Epoch 710/1000, Training Loss (NLML): -960.4609\n",
      "convergence dfGPdfNN Run 6/10, Epoch 711/1000, Training Loss (NLML): -960.4629\n",
      "convergence dfGPdfNN Run 6/10, Epoch 712/1000, Training Loss (NLML): -960.4667\n",
      "convergence dfGPdfNN Run 6/10, Epoch 713/1000, Training Loss (NLML): -960.4669\n",
      "convergence dfGPdfNN Run 6/10, Epoch 714/1000, Training Loss (NLML): -960.4713\n",
      "convergence dfGPdfNN Run 6/10, Epoch 715/1000, Training Loss (NLML): -960.4729\n",
      "convergence dfGPdfNN Run 6/10, Epoch 716/1000, Training Loss (NLML): -960.4766\n",
      "convergence dfGPdfNN Run 6/10, Epoch 717/1000, Training Loss (NLML): -960.4823\n",
      "convergence dfGPdfNN Run 6/10, Epoch 718/1000, Training Loss (NLML): -960.4851\n",
      "convergence dfGPdfNN Run 6/10, Epoch 719/1000, Training Loss (NLML): -960.4872\n",
      "convergence dfGPdfNN Run 6/10, Epoch 720/1000, Training Loss (NLML): -960.4908\n",
      "convergence dfGPdfNN Run 6/10, Epoch 721/1000, Training Loss (NLML): -960.4924\n",
      "convergence dfGPdfNN Run 6/10, Epoch 722/1000, Training Loss (NLML): -960.4956\n",
      "convergence dfGPdfNN Run 6/10, Epoch 723/1000, Training Loss (NLML): -960.4998\n",
      "convergence dfGPdfNN Run 6/10, Epoch 724/1000, Training Loss (NLML): -960.5012\n",
      "convergence dfGPdfNN Run 6/10, Epoch 725/1000, Training Loss (NLML): -960.5035\n",
      "convergence dfGPdfNN Run 6/10, Epoch 726/1000, Training Loss (NLML): -960.5061\n",
      "convergence dfGPdfNN Run 6/10, Epoch 727/1000, Training Loss (NLML): -960.5283\n",
      "convergence dfGPdfNN Run 6/10, Epoch 728/1000, Training Loss (NLML): -960.5293\n",
      "convergence dfGPdfNN Run 6/10, Epoch 729/1000, Training Loss (NLML): -960.5142\n",
      "convergence dfGPdfNN Run 6/10, Epoch 730/1000, Training Loss (NLML): -960.5175\n",
      "convergence dfGPdfNN Run 6/10, Epoch 731/1000, Training Loss (NLML): -960.5211\n",
      "convergence dfGPdfNN Run 6/10, Epoch 732/1000, Training Loss (NLML): -960.5248\n",
      "convergence dfGPdfNN Run 6/10, Epoch 733/1000, Training Loss (NLML): -960.5272\n",
      "convergence dfGPdfNN Run 6/10, Epoch 734/1000, Training Loss (NLML): -960.5289\n",
      "convergence dfGPdfNN Run 6/10, Epoch 735/1000, Training Loss (NLML): -960.5354\n",
      "convergence dfGPdfNN Run 6/10, Epoch 736/1000, Training Loss (NLML): -960.5353\n",
      "convergence dfGPdfNN Run 6/10, Epoch 737/1000, Training Loss (NLML): -960.5389\n",
      "convergence dfGPdfNN Run 6/10, Epoch 738/1000, Training Loss (NLML): -960.5392\n",
      "convergence dfGPdfNN Run 6/10, Epoch 739/1000, Training Loss (NLML): -960.5437\n",
      "convergence dfGPdfNN Run 6/10, Epoch 740/1000, Training Loss (NLML): -960.5459\n",
      "convergence dfGPdfNN Run 6/10, Epoch 741/1000, Training Loss (NLML): -960.5531\n",
      "convergence dfGPdfNN Run 6/10, Epoch 742/1000, Training Loss (NLML): -960.5524\n",
      "convergence dfGPdfNN Run 6/10, Epoch 743/1000, Training Loss (NLML): -960.5558\n",
      "convergence dfGPdfNN Run 6/10, Epoch 744/1000, Training Loss (NLML): -960.5575\n",
      "convergence dfGPdfNN Run 6/10, Epoch 745/1000, Training Loss (NLML): -960.5581\n",
      "convergence dfGPdfNN Run 6/10, Epoch 746/1000, Training Loss (NLML): -960.5626\n",
      "convergence dfGPdfNN Run 6/10, Epoch 747/1000, Training Loss (NLML): -960.5658\n",
      "convergence dfGPdfNN Run 6/10, Epoch 748/1000, Training Loss (NLML): -960.5691\n",
      "convergence dfGPdfNN Run 6/10, Epoch 749/1000, Training Loss (NLML): -960.5736\n",
      "convergence dfGPdfNN Run 6/10, Epoch 750/1000, Training Loss (NLML): -960.5751\n",
      "convergence dfGPdfNN Run 6/10, Epoch 751/1000, Training Loss (NLML): -960.5760\n",
      "convergence dfGPdfNN Run 6/10, Epoch 752/1000, Training Loss (NLML): -960.5773\n",
      "convergence dfGPdfNN Run 6/10, Epoch 753/1000, Training Loss (NLML): -960.5807\n",
      "convergence dfGPdfNN Run 6/10, Epoch 754/1000, Training Loss (NLML): -960.5845\n",
      "convergence dfGPdfNN Run 6/10, Epoch 755/1000, Training Loss (NLML): -960.5852\n",
      "convergence dfGPdfNN Run 6/10, Epoch 756/1000, Training Loss (NLML): -960.5884\n",
      "convergence dfGPdfNN Run 6/10, Epoch 757/1000, Training Loss (NLML): -960.5907\n",
      "convergence dfGPdfNN Run 6/10, Epoch 758/1000, Training Loss (NLML): -960.5962\n",
      "convergence dfGPdfNN Run 6/10, Epoch 759/1000, Training Loss (NLML): -960.5981\n",
      "convergence dfGPdfNN Run 6/10, Epoch 760/1000, Training Loss (NLML): -960.5975\n",
      "convergence dfGPdfNN Run 6/10, Epoch 761/1000, Training Loss (NLML): -960.6001\n",
      "convergence dfGPdfNN Run 6/10, Epoch 762/1000, Training Loss (NLML): -960.6040\n",
      "convergence dfGPdfNN Run 6/10, Epoch 763/1000, Training Loss (NLML): -960.6067\n",
      "convergence dfGPdfNN Run 6/10, Epoch 764/1000, Training Loss (NLML): -960.6152\n",
      "convergence dfGPdfNN Run 6/10, Epoch 765/1000, Training Loss (NLML): -960.6023\n",
      "convergence dfGPdfNN Run 6/10, Epoch 766/1000, Training Loss (NLML): -960.6046\n",
      "convergence dfGPdfNN Run 6/10, Epoch 767/1000, Training Loss (NLML): -960.6104\n",
      "convergence dfGPdfNN Run 6/10, Epoch 768/1000, Training Loss (NLML): -960.6108\n",
      "convergence dfGPdfNN Run 6/10, Epoch 769/1000, Training Loss (NLML): -960.6117\n",
      "convergence dfGPdfNN Run 6/10, Epoch 770/1000, Training Loss (NLML): -960.6191\n",
      "convergence dfGPdfNN Run 6/10, Epoch 771/1000, Training Loss (NLML): -960.6204\n",
      "convergence dfGPdfNN Run 6/10, Epoch 772/1000, Training Loss (NLML): -960.6226\n",
      "convergence dfGPdfNN Run 6/10, Epoch 773/1000, Training Loss (NLML): -960.6239\n",
      "convergence dfGPdfNN Run 6/10, Epoch 774/1000, Training Loss (NLML): -960.6277\n",
      "convergence dfGPdfNN Run 6/10, Epoch 775/1000, Training Loss (NLML): -960.6327\n",
      "convergence dfGPdfNN Run 6/10, Epoch 776/1000, Training Loss (NLML): -960.6345\n",
      "convergence dfGPdfNN Run 6/10, Epoch 777/1000, Training Loss (NLML): -960.6350\n",
      "convergence dfGPdfNN Run 6/10, Epoch 778/1000, Training Loss (NLML): -960.6362\n",
      "convergence dfGPdfNN Run 6/10, Epoch 779/1000, Training Loss (NLML): -960.6400\n",
      "convergence dfGPdfNN Run 6/10, Epoch 780/1000, Training Loss (NLML): -960.6161\n",
      "convergence dfGPdfNN Run 6/10, Epoch 781/1000, Training Loss (NLML): -960.6237\n",
      "convergence dfGPdfNN Run 6/10, Epoch 782/1000, Training Loss (NLML): -960.6238\n",
      "convergence dfGPdfNN Run 6/10, Epoch 783/1000, Training Loss (NLML): -960.6249\n",
      "convergence dfGPdfNN Run 6/10, Epoch 784/1000, Training Loss (NLML): -960.6274\n",
      "convergence dfGPdfNN Run 6/10, Epoch 785/1000, Training Loss (NLML): -960.6400\n",
      "convergence dfGPdfNN Run 6/10, Epoch 786/1000, Training Loss (NLML): -960.6433\n",
      "convergence dfGPdfNN Run 6/10, Epoch 787/1000, Training Loss (NLML): -960.6482\n",
      "convergence dfGPdfNN Run 6/10, Epoch 788/1000, Training Loss (NLML): -960.6371\n",
      "convergence dfGPdfNN Run 6/10, Epoch 789/1000, Training Loss (NLML): -960.6381\n",
      "convergence dfGPdfNN Run 6/10, Epoch 790/1000, Training Loss (NLML): -960.6555\n",
      "convergence dfGPdfNN Run 6/10, Epoch 791/1000, Training Loss (NLML): -960.6564\n",
      "convergence dfGPdfNN Run 6/10, Epoch 792/1000, Training Loss (NLML): -960.6581\n",
      "convergence dfGPdfNN Run 6/10, Epoch 793/1000, Training Loss (NLML): -960.6611\n",
      "convergence dfGPdfNN Run 6/10, Epoch 794/1000, Training Loss (NLML): -960.6680\n",
      "convergence dfGPdfNN Run 6/10, Epoch 795/1000, Training Loss (NLML): -960.6693\n",
      "convergence dfGPdfNN Run 6/10, Epoch 796/1000, Training Loss (NLML): -960.6553\n",
      "convergence dfGPdfNN Run 6/10, Epoch 797/1000, Training Loss (NLML): -960.6597\n",
      "convergence dfGPdfNN Run 6/10, Epoch 798/1000, Training Loss (NLML): -960.6663\n",
      "convergence dfGPdfNN Run 6/10, Epoch 799/1000, Training Loss (NLML): -960.6700\n",
      "convergence dfGPdfNN Run 6/10, Epoch 800/1000, Training Loss (NLML): -960.6777\n",
      "convergence dfGPdfNN Run 6/10, Epoch 801/1000, Training Loss (NLML): -960.6816\n",
      "convergence dfGPdfNN Run 6/10, Epoch 802/1000, Training Loss (NLML): -960.6791\n",
      "convergence dfGPdfNN Run 6/10, Epoch 803/1000, Training Loss (NLML): -960.6796\n",
      "convergence dfGPdfNN Run 6/10, Epoch 804/1000, Training Loss (NLML): -960.6851\n",
      "convergence dfGPdfNN Run 6/10, Epoch 805/1000, Training Loss (NLML): -960.6902\n",
      "convergence dfGPdfNN Run 6/10, Epoch 806/1000, Training Loss (NLML): -960.6915\n",
      "convergence dfGPdfNN Run 6/10, Epoch 807/1000, Training Loss (NLML): -960.6947\n",
      "convergence dfGPdfNN Run 6/10, Epoch 808/1000, Training Loss (NLML): -960.6946\n",
      "convergence dfGPdfNN Run 6/10, Epoch 809/1000, Training Loss (NLML): -960.6884\n",
      "convergence dfGPdfNN Run 6/10, Epoch 810/1000, Training Loss (NLML): -960.6891\n",
      "convergence dfGPdfNN Run 6/10, Epoch 811/1000, Training Loss (NLML): -960.6953\n",
      "convergence dfGPdfNN Run 6/10, Epoch 812/1000, Training Loss (NLML): -960.6984\n",
      "convergence dfGPdfNN Run 6/10, Epoch 813/1000, Training Loss (NLML): -960.6963\n",
      "convergence dfGPdfNN Run 6/10, Epoch 814/1000, Training Loss (NLML): -960.7037\n",
      "convergence dfGPdfNN Run 6/10, Epoch 815/1000, Training Loss (NLML): -960.7054\n",
      "convergence dfGPdfNN Run 6/10, Epoch 816/1000, Training Loss (NLML): -960.7089\n",
      "convergence dfGPdfNN Run 6/10, Epoch 817/1000, Training Loss (NLML): -960.7043\n",
      "convergence dfGPdfNN Run 6/10, Epoch 818/1000, Training Loss (NLML): -960.7114\n",
      "convergence dfGPdfNN Run 6/10, Epoch 819/1000, Training Loss (NLML): -960.7113\n",
      "convergence dfGPdfNN Run 6/10, Epoch 820/1000, Training Loss (NLML): -960.7175\n",
      "convergence dfGPdfNN Run 6/10, Epoch 821/1000, Training Loss (NLML): -960.7195\n",
      "convergence dfGPdfNN Run 6/10, Epoch 822/1000, Training Loss (NLML): -960.7211\n",
      "convergence dfGPdfNN Run 6/10, Epoch 823/1000, Training Loss (NLML): -960.7224\n",
      "convergence dfGPdfNN Run 6/10, Epoch 824/1000, Training Loss (NLML): -960.7246\n",
      "convergence dfGPdfNN Run 6/10, Epoch 825/1000, Training Loss (NLML): -960.7292\n",
      "convergence dfGPdfNN Run 6/10, Epoch 826/1000, Training Loss (NLML): -960.7305\n",
      "convergence dfGPdfNN Run 6/10, Epoch 827/1000, Training Loss (NLML): -960.7340\n",
      "convergence dfGPdfNN Run 6/10, Epoch 828/1000, Training Loss (NLML): -960.7351\n",
      "convergence dfGPdfNN Run 6/10, Epoch 829/1000, Training Loss (NLML): -960.7361\n",
      "convergence dfGPdfNN Run 6/10, Epoch 830/1000, Training Loss (NLML): -960.7366\n",
      "convergence dfGPdfNN Run 6/10, Epoch 831/1000, Training Loss (NLML): -960.7406\n",
      "convergence dfGPdfNN Run 6/10, Epoch 832/1000, Training Loss (NLML): -960.7456\n",
      "convergence dfGPdfNN Run 6/10, Epoch 833/1000, Training Loss (NLML): -960.7471\n",
      "convergence dfGPdfNN Run 6/10, Epoch 834/1000, Training Loss (NLML): -960.7498\n",
      "convergence dfGPdfNN Run 6/10, Epoch 835/1000, Training Loss (NLML): -960.7526\n",
      "convergence dfGPdfNN Run 6/10, Epoch 836/1000, Training Loss (NLML): -960.7548\n",
      "convergence dfGPdfNN Run 6/10, Epoch 837/1000, Training Loss (NLML): -960.7544\n",
      "convergence dfGPdfNN Run 6/10, Epoch 838/1000, Training Loss (NLML): -960.7559\n",
      "convergence dfGPdfNN Run 6/10, Epoch 839/1000, Training Loss (NLML): -960.7594\n",
      "convergence dfGPdfNN Run 6/10, Epoch 840/1000, Training Loss (NLML): -960.7648\n",
      "convergence dfGPdfNN Run 6/10, Epoch 841/1000, Training Loss (NLML): -960.7657\n",
      "convergence dfGPdfNN Run 6/10, Epoch 842/1000, Training Loss (NLML): -960.7653\n",
      "convergence dfGPdfNN Run 6/10, Epoch 843/1000, Training Loss (NLML): -960.7656\n",
      "convergence dfGPdfNN Run 6/10, Epoch 844/1000, Training Loss (NLML): -960.7701\n",
      "convergence dfGPdfNN Run 6/10, Epoch 845/1000, Training Loss (NLML): -960.7750\n",
      "convergence dfGPdfNN Run 6/10, Epoch 846/1000, Training Loss (NLML): -960.7771\n",
      "convergence dfGPdfNN Run 6/10, Epoch 847/1000, Training Loss (NLML): -960.7800\n",
      "convergence dfGPdfNN Run 6/10, Epoch 848/1000, Training Loss (NLML): -960.7816\n",
      "convergence dfGPdfNN Run 6/10, Epoch 849/1000, Training Loss (NLML): -960.7823\n",
      "convergence dfGPdfNN Run 6/10, Epoch 850/1000, Training Loss (NLML): -960.7833\n",
      "convergence dfGPdfNN Run 6/10, Epoch 851/1000, Training Loss (NLML): -960.7839\n",
      "convergence dfGPdfNN Run 6/10, Epoch 852/1000, Training Loss (NLML): -960.7887\n",
      "convergence dfGPdfNN Run 6/10, Epoch 853/1000, Training Loss (NLML): -960.8082\n",
      "convergence dfGPdfNN Run 6/10, Epoch 854/1000, Training Loss (NLML): -960.8092\n",
      "convergence dfGPdfNN Run 6/10, Epoch 855/1000, Training Loss (NLML): -960.8105\n",
      "convergence dfGPdfNN Run 6/10, Epoch 856/1000, Training Loss (NLML): -960.8151\n",
      "convergence dfGPdfNN Run 6/10, Epoch 857/1000, Training Loss (NLML): -960.8134\n",
      "convergence dfGPdfNN Run 6/10, Epoch 858/1000, Training Loss (NLML): -960.8186\n",
      "convergence dfGPdfNN Run 6/10, Epoch 859/1000, Training Loss (NLML): -960.8206\n",
      "convergence dfGPdfNN Run 6/10, Epoch 860/1000, Training Loss (NLML): -960.8345\n",
      "convergence dfGPdfNN Run 6/10, Epoch 861/1000, Training Loss (NLML): -960.8390\n",
      "convergence dfGPdfNN Run 6/10, Epoch 862/1000, Training Loss (NLML): -960.8317\n",
      "convergence dfGPdfNN Run 6/10, Epoch 863/1000, Training Loss (NLML): -960.8352\n",
      "convergence dfGPdfNN Run 6/10, Epoch 864/1000, Training Loss (NLML): -960.8350\n",
      "convergence dfGPdfNN Run 6/10, Epoch 865/1000, Training Loss (NLML): -960.8445\n",
      "convergence dfGPdfNN Run 6/10, Epoch 866/1000, Training Loss (NLML): -960.8162\n",
      "convergence dfGPdfNN Run 6/10, Epoch 867/1000, Training Loss (NLML): -960.8279\n",
      "convergence dfGPdfNN Run 6/10, Epoch 868/1000, Training Loss (NLML): -960.8303\n",
      "convergence dfGPdfNN Run 6/10, Epoch 869/1000, Training Loss (NLML): -960.8458\n",
      "convergence dfGPdfNN Run 6/10, Epoch 870/1000, Training Loss (NLML): -960.8512\n",
      "convergence dfGPdfNN Run 6/10, Epoch 871/1000, Training Loss (NLML): -960.8533\n",
      "convergence dfGPdfNN Run 6/10, Epoch 872/1000, Training Loss (NLML): -960.8372\n",
      "convergence dfGPdfNN Run 6/10, Epoch 873/1000, Training Loss (NLML): -960.8364\n",
      "convergence dfGPdfNN Run 6/10, Epoch 874/1000, Training Loss (NLML): -960.8594\n",
      "convergence dfGPdfNN Run 6/10, Epoch 875/1000, Training Loss (NLML): -960.8588\n",
      "convergence dfGPdfNN Run 6/10, Epoch 876/1000, Training Loss (NLML): -960.8604\n",
      "convergence dfGPdfNN Run 6/10, Epoch 877/1000, Training Loss (NLML): -960.8622\n",
      "convergence dfGPdfNN Run 6/10, Epoch 878/1000, Training Loss (NLML): -960.8702\n",
      "convergence dfGPdfNN Run 6/10, Epoch 879/1000, Training Loss (NLML): -960.8507\n",
      "convergence dfGPdfNN Run 6/10, Epoch 880/1000, Training Loss (NLML): -960.8694\n",
      "convergence dfGPdfNN Run 6/10, Epoch 881/1000, Training Loss (NLML): -960.8702\n",
      "convergence dfGPdfNN Run 6/10, Epoch 882/1000, Training Loss (NLML): -960.8765\n",
      "convergence dfGPdfNN Run 6/10, Epoch 883/1000, Training Loss (NLML): -960.8757\n",
      "convergence dfGPdfNN Run 6/10, Epoch 884/1000, Training Loss (NLML): -960.8818\n",
      "convergence dfGPdfNN Run 6/10, Epoch 885/1000, Training Loss (NLML): -960.8824\n",
      "convergence dfGPdfNN Run 6/10, Epoch 886/1000, Training Loss (NLML): -960.8839\n",
      "convergence dfGPdfNN Run 6/10, Epoch 887/1000, Training Loss (NLML): -960.8666\n",
      "convergence dfGPdfNN Run 6/10, Epoch 888/1000, Training Loss (NLML): -960.8717\n",
      "convergence dfGPdfNN Run 6/10, Epoch 889/1000, Training Loss (NLML): -960.8925\n",
      "convergence dfGPdfNN Run 6/10, Epoch 890/1000, Training Loss (NLML): -960.8936\n",
      "convergence dfGPdfNN Run 6/10, Epoch 891/1000, Training Loss (NLML): -960.8927\n",
      "convergence dfGPdfNN Run 6/10, Epoch 892/1000, Training Loss (NLML): -960.8806\n",
      "convergence dfGPdfNN Run 6/10, Epoch 893/1000, Training Loss (NLML): -960.8944\n",
      "convergence dfGPdfNN Run 6/10, Epoch 894/1000, Training Loss (NLML): -960.8829\n",
      "convergence dfGPdfNN Run 6/10, Epoch 895/1000, Training Loss (NLML): -960.9000\n",
      "convergence dfGPdfNN Run 6/10, Epoch 896/1000, Training Loss (NLML): -960.9022\n",
      "convergence dfGPdfNN Run 6/10, Epoch 897/1000, Training Loss (NLML): -960.9045\n",
      "convergence dfGPdfNN Run 6/10, Epoch 898/1000, Training Loss (NLML): -960.9081\n",
      "convergence dfGPdfNN Run 6/10, Epoch 899/1000, Training Loss (NLML): -960.9108\n",
      "convergence dfGPdfNN Run 6/10, Epoch 900/1000, Training Loss (NLML): -960.8940\n",
      "convergence dfGPdfNN Run 6/10, Epoch 901/1000, Training Loss (NLML): -960.9128\n",
      "convergence dfGPdfNN Run 6/10, Epoch 902/1000, Training Loss (NLML): -960.9115\n",
      "convergence dfGPdfNN Run 6/10, Epoch 903/1000, Training Loss (NLML): -960.9023\n",
      "convergence dfGPdfNN Run 6/10, Epoch 904/1000, Training Loss (NLML): -960.9208\n",
      "convergence dfGPdfNN Run 6/10, Epoch 905/1000, Training Loss (NLML): -960.9207\n",
      "convergence dfGPdfNN Run 6/10, Epoch 906/1000, Training Loss (NLML): -960.9087\n",
      "convergence dfGPdfNN Run 6/10, Epoch 907/1000, Training Loss (NLML): -960.9215\n",
      "convergence dfGPdfNN Run 6/10, Epoch 908/1000, Training Loss (NLML): -960.9253\n",
      "convergence dfGPdfNN Run 6/10, Epoch 909/1000, Training Loss (NLML): -960.9319\n",
      "convergence dfGPdfNN Run 6/10, Epoch 910/1000, Training Loss (NLML): -960.9346\n",
      "convergence dfGPdfNN Run 6/10, Epoch 911/1000, Training Loss (NLML): -960.9174\n",
      "convergence dfGPdfNN Run 6/10, Epoch 912/1000, Training Loss (NLML): -960.9153\n",
      "convergence dfGPdfNN Run 6/10, Epoch 913/1000, Training Loss (NLML): -960.9353\n",
      "convergence dfGPdfNN Run 6/10, Epoch 914/1000, Training Loss (NLML): -960.9365\n",
      "convergence dfGPdfNN Run 6/10, Epoch 915/1000, Training Loss (NLML): -960.9366\n",
      "convergence dfGPdfNN Run 6/10, Epoch 916/1000, Training Loss (NLML): -960.9382\n",
      "convergence dfGPdfNN Run 6/10, Epoch 917/1000, Training Loss (NLML): -960.9484\n",
      "convergence dfGPdfNN Run 6/10, Epoch 918/1000, Training Loss (NLML): -960.9459\n",
      "convergence dfGPdfNN Run 6/10, Epoch 919/1000, Training Loss (NLML): -960.9482\n",
      "convergence dfGPdfNN Run 6/10, Epoch 920/1000, Training Loss (NLML): -960.9506\n",
      "convergence dfGPdfNN Run 6/10, Epoch 921/1000, Training Loss (NLML): -960.9546\n",
      "convergence dfGPdfNN Run 6/10, Epoch 922/1000, Training Loss (NLML): -960.9392\n",
      "convergence dfGPdfNN Run 6/10, Epoch 923/1000, Training Loss (NLML): -960.9395\n",
      "convergence dfGPdfNN Run 6/10, Epoch 924/1000, Training Loss (NLML): -960.9447\n",
      "convergence dfGPdfNN Run 6/10, Epoch 925/1000, Training Loss (NLML): -960.9425\n",
      "convergence dfGPdfNN Run 6/10, Epoch 926/1000, Training Loss (NLML): -960.9572\n",
      "convergence dfGPdfNN Run 6/10, Epoch 927/1000, Training Loss (NLML): -960.9624\n",
      "convergence dfGPdfNN Run 6/10, Epoch 928/1000, Training Loss (NLML): -960.9669\n",
      "convergence dfGPdfNN Run 6/10, Epoch 929/1000, Training Loss (NLML): -960.9674\n",
      "convergence dfGPdfNN Run 6/10, Epoch 930/1000, Training Loss (NLML): -960.9689\n",
      "convergence dfGPdfNN Run 6/10, Epoch 931/1000, Training Loss (NLML): -960.9681\n",
      "convergence dfGPdfNN Run 6/10, Epoch 932/1000, Training Loss (NLML): -960.9717\n",
      "convergence dfGPdfNN Run 6/10, Epoch 933/1000, Training Loss (NLML): -960.9733\n",
      "convergence dfGPdfNN Run 6/10, Epoch 934/1000, Training Loss (NLML): -960.9739\n",
      "convergence dfGPdfNN Run 6/10, Epoch 935/1000, Training Loss (NLML): -960.9803\n",
      "convergence dfGPdfNN Run 6/10, Epoch 936/1000, Training Loss (NLML): -960.9834\n",
      "convergence dfGPdfNN Run 6/10, Epoch 937/1000, Training Loss (NLML): -960.9816\n",
      "convergence dfGPdfNN Run 6/10, Epoch 938/1000, Training Loss (NLML): -960.9845\n",
      "convergence dfGPdfNN Run 6/10, Epoch 939/1000, Training Loss (NLML): -960.9774\n",
      "convergence dfGPdfNN Run 6/10, Epoch 940/1000, Training Loss (NLML): -960.9844\n",
      "convergence dfGPdfNN Run 6/10, Epoch 941/1000, Training Loss (NLML): -960.9861\n",
      "convergence dfGPdfNN Run 6/10, Epoch 942/1000, Training Loss (NLML): -960.9902\n",
      "convergence dfGPdfNN Run 6/10, Epoch 943/1000, Training Loss (NLML): -960.9916\n",
      "convergence dfGPdfNN Run 6/10, Epoch 944/1000, Training Loss (NLML): -960.9943\n",
      "convergence dfGPdfNN Run 6/10, Epoch 945/1000, Training Loss (NLML): -960.9803\n",
      "convergence dfGPdfNN Run 6/10, Epoch 946/1000, Training Loss (NLML): -960.9980\n",
      "convergence dfGPdfNN Run 6/10, Epoch 947/1000, Training Loss (NLML): -960.9999\n",
      "convergence dfGPdfNN Run 6/10, Epoch 948/1000, Training Loss (NLML): -961.0031\n",
      "convergence dfGPdfNN Run 6/10, Epoch 949/1000, Training Loss (NLML): -961.0015\n",
      "convergence dfGPdfNN Run 6/10, Epoch 950/1000, Training Loss (NLML): -961.0039\n",
      "convergence dfGPdfNN Run 6/10, Epoch 951/1000, Training Loss (NLML): -961.0052\n",
      "convergence dfGPdfNN Run 6/10, Epoch 952/1000, Training Loss (NLML): -961.0082\n",
      "convergence dfGPdfNN Run 6/10, Epoch 953/1000, Training Loss (NLML): -961.0106\n",
      "convergence dfGPdfNN Run 6/10, Epoch 954/1000, Training Loss (NLML): -961.0116\n",
      "convergence dfGPdfNN Run 6/10, Epoch 955/1000, Training Loss (NLML): -961.0134\n",
      "convergence dfGPdfNN Run 6/10, Epoch 956/1000, Training Loss (NLML): -961.0194\n",
      "convergence dfGPdfNN Run 6/10, Epoch 957/1000, Training Loss (NLML): -961.0157\n",
      "convergence dfGPdfNN Run 6/10, Epoch 958/1000, Training Loss (NLML): -961.0168\n",
      "convergence dfGPdfNN Run 6/10, Epoch 959/1000, Training Loss (NLML): -961.0173\n",
      "convergence dfGPdfNN Run 6/10, Epoch 960/1000, Training Loss (NLML): -961.0219\n",
      "convergence dfGPdfNN Run 6/10, Epoch 961/1000, Training Loss (NLML): -961.0237\n",
      "convergence dfGPdfNN Run 6/10, Epoch 962/1000, Training Loss (NLML): -961.0265\n",
      "convergence dfGPdfNN Run 6/10, Epoch 963/1000, Training Loss (NLML): -961.0308\n",
      "convergence dfGPdfNN Run 6/10, Epoch 964/1000, Training Loss (NLML): -961.0275\n",
      "convergence dfGPdfNN Run 6/10, Epoch 965/1000, Training Loss (NLML): -961.0304\n",
      "convergence dfGPdfNN Run 6/10, Epoch 966/1000, Training Loss (NLML): -961.0288\n",
      "convergence dfGPdfNN Run 6/10, Epoch 967/1000, Training Loss (NLML): -961.0348\n",
      "convergence dfGPdfNN Run 6/10, Epoch 968/1000, Training Loss (NLML): -961.0367\n",
      "convergence dfGPdfNN Run 6/10, Epoch 969/1000, Training Loss (NLML): -961.0398\n",
      "convergence dfGPdfNN Run 6/10, Epoch 970/1000, Training Loss (NLML): -961.0397\n",
      "convergence dfGPdfNN Run 6/10, Epoch 971/1000, Training Loss (NLML): -961.0386\n",
      "convergence dfGPdfNN Run 6/10, Epoch 972/1000, Training Loss (NLML): -961.0409\n",
      "convergence dfGPdfNN Run 6/10, Epoch 973/1000, Training Loss (NLML): -961.0442\n",
      "convergence dfGPdfNN Run 6/10, Epoch 974/1000, Training Loss (NLML): -961.0460\n",
      "convergence dfGPdfNN Run 6/10, Epoch 975/1000, Training Loss (NLML): -961.0491\n",
      "convergence dfGPdfNN Run 6/10, Epoch 976/1000, Training Loss (NLML): -961.0500\n",
      "convergence dfGPdfNN Run 6/10, Epoch 977/1000, Training Loss (NLML): -961.0519\n",
      "convergence dfGPdfNN Run 6/10, Epoch 978/1000, Training Loss (NLML): -961.0529\n",
      "convergence dfGPdfNN Run 6/10, Epoch 979/1000, Training Loss (NLML): -961.0519\n",
      "convergence dfGPdfNN Run 6/10, Epoch 980/1000, Training Loss (NLML): -961.0558\n",
      "convergence dfGPdfNN Run 6/10, Epoch 981/1000, Training Loss (NLML): -961.0570\n",
      "convergence dfGPdfNN Run 6/10, Epoch 982/1000, Training Loss (NLML): -961.0619\n",
      "convergence dfGPdfNN Run 6/10, Epoch 983/1000, Training Loss (NLML): -961.0601\n",
      "convergence dfGPdfNN Run 6/10, Epoch 984/1000, Training Loss (NLML): -961.0624\n",
      "convergence dfGPdfNN Run 6/10, Epoch 985/1000, Training Loss (NLML): -961.0637\n",
      "convergence dfGPdfNN Run 6/10, Epoch 986/1000, Training Loss (NLML): -961.0651\n",
      "convergence dfGPdfNN Run 6/10, Epoch 987/1000, Training Loss (NLML): -961.0657\n",
      "convergence dfGPdfNN Run 6/10, Epoch 988/1000, Training Loss (NLML): -961.0682\n",
      "convergence dfGPdfNN Run 6/10, Epoch 989/1000, Training Loss (NLML): -961.0737\n",
      "convergence dfGPdfNN Run 6/10, Epoch 990/1000, Training Loss (NLML): -961.0732\n",
      "convergence dfGPdfNN Run 6/10, Epoch 991/1000, Training Loss (NLML): -961.0730\n",
      "convergence dfGPdfNN Run 6/10, Epoch 992/1000, Training Loss (NLML): -961.0774\n",
      "convergence dfGPdfNN Run 6/10, Epoch 993/1000, Training Loss (NLML): -961.0779\n",
      "convergence dfGPdfNN Run 6/10, Epoch 994/1000, Training Loss (NLML): -961.0771\n",
      "convergence dfGPdfNN Run 6/10, Epoch 995/1000, Training Loss (NLML): -961.0770\n",
      "convergence dfGPdfNN Run 6/10, Epoch 996/1000, Training Loss (NLML): -961.0807\n",
      "convergence dfGPdfNN Run 6/10, Epoch 997/1000, Training Loss (NLML): -961.0867\n",
      "convergence dfGPdfNN Run 6/10, Epoch 998/1000, Training Loss (NLML): -961.0836\n",
      "convergence dfGPdfNN Run 6/10, Epoch 999/1000, Training Loss (NLML): -961.0880\n",
      "convergence dfGPdfNN Run 6/10, Epoch 1000/1000, Training Loss (NLML): -961.0917\n",
      "\n",
      "--- Training Run 7/10 ---\n",
      "\n",
      "Start Training\n",
      "convergence dfGPdfNN Run 7/10, Epoch 1/1000, Training Loss (NLML): -872.2396\n",
      "convergence dfGPdfNN Run 7/10, Epoch 2/1000, Training Loss (NLML): -894.9019\n",
      "convergence dfGPdfNN Run 7/10, Epoch 3/1000, Training Loss (NLML): -904.0054\n",
      "convergence dfGPdfNN Run 7/10, Epoch 4/1000, Training Loss (NLML): -905.8263\n",
      "convergence dfGPdfNN Run 7/10, Epoch 5/1000, Training Loss (NLML): -905.2135\n",
      "convergence dfGPdfNN Run 7/10, Epoch 6/1000, Training Loss (NLML): -907.8118\n",
      "convergence dfGPdfNN Run 7/10, Epoch 7/1000, Training Loss (NLML): -911.3348\n",
      "convergence dfGPdfNN Run 7/10, Epoch 8/1000, Training Loss (NLML): -913.7272\n",
      "convergence dfGPdfNN Run 7/10, Epoch 9/1000, Training Loss (NLML): -915.4587\n",
      "convergence dfGPdfNN Run 7/10, Epoch 10/1000, Training Loss (NLML): -916.6794\n",
      "convergence dfGPdfNN Run 7/10, Epoch 11/1000, Training Loss (NLML): -917.9767\n",
      "convergence dfGPdfNN Run 7/10, Epoch 12/1000, Training Loss (NLML): -919.1060\n",
      "convergence dfGPdfNN Run 7/10, Epoch 13/1000, Training Loss (NLML): -920.3397\n",
      "convergence dfGPdfNN Run 7/10, Epoch 14/1000, Training Loss (NLML): -921.4768\n",
      "convergence dfGPdfNN Run 7/10, Epoch 15/1000, Training Loss (NLML): -922.3230\n",
      "convergence dfGPdfNN Run 7/10, Epoch 16/1000, Training Loss (NLML): -922.7747\n",
      "convergence dfGPdfNN Run 7/10, Epoch 17/1000, Training Loss (NLML): -923.9191\n",
      "convergence dfGPdfNN Run 7/10, Epoch 18/1000, Training Loss (NLML): -925.2954\n",
      "convergence dfGPdfNN Run 7/10, Epoch 19/1000, Training Loss (NLML): -926.3226\n",
      "convergence dfGPdfNN Run 7/10, Epoch 20/1000, Training Loss (NLML): -927.2965\n",
      "convergence dfGPdfNN Run 7/10, Epoch 21/1000, Training Loss (NLML): -928.1694\n",
      "convergence dfGPdfNN Run 7/10, Epoch 22/1000, Training Loss (NLML): -929.0256\n",
      "convergence dfGPdfNN Run 7/10, Epoch 23/1000, Training Loss (NLML): -929.8330\n",
      "convergence dfGPdfNN Run 7/10, Epoch 24/1000, Training Loss (NLML): -930.5808\n",
      "convergence dfGPdfNN Run 7/10, Epoch 25/1000, Training Loss (NLML): -931.2783\n",
      "convergence dfGPdfNN Run 7/10, Epoch 26/1000, Training Loss (NLML): -931.9933\n",
      "convergence dfGPdfNN Run 7/10, Epoch 27/1000, Training Loss (NLML): -932.6606\n",
      "convergence dfGPdfNN Run 7/10, Epoch 28/1000, Training Loss (NLML): -933.3353\n",
      "convergence dfGPdfNN Run 7/10, Epoch 29/1000, Training Loss (NLML): -933.9888\n",
      "convergence dfGPdfNN Run 7/10, Epoch 30/1000, Training Loss (NLML): -934.6050\n",
      "convergence dfGPdfNN Run 7/10, Epoch 31/1000, Training Loss (NLML): -935.0939\n",
      "convergence dfGPdfNN Run 7/10, Epoch 32/1000, Training Loss (NLML): -935.7469\n",
      "convergence dfGPdfNN Run 7/10, Epoch 33/1000, Training Loss (NLML): -936.3053\n",
      "convergence dfGPdfNN Run 7/10, Epoch 34/1000, Training Loss (NLML): -936.8920\n",
      "convergence dfGPdfNN Run 7/10, Epoch 35/1000, Training Loss (NLML): -937.4091\n",
      "convergence dfGPdfNN Run 7/10, Epoch 36/1000, Training Loss (NLML): -937.9097\n",
      "convergence dfGPdfNN Run 7/10, Epoch 37/1000, Training Loss (NLML): -938.4053\n",
      "convergence dfGPdfNN Run 7/10, Epoch 38/1000, Training Loss (NLML): -938.8768\n",
      "convergence dfGPdfNN Run 7/10, Epoch 39/1000, Training Loss (NLML): -939.3091\n",
      "convergence dfGPdfNN Run 7/10, Epoch 40/1000, Training Loss (NLML): -939.7805\n",
      "convergence dfGPdfNN Run 7/10, Epoch 41/1000, Training Loss (NLML): -940.0826\n",
      "convergence dfGPdfNN Run 7/10, Epoch 42/1000, Training Loss (NLML): -940.4694\n",
      "convergence dfGPdfNN Run 7/10, Epoch 43/1000, Training Loss (NLML): -940.8462\n",
      "convergence dfGPdfNN Run 7/10, Epoch 44/1000, Training Loss (NLML): -941.2958\n",
      "convergence dfGPdfNN Run 7/10, Epoch 45/1000, Training Loss (NLML): -941.7513\n",
      "convergence dfGPdfNN Run 7/10, Epoch 46/1000, Training Loss (NLML): -942.1398\n",
      "convergence dfGPdfNN Run 7/10, Epoch 47/1000, Training Loss (NLML): -942.5067\n",
      "convergence dfGPdfNN Run 7/10, Epoch 48/1000, Training Loss (NLML): -942.9070\n",
      "convergence dfGPdfNN Run 7/10, Epoch 49/1000, Training Loss (NLML): -943.2528\n",
      "convergence dfGPdfNN Run 7/10, Epoch 50/1000, Training Loss (NLML): -943.5996\n",
      "convergence dfGPdfNN Run 7/10, Epoch 51/1000, Training Loss (NLML): -943.9500\n",
      "convergence dfGPdfNN Run 7/10, Epoch 52/1000, Training Loss (NLML): -944.2623\n",
      "convergence dfGPdfNN Run 7/10, Epoch 53/1000, Training Loss (NLML): -943.3440\n",
      "convergence dfGPdfNN Run 7/10, Epoch 54/1000, Training Loss (NLML): -943.7572\n",
      "convergence dfGPdfNN Run 7/10, Epoch 55/1000, Training Loss (NLML): -944.2816\n",
      "convergence dfGPdfNN Run 7/10, Epoch 56/1000, Training Loss (NLML): -944.5315\n",
      "convergence dfGPdfNN Run 7/10, Epoch 57/1000, Training Loss (NLML): -945.1107\n",
      "convergence dfGPdfNN Run 7/10, Epoch 58/1000, Training Loss (NLML): -945.6239\n",
      "convergence dfGPdfNN Run 7/10, Epoch 59/1000, Training Loss (NLML): -946.0699\n",
      "convergence dfGPdfNN Run 7/10, Epoch 60/1000, Training Loss (NLML): -946.3079\n",
      "convergence dfGPdfNN Run 7/10, Epoch 61/1000, Training Loss (NLML): -946.4933\n",
      "convergence dfGPdfNN Run 7/10, Epoch 62/1000, Training Loss (NLML): -946.6805\n",
      "convergence dfGPdfNN Run 7/10, Epoch 63/1000, Training Loss (NLML): -946.9194\n",
      "convergence dfGPdfNN Run 7/10, Epoch 64/1000, Training Loss (NLML): -947.2439\n",
      "convergence dfGPdfNN Run 7/10, Epoch 65/1000, Training Loss (NLML): -947.5179\n",
      "convergence dfGPdfNN Run 7/10, Epoch 66/1000, Training Loss (NLML): -947.9099\n",
      "convergence dfGPdfNN Run 7/10, Epoch 67/1000, Training Loss (NLML): -948.1083\n",
      "convergence dfGPdfNN Run 7/10, Epoch 68/1000, Training Loss (NLML): -945.2928\n",
      "convergence dfGPdfNN Run 7/10, Epoch 69/1000, Training Loss (NLML): -946.0488\n",
      "convergence dfGPdfNN Run 7/10, Epoch 70/1000, Training Loss (NLML): -947.0570\n",
      "convergence dfGPdfNN Run 7/10, Epoch 71/1000, Training Loss (NLML): -947.8389\n",
      "convergence dfGPdfNN Run 7/10, Epoch 72/1000, Training Loss (NLML): -948.3131\n",
      "convergence dfGPdfNN Run 7/10, Epoch 73/1000, Training Loss (NLML): -948.6007\n",
      "convergence dfGPdfNN Run 7/10, Epoch 74/1000, Training Loss (NLML): -948.9718\n",
      "convergence dfGPdfNN Run 7/10, Epoch 75/1000, Training Loss (NLML): -949.2275\n",
      "convergence dfGPdfNN Run 7/10, Epoch 76/1000, Training Loss (NLML): -949.5317\n",
      "convergence dfGPdfNN Run 7/10, Epoch 77/1000, Training Loss (NLML): -949.8116\n",
      "convergence dfGPdfNN Run 7/10, Epoch 78/1000, Training Loss (NLML): -950.3221\n",
      "convergence dfGPdfNN Run 7/10, Epoch 79/1000, Training Loss (NLML): -950.3030\n",
      "convergence dfGPdfNN Run 7/10, Epoch 80/1000, Training Loss (NLML): -950.4191\n",
      "convergence dfGPdfNN Run 7/10, Epoch 81/1000, Training Loss (NLML): -950.6055\n",
      "convergence dfGPdfNN Run 7/10, Epoch 82/1000, Training Loss (NLML): -950.8560\n",
      "convergence dfGPdfNN Run 7/10, Epoch 83/1000, Training Loss (NLML): -950.9543\n",
      "convergence dfGPdfNN Run 7/10, Epoch 84/1000, Training Loss (NLML): -951.1342\n",
      "convergence dfGPdfNN Run 7/10, Epoch 85/1000, Training Loss (NLML): -950.8805\n",
      "convergence dfGPdfNN Run 7/10, Epoch 86/1000, Training Loss (NLML): -951.3198\n",
      "convergence dfGPdfNN Run 7/10, Epoch 87/1000, Training Loss (NLML): -951.7181\n",
      "convergence dfGPdfNN Run 7/10, Epoch 88/1000, Training Loss (NLML): -951.9601\n",
      "convergence dfGPdfNN Run 7/10, Epoch 89/1000, Training Loss (NLML): -952.2047\n",
      "convergence dfGPdfNN Run 7/10, Epoch 90/1000, Training Loss (NLML): -952.1129\n",
      "convergence dfGPdfNN Run 7/10, Epoch 91/1000, Training Loss (NLML): -952.2335\n",
      "convergence dfGPdfNN Run 7/10, Epoch 92/1000, Training Loss (NLML): -952.3378\n",
      "convergence dfGPdfNN Run 7/10, Epoch 93/1000, Training Loss (NLML): -952.7355\n",
      "convergence dfGPdfNN Run 7/10, Epoch 94/1000, Training Loss (NLML): -953.0193\n",
      "convergence dfGPdfNN Run 7/10, Epoch 95/1000, Training Loss (NLML): -953.1799\n",
      "convergence dfGPdfNN Run 7/10, Epoch 96/1000, Training Loss (NLML): -947.3268\n",
      "convergence dfGPdfNN Run 7/10, Epoch 97/1000, Training Loss (NLML): -949.3232\n",
      "convergence dfGPdfNN Run 7/10, Epoch 98/1000, Training Loss (NLML): -953.0990\n",
      "convergence dfGPdfNN Run 7/10, Epoch 99/1000, Training Loss (NLML): -952.9795\n",
      "convergence dfGPdfNN Run 7/10, Epoch 100/1000, Training Loss (NLML): -952.9722\n",
      "convergence dfGPdfNN Run 7/10, Epoch 101/1000, Training Loss (NLML): -953.0786\n",
      "convergence dfGPdfNN Run 7/10, Epoch 102/1000, Training Loss (NLML): -953.3595\n",
      "convergence dfGPdfNN Run 7/10, Epoch 103/1000, Training Loss (NLML): -953.5930\n",
      "convergence dfGPdfNN Run 7/10, Epoch 104/1000, Training Loss (NLML): -953.9520\n",
      "convergence dfGPdfNN Run 7/10, Epoch 105/1000, Training Loss (NLML): -954.0339\n",
      "convergence dfGPdfNN Run 7/10, Epoch 106/1000, Training Loss (NLML): -954.2223\n",
      "convergence dfGPdfNN Run 7/10, Epoch 107/1000, Training Loss (NLML): -954.1742\n",
      "convergence dfGPdfNN Run 7/10, Epoch 108/1000, Training Loss (NLML): -954.3251\n",
      "convergence dfGPdfNN Run 7/10, Epoch 109/1000, Training Loss (NLML): -954.5900\n",
      "convergence dfGPdfNN Run 7/10, Epoch 110/1000, Training Loss (NLML): -954.4703\n",
      "convergence dfGPdfNN Run 7/10, Epoch 111/1000, Training Loss (NLML): -954.3738\n",
      "convergence dfGPdfNN Run 7/10, Epoch 112/1000, Training Loss (NLML): -954.4054\n",
      "convergence dfGPdfNN Run 7/10, Epoch 113/1000, Training Loss (NLML): -954.4772\n",
      "convergence dfGPdfNN Run 7/10, Epoch 114/1000, Training Loss (NLML): -954.5781\n",
      "convergence dfGPdfNN Run 7/10, Epoch 115/1000, Training Loss (NLML): -954.9092\n",
      "convergence dfGPdfNN Run 7/10, Epoch 116/1000, Training Loss (NLML): -954.9327\n",
      "convergence dfGPdfNN Run 7/10, Epoch 117/1000, Training Loss (NLML): -954.9792\n",
      "convergence dfGPdfNN Run 7/10, Epoch 118/1000, Training Loss (NLML): -955.1855\n",
      "convergence dfGPdfNN Run 7/10, Epoch 119/1000, Training Loss (NLML): -955.3967\n",
      "convergence dfGPdfNN Run 7/10, Epoch 120/1000, Training Loss (NLML): -955.4807\n",
      "convergence dfGPdfNN Run 7/10, Epoch 121/1000, Training Loss (NLML): -955.2000\n",
      "convergence dfGPdfNN Run 7/10, Epoch 122/1000, Training Loss (NLML): -939.9893\n",
      "convergence dfGPdfNN Run 7/10, Epoch 123/1000, Training Loss (NLML): -955.3422\n",
      "convergence dfGPdfNN Run 7/10, Epoch 124/1000, Training Loss (NLML): -955.0208\n",
      "convergence dfGPdfNN Run 7/10, Epoch 125/1000, Training Loss (NLML): -954.9589\n",
      "convergence dfGPdfNN Run 7/10, Epoch 126/1000, Training Loss (NLML): -955.0022\n",
      "convergence dfGPdfNN Run 7/10, Epoch 127/1000, Training Loss (NLML): -955.1982\n",
      "convergence dfGPdfNN Run 7/10, Epoch 128/1000, Training Loss (NLML): -955.1436\n",
      "convergence dfGPdfNN Run 7/10, Epoch 129/1000, Training Loss (NLML): -955.3795\n",
      "convergence dfGPdfNN Run 7/10, Epoch 130/1000, Training Loss (NLML): -955.5090\n",
      "convergence dfGPdfNN Run 7/10, Epoch 131/1000, Training Loss (NLML): -955.5216\n",
      "convergence dfGPdfNN Run 7/10, Epoch 132/1000, Training Loss (NLML): -955.4972\n",
      "convergence dfGPdfNN Run 7/10, Epoch 133/1000, Training Loss (NLML): -955.3185\n",
      "convergence dfGPdfNN Run 7/10, Epoch 134/1000, Training Loss (NLML): -955.0101\n",
      "convergence dfGPdfNN Run 7/10, Epoch 135/1000, Training Loss (NLML): -954.4642\n",
      "convergence dfGPdfNN Run 7/10, Epoch 136/1000, Training Loss (NLML): -954.2292\n",
      "convergence dfGPdfNN Run 7/10, Epoch 137/1000, Training Loss (NLML): -954.5704\n",
      "convergence dfGPdfNN Run 7/10, Epoch 138/1000, Training Loss (NLML): -955.0428\n",
      "convergence dfGPdfNN Run 7/10, Epoch 139/1000, Training Loss (NLML): -955.3837\n",
      "convergence dfGPdfNN Run 7/10, Epoch 140/1000, Training Loss (NLML): -955.6753\n",
      "convergence dfGPdfNN Run 7/10, Epoch 141/1000, Training Loss (NLML): -955.8036\n",
      "convergence dfGPdfNN Run 7/10, Epoch 142/1000, Training Loss (NLML): -955.8362\n",
      "convergence dfGPdfNN Run 7/10, Epoch 143/1000, Training Loss (NLML): -955.8292\n",
      "convergence dfGPdfNN Run 7/10, Epoch 144/1000, Training Loss (NLML): -955.7937\n",
      "convergence dfGPdfNN Run 7/10, Epoch 145/1000, Training Loss (NLML): -955.7496\n",
      "convergence dfGPdfNN Run 7/10, Epoch 146/1000, Training Loss (NLML): -955.7648\n",
      "convergence dfGPdfNN Run 7/10, Epoch 147/1000, Training Loss (NLML): -955.9347\n",
      "convergence dfGPdfNN Run 7/10, Epoch 148/1000, Training Loss (NLML): -955.9344\n",
      "convergence dfGPdfNN Run 7/10, Epoch 149/1000, Training Loss (NLML): -955.4696\n",
      "convergence dfGPdfNN Run 7/10, Epoch 150/1000, Training Loss (NLML): -955.6636\n",
      "convergence dfGPdfNN Run 7/10, Epoch 151/1000, Training Loss (NLML): -955.7800\n",
      "convergence dfGPdfNN Run 7/10, Epoch 152/1000, Training Loss (NLML): -956.0107\n",
      "convergence dfGPdfNN Run 7/10, Epoch 153/1000, Training Loss (NLML): -956.2367\n",
      "convergence dfGPdfNN Run 7/10, Epoch 154/1000, Training Loss (NLML): -956.3081\n",
      "convergence dfGPdfNN Run 7/10, Epoch 155/1000, Training Loss (NLML): -956.3691\n",
      "convergence dfGPdfNN Run 7/10, Epoch 156/1000, Training Loss (NLML): -956.4327\n",
      "convergence dfGPdfNN Run 7/10, Epoch 157/1000, Training Loss (NLML): -956.4821\n",
      "convergence dfGPdfNN Run 7/10, Epoch 158/1000, Training Loss (NLML): -956.5688\n",
      "convergence dfGPdfNN Run 7/10, Epoch 159/1000, Training Loss (NLML): -956.6162\n",
      "convergence dfGPdfNN Run 7/10, Epoch 160/1000, Training Loss (NLML): -956.5901\n",
      "convergence dfGPdfNN Run 7/10, Epoch 161/1000, Training Loss (NLML): -956.5151\n",
      "convergence dfGPdfNN Run 7/10, Epoch 162/1000, Training Loss (NLML): -956.4791\n",
      "convergence dfGPdfNN Run 7/10, Epoch 163/1000, Training Loss (NLML): -956.5193\n",
      "convergence dfGPdfNN Run 7/10, Epoch 164/1000, Training Loss (NLML): -956.5698\n",
      "convergence dfGPdfNN Run 7/10, Epoch 165/1000, Training Loss (NLML): -956.5688\n",
      "convergence dfGPdfNN Run 7/10, Epoch 166/1000, Training Loss (NLML): -956.6169\n",
      "convergence dfGPdfNN Run 7/10, Epoch 167/1000, Training Loss (NLML): -956.6361\n",
      "convergence dfGPdfNN Run 7/10, Epoch 168/1000, Training Loss (NLML): -956.6641\n",
      "convergence dfGPdfNN Run 7/10, Epoch 169/1000, Training Loss (NLML): -956.6366\n",
      "convergence dfGPdfNN Run 7/10, Epoch 170/1000, Training Loss (NLML): -956.6677\n",
      "convergence dfGPdfNN Run 7/10, Epoch 171/1000, Training Loss (NLML): -956.6772\n",
      "convergence dfGPdfNN Run 7/10, Epoch 172/1000, Training Loss (NLML): -956.7449\n",
      "convergence dfGPdfNN Run 7/10, Epoch 173/1000, Training Loss (NLML): -956.7861\n",
      "convergence dfGPdfNN Run 7/10, Epoch 174/1000, Training Loss (NLML): -956.8193\n",
      "convergence dfGPdfNN Run 7/10, Epoch 175/1000, Training Loss (NLML): -956.7532\n",
      "convergence dfGPdfNN Run 7/10, Epoch 176/1000, Training Loss (NLML): -956.6543\n",
      "convergence dfGPdfNN Run 7/10, Epoch 177/1000, Training Loss (NLML): -956.6450\n",
      "convergence dfGPdfNN Run 7/10, Epoch 178/1000, Training Loss (NLML): -956.7542\n",
      "convergence dfGPdfNN Run 7/10, Epoch 179/1000, Training Loss (NLML): -956.7391\n",
      "convergence dfGPdfNN Run 7/10, Epoch 180/1000, Training Loss (NLML): -956.7661\n",
      "convergence dfGPdfNN Run 7/10, Epoch 181/1000, Training Loss (NLML): -956.7896\n",
      "convergence dfGPdfNN Run 7/10, Epoch 182/1000, Training Loss (NLML): -956.7699\n",
      "convergence dfGPdfNN Run 7/10, Epoch 183/1000, Training Loss (NLML): -956.8198\n",
      "convergence dfGPdfNN Run 7/10, Epoch 184/1000, Training Loss (NLML): -956.8864\n",
      "convergence dfGPdfNN Run 7/10, Epoch 185/1000, Training Loss (NLML): -956.9543\n",
      "convergence dfGPdfNN Run 7/10, Epoch 186/1000, Training Loss (NLML): -956.8855\n",
      "convergence dfGPdfNN Run 7/10, Epoch 187/1000, Training Loss (NLML): -956.7162\n",
      "convergence dfGPdfNN Run 7/10, Epoch 188/1000, Training Loss (NLML): -956.5734\n",
      "convergence dfGPdfNN Run 7/10, Epoch 189/1000, Training Loss (NLML): -956.5571\n",
      "convergence dfGPdfNN Run 7/10, Epoch 190/1000, Training Loss (NLML): -956.5980\n",
      "convergence dfGPdfNN Run 7/10, Epoch 191/1000, Training Loss (NLML): -956.7084\n",
      "convergence dfGPdfNN Run 7/10, Epoch 192/1000, Training Loss (NLML): -956.7327\n",
      "convergence dfGPdfNN Run 7/10, Epoch 193/1000, Training Loss (NLML): -956.6793\n",
      "convergence dfGPdfNN Run 7/10, Epoch 194/1000, Training Loss (NLML): -956.6945\n",
      "convergence dfGPdfNN Run 7/10, Epoch 195/1000, Training Loss (NLML): -956.6802\n",
      "convergence dfGPdfNN Run 7/10, Epoch 196/1000, Training Loss (NLML): -956.8147\n",
      "convergence dfGPdfNN Run 7/10, Epoch 197/1000, Training Loss (NLML): -956.8693\n",
      "convergence dfGPdfNN Run 7/10, Epoch 198/1000, Training Loss (NLML): -956.8782\n",
      "convergence dfGPdfNN Run 7/10, Epoch 199/1000, Training Loss (NLML): -956.9255\n",
      "convergence dfGPdfNN Run 7/10, Epoch 200/1000, Training Loss (NLML): -956.9446\n",
      "convergence dfGPdfNN Run 7/10, Epoch 201/1000, Training Loss (NLML): -956.9885\n",
      "convergence dfGPdfNN Run 7/10, Epoch 202/1000, Training Loss (NLML): -957.0289\n",
      "convergence dfGPdfNN Run 7/10, Epoch 203/1000, Training Loss (NLML): -957.0164\n",
      "convergence dfGPdfNN Run 7/10, Epoch 204/1000, Training Loss (NLML): -957.0219\n",
      "convergence dfGPdfNN Run 7/10, Epoch 205/1000, Training Loss (NLML): -957.0265\n",
      "convergence dfGPdfNN Run 7/10, Epoch 206/1000, Training Loss (NLML): -957.1497\n",
      "convergence dfGPdfNN Run 7/10, Epoch 207/1000, Training Loss (NLML): -957.1790\n",
      "convergence dfGPdfNN Run 7/10, Epoch 208/1000, Training Loss (NLML): -957.2144\n",
      "convergence dfGPdfNN Run 7/10, Epoch 209/1000, Training Loss (NLML): -957.2355\n",
      "convergence dfGPdfNN Run 7/10, Epoch 210/1000, Training Loss (NLML): -957.2174\n",
      "convergence dfGPdfNN Run 7/10, Epoch 211/1000, Training Loss (NLML): -957.1721\n",
      "convergence dfGPdfNN Run 7/10, Epoch 212/1000, Training Loss (NLML): -957.1224\n",
      "convergence dfGPdfNN Run 7/10, Epoch 213/1000, Training Loss (NLML): -957.1127\n",
      "convergence dfGPdfNN Run 7/10, Epoch 214/1000, Training Loss (NLML): -957.1710\n",
      "convergence dfGPdfNN Run 7/10, Epoch 215/1000, Training Loss (NLML): -957.2576\n",
      "convergence dfGPdfNN Run 7/10, Epoch 216/1000, Training Loss (NLML): -957.2717\n",
      "convergence dfGPdfNN Run 7/10, Epoch 217/1000, Training Loss (NLML): -957.2935\n",
      "convergence dfGPdfNN Run 7/10, Epoch 218/1000, Training Loss (NLML): -957.3350\n",
      "convergence dfGPdfNN Run 7/10, Epoch 219/1000, Training Loss (NLML): -957.3889\n",
      "convergence dfGPdfNN Run 7/10, Epoch 220/1000, Training Loss (NLML): -957.4073\n",
      "convergence dfGPdfNN Run 7/10, Epoch 221/1000, Training Loss (NLML): -957.4021\n",
      "convergence dfGPdfNN Run 7/10, Epoch 222/1000, Training Loss (NLML): -957.4435\n",
      "convergence dfGPdfNN Run 7/10, Epoch 223/1000, Training Loss (NLML): -957.4352\n",
      "convergence dfGPdfNN Run 7/10, Epoch 224/1000, Training Loss (NLML): -957.4781\n",
      "convergence dfGPdfNN Run 7/10, Epoch 225/1000, Training Loss (NLML): -957.4650\n",
      "convergence dfGPdfNN Run 7/10, Epoch 226/1000, Training Loss (NLML): -957.4503\n",
      "convergence dfGPdfNN Run 7/10, Epoch 227/1000, Training Loss (NLML): -957.4626\n",
      "convergence dfGPdfNN Run 7/10, Epoch 228/1000, Training Loss (NLML): -957.4796\n",
      "convergence dfGPdfNN Run 7/10, Epoch 229/1000, Training Loss (NLML): -957.5490\n",
      "convergence dfGPdfNN Run 7/10, Epoch 230/1000, Training Loss (NLML): -957.5995\n",
      "convergence dfGPdfNN Run 7/10, Epoch 231/1000, Training Loss (NLML): -957.6772\n",
      "convergence dfGPdfNN Run 7/10, Epoch 232/1000, Training Loss (NLML): -957.7120\n",
      "convergence dfGPdfNN Run 7/10, Epoch 233/1000, Training Loss (NLML): -957.7283\n",
      "convergence dfGPdfNN Run 7/10, Epoch 234/1000, Training Loss (NLML): -957.7567\n",
      "convergence dfGPdfNN Run 7/10, Epoch 235/1000, Training Loss (NLML): -957.7139\n",
      "convergence dfGPdfNN Run 7/10, Epoch 236/1000, Training Loss (NLML): -957.7413\n",
      "convergence dfGPdfNN Run 7/10, Epoch 237/1000, Training Loss (NLML): -957.8079\n",
      "convergence dfGPdfNN Run 7/10, Epoch 238/1000, Training Loss (NLML): -957.7775\n",
      "convergence dfGPdfNN Run 7/10, Epoch 239/1000, Training Loss (NLML): -957.6664\n",
      "convergence dfGPdfNN Run 7/10, Epoch 240/1000, Training Loss (NLML): -957.6864\n",
      "convergence dfGPdfNN Run 7/10, Epoch 241/1000, Training Loss (NLML): -957.6945\n",
      "convergence dfGPdfNN Run 7/10, Epoch 242/1000, Training Loss (NLML): -957.7764\n",
      "convergence dfGPdfNN Run 7/10, Epoch 243/1000, Training Loss (NLML): -957.7825\n",
      "convergence dfGPdfNN Run 7/10, Epoch 244/1000, Training Loss (NLML): -957.7930\n",
      "convergence dfGPdfNN Run 7/10, Epoch 245/1000, Training Loss (NLML): -957.8094\n",
      "convergence dfGPdfNN Run 7/10, Epoch 246/1000, Training Loss (NLML): -957.8425\n",
      "convergence dfGPdfNN Run 7/10, Epoch 247/1000, Training Loss (NLML): -957.8536\n",
      "convergence dfGPdfNN Run 7/10, Epoch 248/1000, Training Loss (NLML): -957.8666\n",
      "convergence dfGPdfNN Run 7/10, Epoch 249/1000, Training Loss (NLML): -957.7888\n",
      "convergence dfGPdfNN Run 7/10, Epoch 250/1000, Training Loss (NLML): -957.8137\n",
      "convergence dfGPdfNN Run 7/10, Epoch 251/1000, Training Loss (NLML): -957.8435\n",
      "convergence dfGPdfNN Run 7/10, Epoch 252/1000, Training Loss (NLML): -957.9128\n",
      "convergence dfGPdfNN Run 7/10, Epoch 253/1000, Training Loss (NLML): -957.9075\n",
      "convergence dfGPdfNN Run 7/10, Epoch 254/1000, Training Loss (NLML): -957.9502\n",
      "convergence dfGPdfNN Run 7/10, Epoch 255/1000, Training Loss (NLML): -957.9265\n",
      "convergence dfGPdfNN Run 7/10, Epoch 256/1000, Training Loss (NLML): -957.8690\n",
      "convergence dfGPdfNN Run 7/10, Epoch 257/1000, Training Loss (NLML): -957.8843\n",
      "convergence dfGPdfNN Run 7/10, Epoch 258/1000, Training Loss (NLML): -957.8812\n",
      "convergence dfGPdfNN Run 7/10, Epoch 259/1000, Training Loss (NLML): -957.9695\n",
      "convergence dfGPdfNN Run 7/10, Epoch 260/1000, Training Loss (NLML): -957.9967\n",
      "convergence dfGPdfNN Run 7/10, Epoch 261/1000, Training Loss (NLML): -957.9337\n",
      "convergence dfGPdfNN Run 7/10, Epoch 262/1000, Training Loss (NLML): -957.9990\n",
      "convergence dfGPdfNN Run 7/10, Epoch 263/1000, Training Loss (NLML): -958.0167\n",
      "convergence dfGPdfNN Run 7/10, Epoch 264/1000, Training Loss (NLML): -958.1071\n",
      "convergence dfGPdfNN Run 7/10, Epoch 265/1000, Training Loss (NLML): -958.1318\n",
      "convergence dfGPdfNN Run 7/10, Epoch 266/1000, Training Loss (NLML): -958.1458\n",
      "convergence dfGPdfNN Run 7/10, Epoch 267/1000, Training Loss (NLML): -958.1501\n",
      "convergence dfGPdfNN Run 7/10, Epoch 268/1000, Training Loss (NLML): -958.1263\n",
      "convergence dfGPdfNN Run 7/10, Epoch 269/1000, Training Loss (NLML): -958.1205\n",
      "convergence dfGPdfNN Run 7/10, Epoch 270/1000, Training Loss (NLML): -958.1064\n",
      "convergence dfGPdfNN Run 7/10, Epoch 271/1000, Training Loss (NLML): -958.1224\n",
      "convergence dfGPdfNN Run 7/10, Epoch 272/1000, Training Loss (NLML): -958.1697\n",
      "convergence dfGPdfNN Run 7/10, Epoch 273/1000, Training Loss (NLML): -958.1846\n",
      "convergence dfGPdfNN Run 7/10, Epoch 274/1000, Training Loss (NLML): -958.2202\n",
      "convergence dfGPdfNN Run 7/10, Epoch 275/1000, Training Loss (NLML): -958.2467\n",
      "convergence dfGPdfNN Run 7/10, Epoch 276/1000, Training Loss (NLML): -958.2582\n",
      "convergence dfGPdfNN Run 7/10, Epoch 277/1000, Training Loss (NLML): -958.2368\n",
      "convergence dfGPdfNN Run 7/10, Epoch 278/1000, Training Loss (NLML): -958.2361\n",
      "convergence dfGPdfNN Run 7/10, Epoch 279/1000, Training Loss (NLML): -958.2462\n",
      "convergence dfGPdfNN Run 7/10, Epoch 280/1000, Training Loss (NLML): -958.2372\n",
      "convergence dfGPdfNN Run 7/10, Epoch 281/1000, Training Loss (NLML): -958.3025\n",
      "convergence dfGPdfNN Run 7/10, Epoch 282/1000, Training Loss (NLML): -958.3278\n",
      "convergence dfGPdfNN Run 7/10, Epoch 283/1000, Training Loss (NLML): -958.3081\n",
      "convergence dfGPdfNN Run 7/10, Epoch 284/1000, Training Loss (NLML): -958.3213\n",
      "convergence dfGPdfNN Run 7/10, Epoch 285/1000, Training Loss (NLML): -958.3641\n",
      "convergence dfGPdfNN Run 7/10, Epoch 286/1000, Training Loss (NLML): -958.3759\n",
      "convergence dfGPdfNN Run 7/10, Epoch 287/1000, Training Loss (NLML): -958.3551\n",
      "convergence dfGPdfNN Run 7/10, Epoch 288/1000, Training Loss (NLML): -958.3667\n",
      "convergence dfGPdfNN Run 7/10, Epoch 289/1000, Training Loss (NLML): -958.4102\n",
      "convergence dfGPdfNN Run 7/10, Epoch 290/1000, Training Loss (NLML): -958.3828\n",
      "convergence dfGPdfNN Run 7/10, Epoch 291/1000, Training Loss (NLML): -958.3934\n",
      "convergence dfGPdfNN Run 7/10, Epoch 292/1000, Training Loss (NLML): -958.3937\n",
      "convergence dfGPdfNN Run 7/10, Epoch 293/1000, Training Loss (NLML): -958.4053\n",
      "convergence dfGPdfNN Run 7/10, Epoch 294/1000, Training Loss (NLML): -958.4644\n",
      "convergence dfGPdfNN Run 7/10, Epoch 295/1000, Training Loss (NLML): -958.4741\n",
      "convergence dfGPdfNN Run 7/10, Epoch 296/1000, Training Loss (NLML): -958.4852\n",
      "convergence dfGPdfNN Run 7/10, Epoch 297/1000, Training Loss (NLML): -958.4957\n",
      "convergence dfGPdfNN Run 7/10, Epoch 298/1000, Training Loss (NLML): -958.4744\n",
      "convergence dfGPdfNN Run 7/10, Epoch 299/1000, Training Loss (NLML): -958.4899\n",
      "convergence dfGPdfNN Run 7/10, Epoch 300/1000, Training Loss (NLML): -958.5005\n",
      "convergence dfGPdfNN Run 7/10, Epoch 301/1000, Training Loss (NLML): -958.5104\n",
      "convergence dfGPdfNN Run 7/10, Epoch 302/1000, Training Loss (NLML): -958.5469\n",
      "convergence dfGPdfNN Run 7/10, Epoch 303/1000, Training Loss (NLML): -958.5583\n",
      "convergence dfGPdfNN Run 7/10, Epoch 304/1000, Training Loss (NLML): -958.5687\n",
      "convergence dfGPdfNN Run 7/10, Epoch 305/1000, Training Loss (NLML): -958.5781\n",
      "convergence dfGPdfNN Run 7/10, Epoch 306/1000, Training Loss (NLML): -958.5731\n",
      "convergence dfGPdfNN Run 7/10, Epoch 307/1000, Training Loss (NLML): -958.5824\n",
      "convergence dfGPdfNN Run 7/10, Epoch 308/1000, Training Loss (NLML): -958.5918\n",
      "convergence dfGPdfNN Run 7/10, Epoch 309/1000, Training Loss (NLML): -958.6044\n",
      "convergence dfGPdfNN Run 7/10, Epoch 310/1000, Training Loss (NLML): -958.6307\n",
      "convergence dfGPdfNN Run 7/10, Epoch 311/1000, Training Loss (NLML): -958.6399\n",
      "convergence dfGPdfNN Run 7/10, Epoch 312/1000, Training Loss (NLML): -958.6500\n",
      "convergence dfGPdfNN Run 7/10, Epoch 313/1000, Training Loss (NLML): -958.6606\n",
      "convergence dfGPdfNN Run 7/10, Epoch 314/1000, Training Loss (NLML): -958.6324\n",
      "convergence dfGPdfNN Run 7/10, Epoch 315/1000, Training Loss (NLML): -958.6475\n",
      "convergence dfGPdfNN Run 7/10, Epoch 316/1000, Training Loss (NLML): -958.6571\n",
      "convergence dfGPdfNN Run 7/10, Epoch 317/1000, Training Loss (NLML): -958.6832\n",
      "convergence dfGPdfNN Run 7/10, Epoch 318/1000, Training Loss (NLML): -958.7019\n",
      "convergence dfGPdfNN Run 7/10, Epoch 319/1000, Training Loss (NLML): -958.7109\n",
      "convergence dfGPdfNN Run 7/10, Epoch 320/1000, Training Loss (NLML): -958.7230\n",
      "convergence dfGPdfNN Run 7/10, Epoch 321/1000, Training Loss (NLML): -958.7333\n",
      "convergence dfGPdfNN Run 7/10, Epoch 322/1000, Training Loss (NLML): -958.7317\n",
      "convergence dfGPdfNN Run 7/10, Epoch 323/1000, Training Loss (NLML): -958.7434\n",
      "convergence dfGPdfNN Run 7/10, Epoch 324/1000, Training Loss (NLML): -958.7458\n",
      "convergence dfGPdfNN Run 7/10, Epoch 325/1000, Training Loss (NLML): -958.7692\n",
      "convergence dfGPdfNN Run 7/10, Epoch 326/1000, Training Loss (NLML): -958.7799\n",
      "convergence dfGPdfNN Run 7/10, Epoch 327/1000, Training Loss (NLML): -958.7985\n",
      "convergence dfGPdfNN Run 7/10, Epoch 328/1000, Training Loss (NLML): -958.8079\n",
      "convergence dfGPdfNN Run 7/10, Epoch 329/1000, Training Loss (NLML): -958.8169\n",
      "convergence dfGPdfNN Run 7/10, Epoch 330/1000, Training Loss (NLML): -958.8253\n",
      "convergence dfGPdfNN Run 7/10, Epoch 331/1000, Training Loss (NLML): -958.8278\n",
      "convergence dfGPdfNN Run 7/10, Epoch 332/1000, Training Loss (NLML): -958.8356\n",
      "convergence dfGPdfNN Run 7/10, Epoch 333/1000, Training Loss (NLML): -958.8400\n",
      "convergence dfGPdfNN Run 7/10, Epoch 334/1000, Training Loss (NLML): -958.8324\n",
      "convergence dfGPdfNN Run 7/10, Epoch 335/1000, Training Loss (NLML): -958.8389\n",
      "convergence dfGPdfNN Run 7/10, Epoch 336/1000, Training Loss (NLML): -958.8553\n",
      "convergence dfGPdfNN Run 7/10, Epoch 337/1000, Training Loss (NLML): -958.8569\n",
      "convergence dfGPdfNN Run 7/10, Epoch 338/1000, Training Loss (NLML): -958.8658\n",
      "convergence dfGPdfNN Run 7/10, Epoch 339/1000, Training Loss (NLML): -958.8849\n",
      "convergence dfGPdfNN Run 7/10, Epoch 340/1000, Training Loss (NLML): -958.8884\n",
      "convergence dfGPdfNN Run 7/10, Epoch 341/1000, Training Loss (NLML): -958.8871\n",
      "convergence dfGPdfNN Run 7/10, Epoch 342/1000, Training Loss (NLML): -958.9066\n",
      "convergence dfGPdfNN Run 7/10, Epoch 343/1000, Training Loss (NLML): -958.8912\n",
      "convergence dfGPdfNN Run 7/10, Epoch 344/1000, Training Loss (NLML): -958.9254\n",
      "convergence dfGPdfNN Run 7/10, Epoch 345/1000, Training Loss (NLML): -958.9404\n",
      "convergence dfGPdfNN Run 7/10, Epoch 346/1000, Training Loss (NLML): -958.9388\n",
      "convergence dfGPdfNN Run 7/10, Epoch 347/1000, Training Loss (NLML): -958.9482\n",
      "convergence dfGPdfNN Run 7/10, Epoch 348/1000, Training Loss (NLML): -958.9692\n",
      "convergence dfGPdfNN Run 7/10, Epoch 349/1000, Training Loss (NLML): -958.9785\n",
      "convergence dfGPdfNN Run 7/10, Epoch 350/1000, Training Loss (NLML): -958.9879\n",
      "convergence dfGPdfNN Run 7/10, Epoch 351/1000, Training Loss (NLML): -959.0200\n",
      "convergence dfGPdfNN Run 7/10, Epoch 352/1000, Training Loss (NLML): -959.0044\n",
      "convergence dfGPdfNN Run 7/10, Epoch 353/1000, Training Loss (NLML): -959.0107\n",
      "convergence dfGPdfNN Run 7/10, Epoch 354/1000, Training Loss (NLML): -959.0187\n",
      "convergence dfGPdfNN Run 7/10, Epoch 355/1000, Training Loss (NLML): -959.0291\n",
      "convergence dfGPdfNN Run 7/10, Epoch 356/1000, Training Loss (NLML): -959.0374\n",
      "convergence dfGPdfNN Run 7/10, Epoch 357/1000, Training Loss (NLML): -959.0466\n",
      "convergence dfGPdfNN Run 7/10, Epoch 358/1000, Training Loss (NLML): -959.0570\n",
      "convergence dfGPdfNN Run 7/10, Epoch 359/1000, Training Loss (NLML): -959.0651\n",
      "convergence dfGPdfNN Run 7/10, Epoch 360/1000, Training Loss (NLML): -959.0720\n",
      "convergence dfGPdfNN Run 7/10, Epoch 361/1000, Training Loss (NLML): -959.0814\n",
      "convergence dfGPdfNN Run 7/10, Epoch 362/1000, Training Loss (NLML): -959.0886\n",
      "convergence dfGPdfNN Run 7/10, Epoch 363/1000, Training Loss (NLML): -959.0973\n",
      "convergence dfGPdfNN Run 7/10, Epoch 364/1000, Training Loss (NLML): -959.1068\n",
      "convergence dfGPdfNN Run 7/10, Epoch 365/1000, Training Loss (NLML): -959.1128\n",
      "convergence dfGPdfNN Run 7/10, Epoch 366/1000, Training Loss (NLML): -959.1228\n",
      "convergence dfGPdfNN Run 7/10, Epoch 367/1000, Training Loss (NLML): -959.1293\n",
      "convergence dfGPdfNN Run 7/10, Epoch 368/1000, Training Loss (NLML): -959.1368\n",
      "convergence dfGPdfNN Run 7/10, Epoch 369/1000, Training Loss (NLML): -959.1432\n",
      "convergence dfGPdfNN Run 7/10, Epoch 370/1000, Training Loss (NLML): -959.1514\n",
      "convergence dfGPdfNN Run 7/10, Epoch 371/1000, Training Loss (NLML): -959.1681\n",
      "convergence dfGPdfNN Run 7/10, Epoch 372/1000, Training Loss (NLML): -959.1788\n",
      "convergence dfGPdfNN Run 7/10, Epoch 373/1000, Training Loss (NLML): -959.1854\n",
      "convergence dfGPdfNN Run 7/10, Epoch 374/1000, Training Loss (NLML): -959.1918\n",
      "convergence dfGPdfNN Run 7/10, Epoch 375/1000, Training Loss (NLML): -959.2003\n",
      "convergence dfGPdfNN Run 7/10, Epoch 376/1000, Training Loss (NLML): -959.2078\n",
      "convergence dfGPdfNN Run 7/10, Epoch 377/1000, Training Loss (NLML): -959.2050\n",
      "convergence dfGPdfNN Run 7/10, Epoch 378/1000, Training Loss (NLML): -959.2120\n",
      "convergence dfGPdfNN Run 7/10, Epoch 379/1000, Training Loss (NLML): -959.2172\n",
      "convergence dfGPdfNN Run 7/10, Epoch 380/1000, Training Loss (NLML): -959.2213\n",
      "convergence dfGPdfNN Run 7/10, Epoch 381/1000, Training Loss (NLML): -959.2041\n",
      "convergence dfGPdfNN Run 7/10, Epoch 382/1000, Training Loss (NLML): -959.2035\n",
      "convergence dfGPdfNN Run 7/10, Epoch 383/1000, Training Loss (NLML): -959.2091\n",
      "convergence dfGPdfNN Run 7/10, Epoch 384/1000, Training Loss (NLML): -959.2323\n",
      "convergence dfGPdfNN Run 7/10, Epoch 385/1000, Training Loss (NLML): -959.2351\n",
      "convergence dfGPdfNN Run 7/10, Epoch 386/1000, Training Loss (NLML): -959.2456\n",
      "convergence dfGPdfNN Run 7/10, Epoch 387/1000, Training Loss (NLML): -959.2598\n",
      "convergence dfGPdfNN Run 7/10, Epoch 388/1000, Training Loss (NLML): -959.2704\n",
      "convergence dfGPdfNN Run 7/10, Epoch 389/1000, Training Loss (NLML): -959.2788\n",
      "convergence dfGPdfNN Run 7/10, Epoch 390/1000, Training Loss (NLML): -959.2865\n",
      "convergence dfGPdfNN Run 7/10, Epoch 391/1000, Training Loss (NLML): -959.2950\n",
      "convergence dfGPdfNN Run 7/10, Epoch 392/1000, Training Loss (NLML): -959.3032\n",
      "convergence dfGPdfNN Run 7/10, Epoch 393/1000, Training Loss (NLML): -959.3101\n",
      "convergence dfGPdfNN Run 7/10, Epoch 394/1000, Training Loss (NLML): -959.3180\n",
      "convergence dfGPdfNN Run 7/10, Epoch 395/1000, Training Loss (NLML): -959.3274\n",
      "convergence dfGPdfNN Run 7/10, Epoch 396/1000, Training Loss (NLML): -959.3419\n",
      "convergence dfGPdfNN Run 7/10, Epoch 397/1000, Training Loss (NLML): -959.3469\n",
      "convergence dfGPdfNN Run 7/10, Epoch 398/1000, Training Loss (NLML): -959.3472\n",
      "convergence dfGPdfNN Run 7/10, Epoch 399/1000, Training Loss (NLML): -959.3516\n",
      "convergence dfGPdfNN Run 7/10, Epoch 400/1000, Training Loss (NLML): -959.3674\n",
      "convergence dfGPdfNN Run 7/10, Epoch 401/1000, Training Loss (NLML): -959.3759\n",
      "convergence dfGPdfNN Run 7/10, Epoch 402/1000, Training Loss (NLML): -959.3813\n",
      "convergence dfGPdfNN Run 7/10, Epoch 403/1000, Training Loss (NLML): -959.3776\n",
      "convergence dfGPdfNN Run 7/10, Epoch 404/1000, Training Loss (NLML): -959.3855\n",
      "convergence dfGPdfNN Run 7/10, Epoch 405/1000, Training Loss (NLML): -959.3936\n",
      "convergence dfGPdfNN Run 7/10, Epoch 406/1000, Training Loss (NLML): -959.3942\n",
      "convergence dfGPdfNN Run 7/10, Epoch 407/1000, Training Loss (NLML): -959.4099\n",
      "convergence dfGPdfNN Run 7/10, Epoch 408/1000, Training Loss (NLML): -959.4163\n",
      "convergence dfGPdfNN Run 7/10, Epoch 409/1000, Training Loss (NLML): -959.4225\n",
      "convergence dfGPdfNN Run 7/10, Epoch 410/1000, Training Loss (NLML): -959.4304\n",
      "convergence dfGPdfNN Run 7/10, Epoch 411/1000, Training Loss (NLML): -956.3047\n",
      "convergence dfGPdfNN Run 7/10, Epoch 412/1000, Training Loss (NLML): -959.4210\n",
      "convergence dfGPdfNN Run 7/10, Epoch 413/1000, Training Loss (NLML): -959.4047\n",
      "convergence dfGPdfNN Run 7/10, Epoch 414/1000, Training Loss (NLML): -959.3877\n",
      "convergence dfGPdfNN Run 7/10, Epoch 415/1000, Training Loss (NLML): -959.3749\n",
      "convergence dfGPdfNN Run 7/10, Epoch 416/1000, Training Loss (NLML): -959.3684\n",
      "convergence dfGPdfNN Run 7/10, Epoch 417/1000, Training Loss (NLML): -959.3694\n",
      "convergence dfGPdfNN Run 7/10, Epoch 418/1000, Training Loss (NLML): -959.3671\n",
      "convergence dfGPdfNN Run 7/10, Epoch 419/1000, Training Loss (NLML): -959.3752\n",
      "convergence dfGPdfNN Run 7/10, Epoch 420/1000, Training Loss (NLML): -959.3710\n",
      "convergence dfGPdfNN Run 7/10, Epoch 421/1000, Training Loss (NLML): -959.3877\n",
      "convergence dfGPdfNN Run 7/10, Epoch 422/1000, Training Loss (NLML): -959.3915\n",
      "convergence dfGPdfNN Run 7/10, Epoch 423/1000, Training Loss (NLML): -959.4084\n",
      "convergence dfGPdfNN Run 7/10, Epoch 424/1000, Training Loss (NLML): -959.4210\n",
      "convergence dfGPdfNN Run 7/10, Epoch 425/1000, Training Loss (NLML): -959.4709\n",
      "convergence dfGPdfNN Run 7/10, Epoch 426/1000, Training Loss (NLML): -959.5026\n",
      "convergence dfGPdfNN Run 7/10, Epoch 427/1000, Training Loss (NLML): -959.5292\n",
      "convergence dfGPdfNN Run 7/10, Epoch 428/1000, Training Loss (NLML): -959.5430\n",
      "convergence dfGPdfNN Run 7/10, Epoch 429/1000, Training Loss (NLML): -959.5638\n",
      "convergence dfGPdfNN Run 7/10, Epoch 430/1000, Training Loss (NLML): -959.5720\n",
      "convergence dfGPdfNN Run 7/10, Epoch 431/1000, Training Loss (NLML): -959.5835\n",
      "convergence dfGPdfNN Run 7/10, Epoch 432/1000, Training Loss (NLML): -959.5908\n",
      "convergence dfGPdfNN Run 7/10, Epoch 433/1000, Training Loss (NLML): -959.4541\n",
      "convergence dfGPdfNN Run 7/10, Epoch 434/1000, Training Loss (NLML): -959.4819\n",
      "convergence dfGPdfNN Run 7/10, Epoch 435/1000, Training Loss (NLML): -959.5250\n",
      "convergence dfGPdfNN Run 7/10, Epoch 436/1000, Training Loss (NLML): -959.5586\n",
      "convergence dfGPdfNN Run 7/10, Epoch 437/1000, Training Loss (NLML): -959.5767\n",
      "convergence dfGPdfNN Run 7/10, Epoch 438/1000, Training Loss (NLML): -959.5852\n",
      "convergence dfGPdfNN Run 7/10, Epoch 439/1000, Training Loss (NLML): -959.5824\n",
      "convergence dfGPdfNN Run 7/10, Epoch 440/1000, Training Loss (NLML): -959.5828\n",
      "convergence dfGPdfNN Run 7/10, Epoch 441/1000, Training Loss (NLML): -959.5829\n",
      "convergence dfGPdfNN Run 7/10, Epoch 442/1000, Training Loss (NLML): -959.5968\n",
      "convergence dfGPdfNN Run 7/10, Epoch 443/1000, Training Loss (NLML): -959.6227\n",
      "convergence dfGPdfNN Run 7/10, Epoch 444/1000, Training Loss (NLML): -959.6309\n",
      "convergence dfGPdfNN Run 7/10, Epoch 445/1000, Training Loss (NLML): -959.6400\n",
      "convergence dfGPdfNN Run 7/10, Epoch 446/1000, Training Loss (NLML): -959.6383\n",
      "convergence dfGPdfNN Run 7/10, Epoch 447/1000, Training Loss (NLML): -959.6477\n",
      "convergence dfGPdfNN Run 7/10, Epoch 448/1000, Training Loss (NLML): -959.6493\n",
      "convergence dfGPdfNN Run 7/10, Epoch 449/1000, Training Loss (NLML): -959.6846\n",
      "convergence dfGPdfNN Run 7/10, Epoch 450/1000, Training Loss (NLML): -959.7113\n",
      "convergence dfGPdfNN Run 7/10, Epoch 451/1000, Training Loss (NLML): -959.7229\n",
      "convergence dfGPdfNN Run 7/10, Epoch 452/1000, Training Loss (NLML): -959.7347\n",
      "convergence dfGPdfNN Run 7/10, Epoch 453/1000, Training Loss (NLML): -959.7004\n",
      "convergence dfGPdfNN Run 7/10, Epoch 454/1000, Training Loss (NLML): -959.7152\n",
      "convergence dfGPdfNN Run 7/10, Epoch 455/1000, Training Loss (NLML): -959.8240\n",
      "convergence dfGPdfNN Run 7/10, Epoch 456/1000, Training Loss (NLML): -959.8695\n",
      "convergence dfGPdfNN Run 7/10, Epoch 457/1000, Training Loss (NLML): -959.9521\n",
      "convergence dfGPdfNN Run 7/10, Epoch 458/1000, Training Loss (NLML): -959.9087\n",
      "convergence dfGPdfNN Run 7/10, Epoch 459/1000, Training Loss (NLML): -959.7964\n",
      "convergence dfGPdfNN Run 7/10, Epoch 460/1000, Training Loss (NLML): -959.7889\n",
      "convergence dfGPdfNN Run 7/10, Epoch 461/1000, Training Loss (NLML): -959.7688\n",
      "convergence dfGPdfNN Run 7/10, Epoch 462/1000, Training Loss (NLML): -959.7031\n",
      "convergence dfGPdfNN Run 7/10, Epoch 463/1000, Training Loss (NLML): -959.6866\n",
      "convergence dfGPdfNN Run 7/10, Epoch 464/1000, Training Loss (NLML): -959.7255\n",
      "convergence dfGPdfNN Run 7/10, Epoch 465/1000, Training Loss (NLML): -959.7571\n",
      "convergence dfGPdfNN Run 7/10, Epoch 466/1000, Training Loss (NLML): -959.7798\n",
      "convergence dfGPdfNN Run 7/10, Epoch 467/1000, Training Loss (NLML): -959.8119\n",
      "convergence dfGPdfNN Run 7/10, Epoch 468/1000, Training Loss (NLML): -959.8170\n",
      "convergence dfGPdfNN Run 7/10, Epoch 469/1000, Training Loss (NLML): -959.7903\n",
      "convergence dfGPdfNN Run 7/10, Epoch 470/1000, Training Loss (NLML): -959.7985\n",
      "convergence dfGPdfNN Run 7/10, Epoch 471/1000, Training Loss (NLML): -959.7611\n",
      "convergence dfGPdfNN Run 7/10, Epoch 472/1000, Training Loss (NLML): -959.7367\n",
      "convergence dfGPdfNN Run 7/10, Epoch 473/1000, Training Loss (NLML): -959.7169\n",
      "convergence dfGPdfNN Run 7/10, Epoch 474/1000, Training Loss (NLML): -959.7681\n",
      "convergence dfGPdfNN Run 7/10, Epoch 475/1000, Training Loss (NLML): -959.7860\n",
      "convergence dfGPdfNN Run 7/10, Epoch 476/1000, Training Loss (NLML): -959.7662\n",
      "convergence dfGPdfNN Run 7/10, Epoch 477/1000, Training Loss (NLML): -959.7256\n",
      "convergence dfGPdfNN Run 7/10, Epoch 478/1000, Training Loss (NLML): -959.7812\n",
      "convergence dfGPdfNN Run 7/10, Epoch 479/1000, Training Loss (NLML): -959.8419\n",
      "convergence dfGPdfNN Run 7/10, Epoch 480/1000, Training Loss (NLML): -959.8469\n",
      "convergence dfGPdfNN Run 7/10, Epoch 481/1000, Training Loss (NLML): -959.8557\n",
      "convergence dfGPdfNN Run 7/10, Epoch 482/1000, Training Loss (NLML): -959.8253\n",
      "convergence dfGPdfNN Run 7/10, Epoch 483/1000, Training Loss (NLML): -959.8031\n",
      "convergence dfGPdfNN Run 7/10, Epoch 484/1000, Training Loss (NLML): -959.8126\n",
      "convergence dfGPdfNN Run 7/10, Epoch 485/1000, Training Loss (NLML): -959.7902\n",
      "convergence dfGPdfNN Run 7/10, Epoch 486/1000, Training Loss (NLML): -959.8154\n",
      "convergence dfGPdfNN Run 7/10, Epoch 487/1000, Training Loss (NLML): -959.8254\n",
      "convergence dfGPdfNN Run 7/10, Epoch 488/1000, Training Loss (NLML): -959.8390\n",
      "convergence dfGPdfNN Run 7/10, Epoch 489/1000, Training Loss (NLML): -959.8529\n",
      "convergence dfGPdfNN Run 7/10, Epoch 490/1000, Training Loss (NLML): -959.8564\n",
      "convergence dfGPdfNN Run 7/10, Epoch 491/1000, Training Loss (NLML): -959.8270\n",
      "convergence dfGPdfNN Run 7/10, Epoch 492/1000, Training Loss (NLML): -959.8517\n",
      "convergence dfGPdfNN Run 7/10, Epoch 493/1000, Training Loss (NLML): -959.8552\n",
      "convergence dfGPdfNN Run 7/10, Epoch 494/1000, Training Loss (NLML): -959.8536\n",
      "convergence dfGPdfNN Run 7/10, Epoch 495/1000, Training Loss (NLML): -959.8708\n",
      "convergence dfGPdfNN Run 7/10, Epoch 496/1000, Training Loss (NLML): -959.8750\n",
      "convergence dfGPdfNN Run 7/10, Epoch 497/1000, Training Loss (NLML): -959.8844\n",
      "convergence dfGPdfNN Run 7/10, Epoch 498/1000, Training Loss (NLML): -959.8915\n",
      "convergence dfGPdfNN Run 7/10, Epoch 499/1000, Training Loss (NLML): -959.8970\n",
      "convergence dfGPdfNN Run 7/10, Epoch 500/1000, Training Loss (NLML): -959.8949\n",
      "convergence dfGPdfNN Run 7/10, Epoch 501/1000, Training Loss (NLML): -959.9563\n",
      "convergence dfGPdfNN Run 7/10, Epoch 502/1000, Training Loss (NLML): -959.9652\n",
      "convergence dfGPdfNN Run 7/10, Epoch 503/1000, Training Loss (NLML): -959.9742\n",
      "convergence dfGPdfNN Run 7/10, Epoch 504/1000, Training Loss (NLML): -959.9857\n",
      "convergence dfGPdfNN Run 7/10, Epoch 505/1000, Training Loss (NLML): -959.9922\n",
      "convergence dfGPdfNN Run 7/10, Epoch 506/1000, Training Loss (NLML): -960.0311\n",
      "convergence dfGPdfNN Run 7/10, Epoch 507/1000, Training Loss (NLML): -960.0800\n",
      "convergence dfGPdfNN Run 7/10, Epoch 508/1000, Training Loss (NLML): -960.0793\n",
      "convergence dfGPdfNN Run 7/10, Epoch 509/1000, Training Loss (NLML): -960.0430\n",
      "convergence dfGPdfNN Run 7/10, Epoch 510/1000, Training Loss (NLML): -959.9985\n",
      "convergence dfGPdfNN Run 7/10, Epoch 511/1000, Training Loss (NLML): -959.9556\n",
      "convergence dfGPdfNN Run 7/10, Epoch 512/1000, Training Loss (NLML): -959.9225\n",
      "convergence dfGPdfNN Run 7/10, Epoch 513/1000, Training Loss (NLML): -959.9180\n",
      "convergence dfGPdfNN Run 7/10, Epoch 514/1000, Training Loss (NLML): -959.9487\n",
      "convergence dfGPdfNN Run 7/10, Epoch 515/1000, Training Loss (NLML): -959.9744\n",
      "convergence dfGPdfNN Run 7/10, Epoch 516/1000, Training Loss (NLML): -959.9845\n",
      "convergence dfGPdfNN Run 7/10, Epoch 517/1000, Training Loss (NLML): -960.0348\n",
      "convergence dfGPdfNN Run 7/10, Epoch 518/1000, Training Loss (NLML): -960.0742\n",
      "convergence dfGPdfNN Run 7/10, Epoch 519/1000, Training Loss (NLML): -960.0898\n",
      "convergence dfGPdfNN Run 7/10, Epoch 520/1000, Training Loss (NLML): -960.1193\n",
      "convergence dfGPdfNN Run 7/10, Epoch 521/1000, Training Loss (NLML): -960.1320\n",
      "convergence dfGPdfNN Run 7/10, Epoch 522/1000, Training Loss (NLML): -960.1285\n",
      "convergence dfGPdfNN Run 7/10, Epoch 523/1000, Training Loss (NLML): -960.1168\n",
      "convergence dfGPdfNN Run 7/10, Epoch 524/1000, Training Loss (NLML): -960.1096\n",
      "convergence dfGPdfNN Run 7/10, Epoch 525/1000, Training Loss (NLML): -960.0739\n",
      "convergence dfGPdfNN Run 7/10, Epoch 526/1000, Training Loss (NLML): -960.0566\n",
      "convergence dfGPdfNN Run 7/10, Epoch 527/1000, Training Loss (NLML): -960.0508\n",
      "convergence dfGPdfNN Run 7/10, Epoch 528/1000, Training Loss (NLML): -960.0580\n",
      "convergence dfGPdfNN Run 7/10, Epoch 529/1000, Training Loss (NLML): -960.0663\n",
      "convergence dfGPdfNN Run 7/10, Epoch 530/1000, Training Loss (NLML): -960.0396\n",
      "convergence dfGPdfNN Run 7/10, Epoch 531/1000, Training Loss (NLML): -960.0664\n",
      "convergence dfGPdfNN Run 7/10, Epoch 532/1000, Training Loss (NLML): -960.1055\n",
      "convergence dfGPdfNN Run 7/10, Epoch 533/1000, Training Loss (NLML): -960.1292\n",
      "convergence dfGPdfNN Run 7/10, Epoch 534/1000, Training Loss (NLML): -960.1326\n",
      "convergence dfGPdfNN Run 7/10, Epoch 535/1000, Training Loss (NLML): -960.1353\n",
      "convergence dfGPdfNN Run 7/10, Epoch 536/1000, Training Loss (NLML): -960.1465\n",
      "convergence dfGPdfNN Run 7/10, Epoch 537/1000, Training Loss (NLML): -960.1622\n",
      "convergence dfGPdfNN Run 7/10, Epoch 538/1000, Training Loss (NLML): -960.1699\n",
      "convergence dfGPdfNN Run 7/10, Epoch 539/1000, Training Loss (NLML): -960.1147\n",
      "convergence dfGPdfNN Run 7/10, Epoch 540/1000, Training Loss (NLML): -960.1056\n",
      "convergence dfGPdfNN Run 7/10, Epoch 541/1000, Training Loss (NLML): -960.1240\n",
      "convergence dfGPdfNN Run 7/10, Epoch 542/1000, Training Loss (NLML): -960.1576\n",
      "convergence dfGPdfNN Run 7/10, Epoch 543/1000, Training Loss (NLML): -960.1523\n",
      "convergence dfGPdfNN Run 7/10, Epoch 544/1000, Training Loss (NLML): -960.1550\n",
      "convergence dfGPdfNN Run 7/10, Epoch 545/1000, Training Loss (NLML): -960.1667\n",
      "convergence dfGPdfNN Run 7/10, Epoch 546/1000, Training Loss (NLML): -960.1721\n",
      "convergence dfGPdfNN Run 7/10, Epoch 547/1000, Training Loss (NLML): -960.1763\n",
      "convergence dfGPdfNN Run 7/10, Epoch 548/1000, Training Loss (NLML): -960.1772\n",
      "convergence dfGPdfNN Run 7/10, Epoch 549/1000, Training Loss (NLML): -960.1895\n",
      "convergence dfGPdfNN Run 7/10, Epoch 550/1000, Training Loss (NLML): -960.1907\n",
      "convergence dfGPdfNN Run 7/10, Epoch 551/1000, Training Loss (NLML): -960.1956\n",
      "convergence dfGPdfNN Run 7/10, Epoch 552/1000, Training Loss (NLML): -960.2003\n",
      "convergence dfGPdfNN Run 7/10, Epoch 553/1000, Training Loss (NLML): -960.2036\n",
      "convergence dfGPdfNN Run 7/10, Epoch 554/1000, Training Loss (NLML): -960.2092\n",
      "convergence dfGPdfNN Run 7/10, Epoch 555/1000, Training Loss (NLML): -960.2081\n",
      "convergence dfGPdfNN Run 7/10, Epoch 556/1000, Training Loss (NLML): -960.1973\n",
      "convergence dfGPdfNN Run 7/10, Epoch 557/1000, Training Loss (NLML): -960.1853\n",
      "convergence dfGPdfNN Run 7/10, Epoch 558/1000, Training Loss (NLML): -960.2091\n",
      "convergence dfGPdfNN Run 7/10, Epoch 559/1000, Training Loss (NLML): -960.2117\n",
      "convergence dfGPdfNN Run 7/10, Epoch 560/1000, Training Loss (NLML): -960.2166\n",
      "convergence dfGPdfNN Run 7/10, Epoch 561/1000, Training Loss (NLML): -960.2236\n",
      "convergence dfGPdfNN Run 7/10, Epoch 562/1000, Training Loss (NLML): -960.2379\n",
      "convergence dfGPdfNN Run 7/10, Epoch 563/1000, Training Loss (NLML): -960.2423\n",
      "convergence dfGPdfNN Run 7/10, Epoch 564/1000, Training Loss (NLML): -960.2472\n",
      "convergence dfGPdfNN Run 7/10, Epoch 565/1000, Training Loss (NLML): -960.2498\n",
      "convergence dfGPdfNN Run 7/10, Epoch 566/1000, Training Loss (NLML): -960.2533\n",
      "convergence dfGPdfNN Run 7/10, Epoch 567/1000, Training Loss (NLML): -960.2596\n",
      "convergence dfGPdfNN Run 7/10, Epoch 568/1000, Training Loss (NLML): -960.2289\n",
      "convergence dfGPdfNN Run 7/10, Epoch 569/1000, Training Loss (NLML): -960.2661\n",
      "convergence dfGPdfNN Run 7/10, Epoch 570/1000, Training Loss (NLML): -960.2701\n",
      "convergence dfGPdfNN Run 7/10, Epoch 571/1000, Training Loss (NLML): -960.2717\n",
      "convergence dfGPdfNN Run 7/10, Epoch 572/1000, Training Loss (NLML): -960.2755\n",
      "convergence dfGPdfNN Run 7/10, Epoch 573/1000, Training Loss (NLML): -960.2756\n",
      "convergence dfGPdfNN Run 7/10, Epoch 574/1000, Training Loss (NLML): -960.2728\n",
      "convergence dfGPdfNN Run 7/10, Epoch 575/1000, Training Loss (NLML): -960.2772\n",
      "convergence dfGPdfNN Run 7/10, Epoch 576/1000, Training Loss (NLML): -960.2809\n",
      "convergence dfGPdfNN Run 7/10, Epoch 577/1000, Training Loss (NLML): -960.2825\n",
      "convergence dfGPdfNN Run 7/10, Epoch 578/1000, Training Loss (NLML): -960.2859\n",
      "convergence dfGPdfNN Run 7/10, Epoch 579/1000, Training Loss (NLML): -960.2998\n",
      "convergence dfGPdfNN Run 7/10, Epoch 580/1000, Training Loss (NLML): -960.3031\n",
      "convergence dfGPdfNN Run 7/10, Epoch 581/1000, Training Loss (NLML): -960.3087\n",
      "convergence dfGPdfNN Run 7/10, Epoch 582/1000, Training Loss (NLML): -960.3112\n",
      "convergence dfGPdfNN Run 7/10, Epoch 583/1000, Training Loss (NLML): -960.3138\n",
      "convergence dfGPdfNN Run 7/10, Epoch 584/1000, Training Loss (NLML): -960.3083\n",
      "convergence dfGPdfNN Run 7/10, Epoch 585/1000, Training Loss (NLML): -960.3102\n",
      "convergence dfGPdfNN Run 7/10, Epoch 586/1000, Training Loss (NLML): -960.3140\n",
      "convergence dfGPdfNN Run 7/10, Epoch 587/1000, Training Loss (NLML): -960.3190\n",
      "convergence dfGPdfNN Run 7/10, Epoch 588/1000, Training Loss (NLML): -960.3214\n",
      "convergence dfGPdfNN Run 7/10, Epoch 589/1000, Training Loss (NLML): -960.3152\n",
      "convergence dfGPdfNN Run 7/10, Epoch 590/1000, Training Loss (NLML): -960.3115\n",
      "convergence dfGPdfNN Run 7/10, Epoch 591/1000, Training Loss (NLML): -960.3160\n",
      "convergence dfGPdfNN Run 7/10, Epoch 592/1000, Training Loss (NLML): -960.3210\n",
      "convergence dfGPdfNN Run 7/10, Epoch 593/1000, Training Loss (NLML): -960.3317\n",
      "convergence dfGPdfNN Run 7/10, Epoch 594/1000, Training Loss (NLML): -960.3413\n",
      "convergence dfGPdfNN Run 7/10, Epoch 595/1000, Training Loss (NLML): -960.3325\n",
      "convergence dfGPdfNN Run 7/10, Epoch 596/1000, Training Loss (NLML): -960.3549\n",
      "convergence dfGPdfNN Run 7/10, Epoch 597/1000, Training Loss (NLML): -960.3580\n",
      "convergence dfGPdfNN Run 7/10, Epoch 598/1000, Training Loss (NLML): -960.3617\n",
      "convergence dfGPdfNN Run 7/10, Epoch 599/1000, Training Loss (NLML): -960.3666\n",
      "convergence dfGPdfNN Run 7/10, Epoch 600/1000, Training Loss (NLML): -960.3676\n",
      "convergence dfGPdfNN Run 7/10, Epoch 601/1000, Training Loss (NLML): -960.3752\n",
      "convergence dfGPdfNN Run 7/10, Epoch 602/1000, Training Loss (NLML): -960.3783\n",
      "convergence dfGPdfNN Run 7/10, Epoch 603/1000, Training Loss (NLML): -960.3804\n",
      "convergence dfGPdfNN Run 7/10, Epoch 604/1000, Training Loss (NLML): -960.3853\n",
      "convergence dfGPdfNN Run 7/10, Epoch 605/1000, Training Loss (NLML): -960.3885\n",
      "convergence dfGPdfNN Run 7/10, Epoch 606/1000, Training Loss (NLML): -960.3920\n",
      "convergence dfGPdfNN Run 7/10, Epoch 607/1000, Training Loss (NLML): -960.3951\n",
      "convergence dfGPdfNN Run 7/10, Epoch 608/1000, Training Loss (NLML): -960.3978\n",
      "convergence dfGPdfNN Run 7/10, Epoch 609/1000, Training Loss (NLML): -960.3986\n",
      "convergence dfGPdfNN Run 7/10, Epoch 610/1000, Training Loss (NLML): -960.4025\n",
      "convergence dfGPdfNN Run 7/10, Epoch 611/1000, Training Loss (NLML): -960.4050\n",
      "convergence dfGPdfNN Run 7/10, Epoch 612/1000, Training Loss (NLML): -960.4088\n",
      "convergence dfGPdfNN Run 7/10, Epoch 613/1000, Training Loss (NLML): -960.4100\n",
      "convergence dfGPdfNN Run 7/10, Epoch 614/1000, Training Loss (NLML): -960.4148\n",
      "convergence dfGPdfNN Run 7/10, Epoch 615/1000, Training Loss (NLML): -960.4183\n",
      "convergence dfGPdfNN Run 7/10, Epoch 616/1000, Training Loss (NLML): -960.4138\n",
      "convergence dfGPdfNN Run 7/10, Epoch 617/1000, Training Loss (NLML): -960.4250\n",
      "convergence dfGPdfNN Run 7/10, Epoch 618/1000, Training Loss (NLML): -960.4281\n",
      "convergence dfGPdfNN Run 7/10, Epoch 619/1000, Training Loss (NLML): -960.4308\n",
      "convergence dfGPdfNN Run 7/10, Epoch 620/1000, Training Loss (NLML): -960.4335\n",
      "convergence dfGPdfNN Run 7/10, Epoch 621/1000, Training Loss (NLML): -960.4362\n",
      "convergence dfGPdfNN Run 7/10, Epoch 622/1000, Training Loss (NLML): -960.4387\n",
      "convergence dfGPdfNN Run 7/10, Epoch 623/1000, Training Loss (NLML): -960.4426\n",
      "convergence dfGPdfNN Run 7/10, Epoch 624/1000, Training Loss (NLML): -960.4451\n",
      "convergence dfGPdfNN Run 7/10, Epoch 625/1000, Training Loss (NLML): -960.4482\n",
      "convergence dfGPdfNN Run 7/10, Epoch 626/1000, Training Loss (NLML): -960.4512\n",
      "convergence dfGPdfNN Run 7/10, Epoch 627/1000, Training Loss (NLML): -960.4547\n",
      "convergence dfGPdfNN Run 7/10, Epoch 628/1000, Training Loss (NLML): -960.4592\n",
      "convergence dfGPdfNN Run 7/10, Epoch 629/1000, Training Loss (NLML): -960.4609\n",
      "convergence dfGPdfNN Run 7/10, Epoch 630/1000, Training Loss (NLML): -960.4635\n",
      "convergence dfGPdfNN Run 7/10, Epoch 631/1000, Training Loss (NLML): -960.4673\n",
      "convergence dfGPdfNN Run 7/10, Epoch 632/1000, Training Loss (NLML): -960.4695\n",
      "convergence dfGPdfNN Run 7/10, Epoch 633/1000, Training Loss (NLML): -960.4733\n",
      "convergence dfGPdfNN Run 7/10, Epoch 634/1000, Training Loss (NLML): -960.4772\n",
      "convergence dfGPdfNN Run 7/10, Epoch 635/1000, Training Loss (NLML): -960.4792\n",
      "convergence dfGPdfNN Run 7/10, Epoch 636/1000, Training Loss (NLML): -960.4834\n",
      "convergence dfGPdfNN Run 7/10, Epoch 637/1000, Training Loss (NLML): -960.4865\n",
      "convergence dfGPdfNN Run 7/10, Epoch 638/1000, Training Loss (NLML): -960.4889\n",
      "convergence dfGPdfNN Run 7/10, Epoch 639/1000, Training Loss (NLML): -960.4921\n",
      "convergence dfGPdfNN Run 7/10, Epoch 640/1000, Training Loss (NLML): -960.4941\n",
      "convergence dfGPdfNN Run 7/10, Epoch 641/1000, Training Loss (NLML): -960.4980\n",
      "convergence dfGPdfNN Run 7/10, Epoch 642/1000, Training Loss (NLML): -960.5009\n",
      "convergence dfGPdfNN Run 7/10, Epoch 643/1000, Training Loss (NLML): -960.5038\n",
      "convergence dfGPdfNN Run 7/10, Epoch 644/1000, Training Loss (NLML): -960.5055\n",
      "convergence dfGPdfNN Run 7/10, Epoch 645/1000, Training Loss (NLML): -960.5087\n",
      "convergence dfGPdfNN Run 7/10, Epoch 646/1000, Training Loss (NLML): -960.5116\n",
      "convergence dfGPdfNN Run 7/10, Epoch 647/1000, Training Loss (NLML): -960.5140\n",
      "convergence dfGPdfNN Run 7/10, Epoch 648/1000, Training Loss (NLML): -960.5179\n",
      "convergence dfGPdfNN Run 7/10, Epoch 649/1000, Training Loss (NLML): -960.5210\n",
      "convergence dfGPdfNN Run 7/10, Epoch 650/1000, Training Loss (NLML): -960.5254\n",
      "convergence dfGPdfNN Run 7/10, Epoch 651/1000, Training Loss (NLML): -960.5289\n",
      "convergence dfGPdfNN Run 7/10, Epoch 652/1000, Training Loss (NLML): -960.5304\n",
      "convergence dfGPdfNN Run 7/10, Epoch 653/1000, Training Loss (NLML): -960.5337\n",
      "convergence dfGPdfNN Run 7/10, Epoch 654/1000, Training Loss (NLML): -960.5348\n",
      "convergence dfGPdfNN Run 7/10, Epoch 655/1000, Training Loss (NLML): -960.5387\n",
      "convergence dfGPdfNN Run 7/10, Epoch 656/1000, Training Loss (NLML): -960.5416\n",
      "convergence dfGPdfNN Run 7/10, Epoch 657/1000, Training Loss (NLML): -960.5454\n",
      "convergence dfGPdfNN Run 7/10, Epoch 658/1000, Training Loss (NLML): -960.5475\n",
      "convergence dfGPdfNN Run 7/10, Epoch 659/1000, Training Loss (NLML): -960.5486\n",
      "convergence dfGPdfNN Run 7/10, Epoch 660/1000, Training Loss (NLML): -960.5532\n",
      "convergence dfGPdfNN Run 7/10, Epoch 661/1000, Training Loss (NLML): -960.5565\n",
      "convergence dfGPdfNN Run 7/10, Epoch 662/1000, Training Loss (NLML): -960.5468\n",
      "convergence dfGPdfNN Run 7/10, Epoch 663/1000, Training Loss (NLML): -960.5555\n",
      "convergence dfGPdfNN Run 7/10, Epoch 664/1000, Training Loss (NLML): -960.5597\n",
      "convergence dfGPdfNN Run 7/10, Epoch 665/1000, Training Loss (NLML): -960.5623\n",
      "convergence dfGPdfNN Run 7/10, Epoch 666/1000, Training Loss (NLML): -960.5627\n",
      "convergence dfGPdfNN Run 7/10, Epoch 667/1000, Training Loss (NLML): -960.5649\n",
      "convergence dfGPdfNN Run 7/10, Epoch 668/1000, Training Loss (NLML): -960.5656\n",
      "convergence dfGPdfNN Run 7/10, Epoch 669/1000, Training Loss (NLML): -960.5710\n",
      "convergence dfGPdfNN Run 7/10, Epoch 670/1000, Training Loss (NLML): -960.5735\n",
      "convergence dfGPdfNN Run 7/10, Epoch 671/1000, Training Loss (NLML): -960.5778\n",
      "convergence dfGPdfNN Run 7/10, Epoch 672/1000, Training Loss (NLML): -960.5797\n",
      "convergence dfGPdfNN Run 7/10, Epoch 673/1000, Training Loss (NLML): -960.5857\n",
      "convergence dfGPdfNN Run 7/10, Epoch 674/1000, Training Loss (NLML): -960.5867\n",
      "convergence dfGPdfNN Run 7/10, Epoch 675/1000, Training Loss (NLML): -960.5890\n",
      "convergence dfGPdfNN Run 7/10, Epoch 676/1000, Training Loss (NLML): -960.5925\n",
      "convergence dfGPdfNN Run 7/10, Epoch 677/1000, Training Loss (NLML): -960.5920\n",
      "convergence dfGPdfNN Run 7/10, Epoch 678/1000, Training Loss (NLML): -960.5989\n",
      "convergence dfGPdfNN Run 7/10, Epoch 679/1000, Training Loss (NLML): -960.5861\n",
      "convergence dfGPdfNN Run 7/10, Epoch 680/1000, Training Loss (NLML): -960.6060\n",
      "convergence dfGPdfNN Run 7/10, Epoch 681/1000, Training Loss (NLML): -960.6105\n",
      "convergence dfGPdfNN Run 7/10, Epoch 682/1000, Training Loss (NLML): -960.6096\n",
      "convergence dfGPdfNN Run 7/10, Epoch 683/1000, Training Loss (NLML): -960.6195\n",
      "convergence dfGPdfNN Run 7/10, Epoch 684/1000, Training Loss (NLML): -960.6230\n",
      "convergence dfGPdfNN Run 7/10, Epoch 685/1000, Training Loss (NLML): -960.6012\n",
      "convergence dfGPdfNN Run 7/10, Epoch 686/1000, Training Loss (NLML): -960.6161\n",
      "convergence dfGPdfNN Run 7/10, Epoch 687/1000, Training Loss (NLML): -960.6211\n",
      "convergence dfGPdfNN Run 7/10, Epoch 688/1000, Training Loss (NLML): -960.5999\n",
      "convergence dfGPdfNN Run 7/10, Epoch 689/1000, Training Loss (NLML): -960.6277\n",
      "convergence dfGPdfNN Run 7/10, Epoch 690/1000, Training Loss (NLML): -960.6356\n",
      "convergence dfGPdfNN Run 7/10, Epoch 691/1000, Training Loss (NLML): -960.5996\n",
      "convergence dfGPdfNN Run 7/10, Epoch 692/1000, Training Loss (NLML): -960.6151\n",
      "convergence dfGPdfNN Run 7/10, Epoch 693/1000, Training Loss (NLML): -960.6453\n",
      "convergence dfGPdfNN Run 7/10, Epoch 694/1000, Training Loss (NLML): -960.6475\n",
      "convergence dfGPdfNN Run 7/10, Epoch 695/1000, Training Loss (NLML): -960.6482\n",
      "convergence dfGPdfNN Run 7/10, Epoch 696/1000, Training Loss (NLML): -960.6162\n",
      "convergence dfGPdfNN Run 7/10, Epoch 697/1000, Training Loss (NLML): -960.6442\n",
      "convergence dfGPdfNN Run 7/10, Epoch 698/1000, Training Loss (NLML): -960.6124\n",
      "convergence dfGPdfNN Run 7/10, Epoch 699/1000, Training Loss (NLML): -960.6519\n",
      "convergence dfGPdfNN Run 7/10, Epoch 700/1000, Training Loss (NLML): -960.6434\n",
      "convergence dfGPdfNN Run 7/10, Epoch 701/1000, Training Loss (NLML): -960.6349\n",
      "convergence dfGPdfNN Run 7/10, Epoch 702/1000, Training Loss (NLML): -960.6367\n",
      "convergence dfGPdfNN Run 7/10, Epoch 703/1000, Training Loss (NLML): -960.6240\n",
      "convergence dfGPdfNN Run 7/10, Epoch 704/1000, Training Loss (NLML): -960.6506\n",
      "convergence dfGPdfNN Run 7/10, Epoch 705/1000, Training Loss (NLML): -960.6469\n",
      "convergence dfGPdfNN Run 7/10, Epoch 706/1000, Training Loss (NLML): -960.6469\n",
      "convergence dfGPdfNN Run 7/10, Epoch 707/1000, Training Loss (NLML): -960.6342\n",
      "convergence dfGPdfNN Run 7/10, Epoch 708/1000, Training Loss (NLML): -960.6279\n",
      "convergence dfGPdfNN Run 7/10, Epoch 709/1000, Training Loss (NLML): -960.6289\n",
      "convergence dfGPdfNN Run 7/10, Epoch 710/1000, Training Loss (NLML): -960.6139\n",
      "convergence dfGPdfNN Run 7/10, Epoch 711/1000, Training Loss (NLML): -960.6213\n",
      "convergence dfGPdfNN Run 7/10, Epoch 712/1000, Training Loss (NLML): -960.6234\n",
      "convergence dfGPdfNN Run 7/10, Epoch 713/1000, Training Loss (NLML): -960.6281\n",
      "convergence dfGPdfNN Run 7/10, Epoch 714/1000, Training Loss (NLML): -960.6324\n",
      "convergence dfGPdfNN Run 7/10, Epoch 715/1000, Training Loss (NLML): -960.6550\n",
      "convergence dfGPdfNN Run 7/10, Epoch 716/1000, Training Loss (NLML): -960.6610\n",
      "convergence dfGPdfNN Run 7/10, Epoch 717/1000, Training Loss (NLML): -960.6755\n",
      "convergence dfGPdfNN Run 7/10, Epoch 718/1000, Training Loss (NLML): -960.6790\n",
      "convergence dfGPdfNN Run 7/10, Epoch 719/1000, Training Loss (NLML): -960.6770\n",
      "convergence dfGPdfNN Run 7/10, Epoch 720/1000, Training Loss (NLML): -960.6591\n",
      "convergence dfGPdfNN Run 7/10, Epoch 721/1000, Training Loss (NLML): -960.6594\n",
      "convergence dfGPdfNN Run 7/10, Epoch 722/1000, Training Loss (NLML): -960.6426\n",
      "convergence dfGPdfNN Run 7/10, Epoch 723/1000, Training Loss (NLML): -960.6345\n",
      "convergence dfGPdfNN Run 7/10, Epoch 724/1000, Training Loss (NLML): -960.6803\n",
      "convergence dfGPdfNN Run 7/10, Epoch 725/1000, Training Loss (NLML): -960.6813\n",
      "convergence dfGPdfNN Run 7/10, Epoch 726/1000, Training Loss (NLML): -960.6688\n",
      "convergence dfGPdfNN Run 7/10, Epoch 727/1000, Training Loss (NLML): -960.6733\n",
      "convergence dfGPdfNN Run 7/10, Epoch 728/1000, Training Loss (NLML): -960.6759\n",
      "convergence dfGPdfNN Run 7/10, Epoch 729/1000, Training Loss (NLML): -960.6925\n",
      "convergence dfGPdfNN Run 7/10, Epoch 730/1000, Training Loss (NLML): -960.6936\n",
      "convergence dfGPdfNN Run 7/10, Epoch 731/1000, Training Loss (NLML): -960.7108\n",
      "convergence dfGPdfNN Run 7/10, Epoch 732/1000, Training Loss (NLML): -960.7133\n",
      "convergence dfGPdfNN Run 7/10, Epoch 733/1000, Training Loss (NLML): -960.7036\n",
      "convergence dfGPdfNN Run 7/10, Epoch 734/1000, Training Loss (NLML): -960.7030\n",
      "convergence dfGPdfNN Run 7/10, Epoch 735/1000, Training Loss (NLML): -960.7083\n",
      "convergence dfGPdfNN Run 7/10, Epoch 736/1000, Training Loss (NLML): -960.7095\n",
      "convergence dfGPdfNN Run 7/10, Epoch 737/1000, Training Loss (NLML): -960.7173\n",
      "convergence dfGPdfNN Run 7/10, Epoch 738/1000, Training Loss (NLML): -960.7183\n",
      "convergence dfGPdfNN Run 7/10, Epoch 739/1000, Training Loss (NLML): -960.7197\n",
      "convergence dfGPdfNN Run 7/10, Epoch 740/1000, Training Loss (NLML): -960.7194\n",
      "convergence dfGPdfNN Run 7/10, Epoch 741/1000, Training Loss (NLML): -960.7235\n",
      "convergence dfGPdfNN Run 7/10, Epoch 742/1000, Training Loss (NLML): -960.7264\n",
      "convergence dfGPdfNN Run 7/10, Epoch 743/1000, Training Loss (NLML): -960.7279\n",
      "convergence dfGPdfNN Run 7/10, Epoch 744/1000, Training Loss (NLML): -960.7317\n",
      "convergence dfGPdfNN Run 7/10, Epoch 745/1000, Training Loss (NLML): -960.7338\n",
      "convergence dfGPdfNN Run 7/10, Epoch 746/1000, Training Loss (NLML): -960.7333\n",
      "convergence dfGPdfNN Run 7/10, Epoch 747/1000, Training Loss (NLML): -960.7386\n",
      "convergence dfGPdfNN Run 7/10, Epoch 748/1000, Training Loss (NLML): -960.7404\n",
      "convergence dfGPdfNN Run 7/10, Epoch 749/1000, Training Loss (NLML): -960.7441\n",
      "convergence dfGPdfNN Run 7/10, Epoch 750/1000, Training Loss (NLML): -960.7283\n",
      "convergence dfGPdfNN Run 7/10, Epoch 751/1000, Training Loss (NLML): -960.7466\n",
      "convergence dfGPdfNN Run 7/10, Epoch 752/1000, Training Loss (NLML): -960.7498\n",
      "convergence dfGPdfNN Run 7/10, Epoch 753/1000, Training Loss (NLML): -960.7499\n",
      "convergence dfGPdfNN Run 7/10, Epoch 754/1000, Training Loss (NLML): -960.7548\n",
      "convergence dfGPdfNN Run 7/10, Epoch 755/1000, Training Loss (NLML): -960.7390\n",
      "convergence dfGPdfNN Run 7/10, Epoch 756/1000, Training Loss (NLML): -960.7589\n",
      "convergence dfGPdfNN Run 7/10, Epoch 757/1000, Training Loss (NLML): -960.7478\n",
      "convergence dfGPdfNN Run 7/10, Epoch 758/1000, Training Loss (NLML): -960.7627\n",
      "convergence dfGPdfNN Run 7/10, Epoch 759/1000, Training Loss (NLML): -960.7679\n",
      "convergence dfGPdfNN Run 7/10, Epoch 760/1000, Training Loss (NLML): -960.7700\n",
      "convergence dfGPdfNN Run 7/10, Epoch 761/1000, Training Loss (NLML): -960.7729\n",
      "convergence dfGPdfNN Run 7/10, Epoch 762/1000, Training Loss (NLML): -960.7614\n",
      "convergence dfGPdfNN Run 7/10, Epoch 763/1000, Training Loss (NLML): -960.7749\n",
      "convergence dfGPdfNN Run 7/10, Epoch 764/1000, Training Loss (NLML): -960.7793\n",
      "convergence dfGPdfNN Run 7/10, Epoch 765/1000, Training Loss (NLML): -960.7922\n",
      "convergence dfGPdfNN Run 7/10, Epoch 766/1000, Training Loss (NLML): -960.7899\n",
      "convergence dfGPdfNN Run 7/10, Epoch 767/1000, Training Loss (NLML): -960.7960\n",
      "convergence dfGPdfNN Run 7/10, Epoch 768/1000, Training Loss (NLML): -960.7859\n",
      "convergence dfGPdfNN Run 7/10, Epoch 769/1000, Training Loss (NLML): -960.7904\n",
      "convergence dfGPdfNN Run 7/10, Epoch 770/1000, Training Loss (NLML): -960.7921\n",
      "convergence dfGPdfNN Run 7/10, Epoch 771/1000, Training Loss (NLML): -960.7955\n",
      "convergence dfGPdfNN Run 7/10, Epoch 772/1000, Training Loss (NLML): -960.7827\n",
      "convergence dfGPdfNN Run 7/10, Epoch 773/1000, Training Loss (NLML): -960.7968\n",
      "convergence dfGPdfNN Run 7/10, Epoch 774/1000, Training Loss (NLML): -960.8109\n",
      "convergence dfGPdfNN Run 7/10, Epoch 775/1000, Training Loss (NLML): -960.8152\n",
      "convergence dfGPdfNN Run 7/10, Epoch 776/1000, Training Loss (NLML): -960.8169\n",
      "convergence dfGPdfNN Run 7/10, Epoch 777/1000, Training Loss (NLML): -960.8212\n",
      "convergence dfGPdfNN Run 7/10, Epoch 778/1000, Training Loss (NLML): -960.8065\n",
      "convergence dfGPdfNN Run 7/10, Epoch 779/1000, Training Loss (NLML): -960.8224\n",
      "convergence dfGPdfNN Run 7/10, Epoch 780/1000, Training Loss (NLML): -960.8011\n",
      "convergence dfGPdfNN Run 7/10, Epoch 781/1000, Training Loss (NLML): -960.8187\n",
      "convergence dfGPdfNN Run 7/10, Epoch 782/1000, Training Loss (NLML): -960.8184\n",
      "convergence dfGPdfNN Run 7/10, Epoch 783/1000, Training Loss (NLML): -960.8069\n",
      "convergence dfGPdfNN Run 7/10, Epoch 784/1000, Training Loss (NLML): -960.8098\n",
      "convergence dfGPdfNN Run 7/10, Epoch 785/1000, Training Loss (NLML): -960.8274\n",
      "convergence dfGPdfNN Run 7/10, Epoch 786/1000, Training Loss (NLML): -960.8278\n",
      "convergence dfGPdfNN Run 7/10, Epoch 787/1000, Training Loss (NLML): -960.8312\n",
      "convergence dfGPdfNN Run 7/10, Epoch 788/1000, Training Loss (NLML): -960.8447\n",
      "convergence dfGPdfNN Run 7/10, Epoch 789/1000, Training Loss (NLML): -960.8467\n",
      "convergence dfGPdfNN Run 7/10, Epoch 790/1000, Training Loss (NLML): -960.8225\n",
      "convergence dfGPdfNN Run 7/10, Epoch 791/1000, Training Loss (NLML): -960.8414\n",
      "convergence dfGPdfNN Run 7/10, Epoch 792/1000, Training Loss (NLML): -960.8420\n",
      "convergence dfGPdfNN Run 7/10, Epoch 793/1000, Training Loss (NLML): -960.8448\n",
      "convergence dfGPdfNN Run 7/10, Epoch 794/1000, Training Loss (NLML): -960.8473\n",
      "convergence dfGPdfNN Run 7/10, Epoch 795/1000, Training Loss (NLML): -960.8502\n",
      "convergence dfGPdfNN Run 7/10, Epoch 796/1000, Training Loss (NLML): -960.8608\n",
      "convergence dfGPdfNN Run 7/10, Epoch 797/1000, Training Loss (NLML): -960.8657\n",
      "convergence dfGPdfNN Run 7/10, Epoch 798/1000, Training Loss (NLML): -960.8557\n",
      "convergence dfGPdfNN Run 7/10, Epoch 799/1000, Training Loss (NLML): -960.8571\n",
      "convergence dfGPdfNN Run 7/10, Epoch 800/1000, Training Loss (NLML): -960.8610\n",
      "convergence dfGPdfNN Run 7/10, Epoch 801/1000, Training Loss (NLML): -960.8621\n",
      "convergence dfGPdfNN Run 7/10, Epoch 802/1000, Training Loss (NLML): -960.8636\n",
      "convergence dfGPdfNN Run 7/10, Epoch 803/1000, Training Loss (NLML): -960.8671\n",
      "convergence dfGPdfNN Run 7/10, Epoch 804/1000, Training Loss (NLML): -960.8541\n",
      "convergence dfGPdfNN Run 7/10, Epoch 805/1000, Training Loss (NLML): -960.8636\n",
      "convergence dfGPdfNN Run 7/10, Epoch 806/1000, Training Loss (NLML): -960.8824\n",
      "convergence dfGPdfNN Run 7/10, Epoch 807/1000, Training Loss (NLML): -960.8844\n",
      "convergence dfGPdfNN Run 7/10, Epoch 808/1000, Training Loss (NLML): -960.8846\n",
      "convergence dfGPdfNN Run 7/10, Epoch 809/1000, Training Loss (NLML): -960.8903\n",
      "convergence dfGPdfNN Run 7/10, Epoch 810/1000, Training Loss (NLML): -960.8798\n",
      "convergence dfGPdfNN Run 7/10, Epoch 811/1000, Training Loss (NLML): -960.8839\n",
      "convergence dfGPdfNN Run 7/10, Epoch 812/1000, Training Loss (NLML): -960.8716\n",
      "convergence dfGPdfNN Run 7/10, Epoch 813/1000, Training Loss (NLML): -960.8723\n",
      "convergence dfGPdfNN Run 7/10, Epoch 814/1000, Training Loss (NLML): -960.8914\n",
      "convergence dfGPdfNN Run 7/10, Epoch 815/1000, Training Loss (NLML): -960.8901\n",
      "convergence dfGPdfNN Run 7/10, Epoch 816/1000, Training Loss (NLML): -960.8944\n",
      "convergence dfGPdfNN Run 7/10, Epoch 817/1000, Training Loss (NLML): -960.9052\n",
      "convergence dfGPdfNN Run 7/10, Epoch 818/1000, Training Loss (NLML): -960.9102\n",
      "convergence dfGPdfNN Run 7/10, Epoch 819/1000, Training Loss (NLML): -960.8955\n",
      "convergence dfGPdfNN Run 7/10, Epoch 820/1000, Training Loss (NLML): -960.9108\n",
      "convergence dfGPdfNN Run 7/10, Epoch 821/1000, Training Loss (NLML): -960.9147\n",
      "convergence dfGPdfNN Run 7/10, Epoch 822/1000, Training Loss (NLML): -960.9177\n",
      "convergence dfGPdfNN Run 7/10, Epoch 823/1000, Training Loss (NLML): -960.9182\n",
      "convergence dfGPdfNN Run 7/10, Epoch 824/1000, Training Loss (NLML): -960.9233\n",
      "convergence dfGPdfNN Run 7/10, Epoch 825/1000, Training Loss (NLML): -960.9216\n",
      "convergence dfGPdfNN Run 7/10, Epoch 826/1000, Training Loss (NLML): -960.9026\n",
      "convergence dfGPdfNN Run 7/10, Epoch 827/1000, Training Loss (NLML): -960.9191\n",
      "convergence dfGPdfNN Run 7/10, Epoch 828/1000, Training Loss (NLML): -960.9188\n",
      "convergence dfGPdfNN Run 7/10, Epoch 829/1000, Training Loss (NLML): -960.9180\n",
      "convergence dfGPdfNN Run 7/10, Epoch 830/1000, Training Loss (NLML): -960.9227\n",
      "convergence dfGPdfNN Run 7/10, Epoch 831/1000, Training Loss (NLML): -960.9242\n",
      "convergence dfGPdfNN Run 7/10, Epoch 832/1000, Training Loss (NLML): -960.9305\n",
      "convergence dfGPdfNN Run 7/10, Epoch 833/1000, Training Loss (NLML): -960.9304\n",
      "convergence dfGPdfNN Run 7/10, Epoch 834/1000, Training Loss (NLML): -960.9424\n",
      "convergence dfGPdfNN Run 7/10, Epoch 835/1000, Training Loss (NLML): -960.9443\n",
      "convergence dfGPdfNN Run 7/10, Epoch 836/1000, Training Loss (NLML): -960.9438\n",
      "convergence dfGPdfNN Run 7/10, Epoch 837/1000, Training Loss (NLML): -960.9456\n",
      "convergence dfGPdfNN Run 7/10, Epoch 838/1000, Training Loss (NLML): -960.9480\n",
      "convergence dfGPdfNN Run 7/10, Epoch 839/1000, Training Loss (NLML): -960.9513\n",
      "convergence dfGPdfNN Run 7/10, Epoch 840/1000, Training Loss (NLML): -960.9531\n",
      "convergence dfGPdfNN Run 7/10, Epoch 841/1000, Training Loss (NLML): -960.9540\n",
      "convergence dfGPdfNN Run 7/10, Epoch 842/1000, Training Loss (NLML): -960.9579\n",
      "convergence dfGPdfNN Run 7/10, Epoch 843/1000, Training Loss (NLML): -960.9584\n",
      "convergence dfGPdfNN Run 7/10, Epoch 844/1000, Training Loss (NLML): -960.9614\n",
      "convergence dfGPdfNN Run 7/10, Epoch 845/1000, Training Loss (NLML): -960.9652\n",
      "convergence dfGPdfNN Run 7/10, Epoch 846/1000, Training Loss (NLML): -960.9629\n",
      "convergence dfGPdfNN Run 7/10, Epoch 847/1000, Training Loss (NLML): -960.9675\n",
      "convergence dfGPdfNN Run 7/10, Epoch 848/1000, Training Loss (NLML): -960.9688\n",
      "convergence dfGPdfNN Run 7/10, Epoch 849/1000, Training Loss (NLML): -960.9705\n",
      "convergence dfGPdfNN Run 7/10, Epoch 850/1000, Training Loss (NLML): -960.9741\n",
      "convergence dfGPdfNN Run 7/10, Epoch 851/1000, Training Loss (NLML): -960.9761\n",
      "convergence dfGPdfNN Run 7/10, Epoch 852/1000, Training Loss (NLML): -960.9761\n",
      "convergence dfGPdfNN Run 7/10, Epoch 853/1000, Training Loss (NLML): -960.9790\n",
      "convergence dfGPdfNN Run 7/10, Epoch 854/1000, Training Loss (NLML): -960.9752\n",
      "convergence dfGPdfNN Run 7/10, Epoch 855/1000, Training Loss (NLML): -960.9824\n",
      "convergence dfGPdfNN Run 7/10, Epoch 856/1000, Training Loss (NLML): -960.9839\n",
      "convergence dfGPdfNN Run 7/10, Epoch 857/1000, Training Loss (NLML): -960.9838\n",
      "convergence dfGPdfNN Run 7/10, Epoch 858/1000, Training Loss (NLML): -960.9879\n",
      "convergence dfGPdfNN Run 7/10, Epoch 859/1000, Training Loss (NLML): -960.9910\n",
      "convergence dfGPdfNN Run 7/10, Epoch 860/1000, Training Loss (NLML): -960.9915\n",
      "convergence dfGPdfNN Run 7/10, Epoch 861/1000, Training Loss (NLML): -960.9944\n",
      "convergence dfGPdfNN Run 7/10, Epoch 862/1000, Training Loss (NLML): -960.9951\n",
      "convergence dfGPdfNN Run 7/10, Epoch 863/1000, Training Loss (NLML): -960.9963\n",
      "convergence dfGPdfNN Run 7/10, Epoch 864/1000, Training Loss (NLML): -960.9968\n",
      "convergence dfGPdfNN Run 7/10, Epoch 865/1000, Training Loss (NLML): -961.0017\n",
      "convergence dfGPdfNN Run 7/10, Epoch 866/1000, Training Loss (NLML): -961.0045\n",
      "convergence dfGPdfNN Run 7/10, Epoch 867/1000, Training Loss (NLML): -961.0054\n",
      "convergence dfGPdfNN Run 7/10, Epoch 868/1000, Training Loss (NLML): -961.0073\n",
      "convergence dfGPdfNN Run 7/10, Epoch 869/1000, Training Loss (NLML): -961.0092\n",
      "convergence dfGPdfNN Run 7/10, Epoch 870/1000, Training Loss (NLML): -961.0122\n",
      "convergence dfGPdfNN Run 7/10, Epoch 871/1000, Training Loss (NLML): -961.0155\n",
      "convergence dfGPdfNN Run 7/10, Epoch 872/1000, Training Loss (NLML): -961.0166\n",
      "convergence dfGPdfNN Run 7/10, Epoch 873/1000, Training Loss (NLML): -961.0161\n",
      "convergence dfGPdfNN Run 7/10, Epoch 874/1000, Training Loss (NLML): -961.0179\n",
      "convergence dfGPdfNN Run 7/10, Epoch 875/1000, Training Loss (NLML): -961.0098\n",
      "convergence dfGPdfNN Run 7/10, Epoch 876/1000, Training Loss (NLML): -961.0106\n",
      "convergence dfGPdfNN Run 7/10, Epoch 877/1000, Training Loss (NLML): -961.0132\n",
      "convergence dfGPdfNN Run 7/10, Epoch 878/1000, Training Loss (NLML): -961.0172\n",
      "convergence dfGPdfNN Run 7/10, Epoch 879/1000, Training Loss (NLML): -961.0265\n",
      "convergence dfGPdfNN Run 7/10, Epoch 880/1000, Training Loss (NLML): -961.0295\n",
      "convergence dfGPdfNN Run 7/10, Epoch 881/1000, Training Loss (NLML): -961.0302\n",
      "convergence dfGPdfNN Run 7/10, Epoch 882/1000, Training Loss (NLML): -961.0361\n",
      "convergence dfGPdfNN Run 7/10, Epoch 883/1000, Training Loss (NLML): -961.0361\n",
      "convergence dfGPdfNN Run 7/10, Epoch 884/1000, Training Loss (NLML): -961.0371\n",
      "convergence dfGPdfNN Run 7/10, Epoch 885/1000, Training Loss (NLML): -961.0366\n",
      "convergence dfGPdfNN Run 7/10, Epoch 886/1000, Training Loss (NLML): -961.0414\n",
      "convergence dfGPdfNN Run 7/10, Epoch 887/1000, Training Loss (NLML): -961.0413\n",
      "convergence dfGPdfNN Run 7/10, Epoch 888/1000, Training Loss (NLML): -961.0369\n",
      "convergence dfGPdfNN Run 7/10, Epoch 889/1000, Training Loss (NLML): -961.0452\n",
      "convergence dfGPdfNN Run 7/10, Epoch 890/1000, Training Loss (NLML): -961.0476\n",
      "convergence dfGPdfNN Run 7/10, Epoch 891/1000, Training Loss (NLML): -961.0509\n",
      "convergence dfGPdfNN Run 7/10, Epoch 892/1000, Training Loss (NLML): -961.0513\n",
      "convergence dfGPdfNN Run 7/10, Epoch 893/1000, Training Loss (NLML): -961.0531\n",
      "convergence dfGPdfNN Run 7/10, Epoch 894/1000, Training Loss (NLML): -961.0544\n",
      "convergence dfGPdfNN Run 7/10, Epoch 895/1000, Training Loss (NLML): -961.0548\n",
      "convergence dfGPdfNN Run 7/10, Epoch 896/1000, Training Loss (NLML): -961.0592\n",
      "convergence dfGPdfNN Run 7/10, Epoch 897/1000, Training Loss (NLML): -961.0586\n",
      "convergence dfGPdfNN Run 7/10, Epoch 898/1000, Training Loss (NLML): -961.0603\n",
      "convergence dfGPdfNN Run 7/10, Epoch 899/1000, Training Loss (NLML): -961.0627\n",
      "convergence dfGPdfNN Run 7/10, Epoch 900/1000, Training Loss (NLML): -961.0629\n",
      "convergence dfGPdfNN Run 7/10, Epoch 901/1000, Training Loss (NLML): -961.0670\n",
      "convergence dfGPdfNN Run 7/10, Epoch 902/1000, Training Loss (NLML): -961.0657\n",
      "convergence dfGPdfNN Run 7/10, Epoch 903/1000, Training Loss (NLML): -961.0718\n",
      "convergence dfGPdfNN Run 7/10, Epoch 904/1000, Training Loss (NLML): -961.0739\n",
      "convergence dfGPdfNN Run 7/10, Epoch 905/1000, Training Loss (NLML): -961.0741\n",
      "convergence dfGPdfNN Run 7/10, Epoch 906/1000, Training Loss (NLML): -961.0748\n",
      "convergence dfGPdfNN Run 7/10, Epoch 907/1000, Training Loss (NLML): -961.0784\n",
      "convergence dfGPdfNN Run 7/10, Epoch 908/1000, Training Loss (NLML): -961.0778\n",
      "convergence dfGPdfNN Run 7/10, Epoch 909/1000, Training Loss (NLML): -961.0790\n",
      "convergence dfGPdfNN Run 7/10, Epoch 910/1000, Training Loss (NLML): -961.0806\n",
      "convergence dfGPdfNN Run 7/10, Epoch 911/1000, Training Loss (NLML): -961.0848\n",
      "convergence dfGPdfNN Run 7/10, Epoch 912/1000, Training Loss (NLML): -961.0853\n",
      "convergence dfGPdfNN Run 7/10, Epoch 913/1000, Training Loss (NLML): -961.0851\n",
      "convergence dfGPdfNN Run 7/10, Epoch 914/1000, Training Loss (NLML): -961.0898\n",
      "convergence dfGPdfNN Run 7/10, Epoch 915/1000, Training Loss (NLML): -961.0919\n",
      "convergence dfGPdfNN Run 7/10, Epoch 916/1000, Training Loss (NLML): -961.0914\n",
      "convergence dfGPdfNN Run 7/10, Epoch 917/1000, Training Loss (NLML): -961.0953\n",
      "convergence dfGPdfNN Run 7/10, Epoch 918/1000, Training Loss (NLML): -961.0950\n",
      "convergence dfGPdfNN Run 7/10, Epoch 919/1000, Training Loss (NLML): -961.0981\n",
      "convergence dfGPdfNN Run 7/10, Epoch 920/1000, Training Loss (NLML): -961.0999\n",
      "convergence dfGPdfNN Run 7/10, Epoch 921/1000, Training Loss (NLML): -961.1006\n",
      "convergence dfGPdfNN Run 7/10, Epoch 922/1000, Training Loss (NLML): -961.1022\n",
      "convergence dfGPdfNN Run 7/10, Epoch 923/1000, Training Loss (NLML): -961.1045\n",
      "convergence dfGPdfNN Run 7/10, Epoch 924/1000, Training Loss (NLML): -961.1079\n",
      "convergence dfGPdfNN Run 7/10, Epoch 925/1000, Training Loss (NLML): -961.1058\n",
      "convergence dfGPdfNN Run 7/10, Epoch 926/1000, Training Loss (NLML): -961.1215\n",
      "convergence dfGPdfNN Run 7/10, Epoch 927/1000, Training Loss (NLML): -961.1232\n",
      "convergence dfGPdfNN Run 7/10, Epoch 928/1000, Training Loss (NLML): -961.1268\n",
      "convergence dfGPdfNN Run 7/10, Epoch 929/1000, Training Loss (NLML): -961.1290\n",
      "convergence dfGPdfNN Run 7/10, Epoch 930/1000, Training Loss (NLML): -961.1294\n",
      "convergence dfGPdfNN Run 7/10, Epoch 931/1000, Training Loss (NLML): -961.1307\n",
      "convergence dfGPdfNN Run 7/10, Epoch 932/1000, Training Loss (NLML): -961.1342\n",
      "convergence dfGPdfNN Run 7/10, Epoch 933/1000, Training Loss (NLML): -961.1355\n",
      "convergence dfGPdfNN Run 7/10, Epoch 934/1000, Training Loss (NLML): -961.1361\n",
      "convergence dfGPdfNN Run 7/10, Epoch 935/1000, Training Loss (NLML): -961.1333\n",
      "convergence dfGPdfNN Run 7/10, Epoch 936/1000, Training Loss (NLML): -961.1362\n",
      "convergence dfGPdfNN Run 7/10, Epoch 937/1000, Training Loss (NLML): -961.1373\n",
      "convergence dfGPdfNN Run 7/10, Epoch 938/1000, Training Loss (NLML): -961.1406\n",
      "convergence dfGPdfNN Run 7/10, Epoch 939/1000, Training Loss (NLML): -961.1407\n",
      "convergence dfGPdfNN Run 7/10, Epoch 940/1000, Training Loss (NLML): -961.1444\n",
      "convergence dfGPdfNN Run 7/10, Epoch 941/1000, Training Loss (NLML): -961.1440\n",
      "convergence dfGPdfNN Run 7/10, Epoch 942/1000, Training Loss (NLML): -961.1481\n",
      "convergence dfGPdfNN Run 7/10, Epoch 943/1000, Training Loss (NLML): -961.1458\n",
      "convergence dfGPdfNN Run 7/10, Epoch 944/1000, Training Loss (NLML): -961.1479\n",
      "convergence dfGPdfNN Run 7/10, Epoch 945/1000, Training Loss (NLML): -961.1519\n",
      "convergence dfGPdfNN Run 7/10, Epoch 946/1000, Training Loss (NLML): -961.1533\n",
      "convergence dfGPdfNN Run 7/10, Epoch 947/1000, Training Loss (NLML): -961.1577\n",
      "convergence dfGPdfNN Run 7/10, Epoch 948/1000, Training Loss (NLML): -961.1565\n",
      "convergence dfGPdfNN Run 7/10, Epoch 949/1000, Training Loss (NLML): -961.1575\n",
      "convergence dfGPdfNN Run 7/10, Epoch 950/1000, Training Loss (NLML): -961.1609\n",
      "convergence dfGPdfNN Run 7/10, Epoch 951/1000, Training Loss (NLML): -961.1603\n",
      "convergence dfGPdfNN Run 7/10, Epoch 952/1000, Training Loss (NLML): -961.1639\n",
      "convergence dfGPdfNN Run 7/10, Epoch 953/1000, Training Loss (NLML): -961.1667\n",
      "convergence dfGPdfNN Run 7/10, Epoch 954/1000, Training Loss (NLML): -961.1642\n",
      "convergence dfGPdfNN Run 7/10, Epoch 955/1000, Training Loss (NLML): -961.1669\n",
      "convergence dfGPdfNN Run 7/10, Epoch 956/1000, Training Loss (NLML): -961.1676\n",
      "convergence dfGPdfNN Run 7/10, Epoch 957/1000, Training Loss (NLML): -961.1702\n",
      "convergence dfGPdfNN Run 7/10, Epoch 958/1000, Training Loss (NLML): -961.1746\n",
      "convergence dfGPdfNN Run 7/10, Epoch 959/1000, Training Loss (NLML): -961.1737\n",
      "convergence dfGPdfNN Run 7/10, Epoch 960/1000, Training Loss (NLML): -961.1768\n",
      "convergence dfGPdfNN Run 7/10, Epoch 961/1000, Training Loss (NLML): -961.1760\n",
      "convergence dfGPdfNN Run 7/10, Epoch 962/1000, Training Loss (NLML): -961.1804\n",
      "convergence dfGPdfNN Run 7/10, Epoch 963/1000, Training Loss (NLML): -961.1804\n",
      "convergence dfGPdfNN Run 7/10, Epoch 964/1000, Training Loss (NLML): -961.1820\n",
      "convergence dfGPdfNN Run 7/10, Epoch 965/1000, Training Loss (NLML): -961.1857\n",
      "convergence dfGPdfNN Run 7/10, Epoch 966/1000, Training Loss (NLML): -961.2008\n",
      "convergence dfGPdfNN Run 7/10, Epoch 967/1000, Training Loss (NLML): -961.1865\n",
      "convergence dfGPdfNN Run 7/10, Epoch 968/1000, Training Loss (NLML): -961.1859\n",
      "convergence dfGPdfNN Run 7/10, Epoch 969/1000, Training Loss (NLML): -961.1866\n",
      "convergence dfGPdfNN Run 7/10, Epoch 970/1000, Training Loss (NLML): -961.1913\n",
      "convergence dfGPdfNN Run 7/10, Epoch 971/1000, Training Loss (NLML): -961.1915\n",
      "convergence dfGPdfNN Run 7/10, Epoch 972/1000, Training Loss (NLML): -961.1936\n",
      "convergence dfGPdfNN Run 7/10, Epoch 973/1000, Training Loss (NLML): -961.1936\n",
      "convergence dfGPdfNN Run 7/10, Epoch 974/1000, Training Loss (NLML): -961.1981\n",
      "convergence dfGPdfNN Run 7/10, Epoch 975/1000, Training Loss (NLML): -961.1969\n",
      "convergence dfGPdfNN Run 7/10, Epoch 976/1000, Training Loss (NLML): -961.2030\n",
      "convergence dfGPdfNN Run 7/10, Epoch 977/1000, Training Loss (NLML): -961.2114\n",
      "convergence dfGPdfNN Run 7/10, Epoch 978/1000, Training Loss (NLML): -961.2040\n",
      "convergence dfGPdfNN Run 7/10, Epoch 979/1000, Training Loss (NLML): -961.2045\n",
      "convergence dfGPdfNN Run 7/10, Epoch 980/1000, Training Loss (NLML): -961.2144\n",
      "convergence dfGPdfNN Run 7/10, Epoch 981/1000, Training Loss (NLML): -961.2152\n",
      "convergence dfGPdfNN Run 7/10, Epoch 982/1000, Training Loss (NLML): -961.2086\n",
      "convergence dfGPdfNN Run 7/10, Epoch 983/1000, Training Loss (NLML): -961.2096\n",
      "convergence dfGPdfNN Run 7/10, Epoch 984/1000, Training Loss (NLML): -961.2202\n",
      "convergence dfGPdfNN Run 7/10, Epoch 985/1000, Training Loss (NLML): -961.2220\n",
      "convergence dfGPdfNN Run 7/10, Epoch 986/1000, Training Loss (NLML): -961.2152\n",
      "convergence dfGPdfNN Run 7/10, Epoch 987/1000, Training Loss (NLML): -961.2162\n",
      "convergence dfGPdfNN Run 7/10, Epoch 988/1000, Training Loss (NLML): -961.2184\n",
      "convergence dfGPdfNN Run 7/10, Epoch 989/1000, Training Loss (NLML): -961.2213\n",
      "convergence dfGPdfNN Run 7/10, Epoch 990/1000, Training Loss (NLML): -961.2217\n",
      "convergence dfGPdfNN Run 7/10, Epoch 991/1000, Training Loss (NLML): -961.2239\n",
      "convergence dfGPdfNN Run 7/10, Epoch 992/1000, Training Loss (NLML): -961.2242\n",
      "convergence dfGPdfNN Run 7/10, Epoch 993/1000, Training Loss (NLML): -961.2250\n",
      "convergence dfGPdfNN Run 7/10, Epoch 994/1000, Training Loss (NLML): -961.2262\n",
      "convergence dfGPdfNN Run 7/10, Epoch 995/1000, Training Loss (NLML): -961.2291\n",
      "convergence dfGPdfNN Run 7/10, Epoch 996/1000, Training Loss (NLML): -961.2390\n",
      "convergence dfGPdfNN Run 7/10, Epoch 997/1000, Training Loss (NLML): -961.2397\n",
      "convergence dfGPdfNN Run 7/10, Epoch 998/1000, Training Loss (NLML): -961.2400\n",
      "convergence dfGPdfNN Run 7/10, Epoch 999/1000, Training Loss (NLML): -961.2427\n",
      "convergence dfGPdfNN Run 7/10, Epoch 1000/1000, Training Loss (NLML): -961.2448\n",
      "\n",
      "--- Training Run 8/10 ---\n",
      "\n",
      "Start Training\n",
      "convergence dfGPdfNN Run 8/10, Epoch 1/1000, Training Loss (NLML): -902.2197\n",
      "convergence dfGPdfNN Run 8/10, Epoch 2/1000, Training Loss (NLML): -903.8193\n",
      "convergence dfGPdfNN Run 8/10, Epoch 3/1000, Training Loss (NLML): -909.5345\n",
      "convergence dfGPdfNN Run 8/10, Epoch 4/1000, Training Loss (NLML): -914.0803\n",
      "convergence dfGPdfNN Run 8/10, Epoch 5/1000, Training Loss (NLML): -917.7003\n",
      "convergence dfGPdfNN Run 8/10, Epoch 6/1000, Training Loss (NLML): -919.3894\n",
      "convergence dfGPdfNN Run 8/10, Epoch 7/1000, Training Loss (NLML): -920.6093\n",
      "convergence dfGPdfNN Run 8/10, Epoch 8/1000, Training Loss (NLML): -922.4015\n",
      "convergence dfGPdfNN Run 8/10, Epoch 9/1000, Training Loss (NLML): -923.6508\n",
      "convergence dfGPdfNN Run 8/10, Epoch 10/1000, Training Loss (NLML): -924.1084\n",
      "convergence dfGPdfNN Run 8/10, Epoch 11/1000, Training Loss (NLML): -925.1301\n",
      "convergence dfGPdfNN Run 8/10, Epoch 12/1000, Training Loss (NLML): -926.0027\n",
      "convergence dfGPdfNN Run 8/10, Epoch 13/1000, Training Loss (NLML): -927.1559\n",
      "convergence dfGPdfNN Run 8/10, Epoch 14/1000, Training Loss (NLML): -928.2465\n",
      "convergence dfGPdfNN Run 8/10, Epoch 15/1000, Training Loss (NLML): -929.2739\n",
      "convergence dfGPdfNN Run 8/10, Epoch 16/1000, Training Loss (NLML): -930.2252\n",
      "convergence dfGPdfNN Run 8/10, Epoch 17/1000, Training Loss (NLML): -931.0984\n",
      "convergence dfGPdfNN Run 8/10, Epoch 18/1000, Training Loss (NLML): -931.9172\n",
      "convergence dfGPdfNN Run 8/10, Epoch 19/1000, Training Loss (NLML): -932.6633\n",
      "convergence dfGPdfNN Run 8/10, Epoch 20/1000, Training Loss (NLML): -933.3950\n",
      "convergence dfGPdfNN Run 8/10, Epoch 21/1000, Training Loss (NLML): -934.0946\n",
      "convergence dfGPdfNN Run 8/10, Epoch 22/1000, Training Loss (NLML): -934.6799\n",
      "convergence dfGPdfNN Run 8/10, Epoch 23/1000, Training Loss (NLML): -935.2349\n",
      "convergence dfGPdfNN Run 8/10, Epoch 24/1000, Training Loss (NLML): -936.0455\n",
      "convergence dfGPdfNN Run 8/10, Epoch 25/1000, Training Loss (NLML): -936.4840\n",
      "convergence dfGPdfNN Run 8/10, Epoch 26/1000, Training Loss (NLML): -936.9470\n",
      "convergence dfGPdfNN Run 8/10, Epoch 27/1000, Training Loss (NLML): -937.5073\n",
      "convergence dfGPdfNN Run 8/10, Epoch 28/1000, Training Loss (NLML): -938.1357\n",
      "convergence dfGPdfNN Run 8/10, Epoch 29/1000, Training Loss (NLML): -938.2432\n",
      "convergence dfGPdfNN Run 8/10, Epoch 30/1000, Training Loss (NLML): -939.0446\n",
      "convergence dfGPdfNN Run 8/10, Epoch 31/1000, Training Loss (NLML): -939.7673\n",
      "convergence dfGPdfNN Run 8/10, Epoch 32/1000, Training Loss (NLML): -940.2031\n",
      "convergence dfGPdfNN Run 8/10, Epoch 33/1000, Training Loss (NLML): -940.5603\n",
      "convergence dfGPdfNN Run 8/10, Epoch 34/1000, Training Loss (NLML): -941.0662\n",
      "convergence dfGPdfNN Run 8/10, Epoch 35/1000, Training Loss (NLML): -941.4744\n",
      "convergence dfGPdfNN Run 8/10, Epoch 36/1000, Training Loss (NLML): -941.9115\n",
      "convergence dfGPdfNN Run 8/10, Epoch 37/1000, Training Loss (NLML): -942.3325\n",
      "convergence dfGPdfNN Run 8/10, Epoch 38/1000, Training Loss (NLML): -942.7668\n",
      "convergence dfGPdfNN Run 8/10, Epoch 39/1000, Training Loss (NLML): -943.1788\n",
      "convergence dfGPdfNN Run 8/10, Epoch 40/1000, Training Loss (NLML): -943.5658\n",
      "convergence dfGPdfNN Run 8/10, Epoch 41/1000, Training Loss (NLML): -943.9316\n",
      "convergence dfGPdfNN Run 8/10, Epoch 42/1000, Training Loss (NLML): -944.2812\n",
      "convergence dfGPdfNN Run 8/10, Epoch 43/1000, Training Loss (NLML): -944.6316\n",
      "convergence dfGPdfNN Run 8/10, Epoch 44/1000, Training Loss (NLML): -944.4368\n",
      "convergence dfGPdfNN Run 8/10, Epoch 45/1000, Training Loss (NLML): -945.3156\n",
      "convergence dfGPdfNN Run 8/10, Epoch 46/1000, Training Loss (NLML): -945.6615\n",
      "convergence dfGPdfNN Run 8/10, Epoch 47/1000, Training Loss (NLML): -946.0148\n",
      "convergence dfGPdfNN Run 8/10, Epoch 48/1000, Training Loss (NLML): -946.3334\n",
      "convergence dfGPdfNN Run 8/10, Epoch 49/1000, Training Loss (NLML): -946.5898\n",
      "convergence dfGPdfNN Run 8/10, Epoch 50/1000, Training Loss (NLML): -946.8759\n",
      "convergence dfGPdfNN Run 8/10, Epoch 51/1000, Training Loss (NLML): -947.2178\n",
      "convergence dfGPdfNN Run 8/10, Epoch 52/1000, Training Loss (NLML): -947.5363\n",
      "convergence dfGPdfNN Run 8/10, Epoch 53/1000, Training Loss (NLML): -945.5184\n",
      "convergence dfGPdfNN Run 8/10, Epoch 54/1000, Training Loss (NLML): -940.5493\n",
      "convergence dfGPdfNN Run 8/10, Epoch 55/1000, Training Loss (NLML): -941.9308\n",
      "convergence dfGPdfNN Run 8/10, Epoch 56/1000, Training Loss (NLML): -948.4169\n",
      "convergence dfGPdfNN Run 8/10, Epoch 57/1000, Training Loss (NLML): -948.5485\n",
      "convergence dfGPdfNN Run 8/10, Epoch 58/1000, Training Loss (NLML): -948.6898\n",
      "convergence dfGPdfNN Run 8/10, Epoch 59/1000, Training Loss (NLML): -948.8591\n",
      "convergence dfGPdfNN Run 8/10, Epoch 60/1000, Training Loss (NLML): -949.0511\n",
      "convergence dfGPdfNN Run 8/10, Epoch 61/1000, Training Loss (NLML): -949.2079\n",
      "convergence dfGPdfNN Run 8/10, Epoch 62/1000, Training Loss (NLML): -949.4261\n",
      "convergence dfGPdfNN Run 8/10, Epoch 63/1000, Training Loss (NLML): -949.5945\n",
      "convergence dfGPdfNN Run 8/10, Epoch 64/1000, Training Loss (NLML): -949.7371\n",
      "convergence dfGPdfNN Run 8/10, Epoch 65/1000, Training Loss (NLML): -949.8999\n",
      "convergence dfGPdfNN Run 8/10, Epoch 66/1000, Training Loss (NLML): -950.0948\n",
      "convergence dfGPdfNN Run 8/10, Epoch 67/1000, Training Loss (NLML): -950.2704\n",
      "convergence dfGPdfNN Run 8/10, Epoch 68/1000, Training Loss (NLML): -950.4810\n",
      "convergence dfGPdfNN Run 8/10, Epoch 69/1000, Training Loss (NLML): -950.6655\n",
      "convergence dfGPdfNN Run 8/10, Epoch 70/1000, Training Loss (NLML): -950.8312\n",
      "convergence dfGPdfNN Run 8/10, Epoch 71/1000, Training Loss (NLML): -950.1340\n",
      "convergence dfGPdfNN Run 8/10, Epoch 72/1000, Training Loss (NLML): -951.1251\n",
      "convergence dfGPdfNN Run 8/10, Epoch 73/1000, Training Loss (NLML): -938.8378\n",
      "convergence dfGPdfNN Run 8/10, Epoch 74/1000, Training Loss (NLML): -943.6316\n",
      "convergence dfGPdfNN Run 8/10, Epoch 75/1000, Training Loss (NLML): -951.4381\n",
      "convergence dfGPdfNN Run 8/10, Epoch 76/1000, Training Loss (NLML): -951.3998\n",
      "convergence dfGPdfNN Run 8/10, Epoch 77/1000, Training Loss (NLML): -951.4135\n",
      "convergence dfGPdfNN Run 8/10, Epoch 78/1000, Training Loss (NLML): -951.5570\n",
      "convergence dfGPdfNN Run 8/10, Epoch 79/1000, Training Loss (NLML): -950.6741\n",
      "convergence dfGPdfNN Run 8/10, Epoch 80/1000, Training Loss (NLML): -951.5641\n",
      "convergence dfGPdfNN Run 8/10, Epoch 81/1000, Training Loss (NLML): -951.7295\n",
      "convergence dfGPdfNN Run 8/10, Epoch 82/1000, Training Loss (NLML): -951.7491\n",
      "convergence dfGPdfNN Run 8/10, Epoch 83/1000, Training Loss (NLML): -952.0919\n",
      "convergence dfGPdfNN Run 8/10, Epoch 84/1000, Training Loss (NLML): -952.2244\n",
      "convergence dfGPdfNN Run 8/10, Epoch 85/1000, Training Loss (NLML): -952.3434\n",
      "convergence dfGPdfNN Run 8/10, Epoch 86/1000, Training Loss (NLML): -952.4675\n",
      "convergence dfGPdfNN Run 8/10, Epoch 87/1000, Training Loss (NLML): -952.5364\n",
      "convergence dfGPdfNN Run 8/10, Epoch 88/1000, Training Loss (NLML): -952.5471\n",
      "convergence dfGPdfNN Run 8/10, Epoch 89/1000, Training Loss (NLML): -952.5094\n",
      "convergence dfGPdfNN Run 8/10, Epoch 90/1000, Training Loss (NLML): -952.7803\n",
      "convergence dfGPdfNN Run 8/10, Epoch 91/1000, Training Loss (NLML): -952.9548\n",
      "convergence dfGPdfNN Run 8/10, Epoch 92/1000, Training Loss (NLML): -953.1383\n",
      "convergence dfGPdfNN Run 8/10, Epoch 93/1000, Training Loss (NLML): -953.2001\n",
      "convergence dfGPdfNN Run 8/10, Epoch 94/1000, Training Loss (NLML): -953.2285\n",
      "convergence dfGPdfNN Run 8/10, Epoch 95/1000, Training Loss (NLML): -953.1488\n",
      "convergence dfGPdfNN Run 8/10, Epoch 96/1000, Training Loss (NLML): -952.8231\n",
      "convergence dfGPdfNN Run 8/10, Epoch 97/1000, Training Loss (NLML): -953.6246\n",
      "convergence dfGPdfNN Run 8/10, Epoch 98/1000, Training Loss (NLML): -953.0575\n",
      "convergence dfGPdfNN Run 8/10, Epoch 99/1000, Training Loss (NLML): -953.8872\n",
      "convergence dfGPdfNN Run 8/10, Epoch 100/1000, Training Loss (NLML): -954.0724\n",
      "convergence dfGPdfNN Run 8/10, Epoch 101/1000, Training Loss (NLML): -954.0610\n",
      "convergence dfGPdfNN Run 8/10, Epoch 102/1000, Training Loss (NLML): -954.2435\n",
      "convergence dfGPdfNN Run 8/10, Epoch 103/1000, Training Loss (NLML): -954.5077\n",
      "convergence dfGPdfNN Run 8/10, Epoch 104/1000, Training Loss (NLML): -954.9520\n",
      "convergence dfGPdfNN Run 8/10, Epoch 105/1000, Training Loss (NLML): -949.5989\n",
      "convergence dfGPdfNN Run 8/10, Epoch 106/1000, Training Loss (NLML): -955.2684\n",
      "convergence dfGPdfNN Run 8/10, Epoch 107/1000, Training Loss (NLML): -955.2480\n",
      "convergence dfGPdfNN Run 8/10, Epoch 108/1000, Training Loss (NLML): -955.1771\n",
      "convergence dfGPdfNN Run 8/10, Epoch 109/1000, Training Loss (NLML): -955.0592\n",
      "convergence dfGPdfNN Run 8/10, Epoch 110/1000, Training Loss (NLML): -955.2096\n",
      "convergence dfGPdfNN Run 8/10, Epoch 111/1000, Training Loss (NLML): -955.2034\n",
      "convergence dfGPdfNN Run 8/10, Epoch 112/1000, Training Loss (NLML): -955.5685\n",
      "convergence dfGPdfNN Run 8/10, Epoch 113/1000, Training Loss (NLML): -955.5801\n",
      "convergence dfGPdfNN Run 8/10, Epoch 114/1000, Training Loss (NLML): -955.6573\n",
      "convergence dfGPdfNN Run 8/10, Epoch 115/1000, Training Loss (NLML): -955.7333\n",
      "convergence dfGPdfNN Run 8/10, Epoch 116/1000, Training Loss (NLML): -955.7644\n",
      "convergence dfGPdfNN Run 8/10, Epoch 117/1000, Training Loss (NLML): -955.8271\n",
      "convergence dfGPdfNN Run 8/10, Epoch 118/1000, Training Loss (NLML): -955.7274\n",
      "convergence dfGPdfNN Run 8/10, Epoch 119/1000, Training Loss (NLML): -955.8276\n",
      "convergence dfGPdfNN Run 8/10, Epoch 120/1000, Training Loss (NLML): -956.0503\n",
      "convergence dfGPdfNN Run 8/10, Epoch 121/1000, Training Loss (NLML): -956.1450\n",
      "convergence dfGPdfNN Run 8/10, Epoch 122/1000, Training Loss (NLML): -956.1990\n",
      "convergence dfGPdfNN Run 8/10, Epoch 123/1000, Training Loss (NLML): -956.1592\n",
      "convergence dfGPdfNN Run 8/10, Epoch 124/1000, Training Loss (NLML): -955.9877\n",
      "convergence dfGPdfNN Run 8/10, Epoch 125/1000, Training Loss (NLML): -956.0072\n",
      "convergence dfGPdfNN Run 8/10, Epoch 126/1000, Training Loss (NLML): -955.9717\n",
      "convergence dfGPdfNN Run 8/10, Epoch 127/1000, Training Loss (NLML): -956.0923\n",
      "convergence dfGPdfNN Run 8/10, Epoch 128/1000, Training Loss (NLML): -956.1921\n",
      "convergence dfGPdfNN Run 8/10, Epoch 129/1000, Training Loss (NLML): -956.2683\n",
      "convergence dfGPdfNN Run 8/10, Epoch 130/1000, Training Loss (NLML): -956.2977\n",
      "convergence dfGPdfNN Run 8/10, Epoch 131/1000, Training Loss (NLML): -956.2639\n",
      "convergence dfGPdfNN Run 8/10, Epoch 132/1000, Training Loss (NLML): -956.0757\n",
      "convergence dfGPdfNN Run 8/10, Epoch 133/1000, Training Loss (NLML): -956.0896\n",
      "convergence dfGPdfNN Run 8/10, Epoch 134/1000, Training Loss (NLML): -956.3475\n",
      "convergence dfGPdfNN Run 8/10, Epoch 135/1000, Training Loss (NLML): -956.4517\n",
      "convergence dfGPdfNN Run 8/10, Epoch 136/1000, Training Loss (NLML): -956.4858\n",
      "convergence dfGPdfNN Run 8/10, Epoch 137/1000, Training Loss (NLML): -956.4277\n",
      "convergence dfGPdfNN Run 8/10, Epoch 138/1000, Training Loss (NLML): -956.4139\n",
      "convergence dfGPdfNN Run 8/10, Epoch 139/1000, Training Loss (NLML): -956.3303\n",
      "convergence dfGPdfNN Run 8/10, Epoch 140/1000, Training Loss (NLML): -956.3835\n",
      "convergence dfGPdfNN Run 8/10, Epoch 141/1000, Training Loss (NLML): -956.5065\n",
      "convergence dfGPdfNN Run 8/10, Epoch 142/1000, Training Loss (NLML): -956.5275\n",
      "convergence dfGPdfNN Run 8/10, Epoch 143/1000, Training Loss (NLML): -956.5483\n",
      "convergence dfGPdfNN Run 8/10, Epoch 144/1000, Training Loss (NLML): -956.5662\n",
      "convergence dfGPdfNN Run 8/10, Epoch 145/1000, Training Loss (NLML): -956.4956\n",
      "convergence dfGPdfNN Run 8/10, Epoch 146/1000, Training Loss (NLML): -956.5021\n",
      "convergence dfGPdfNN Run 8/10, Epoch 147/1000, Training Loss (NLML): -956.5680\n",
      "convergence dfGPdfNN Run 8/10, Epoch 148/1000, Training Loss (NLML): -956.6761\n",
      "convergence dfGPdfNN Run 8/10, Epoch 149/1000, Training Loss (NLML): -956.7050\n",
      "convergence dfGPdfNN Run 8/10, Epoch 150/1000, Training Loss (NLML): -956.7183\n",
      "convergence dfGPdfNN Run 8/10, Epoch 151/1000, Training Loss (NLML): -956.7057\n",
      "convergence dfGPdfNN Run 8/10, Epoch 152/1000, Training Loss (NLML): -956.6270\n",
      "convergence dfGPdfNN Run 8/10, Epoch 153/1000, Training Loss (NLML): -956.7056\n",
      "convergence dfGPdfNN Run 8/10, Epoch 154/1000, Training Loss (NLML): -956.7478\n",
      "convergence dfGPdfNN Run 8/10, Epoch 155/1000, Training Loss (NLML): -956.8605\n",
      "convergence dfGPdfNN Run 8/10, Epoch 156/1000, Training Loss (NLML): -956.8745\n",
      "convergence dfGPdfNN Run 8/10, Epoch 157/1000, Training Loss (NLML): -956.9724\n",
      "convergence dfGPdfNN Run 8/10, Epoch 158/1000, Training Loss (NLML): -956.9279\n",
      "convergence dfGPdfNN Run 8/10, Epoch 159/1000, Training Loss (NLML): -956.8578\n",
      "convergence dfGPdfNN Run 8/10, Epoch 160/1000, Training Loss (NLML): -956.8698\n",
      "convergence dfGPdfNN Run 8/10, Epoch 161/1000, Training Loss (NLML): -956.8678\n",
      "convergence dfGPdfNN Run 8/10, Epoch 162/1000, Training Loss (NLML): -956.9874\n",
      "convergence dfGPdfNN Run 8/10, Epoch 163/1000, Training Loss (NLML): -957.0828\n",
      "convergence dfGPdfNN Run 8/10, Epoch 164/1000, Training Loss (NLML): -957.1619\n",
      "convergence dfGPdfNN Run 8/10, Epoch 165/1000, Training Loss (NLML): -957.1412\n",
      "convergence dfGPdfNN Run 8/10, Epoch 166/1000, Training Loss (NLML): -956.9700\n",
      "convergence dfGPdfNN Run 8/10, Epoch 167/1000, Training Loss (NLML): -956.9395\n",
      "convergence dfGPdfNN Run 8/10, Epoch 168/1000, Training Loss (NLML): -957.0570\n",
      "convergence dfGPdfNN Run 8/10, Epoch 169/1000, Training Loss (NLML): -957.1670\n",
      "convergence dfGPdfNN Run 8/10, Epoch 170/1000, Training Loss (NLML): -957.2421\n",
      "convergence dfGPdfNN Run 8/10, Epoch 171/1000, Training Loss (NLML): -957.2759\n",
      "convergence dfGPdfNN Run 8/10, Epoch 172/1000, Training Loss (NLML): -957.2175\n",
      "convergence dfGPdfNN Run 8/10, Epoch 173/1000, Training Loss (NLML): -957.1238\n",
      "convergence dfGPdfNN Run 8/10, Epoch 174/1000, Training Loss (NLML): -957.1409\n",
      "convergence dfGPdfNN Run 8/10, Epoch 175/1000, Training Loss (NLML): -957.1687\n",
      "convergence dfGPdfNN Run 8/10, Epoch 176/1000, Training Loss (NLML): -957.1552\n",
      "convergence dfGPdfNN Run 8/10, Epoch 177/1000, Training Loss (NLML): -957.2854\n",
      "convergence dfGPdfNN Run 8/10, Epoch 178/1000, Training Loss (NLML): -957.3140\n",
      "convergence dfGPdfNN Run 8/10, Epoch 179/1000, Training Loss (NLML): -957.3263\n",
      "convergence dfGPdfNN Run 8/10, Epoch 180/1000, Training Loss (NLML): -957.2891\n",
      "convergence dfGPdfNN Run 8/10, Epoch 181/1000, Training Loss (NLML): -957.2994\n",
      "convergence dfGPdfNN Run 8/10, Epoch 182/1000, Training Loss (NLML): -957.3175\n",
      "convergence dfGPdfNN Run 8/10, Epoch 183/1000, Training Loss (NLML): -957.2756\n",
      "convergence dfGPdfNN Run 8/10, Epoch 184/1000, Training Loss (NLML): -957.3977\n",
      "convergence dfGPdfNN Run 8/10, Epoch 185/1000, Training Loss (NLML): -957.4716\n",
      "convergence dfGPdfNN Run 8/10, Epoch 186/1000, Training Loss (NLML): -957.4327\n",
      "convergence dfGPdfNN Run 8/10, Epoch 187/1000, Training Loss (NLML): -957.3364\n",
      "convergence dfGPdfNN Run 8/10, Epoch 188/1000, Training Loss (NLML): -957.2671\n",
      "convergence dfGPdfNN Run 8/10, Epoch 189/1000, Training Loss (NLML): -957.3092\n",
      "convergence dfGPdfNN Run 8/10, Epoch 190/1000, Training Loss (NLML): -957.4106\n",
      "convergence dfGPdfNN Run 8/10, Epoch 191/1000, Training Loss (NLML): -957.4161\n",
      "convergence dfGPdfNN Run 8/10, Epoch 192/1000, Training Loss (NLML): -957.4260\n",
      "convergence dfGPdfNN Run 8/10, Epoch 193/1000, Training Loss (NLML): -957.4122\n",
      "convergence dfGPdfNN Run 8/10, Epoch 194/1000, Training Loss (NLML): -957.4114\n",
      "convergence dfGPdfNN Run 8/10, Epoch 195/1000, Training Loss (NLML): -957.4327\n",
      "convergence dfGPdfNN Run 8/10, Epoch 196/1000, Training Loss (NLML): -957.4637\n",
      "convergence dfGPdfNN Run 8/10, Epoch 197/1000, Training Loss (NLML): -957.5696\n",
      "convergence dfGPdfNN Run 8/10, Epoch 198/1000, Training Loss (NLML): -957.6763\n",
      "convergence dfGPdfNN Run 8/10, Epoch 199/1000, Training Loss (NLML): -957.7515\n",
      "convergence dfGPdfNN Run 8/10, Epoch 200/1000, Training Loss (NLML): -957.7198\n",
      "convergence dfGPdfNN Run 8/10, Epoch 201/1000, Training Loss (NLML): -957.6085\n",
      "convergence dfGPdfNN Run 8/10, Epoch 202/1000, Training Loss (NLML): -957.6200\n",
      "convergence dfGPdfNN Run 8/10, Epoch 203/1000, Training Loss (NLML): -957.6497\n",
      "convergence dfGPdfNN Run 8/10, Epoch 204/1000, Training Loss (NLML): -957.6765\n",
      "convergence dfGPdfNN Run 8/10, Epoch 205/1000, Training Loss (NLML): -957.6849\n",
      "convergence dfGPdfNN Run 8/10, Epoch 206/1000, Training Loss (NLML): -957.7507\n",
      "convergence dfGPdfNN Run 8/10, Epoch 207/1000, Training Loss (NLML): -957.7068\n",
      "convergence dfGPdfNN Run 8/10, Epoch 208/1000, Training Loss (NLML): -957.7515\n",
      "convergence dfGPdfNN Run 8/10, Epoch 209/1000, Training Loss (NLML): -957.7109\n",
      "convergence dfGPdfNN Run 8/10, Epoch 210/1000, Training Loss (NLML): -957.7244\n",
      "convergence dfGPdfNN Run 8/10, Epoch 211/1000, Training Loss (NLML): -957.8038\n",
      "convergence dfGPdfNN Run 8/10, Epoch 212/1000, Training Loss (NLML): -957.9415\n",
      "convergence dfGPdfNN Run 8/10, Epoch 213/1000, Training Loss (NLML): -957.9622\n",
      "convergence dfGPdfNN Run 8/10, Epoch 214/1000, Training Loss (NLML): -957.9862\n",
      "convergence dfGPdfNN Run 8/10, Epoch 215/1000, Training Loss (NLML): -957.9418\n",
      "convergence dfGPdfNN Run 8/10, Epoch 216/1000, Training Loss (NLML): -957.8511\n",
      "convergence dfGPdfNN Run 8/10, Epoch 217/1000, Training Loss (NLML): -957.8784\n",
      "convergence dfGPdfNN Run 8/10, Epoch 218/1000, Training Loss (NLML): -958.0596\n",
      "convergence dfGPdfNN Run 8/10, Epoch 219/1000, Training Loss (NLML): -958.0652\n",
      "convergence dfGPdfNN Run 8/10, Epoch 220/1000, Training Loss (NLML): -958.0742\n",
      "convergence dfGPdfNN Run 8/10, Epoch 221/1000, Training Loss (NLML): -958.0474\n",
      "convergence dfGPdfNN Run 8/10, Epoch 222/1000, Training Loss (NLML): -957.9720\n",
      "convergence dfGPdfNN Run 8/10, Epoch 223/1000, Training Loss (NLML): -957.9868\n",
      "convergence dfGPdfNN Run 8/10, Epoch 224/1000, Training Loss (NLML): -958.0031\n",
      "convergence dfGPdfNN Run 8/10, Epoch 225/1000, Training Loss (NLML): -958.1030\n",
      "convergence dfGPdfNN Run 8/10, Epoch 226/1000, Training Loss (NLML): -958.1202\n",
      "convergence dfGPdfNN Run 8/10, Epoch 227/1000, Training Loss (NLML): -958.1445\n",
      "convergence dfGPdfNN Run 8/10, Epoch 228/1000, Training Loss (NLML): -958.1442\n",
      "convergence dfGPdfNN Run 8/10, Epoch 229/1000, Training Loss (NLML): -958.1382\n",
      "convergence dfGPdfNN Run 8/10, Epoch 230/1000, Training Loss (NLML): -958.1506\n",
      "convergence dfGPdfNN Run 8/10, Epoch 231/1000, Training Loss (NLML): -958.1880\n",
      "convergence dfGPdfNN Run 8/10, Epoch 232/1000, Training Loss (NLML): -958.1915\n",
      "convergence dfGPdfNN Run 8/10, Epoch 233/1000, Training Loss (NLML): -958.1952\n",
      "convergence dfGPdfNN Run 8/10, Epoch 234/1000, Training Loss (NLML): -958.2163\n",
      "convergence dfGPdfNN Run 8/10, Epoch 235/1000, Training Loss (NLML): -958.2212\n",
      "convergence dfGPdfNN Run 8/10, Epoch 236/1000, Training Loss (NLML): -958.2329\n",
      "convergence dfGPdfNN Run 8/10, Epoch 237/1000, Training Loss (NLML): -958.2428\n",
      "convergence dfGPdfNN Run 8/10, Epoch 238/1000, Training Loss (NLML): -958.2623\n",
      "convergence dfGPdfNN Run 8/10, Epoch 239/1000, Training Loss (NLML): -958.2896\n",
      "convergence dfGPdfNN Run 8/10, Epoch 240/1000, Training Loss (NLML): -958.3041\n",
      "convergence dfGPdfNN Run 8/10, Epoch 241/1000, Training Loss (NLML): -958.3282\n",
      "convergence dfGPdfNN Run 8/10, Epoch 242/1000, Training Loss (NLML): -958.3479\n",
      "convergence dfGPdfNN Run 8/10, Epoch 243/1000, Training Loss (NLML): -958.3679\n",
      "convergence dfGPdfNN Run 8/10, Epoch 244/1000, Training Loss (NLML): -958.3801\n",
      "convergence dfGPdfNN Run 8/10, Epoch 245/1000, Training Loss (NLML): -958.3954\n",
      "convergence dfGPdfNN Run 8/10, Epoch 246/1000, Training Loss (NLML): -958.3464\n",
      "convergence dfGPdfNN Run 8/10, Epoch 247/1000, Training Loss (NLML): -958.3525\n",
      "convergence dfGPdfNN Run 8/10, Epoch 248/1000, Training Loss (NLML): -958.4227\n",
      "convergence dfGPdfNN Run 8/10, Epoch 249/1000, Training Loss (NLML): -958.4321\n",
      "convergence dfGPdfNN Run 8/10, Epoch 250/1000, Training Loss (NLML): -958.4019\n",
      "convergence dfGPdfNN Run 8/10, Epoch 251/1000, Training Loss (NLML): -958.3900\n",
      "convergence dfGPdfNN Run 8/10, Epoch 252/1000, Training Loss (NLML): -958.4677\n",
      "convergence dfGPdfNN Run 8/10, Epoch 253/1000, Training Loss (NLML): -958.4368\n",
      "convergence dfGPdfNN Run 8/10, Epoch 254/1000, Training Loss (NLML): -958.4927\n",
      "convergence dfGPdfNN Run 8/10, Epoch 255/1000, Training Loss (NLML): -958.4294\n",
      "convergence dfGPdfNN Run 8/10, Epoch 256/1000, Training Loss (NLML): -958.4812\n",
      "convergence dfGPdfNN Run 8/10, Epoch 257/1000, Training Loss (NLML): -958.5452\n",
      "convergence dfGPdfNN Run 8/10, Epoch 258/1000, Training Loss (NLML): -958.5787\n",
      "convergence dfGPdfNN Run 8/10, Epoch 259/1000, Training Loss (NLML): -958.5975\n",
      "convergence dfGPdfNN Run 8/10, Epoch 260/1000, Training Loss (NLML): -958.5524\n",
      "convergence dfGPdfNN Run 8/10, Epoch 261/1000, Training Loss (NLML): -958.5704\n",
      "convergence dfGPdfNN Run 8/10, Epoch 262/1000, Training Loss (NLML): -958.5773\n",
      "convergence dfGPdfNN Run 8/10, Epoch 263/1000, Training Loss (NLML): -958.5826\n",
      "convergence dfGPdfNN Run 8/10, Epoch 264/1000, Training Loss (NLML): -958.5983\n",
      "convergence dfGPdfNN Run 8/10, Epoch 265/1000, Training Loss (NLML): -958.5538\n",
      "convergence dfGPdfNN Run 8/10, Epoch 266/1000, Training Loss (NLML): -958.5732\n",
      "convergence dfGPdfNN Run 8/10, Epoch 267/1000, Training Loss (NLML): -958.5833\n",
      "convergence dfGPdfNN Run 8/10, Epoch 268/1000, Training Loss (NLML): -958.6426\n",
      "convergence dfGPdfNN Run 8/10, Epoch 269/1000, Training Loss (NLML): -958.6531\n",
      "convergence dfGPdfNN Run 8/10, Epoch 270/1000, Training Loss (NLML): -958.7047\n",
      "convergence dfGPdfNN Run 8/10, Epoch 271/1000, Training Loss (NLML): -958.7180\n",
      "convergence dfGPdfNN Run 8/10, Epoch 272/1000, Training Loss (NLML): -958.6794\n",
      "convergence dfGPdfNN Run 8/10, Epoch 273/1000, Training Loss (NLML): -958.6925\n",
      "convergence dfGPdfNN Run 8/10, Epoch 274/1000, Training Loss (NLML): -958.7184\n",
      "convergence dfGPdfNN Run 8/10, Epoch 275/1000, Training Loss (NLML): -958.6490\n",
      "convergence dfGPdfNN Run 8/10, Epoch 276/1000, Training Loss (NLML): -958.7415\n",
      "convergence dfGPdfNN Run 8/10, Epoch 277/1000, Training Loss (NLML): -958.7474\n",
      "convergence dfGPdfNN Run 8/10, Epoch 278/1000, Training Loss (NLML): -958.6913\n",
      "convergence dfGPdfNN Run 8/10, Epoch 279/1000, Training Loss (NLML): -958.7498\n",
      "convergence dfGPdfNN Run 8/10, Epoch 280/1000, Training Loss (NLML): -958.7167\n",
      "convergence dfGPdfNN Run 8/10, Epoch 281/1000, Training Loss (NLML): -958.7268\n",
      "convergence dfGPdfNN Run 8/10, Epoch 282/1000, Training Loss (NLML): -958.8145\n",
      "convergence dfGPdfNN Run 8/10, Epoch 283/1000, Training Loss (NLML): -958.8123\n",
      "convergence dfGPdfNN Run 8/10, Epoch 284/1000, Training Loss (NLML): -958.8234\n",
      "convergence dfGPdfNN Run 8/10, Epoch 285/1000, Training Loss (NLML): -958.8319\n",
      "convergence dfGPdfNN Run 8/10, Epoch 286/1000, Training Loss (NLML): -958.8412\n",
      "convergence dfGPdfNN Run 8/10, Epoch 287/1000, Training Loss (NLML): -958.8495\n",
      "convergence dfGPdfNN Run 8/10, Epoch 288/1000, Training Loss (NLML): -958.8679\n",
      "convergence dfGPdfNN Run 8/10, Epoch 289/1000, Training Loss (NLML): -958.8790\n",
      "convergence dfGPdfNN Run 8/10, Epoch 290/1000, Training Loss (NLML): -958.9196\n",
      "convergence dfGPdfNN Run 8/10, Epoch 291/1000, Training Loss (NLML): -958.8892\n",
      "convergence dfGPdfNN Run 8/10, Epoch 292/1000, Training Loss (NLML): -958.8999\n",
      "convergence dfGPdfNN Run 8/10, Epoch 293/1000, Training Loss (NLML): -958.8434\n",
      "convergence dfGPdfNN Run 8/10, Epoch 294/1000, Training Loss (NLML): -958.8568\n",
      "convergence dfGPdfNN Run 8/10, Epoch 295/1000, Training Loss (NLML): -958.9401\n",
      "convergence dfGPdfNN Run 8/10, Epoch 296/1000, Training Loss (NLML): -958.9340\n",
      "convergence dfGPdfNN Run 8/10, Epoch 297/1000, Training Loss (NLML): -958.9427\n",
      "convergence dfGPdfNN Run 8/10, Epoch 298/1000, Training Loss (NLML): -958.9548\n",
      "convergence dfGPdfNN Run 8/10, Epoch 299/1000, Training Loss (NLML): -958.9181\n",
      "convergence dfGPdfNN Run 8/10, Epoch 300/1000, Training Loss (NLML): -958.9028\n",
      "convergence dfGPdfNN Run 8/10, Epoch 301/1000, Training Loss (NLML): -958.9836\n",
      "convergence dfGPdfNN Run 8/10, Epoch 302/1000, Training Loss (NLML): -958.9930\n",
      "convergence dfGPdfNN Run 8/10, Epoch 303/1000, Training Loss (NLML): -959.0372\n",
      "convergence dfGPdfNN Run 8/10, Epoch 304/1000, Training Loss (NLML): -959.0134\n",
      "convergence dfGPdfNN Run 8/10, Epoch 305/1000, Training Loss (NLML): -959.0214\n",
      "convergence dfGPdfNN Run 8/10, Epoch 306/1000, Training Loss (NLML): -959.0292\n",
      "convergence dfGPdfNN Run 8/10, Epoch 307/1000, Training Loss (NLML): -958.9674\n",
      "convergence dfGPdfNN Run 8/10, Epoch 308/1000, Training Loss (NLML): -959.0430\n",
      "convergence dfGPdfNN Run 8/10, Epoch 309/1000, Training Loss (NLML): -959.0490\n",
      "convergence dfGPdfNN Run 8/10, Epoch 310/1000, Training Loss (NLML): -959.0824\n",
      "convergence dfGPdfNN Run 8/10, Epoch 311/1000, Training Loss (NLML): -959.0914\n",
      "convergence dfGPdfNN Run 8/10, Epoch 312/1000, Training Loss (NLML): -959.0848\n",
      "convergence dfGPdfNN Run 8/10, Epoch 313/1000, Training Loss (NLML): -959.0936\n",
      "convergence dfGPdfNN Run 8/10, Epoch 314/1000, Training Loss (NLML): -959.0298\n",
      "convergence dfGPdfNN Run 8/10, Epoch 315/1000, Training Loss (NLML): -959.1130\n",
      "convergence dfGPdfNN Run 8/10, Epoch 316/1000, Training Loss (NLML): -959.1212\n",
      "convergence dfGPdfNN Run 8/10, Epoch 317/1000, Training Loss (NLML): -959.1289\n",
      "convergence dfGPdfNN Run 8/10, Epoch 318/1000, Training Loss (NLML): -959.1370\n",
      "convergence dfGPdfNN Run 8/10, Epoch 319/1000, Training Loss (NLML): -959.1807\n",
      "convergence dfGPdfNN Run 8/10, Epoch 320/1000, Training Loss (NLML): -959.1680\n",
      "convergence dfGPdfNN Run 8/10, Epoch 321/1000, Training Loss (NLML): -959.1766\n",
      "convergence dfGPdfNN Run 8/10, Epoch 322/1000, Training Loss (NLML): -959.1840\n",
      "convergence dfGPdfNN Run 8/10, Epoch 323/1000, Training Loss (NLML): -959.1919\n",
      "convergence dfGPdfNN Run 8/10, Epoch 324/1000, Training Loss (NLML): -959.2009\n",
      "convergence dfGPdfNN Run 8/10, Epoch 325/1000, Training Loss (NLML): -959.2463\n",
      "convergence dfGPdfNN Run 8/10, Epoch 326/1000, Training Loss (NLML): -959.2184\n",
      "convergence dfGPdfNN Run 8/10, Epoch 327/1000, Training Loss (NLML): -959.2258\n",
      "convergence dfGPdfNN Run 8/10, Epoch 328/1000, Training Loss (NLML): -959.2726\n",
      "convergence dfGPdfNN Run 8/10, Epoch 329/1000, Training Loss (NLML): -959.2406\n",
      "convergence dfGPdfNN Run 8/10, Epoch 330/1000, Training Loss (NLML): -959.2490\n",
      "convergence dfGPdfNN Run 8/10, Epoch 331/1000, Training Loss (NLML): -959.2578\n",
      "convergence dfGPdfNN Run 8/10, Epoch 332/1000, Training Loss (NLML): -959.2659\n",
      "convergence dfGPdfNN Run 8/10, Epoch 333/1000, Training Loss (NLML): -959.2744\n",
      "convergence dfGPdfNN Run 8/10, Epoch 334/1000, Training Loss (NLML): -959.3147\n",
      "convergence dfGPdfNN Run 8/10, Epoch 335/1000, Training Loss (NLML): -959.2894\n",
      "convergence dfGPdfNN Run 8/10, Epoch 336/1000, Training Loss (NLML): -959.2991\n",
      "convergence dfGPdfNN Run 8/10, Epoch 337/1000, Training Loss (NLML): -959.3058\n",
      "convergence dfGPdfNN Run 8/10, Epoch 338/1000, Training Loss (NLML): -959.3121\n",
      "convergence dfGPdfNN Run 8/10, Epoch 339/1000, Training Loss (NLML): -959.3235\n",
      "convergence dfGPdfNN Run 8/10, Epoch 340/1000, Training Loss (NLML): -959.3159\n",
      "convergence dfGPdfNN Run 8/10, Epoch 341/1000, Training Loss (NLML): -959.3252\n",
      "convergence dfGPdfNN Run 8/10, Epoch 342/1000, Training Loss (NLML): -959.3330\n",
      "convergence dfGPdfNN Run 8/10, Epoch 343/1000, Training Loss (NLML): -959.3379\n",
      "convergence dfGPdfNN Run 8/10, Epoch 344/1000, Training Loss (NLML): -959.3462\n",
      "convergence dfGPdfNN Run 8/10, Epoch 345/1000, Training Loss (NLML): -959.3558\n",
      "convergence dfGPdfNN Run 8/10, Epoch 346/1000, Training Loss (NLML): -959.3632\n",
      "convergence dfGPdfNN Run 8/10, Epoch 347/1000, Training Loss (NLML): -959.3065\n",
      "convergence dfGPdfNN Run 8/10, Epoch 348/1000, Training Loss (NLML): -959.3765\n",
      "convergence dfGPdfNN Run 8/10, Epoch 349/1000, Training Loss (NLML): -959.4026\n",
      "convergence dfGPdfNN Run 8/10, Epoch 350/1000, Training Loss (NLML): -959.4131\n",
      "convergence dfGPdfNN Run 8/10, Epoch 351/1000, Training Loss (NLML): -959.3998\n",
      "convergence dfGPdfNN Run 8/10, Epoch 352/1000, Training Loss (NLML): -959.4088\n",
      "convergence dfGPdfNN Run 8/10, Epoch 353/1000, Training Loss (NLML): -959.4154\n",
      "convergence dfGPdfNN Run 8/10, Epoch 354/1000, Training Loss (NLML): -959.4209\n",
      "convergence dfGPdfNN Run 8/10, Epoch 355/1000, Training Loss (NLML): -959.4298\n",
      "convergence dfGPdfNN Run 8/10, Epoch 356/1000, Training Loss (NLML): -959.4354\n",
      "convergence dfGPdfNN Run 8/10, Epoch 357/1000, Training Loss (NLML): -959.4436\n",
      "convergence dfGPdfNN Run 8/10, Epoch 358/1000, Training Loss (NLML): -959.4503\n",
      "convergence dfGPdfNN Run 8/10, Epoch 359/1000, Training Loss (NLML): -959.4711\n",
      "convergence dfGPdfNN Run 8/10, Epoch 360/1000, Training Loss (NLML): -959.4801\n",
      "convergence dfGPdfNN Run 8/10, Epoch 361/1000, Training Loss (NLML): -959.4858\n",
      "convergence dfGPdfNN Run 8/10, Epoch 362/1000, Training Loss (NLML): -959.4890\n",
      "convergence dfGPdfNN Run 8/10, Epoch 363/1000, Training Loss (NLML): -959.4987\n",
      "convergence dfGPdfNN Run 8/10, Epoch 364/1000, Training Loss (NLML): -959.5059\n",
      "convergence dfGPdfNN Run 8/10, Epoch 365/1000, Training Loss (NLML): -959.5117\n",
      "convergence dfGPdfNN Run 8/10, Epoch 366/1000, Training Loss (NLML): -959.5178\n",
      "convergence dfGPdfNN Run 8/10, Epoch 367/1000, Training Loss (NLML): -959.5596\n",
      "convergence dfGPdfNN Run 8/10, Epoch 368/1000, Training Loss (NLML): -959.5670\n",
      "convergence dfGPdfNN Run 8/10, Epoch 369/1000, Training Loss (NLML): -959.5382\n",
      "convergence dfGPdfNN Run 8/10, Epoch 370/1000, Training Loss (NLML): -959.5441\n",
      "convergence dfGPdfNN Run 8/10, Epoch 371/1000, Training Loss (NLML): -959.5510\n",
      "convergence dfGPdfNN Run 8/10, Epoch 372/1000, Training Loss (NLML): -959.5994\n",
      "convergence dfGPdfNN Run 8/10, Epoch 373/1000, Training Loss (NLML): -959.6035\n",
      "convergence dfGPdfNN Run 8/10, Epoch 374/1000, Training Loss (NLML): -959.5751\n",
      "convergence dfGPdfNN Run 8/10, Epoch 375/1000, Training Loss (NLML): -959.5808\n",
      "convergence dfGPdfNN Run 8/10, Epoch 376/1000, Training Loss (NLML): -959.5880\n",
      "convergence dfGPdfNN Run 8/10, Epoch 377/1000, Training Loss (NLML): -959.5815\n",
      "convergence dfGPdfNN Run 8/10, Epoch 378/1000, Training Loss (NLML): -959.6215\n",
      "convergence dfGPdfNN Run 8/10, Epoch 379/1000, Training Loss (NLML): -959.5939\n",
      "convergence dfGPdfNN Run 8/10, Epoch 380/1000, Training Loss (NLML): -959.6018\n",
      "convergence dfGPdfNN Run 8/10, Epoch 381/1000, Training Loss (NLML): -959.6171\n",
      "convergence dfGPdfNN Run 8/10, Epoch 382/1000, Training Loss (NLML): -959.6145\n",
      "convergence dfGPdfNN Run 8/10, Epoch 383/1000, Training Loss (NLML): -959.6528\n",
      "convergence dfGPdfNN Run 8/10, Epoch 384/1000, Training Loss (NLML): -959.6616\n",
      "convergence dfGPdfNN Run 8/10, Epoch 385/1000, Training Loss (NLML): -959.6310\n",
      "convergence dfGPdfNN Run 8/10, Epoch 386/1000, Training Loss (NLML): -959.6361\n",
      "convergence dfGPdfNN Run 8/10, Epoch 387/1000, Training Loss (NLML): -959.6429\n",
      "convergence dfGPdfNN Run 8/10, Epoch 388/1000, Training Loss (NLML): -959.6851\n",
      "convergence dfGPdfNN Run 8/10, Epoch 389/1000, Training Loss (NLML): -959.7247\n",
      "convergence dfGPdfNN Run 8/10, Epoch 390/1000, Training Loss (NLML): -959.6879\n",
      "convergence dfGPdfNN Run 8/10, Epoch 391/1000, Training Loss (NLML): -959.6953\n",
      "convergence dfGPdfNN Run 8/10, Epoch 392/1000, Training Loss (NLML): -959.6877\n",
      "convergence dfGPdfNN Run 8/10, Epoch 393/1000, Training Loss (NLML): -959.6948\n",
      "convergence dfGPdfNN Run 8/10, Epoch 394/1000, Training Loss (NLML): -959.7633\n",
      "convergence dfGPdfNN Run 8/10, Epoch 395/1000, Training Loss (NLML): -959.7705\n",
      "convergence dfGPdfNN Run 8/10, Epoch 396/1000, Training Loss (NLML): -959.7821\n",
      "convergence dfGPdfNN Run 8/10, Epoch 397/1000, Training Loss (NLML): -959.7911\n",
      "convergence dfGPdfNN Run 8/10, Epoch 398/1000, Training Loss (NLML): -959.7428\n",
      "convergence dfGPdfNN Run 8/10, Epoch 399/1000, Training Loss (NLML): -959.7478\n",
      "convergence dfGPdfNN Run 8/10, Epoch 400/1000, Training Loss (NLML): -959.8105\n",
      "convergence dfGPdfNN Run 8/10, Epoch 401/1000, Training Loss (NLML): -959.8158\n",
      "convergence dfGPdfNN Run 8/10, Epoch 402/1000, Training Loss (NLML): -959.8202\n",
      "convergence dfGPdfNN Run 8/10, Epoch 403/1000, Training Loss (NLML): -959.8239\n",
      "convergence dfGPdfNN Run 8/10, Epoch 404/1000, Training Loss (NLML): -959.8256\n",
      "convergence dfGPdfNN Run 8/10, Epoch 405/1000, Training Loss (NLML): -959.8328\n",
      "convergence dfGPdfNN Run 8/10, Epoch 406/1000, Training Loss (NLML): -959.7935\n",
      "convergence dfGPdfNN Run 8/10, Epoch 407/1000, Training Loss (NLML): -959.8433\n",
      "convergence dfGPdfNN Run 8/10, Epoch 408/1000, Training Loss (NLML): -959.8484\n",
      "convergence dfGPdfNN Run 8/10, Epoch 409/1000, Training Loss (NLML): -959.8544\n",
      "convergence dfGPdfNN Run 8/10, Epoch 410/1000, Training Loss (NLML): -959.8590\n",
      "convergence dfGPdfNN Run 8/10, Epoch 411/1000, Training Loss (NLML): -959.8665\n",
      "convergence dfGPdfNN Run 8/10, Epoch 412/1000, Training Loss (NLML): -959.8717\n",
      "convergence dfGPdfNN Run 8/10, Epoch 413/1000, Training Loss (NLML): -959.8867\n",
      "convergence dfGPdfNN Run 8/10, Epoch 414/1000, Training Loss (NLML): -959.8920\n",
      "convergence dfGPdfNN Run 8/10, Epoch 415/1000, Training Loss (NLML): -959.8853\n",
      "convergence dfGPdfNN Run 8/10, Epoch 416/1000, Training Loss (NLML): -959.8907\n",
      "convergence dfGPdfNN Run 8/10, Epoch 417/1000, Training Loss (NLML): -959.8967\n",
      "convergence dfGPdfNN Run 8/10, Epoch 418/1000, Training Loss (NLML): -959.9150\n",
      "convergence dfGPdfNN Run 8/10, Epoch 419/1000, Training Loss (NLML): -959.9076\n",
      "convergence dfGPdfNN Run 8/10, Epoch 420/1000, Training Loss (NLML): -959.9064\n",
      "convergence dfGPdfNN Run 8/10, Epoch 421/1000, Training Loss (NLML): -959.9125\n",
      "convergence dfGPdfNN Run 8/10, Epoch 422/1000, Training Loss (NLML): -959.9178\n",
      "convergence dfGPdfNN Run 8/10, Epoch 423/1000, Training Loss (NLML): -959.9337\n",
      "convergence dfGPdfNN Run 8/10, Epoch 424/1000, Training Loss (NLML): -959.9326\n",
      "convergence dfGPdfNN Run 8/10, Epoch 425/1000, Training Loss (NLML): -959.9465\n",
      "convergence dfGPdfNN Run 8/10, Epoch 426/1000, Training Loss (NLML): -959.9431\n",
      "convergence dfGPdfNN Run 8/10, Epoch 427/1000, Training Loss (NLML): -959.9580\n",
      "convergence dfGPdfNN Run 8/10, Epoch 428/1000, Training Loss (NLML): -959.9626\n",
      "convergence dfGPdfNN Run 8/10, Epoch 429/1000, Training Loss (NLML): -959.9602\n",
      "convergence dfGPdfNN Run 8/10, Epoch 430/1000, Training Loss (NLML): -959.9656\n",
      "convergence dfGPdfNN Run 8/10, Epoch 431/1000, Training Loss (NLML): -959.9552\n",
      "convergence dfGPdfNN Run 8/10, Epoch 432/1000, Training Loss (NLML): -959.9532\n",
      "convergence dfGPdfNN Run 8/10, Epoch 433/1000, Training Loss (NLML): -959.9309\n",
      "convergence dfGPdfNN Run 8/10, Epoch 434/1000, Training Loss (NLML): -959.9850\n",
      "convergence dfGPdfNN Run 8/10, Epoch 435/1000, Training Loss (NLML): -959.9883\n",
      "convergence dfGPdfNN Run 8/10, Epoch 436/1000, Training Loss (NLML): -959.9496\n",
      "convergence dfGPdfNN Run 8/10, Epoch 437/1000, Training Loss (NLML): -959.9485\n",
      "convergence dfGPdfNN Run 8/10, Epoch 438/1000, Training Loss (NLML): -959.9597\n",
      "convergence dfGPdfNN Run 8/10, Epoch 439/1000, Training Loss (NLML): -960.0050\n",
      "convergence dfGPdfNN Run 8/10, Epoch 440/1000, Training Loss (NLML): -959.9686\n",
      "convergence dfGPdfNN Run 8/10, Epoch 441/1000, Training Loss (NLML): -959.9746\n",
      "convergence dfGPdfNN Run 8/10, Epoch 442/1000, Training Loss (NLML): -960.0228\n",
      "convergence dfGPdfNN Run 8/10, Epoch 443/1000, Training Loss (NLML): -959.9823\n",
      "convergence dfGPdfNN Run 8/10, Epoch 444/1000, Training Loss (NLML): -960.0332\n",
      "convergence dfGPdfNN Run 8/10, Epoch 445/1000, Training Loss (NLML): -959.9928\n",
      "convergence dfGPdfNN Run 8/10, Epoch 446/1000, Training Loss (NLML): -960.0449\n",
      "convergence dfGPdfNN Run 8/10, Epoch 447/1000, Training Loss (NLML): -960.0497\n",
      "convergence dfGPdfNN Run 8/10, Epoch 448/1000, Training Loss (NLML): -960.0057\n",
      "convergence dfGPdfNN Run 8/10, Epoch 449/1000, Training Loss (NLML): -960.0580\n",
      "convergence dfGPdfNN Run 8/10, Epoch 450/1000, Training Loss (NLML): -960.0637\n",
      "convergence dfGPdfNN Run 8/10, Epoch 451/1000, Training Loss (NLML): -960.0679\n",
      "convergence dfGPdfNN Run 8/10, Epoch 452/1000, Training Loss (NLML): -960.0256\n",
      "convergence dfGPdfNN Run 8/10, Epoch 453/1000, Training Loss (NLML): -960.0782\n",
      "convergence dfGPdfNN Run 8/10, Epoch 454/1000, Training Loss (NLML): -960.0763\n",
      "convergence dfGPdfNN Run 8/10, Epoch 455/1000, Training Loss (NLML): -960.0826\n",
      "convergence dfGPdfNN Run 8/10, Epoch 456/1000, Training Loss (NLML): -960.0851\n",
      "convergence dfGPdfNN Run 8/10, Epoch 457/1000, Training Loss (NLML): -960.0277\n",
      "convergence dfGPdfNN Run 8/10, Epoch 458/1000, Training Loss (NLML): -960.0823\n",
      "convergence dfGPdfNN Run 8/10, Epoch 459/1000, Training Loss (NLML): -960.0878\n",
      "convergence dfGPdfNN Run 8/10, Epoch 460/1000, Training Loss (NLML): -960.0924\n",
      "convergence dfGPdfNN Run 8/10, Epoch 461/1000, Training Loss (NLML): -960.0978\n",
      "convergence dfGPdfNN Run 8/10, Epoch 462/1000, Training Loss (NLML): -960.1050\n",
      "convergence dfGPdfNN Run 8/10, Epoch 463/1000, Training Loss (NLML): -960.1099\n",
      "convergence dfGPdfNN Run 8/10, Epoch 464/1000, Training Loss (NLML): -960.1167\n",
      "convergence dfGPdfNN Run 8/10, Epoch 465/1000, Training Loss (NLML): -960.1431\n",
      "convergence dfGPdfNN Run 8/10, Epoch 466/1000, Training Loss (NLML): -960.1489\n",
      "convergence dfGPdfNN Run 8/10, Epoch 467/1000, Training Loss (NLML): -960.1558\n",
      "convergence dfGPdfNN Run 8/10, Epoch 468/1000, Training Loss (NLML): -960.1591\n",
      "convergence dfGPdfNN Run 8/10, Epoch 469/1000, Training Loss (NLML): -960.1637\n",
      "convergence dfGPdfNN Run 8/10, Epoch 470/1000, Training Loss (NLML): -960.1836\n",
      "convergence dfGPdfNN Run 8/10, Epoch 471/1000, Training Loss (NLML): -960.1858\n",
      "convergence dfGPdfNN Run 8/10, Epoch 472/1000, Training Loss (NLML): -960.1904\n",
      "convergence dfGPdfNN Run 8/10, Epoch 473/1000, Training Loss (NLML): -960.1561\n",
      "convergence dfGPdfNN Run 8/10, Epoch 474/1000, Training Loss (NLML): -960.1638\n",
      "convergence dfGPdfNN Run 8/10, Epoch 475/1000, Training Loss (NLML): -960.1688\n",
      "convergence dfGPdfNN Run 8/10, Epoch 476/1000, Training Loss (NLML): -960.1965\n",
      "convergence dfGPdfNN Run 8/10, Epoch 477/1000, Training Loss (NLML): -960.2024\n",
      "convergence dfGPdfNN Run 8/10, Epoch 478/1000, Training Loss (NLML): -960.1768\n",
      "convergence dfGPdfNN Run 8/10, Epoch 479/1000, Training Loss (NLML): -960.1808\n",
      "convergence dfGPdfNN Run 8/10, Epoch 480/1000, Training Loss (NLML): -960.1864\n",
      "convergence dfGPdfNN Run 8/10, Epoch 481/1000, Training Loss (NLML): -960.1908\n",
      "convergence dfGPdfNN Run 8/10, Epoch 482/1000, Training Loss (NLML): -960.2343\n",
      "convergence dfGPdfNN Run 8/10, Epoch 483/1000, Training Loss (NLML): -960.2227\n",
      "convergence dfGPdfNN Run 8/10, Epoch 484/1000, Training Loss (NLML): -960.2367\n",
      "convergence dfGPdfNN Run 8/10, Epoch 485/1000, Training Loss (NLML): -960.2535\n",
      "convergence dfGPdfNN Run 8/10, Epoch 486/1000, Training Loss (NLML): -960.2513\n",
      "convergence dfGPdfNN Run 8/10, Epoch 487/1000, Training Loss (NLML): -960.2480\n",
      "convergence dfGPdfNN Run 8/10, Epoch 488/1000, Training Loss (NLML): -960.2544\n",
      "convergence dfGPdfNN Run 8/10, Epoch 489/1000, Training Loss (NLML): -960.2588\n",
      "convergence dfGPdfNN Run 8/10, Epoch 490/1000, Training Loss (NLML): -960.2347\n",
      "convergence dfGPdfNN Run 8/10, Epoch 491/1000, Training Loss (NLML): -960.2529\n",
      "convergence dfGPdfNN Run 8/10, Epoch 492/1000, Training Loss (NLML): -960.2723\n",
      "convergence dfGPdfNN Run 8/10, Epoch 493/1000, Training Loss (NLML): -960.2689\n",
      "convergence dfGPdfNN Run 8/10, Epoch 494/1000, Training Loss (NLML): -960.2479\n",
      "convergence dfGPdfNN Run 8/10, Epoch 495/1000, Training Loss (NLML): -960.2551\n",
      "convergence dfGPdfNN Run 8/10, Epoch 496/1000, Training Loss (NLML): -960.2572\n",
      "convergence dfGPdfNN Run 8/10, Epoch 497/1000, Training Loss (NLML): -960.2676\n",
      "convergence dfGPdfNN Run 8/10, Epoch 498/1000, Training Loss (NLML): -960.2428\n",
      "convergence dfGPdfNN Run 8/10, Epoch 499/1000, Training Loss (NLML): -960.2510\n",
      "convergence dfGPdfNN Run 8/10, Epoch 500/1000, Training Loss (NLML): -960.2537\n",
      "convergence dfGPdfNN Run 8/10, Epoch 501/1000, Training Loss (NLML): -960.2632\n",
      "convergence dfGPdfNN Run 8/10, Epoch 502/1000, Training Loss (NLML): -960.2794\n",
      "convergence dfGPdfNN Run 8/10, Epoch 503/1000, Training Loss (NLML): -960.2828\n",
      "convergence dfGPdfNN Run 8/10, Epoch 504/1000, Training Loss (NLML): -960.2877\n",
      "convergence dfGPdfNN Run 8/10, Epoch 505/1000, Training Loss (NLML): -960.2922\n",
      "convergence dfGPdfNN Run 8/10, Epoch 506/1000, Training Loss (NLML): -960.2799\n",
      "convergence dfGPdfNN Run 8/10, Epoch 507/1000, Training Loss (NLML): -960.2998\n",
      "convergence dfGPdfNN Run 8/10, Epoch 508/1000, Training Loss (NLML): -960.3041\n",
      "convergence dfGPdfNN Run 8/10, Epoch 509/1000, Training Loss (NLML): -960.3090\n",
      "convergence dfGPdfNN Run 8/10, Epoch 510/1000, Training Loss (NLML): -960.3459\n",
      "convergence dfGPdfNN Run 8/10, Epoch 511/1000, Training Loss (NLML): -960.3478\n",
      "convergence dfGPdfNN Run 8/10, Epoch 512/1000, Training Loss (NLML): -960.3527\n",
      "convergence dfGPdfNN Run 8/10, Epoch 513/1000, Training Loss (NLML): -960.3232\n",
      "convergence dfGPdfNN Run 8/10, Epoch 514/1000, Training Loss (NLML): -960.3258\n",
      "convergence dfGPdfNN Run 8/10, Epoch 515/1000, Training Loss (NLML): -960.3324\n",
      "convergence dfGPdfNN Run 8/10, Epoch 516/1000, Training Loss (NLML): -960.3376\n",
      "convergence dfGPdfNN Run 8/10, Epoch 517/1000, Training Loss (NLML): -960.3689\n",
      "convergence dfGPdfNN Run 8/10, Epoch 518/1000, Training Loss (NLML): -960.3723\n",
      "convergence dfGPdfNN Run 8/10, Epoch 519/1000, Training Loss (NLML): -960.3483\n",
      "convergence dfGPdfNN Run 8/10, Epoch 520/1000, Training Loss (NLML): -960.3265\n",
      "convergence dfGPdfNN Run 8/10, Epoch 521/1000, Training Loss (NLML): -960.3328\n",
      "convergence dfGPdfNN Run 8/10, Epoch 522/1000, Training Loss (NLML): -960.3602\n",
      "convergence dfGPdfNN Run 8/10, Epoch 523/1000, Training Loss (NLML): -960.3615\n",
      "convergence dfGPdfNN Run 8/10, Epoch 524/1000, Training Loss (NLML): -960.3654\n",
      "convergence dfGPdfNN Run 8/10, Epoch 525/1000, Training Loss (NLML): -960.3701\n",
      "convergence dfGPdfNN Run 8/10, Epoch 526/1000, Training Loss (NLML): -960.3513\n",
      "convergence dfGPdfNN Run 8/10, Epoch 527/1000, Training Loss (NLML): -960.3571\n",
      "convergence dfGPdfNN Run 8/10, Epoch 528/1000, Training Loss (NLML): -960.3595\n",
      "convergence dfGPdfNN Run 8/10, Epoch 529/1000, Training Loss (NLML): -960.3651\n",
      "convergence dfGPdfNN Run 8/10, Epoch 530/1000, Training Loss (NLML): -960.3878\n",
      "convergence dfGPdfNN Run 8/10, Epoch 531/1000, Training Loss (NLML): -960.3906\n",
      "convergence dfGPdfNN Run 8/10, Epoch 532/1000, Training Loss (NLML): -960.1676\n",
      "convergence dfGPdfNN Run 8/10, Epoch 533/1000, Training Loss (NLML): -960.3771\n",
      "convergence dfGPdfNN Run 8/10, Epoch 534/1000, Training Loss (NLML): -960.3807\n",
      "convergence dfGPdfNN Run 8/10, Epoch 535/1000, Training Loss (NLML): -960.4067\n",
      "convergence dfGPdfNN Run 8/10, Epoch 536/1000, Training Loss (NLML): -960.4092\n",
      "convergence dfGPdfNN Run 8/10, Epoch 537/1000, Training Loss (NLML): -960.3918\n",
      "convergence dfGPdfNN Run 8/10, Epoch 538/1000, Training Loss (NLML): -960.4030\n",
      "convergence dfGPdfNN Run 8/10, Epoch 539/1000, Training Loss (NLML): -960.4061\n",
      "convergence dfGPdfNN Run 8/10, Epoch 540/1000, Training Loss (NLML): -960.4235\n",
      "convergence dfGPdfNN Run 8/10, Epoch 541/1000, Training Loss (NLML): -960.4264\n",
      "convergence dfGPdfNN Run 8/10, Epoch 542/1000, Training Loss (NLML): -960.4103\n",
      "convergence dfGPdfNN Run 8/10, Epoch 543/1000, Training Loss (NLML): -960.4160\n",
      "convergence dfGPdfNN Run 8/10, Epoch 544/1000, Training Loss (NLML): -960.4192\n",
      "convergence dfGPdfNN Run 8/10, Epoch 545/1000, Training Loss (NLML): -960.4216\n",
      "convergence dfGPdfNN Run 8/10, Epoch 546/1000, Training Loss (NLML): -960.4258\n",
      "convergence dfGPdfNN Run 8/10, Epoch 547/1000, Training Loss (NLML): -960.4292\n",
      "convergence dfGPdfNN Run 8/10, Epoch 548/1000, Training Loss (NLML): -960.4315\n",
      "convergence dfGPdfNN Run 8/10, Epoch 549/1000, Training Loss (NLML): -960.4358\n",
      "convergence dfGPdfNN Run 8/10, Epoch 550/1000, Training Loss (NLML): -960.4395\n",
      "convergence dfGPdfNN Run 8/10, Epoch 551/1000, Training Loss (NLML): -960.4436\n",
      "convergence dfGPdfNN Run 8/10, Epoch 552/1000, Training Loss (NLML): -960.4475\n",
      "convergence dfGPdfNN Run 8/10, Epoch 553/1000, Training Loss (NLML): -960.4503\n",
      "convergence dfGPdfNN Run 8/10, Epoch 554/1000, Training Loss (NLML): -960.4539\n",
      "convergence dfGPdfNN Run 8/10, Epoch 555/1000, Training Loss (NLML): -960.4573\n",
      "convergence dfGPdfNN Run 8/10, Epoch 556/1000, Training Loss (NLML): -960.4597\n",
      "convergence dfGPdfNN Run 8/10, Epoch 557/1000, Training Loss (NLML): -960.4811\n",
      "convergence dfGPdfNN Run 8/10, Epoch 558/1000, Training Loss (NLML): -960.4858\n",
      "convergence dfGPdfNN Run 8/10, Epoch 559/1000, Training Loss (NLML): -960.4692\n",
      "convergence dfGPdfNN Run 8/10, Epoch 560/1000, Training Loss (NLML): -960.4718\n",
      "convergence dfGPdfNN Run 8/10, Epoch 561/1000, Training Loss (NLML): -960.4746\n",
      "convergence dfGPdfNN Run 8/10, Epoch 562/1000, Training Loss (NLML): -960.4800\n",
      "convergence dfGPdfNN Run 8/10, Epoch 563/1000, Training Loss (NLML): -960.4844\n",
      "convergence dfGPdfNN Run 8/10, Epoch 564/1000, Training Loss (NLML): -960.4860\n",
      "convergence dfGPdfNN Run 8/10, Epoch 565/1000, Training Loss (NLML): -960.5049\n",
      "convergence dfGPdfNN Run 8/10, Epoch 566/1000, Training Loss (NLML): -960.4910\n",
      "convergence dfGPdfNN Run 8/10, Epoch 567/1000, Training Loss (NLML): -960.4972\n",
      "convergence dfGPdfNN Run 8/10, Epoch 568/1000, Training Loss (NLML): -960.4999\n",
      "convergence dfGPdfNN Run 8/10, Epoch 569/1000, Training Loss (NLML): -960.5026\n",
      "convergence dfGPdfNN Run 8/10, Epoch 570/1000, Training Loss (NLML): -960.5055\n",
      "convergence dfGPdfNN Run 8/10, Epoch 571/1000, Training Loss (NLML): -960.5109\n",
      "convergence dfGPdfNN Run 8/10, Epoch 572/1000, Training Loss (NLML): -960.5138\n",
      "convergence dfGPdfNN Run 8/10, Epoch 573/1000, Training Loss (NLML): -960.5148\n",
      "convergence dfGPdfNN Run 8/10, Epoch 574/1000, Training Loss (NLML): -960.5359\n",
      "convergence dfGPdfNN Run 8/10, Epoch 575/1000, Training Loss (NLML): -960.5227\n",
      "convergence dfGPdfNN Run 8/10, Epoch 576/1000, Training Loss (NLML): -960.5244\n",
      "convergence dfGPdfNN Run 8/10, Epoch 577/1000, Training Loss (NLML): -960.5308\n",
      "convergence dfGPdfNN Run 8/10, Epoch 578/1000, Training Loss (NLML): -960.5321\n",
      "convergence dfGPdfNN Run 8/10, Epoch 579/1000, Training Loss (NLML): -960.5352\n",
      "convergence dfGPdfNN Run 8/10, Epoch 580/1000, Training Loss (NLML): -960.5392\n",
      "convergence dfGPdfNN Run 8/10, Epoch 581/1000, Training Loss (NLML): -960.5609\n",
      "convergence dfGPdfNN Run 8/10, Epoch 582/1000, Training Loss (NLML): -960.5448\n",
      "convergence dfGPdfNN Run 8/10, Epoch 583/1000, Training Loss (NLML): -960.5493\n",
      "convergence dfGPdfNN Run 8/10, Epoch 584/1000, Training Loss (NLML): -960.5516\n",
      "convergence dfGPdfNN Run 8/10, Epoch 585/1000, Training Loss (NLML): -960.5551\n",
      "convergence dfGPdfNN Run 8/10, Epoch 586/1000, Training Loss (NLML): -960.5580\n",
      "convergence dfGPdfNN Run 8/10, Epoch 587/1000, Training Loss (NLML): -960.5593\n",
      "convergence dfGPdfNN Run 8/10, Epoch 588/1000, Training Loss (NLML): -960.5808\n",
      "convergence dfGPdfNN Run 8/10, Epoch 589/1000, Training Loss (NLML): -960.5652\n",
      "convergence dfGPdfNN Run 8/10, Epoch 590/1000, Training Loss (NLML): -960.5697\n",
      "convergence dfGPdfNN Run 8/10, Epoch 591/1000, Training Loss (NLML): -960.5737\n",
      "convergence dfGPdfNN Run 8/10, Epoch 592/1000, Training Loss (NLML): -960.5769\n",
      "convergence dfGPdfNN Run 8/10, Epoch 593/1000, Training Loss (NLML): -960.5781\n",
      "convergence dfGPdfNN Run 8/10, Epoch 594/1000, Training Loss (NLML): -960.5981\n",
      "convergence dfGPdfNN Run 8/10, Epoch 595/1000, Training Loss (NLML): -960.5853\n",
      "convergence dfGPdfNN Run 8/10, Epoch 596/1000, Training Loss (NLML): -960.5886\n",
      "convergence dfGPdfNN Run 8/10, Epoch 597/1000, Training Loss (NLML): -960.5924\n",
      "convergence dfGPdfNN Run 8/10, Epoch 598/1000, Training Loss (NLML): -960.5958\n",
      "convergence dfGPdfNN Run 8/10, Epoch 599/1000, Training Loss (NLML): -960.6145\n",
      "convergence dfGPdfNN Run 8/10, Epoch 600/1000, Training Loss (NLML): -960.5999\n",
      "convergence dfGPdfNN Run 8/10, Epoch 601/1000, Training Loss (NLML): -960.6045\n",
      "convergence dfGPdfNN Run 8/10, Epoch 602/1000, Training Loss (NLML): -960.6055\n",
      "convergence dfGPdfNN Run 8/10, Epoch 603/1000, Training Loss (NLML): -960.6115\n",
      "convergence dfGPdfNN Run 8/10, Epoch 604/1000, Training Loss (NLML): -960.6289\n",
      "convergence dfGPdfNN Run 8/10, Epoch 605/1000, Training Loss (NLML): -960.6167\n",
      "convergence dfGPdfNN Run 8/10, Epoch 606/1000, Training Loss (NLML): -960.6182\n",
      "convergence dfGPdfNN Run 8/10, Epoch 607/1000, Training Loss (NLML): -960.6210\n",
      "convergence dfGPdfNN Run 8/10, Epoch 608/1000, Training Loss (NLML): -960.6243\n",
      "convergence dfGPdfNN Run 8/10, Epoch 609/1000, Training Loss (NLML): -960.6437\n",
      "convergence dfGPdfNN Run 8/10, Epoch 610/1000, Training Loss (NLML): -960.6290\n",
      "convergence dfGPdfNN Run 8/10, Epoch 611/1000, Training Loss (NLML): -960.6333\n",
      "convergence dfGPdfNN Run 8/10, Epoch 612/1000, Training Loss (NLML): -960.6356\n",
      "convergence dfGPdfNN Run 8/10, Epoch 613/1000, Training Loss (NLML): -960.6377\n",
      "convergence dfGPdfNN Run 8/10, Epoch 614/1000, Training Loss (NLML): -960.6584\n",
      "convergence dfGPdfNN Run 8/10, Epoch 615/1000, Training Loss (NLML): -960.6471\n",
      "convergence dfGPdfNN Run 8/10, Epoch 616/1000, Training Loss (NLML): -960.6470\n",
      "convergence dfGPdfNN Run 8/10, Epoch 617/1000, Training Loss (NLML): -960.6521\n",
      "convergence dfGPdfNN Run 8/10, Epoch 618/1000, Training Loss (NLML): -960.6525\n",
      "convergence dfGPdfNN Run 8/10, Epoch 619/1000, Training Loss (NLML): -960.6711\n",
      "convergence dfGPdfNN Run 8/10, Epoch 620/1000, Training Loss (NLML): -960.6603\n",
      "convergence dfGPdfNN Run 8/10, Epoch 621/1000, Training Loss (NLML): -960.6615\n",
      "convergence dfGPdfNN Run 8/10, Epoch 622/1000, Training Loss (NLML): -960.6656\n",
      "convergence dfGPdfNN Run 8/10, Epoch 623/1000, Training Loss (NLML): -960.6683\n",
      "convergence dfGPdfNN Run 8/10, Epoch 624/1000, Training Loss (NLML): -960.6884\n",
      "convergence dfGPdfNN Run 8/10, Epoch 625/1000, Training Loss (NLML): -960.6740\n",
      "convergence dfGPdfNN Run 8/10, Epoch 626/1000, Training Loss (NLML): -960.6761\n",
      "convergence dfGPdfNN Run 8/10, Epoch 627/1000, Training Loss (NLML): -960.6790\n",
      "convergence dfGPdfNN Run 8/10, Epoch 628/1000, Training Loss (NLML): -960.6825\n",
      "convergence dfGPdfNN Run 8/10, Epoch 629/1000, Training Loss (NLML): -960.7012\n",
      "convergence dfGPdfNN Run 8/10, Epoch 630/1000, Training Loss (NLML): -960.6880\n",
      "convergence dfGPdfNN Run 8/10, Epoch 631/1000, Training Loss (NLML): -960.6896\n",
      "convergence dfGPdfNN Run 8/10, Epoch 632/1000, Training Loss (NLML): -960.6942\n",
      "convergence dfGPdfNN Run 8/10, Epoch 633/1000, Training Loss (NLML): -960.6964\n",
      "convergence dfGPdfNN Run 8/10, Epoch 634/1000, Training Loss (NLML): -960.7137\n",
      "convergence dfGPdfNN Run 8/10, Epoch 635/1000, Training Loss (NLML): -960.7013\n",
      "convergence dfGPdfNN Run 8/10, Epoch 636/1000, Training Loss (NLML): -960.7056\n",
      "convergence dfGPdfNN Run 8/10, Epoch 637/1000, Training Loss (NLML): -960.7083\n",
      "convergence dfGPdfNN Run 8/10, Epoch 638/1000, Training Loss (NLML): -960.7097\n",
      "convergence dfGPdfNN Run 8/10, Epoch 639/1000, Training Loss (NLML): -960.7277\n",
      "convergence dfGPdfNN Run 8/10, Epoch 640/1000, Training Loss (NLML): -960.7162\n",
      "convergence dfGPdfNN Run 8/10, Epoch 641/1000, Training Loss (NLML): -960.7185\n",
      "convergence dfGPdfNN Run 8/10, Epoch 642/1000, Training Loss (NLML): -960.7209\n",
      "convergence dfGPdfNN Run 8/10, Epoch 643/1000, Training Loss (NLML): -960.7249\n",
      "convergence dfGPdfNN Run 8/10, Epoch 644/1000, Training Loss (NLML): -960.7423\n",
      "convergence dfGPdfNN Run 8/10, Epoch 645/1000, Training Loss (NLML): -960.7269\n",
      "convergence dfGPdfNN Run 8/10, Epoch 646/1000, Training Loss (NLML): -960.7305\n",
      "convergence dfGPdfNN Run 8/10, Epoch 647/1000, Training Loss (NLML): -960.7344\n",
      "convergence dfGPdfNN Run 8/10, Epoch 648/1000, Training Loss (NLML): -960.7385\n",
      "convergence dfGPdfNN Run 8/10, Epoch 649/1000, Training Loss (NLML): -960.7548\n",
      "convergence dfGPdfNN Run 8/10, Epoch 650/1000, Training Loss (NLML): -960.7423\n",
      "convergence dfGPdfNN Run 8/10, Epoch 651/1000, Training Loss (NLML): -960.7444\n",
      "convergence dfGPdfNN Run 8/10, Epoch 652/1000, Training Loss (NLML): -960.7467\n",
      "convergence dfGPdfNN Run 8/10, Epoch 653/1000, Training Loss (NLML): -960.7520\n",
      "convergence dfGPdfNN Run 8/10, Epoch 654/1000, Training Loss (NLML): -960.7682\n",
      "convergence dfGPdfNN Run 8/10, Epoch 655/1000, Training Loss (NLML): -960.7535\n",
      "convergence dfGPdfNN Run 8/10, Epoch 656/1000, Training Loss (NLML): -960.7563\n",
      "convergence dfGPdfNN Run 8/10, Epoch 657/1000, Training Loss (NLML): -960.7600\n",
      "convergence dfGPdfNN Run 8/10, Epoch 658/1000, Training Loss (NLML): -960.7615\n",
      "convergence dfGPdfNN Run 8/10, Epoch 659/1000, Training Loss (NLML): -960.7795\n",
      "convergence dfGPdfNN Run 8/10, Epoch 660/1000, Training Loss (NLML): -960.7701\n",
      "convergence dfGPdfNN Run 8/10, Epoch 661/1000, Training Loss (NLML): -960.7728\n",
      "convergence dfGPdfNN Run 8/10, Epoch 662/1000, Training Loss (NLML): -960.7726\n",
      "convergence dfGPdfNN Run 8/10, Epoch 663/1000, Training Loss (NLML): -960.7747\n",
      "convergence dfGPdfNN Run 8/10, Epoch 664/1000, Training Loss (NLML): -960.7955\n",
      "convergence dfGPdfNN Run 8/10, Epoch 665/1000, Training Loss (NLML): -960.7798\n",
      "convergence dfGPdfNN Run 8/10, Epoch 666/1000, Training Loss (NLML): -960.7848\n",
      "convergence dfGPdfNN Run 8/10, Epoch 667/1000, Training Loss (NLML): -960.7872\n",
      "convergence dfGPdfNN Run 8/10, Epoch 668/1000, Training Loss (NLML): -960.7891\n",
      "convergence dfGPdfNN Run 8/10, Epoch 669/1000, Training Loss (NLML): -960.8052\n",
      "convergence dfGPdfNN Run 8/10, Epoch 670/1000, Training Loss (NLML): -960.7926\n",
      "convergence dfGPdfNN Run 8/10, Epoch 671/1000, Training Loss (NLML): -960.7961\n",
      "convergence dfGPdfNN Run 8/10, Epoch 672/1000, Training Loss (NLML): -960.7999\n",
      "convergence dfGPdfNN Run 8/10, Epoch 673/1000, Training Loss (NLML): -960.8020\n",
      "convergence dfGPdfNN Run 8/10, Epoch 674/1000, Training Loss (NLML): -960.8168\n",
      "convergence dfGPdfNN Run 8/10, Epoch 675/1000, Training Loss (NLML): -960.8055\n",
      "convergence dfGPdfNN Run 8/10, Epoch 676/1000, Training Loss (NLML): -960.8081\n",
      "convergence dfGPdfNN Run 8/10, Epoch 677/1000, Training Loss (NLML): -960.8104\n",
      "convergence dfGPdfNN Run 8/10, Epoch 678/1000, Training Loss (NLML): -960.8134\n",
      "convergence dfGPdfNN Run 8/10, Epoch 679/1000, Training Loss (NLML): -960.8317\n",
      "convergence dfGPdfNN Run 8/10, Epoch 680/1000, Training Loss (NLML): -960.8170\n",
      "convergence dfGPdfNN Run 8/10, Epoch 681/1000, Training Loss (NLML): -960.8192\n",
      "convergence dfGPdfNN Run 8/10, Epoch 682/1000, Training Loss (NLML): -960.8230\n",
      "convergence dfGPdfNN Run 8/10, Epoch 683/1000, Training Loss (NLML): -960.8253\n",
      "convergence dfGPdfNN Run 8/10, Epoch 684/1000, Training Loss (NLML): -960.8450\n",
      "convergence dfGPdfNN Run 8/10, Epoch 685/1000, Training Loss (NLML): -960.8309\n",
      "convergence dfGPdfNN Run 8/10, Epoch 686/1000, Training Loss (NLML): -960.8347\n",
      "convergence dfGPdfNN Run 8/10, Epoch 687/1000, Training Loss (NLML): -960.8361\n",
      "convergence dfGPdfNN Run 8/10, Epoch 688/1000, Training Loss (NLML): -960.8403\n",
      "convergence dfGPdfNN Run 8/10, Epoch 689/1000, Training Loss (NLML): -960.8529\n",
      "convergence dfGPdfNN Run 8/10, Epoch 690/1000, Training Loss (NLML): -960.8510\n",
      "convergence dfGPdfNN Run 8/10, Epoch 691/1000, Training Loss (NLML): -960.8446\n",
      "convergence dfGPdfNN Run 8/10, Epoch 692/1000, Training Loss (NLML): -960.8458\n",
      "convergence dfGPdfNN Run 8/10, Epoch 693/1000, Training Loss (NLML): -960.8645\n",
      "convergence dfGPdfNN Run 8/10, Epoch 694/1000, Training Loss (NLML): -960.8661\n",
      "convergence dfGPdfNN Run 8/10, Epoch 695/1000, Training Loss (NLML): -960.8557\n",
      "convergence dfGPdfNN Run 8/10, Epoch 696/1000, Training Loss (NLML): -960.8555\n",
      "convergence dfGPdfNN Run 8/10, Epoch 697/1000, Training Loss (NLML): -960.8601\n",
      "convergence dfGPdfNN Run 8/10, Epoch 698/1000, Training Loss (NLML): -960.8757\n",
      "convergence dfGPdfNN Run 8/10, Epoch 699/1000, Training Loss (NLML): -960.8781\n",
      "convergence dfGPdfNN Run 8/10, Epoch 700/1000, Training Loss (NLML): -960.8807\n",
      "convergence dfGPdfNN Run 8/10, Epoch 701/1000, Training Loss (NLML): -960.8668\n",
      "convergence dfGPdfNN Run 8/10, Epoch 702/1000, Training Loss (NLML): -960.8698\n",
      "convergence dfGPdfNN Run 8/10, Epoch 703/1000, Training Loss (NLML): -960.8729\n",
      "convergence dfGPdfNN Run 8/10, Epoch 704/1000, Training Loss (NLML): -960.8882\n",
      "convergence dfGPdfNN Run 8/10, Epoch 705/1000, Training Loss (NLML): -960.8906\n",
      "convergence dfGPdfNN Run 8/10, Epoch 706/1000, Training Loss (NLML): -960.8921\n",
      "convergence dfGPdfNN Run 8/10, Epoch 707/1000, Training Loss (NLML): -960.8969\n",
      "convergence dfGPdfNN Run 8/10, Epoch 708/1000, Training Loss (NLML): -960.8851\n",
      "convergence dfGPdfNN Run 8/10, Epoch 709/1000, Training Loss (NLML): -960.8865\n",
      "convergence dfGPdfNN Run 8/10, Epoch 710/1000, Training Loss (NLML): -960.9039\n",
      "convergence dfGPdfNN Run 8/10, Epoch 711/1000, Training Loss (NLML): -960.9061\n",
      "convergence dfGPdfNN Run 8/10, Epoch 712/1000, Training Loss (NLML): -960.9052\n",
      "convergence dfGPdfNN Run 8/10, Epoch 713/1000, Training Loss (NLML): -960.8975\n",
      "convergence dfGPdfNN Run 8/10, Epoch 714/1000, Training Loss (NLML): -960.9000\n",
      "convergence dfGPdfNN Run 8/10, Epoch 715/1000, Training Loss (NLML): -960.9154\n",
      "convergence dfGPdfNN Run 8/10, Epoch 716/1000, Training Loss (NLML): -960.9014\n",
      "convergence dfGPdfNN Run 8/10, Epoch 717/1000, Training Loss (NLML): -960.9036\n",
      "convergence dfGPdfNN Run 8/10, Epoch 718/1000, Training Loss (NLML): -960.9213\n",
      "convergence dfGPdfNN Run 8/10, Epoch 719/1000, Training Loss (NLML): -960.9100\n",
      "convergence dfGPdfNN Run 8/10, Epoch 720/1000, Training Loss (NLML): -960.9128\n",
      "convergence dfGPdfNN Run 8/10, Epoch 721/1000, Training Loss (NLML): -960.9286\n",
      "convergence dfGPdfNN Run 8/10, Epoch 722/1000, Training Loss (NLML): -960.9161\n",
      "convergence dfGPdfNN Run 8/10, Epoch 723/1000, Training Loss (NLML): -960.9177\n",
      "convergence dfGPdfNN Run 8/10, Epoch 724/1000, Training Loss (NLML): -960.9360\n",
      "convergence dfGPdfNN Run 8/10, Epoch 725/1000, Training Loss (NLML): -960.9385\n",
      "convergence dfGPdfNN Run 8/10, Epoch 726/1000, Training Loss (NLML): -960.9232\n",
      "convergence dfGPdfNN Run 8/10, Epoch 727/1000, Training Loss (NLML): -960.9276\n",
      "convergence dfGPdfNN Run 8/10, Epoch 728/1000, Training Loss (NLML): -960.9446\n",
      "convergence dfGPdfNN Run 8/10, Epoch 729/1000, Training Loss (NLML): -960.9457\n",
      "convergence dfGPdfNN Run 8/10, Epoch 730/1000, Training Loss (NLML): -960.9478\n",
      "convergence dfGPdfNN Run 8/10, Epoch 731/1000, Training Loss (NLML): -960.9351\n",
      "convergence dfGPdfNN Run 8/10, Epoch 732/1000, Training Loss (NLML): -960.9528\n",
      "convergence dfGPdfNN Run 8/10, Epoch 733/1000, Training Loss (NLML): -960.9470\n",
      "convergence dfGPdfNN Run 8/10, Epoch 734/1000, Training Loss (NLML): -960.9552\n",
      "convergence dfGPdfNN Run 8/10, Epoch 735/1000, Training Loss (NLML): -960.9581\n",
      "convergence dfGPdfNN Run 8/10, Epoch 736/1000, Training Loss (NLML): -960.9626\n",
      "convergence dfGPdfNN Run 8/10, Epoch 737/1000, Training Loss (NLML): -960.9658\n",
      "convergence dfGPdfNN Run 8/10, Epoch 738/1000, Training Loss (NLML): -960.9674\n",
      "convergence dfGPdfNN Run 8/10, Epoch 739/1000, Training Loss (NLML): -960.9510\n",
      "convergence dfGPdfNN Run 8/10, Epoch 740/1000, Training Loss (NLML): -960.9679\n",
      "convergence dfGPdfNN Run 8/10, Epoch 741/1000, Training Loss (NLML): -960.9634\n",
      "convergence dfGPdfNN Run 8/10, Epoch 742/1000, Training Loss (NLML): -960.9677\n",
      "convergence dfGPdfNN Run 8/10, Epoch 743/1000, Training Loss (NLML): -960.9698\n",
      "convergence dfGPdfNN Run 8/10, Epoch 744/1000, Training Loss (NLML): -960.9762\n",
      "convergence dfGPdfNN Run 8/10, Epoch 745/1000, Training Loss (NLML): -960.9801\n",
      "convergence dfGPdfNN Run 8/10, Epoch 746/1000, Training Loss (NLML): -960.9833\n",
      "convergence dfGPdfNN Run 8/10, Epoch 747/1000, Training Loss (NLML): -960.9833\n",
      "convergence dfGPdfNN Run 8/10, Epoch 748/1000, Training Loss (NLML): -960.9854\n",
      "convergence dfGPdfNN Run 8/10, Epoch 749/1000, Training Loss (NLML): -960.9894\n",
      "convergence dfGPdfNN Run 8/10, Epoch 750/1000, Training Loss (NLML): -960.9921\n",
      "convergence dfGPdfNN Run 8/10, Epoch 751/1000, Training Loss (NLML): -960.9949\n",
      "convergence dfGPdfNN Run 8/10, Epoch 752/1000, Training Loss (NLML): -960.9968\n",
      "convergence dfGPdfNN Run 8/10, Epoch 753/1000, Training Loss (NLML): -960.9979\n",
      "convergence dfGPdfNN Run 8/10, Epoch 754/1000, Training Loss (NLML): -961.0006\n",
      "convergence dfGPdfNN Run 8/10, Epoch 755/1000, Training Loss (NLML): -961.0006\n",
      "convergence dfGPdfNN Run 8/10, Epoch 756/1000, Training Loss (NLML): -960.9932\n",
      "convergence dfGPdfNN Run 8/10, Epoch 757/1000, Training Loss (NLML): -960.9974\n",
      "convergence dfGPdfNN Run 8/10, Epoch 758/1000, Training Loss (NLML): -961.0059\n",
      "convergence dfGPdfNN Run 8/10, Epoch 759/1000, Training Loss (NLML): -961.0089\n",
      "convergence dfGPdfNN Run 8/10, Epoch 760/1000, Training Loss (NLML): -961.0112\n",
      "convergence dfGPdfNN Run 8/10, Epoch 761/1000, Training Loss (NLML): -960.9993\n",
      "convergence dfGPdfNN Run 8/10, Epoch 762/1000, Training Loss (NLML): -961.0016\n",
      "convergence dfGPdfNN Run 8/10, Epoch 763/1000, Training Loss (NLML): -961.0039\n",
      "convergence dfGPdfNN Run 8/10, Epoch 764/1000, Training Loss (NLML): -961.0061\n",
      "convergence dfGPdfNN Run 8/10, Epoch 765/1000, Training Loss (NLML): -961.0084\n",
      "convergence dfGPdfNN Run 8/10, Epoch 766/1000, Training Loss (NLML): -961.0221\n",
      "convergence dfGPdfNN Run 8/10, Epoch 767/1000, Training Loss (NLML): -961.0121\n",
      "convergence dfGPdfNN Run 8/10, Epoch 768/1000, Training Loss (NLML): -961.0150\n",
      "convergence dfGPdfNN Run 8/10, Epoch 769/1000, Training Loss (NLML): -961.0302\n",
      "convergence dfGPdfNN Run 8/10, Epoch 770/1000, Training Loss (NLML): -961.0314\n",
      "convergence dfGPdfNN Run 8/10, Epoch 771/1000, Training Loss (NLML): -961.0341\n",
      "convergence dfGPdfNN Run 8/10, Epoch 772/1000, Training Loss (NLML): -961.0353\n",
      "convergence dfGPdfNN Run 8/10, Epoch 773/1000, Training Loss (NLML): -961.0393\n",
      "convergence dfGPdfNN Run 8/10, Epoch 774/1000, Training Loss (NLML): -961.0400\n",
      "convergence dfGPdfNN Run 8/10, Epoch 775/1000, Training Loss (NLML): -961.0591\n",
      "convergence dfGPdfNN Run 8/10, Epoch 776/1000, Training Loss (NLML): -961.0424\n",
      "convergence dfGPdfNN Run 8/10, Epoch 777/1000, Training Loss (NLML): -961.0476\n",
      "convergence dfGPdfNN Run 8/10, Epoch 778/1000, Training Loss (NLML): -961.0492\n",
      "convergence dfGPdfNN Run 8/10, Epoch 779/1000, Training Loss (NLML): -961.0497\n",
      "convergence dfGPdfNN Run 8/10, Epoch 780/1000, Training Loss (NLML): -961.0543\n",
      "convergence dfGPdfNN Run 8/10, Epoch 781/1000, Training Loss (NLML): -961.0521\n",
      "convergence dfGPdfNN Run 8/10, Epoch 782/1000, Training Loss (NLML): -961.0546\n",
      "convergence dfGPdfNN Run 8/10, Epoch 783/1000, Training Loss (NLML): -961.0579\n",
      "convergence dfGPdfNN Run 8/10, Epoch 784/1000, Training Loss (NLML): -961.0599\n",
      "convergence dfGPdfNN Run 8/10, Epoch 785/1000, Training Loss (NLML): -961.0607\n",
      "convergence dfGPdfNN Run 8/10, Epoch 786/1000, Training Loss (NLML): -961.0613\n",
      "convergence dfGPdfNN Run 8/10, Epoch 787/1000, Training Loss (NLML): -961.0657\n",
      "convergence dfGPdfNN Run 8/10, Epoch 788/1000, Training Loss (NLML): -961.0645\n",
      "convergence dfGPdfNN Run 8/10, Epoch 789/1000, Training Loss (NLML): -961.0665\n",
      "convergence dfGPdfNN Run 8/10, Epoch 790/1000, Training Loss (NLML): -961.0691\n",
      "convergence dfGPdfNN Run 8/10, Epoch 791/1000, Training Loss (NLML): -961.0681\n",
      "convergence dfGPdfNN Run 8/10, Epoch 792/1000, Training Loss (NLML): -961.0701\n",
      "convergence dfGPdfNN Run 8/10, Epoch 793/1000, Training Loss (NLML): -961.0906\n",
      "convergence dfGPdfNN Run 8/10, Epoch 794/1000, Training Loss (NLML): -961.0748\n",
      "convergence dfGPdfNN Run 8/10, Epoch 795/1000, Training Loss (NLML): -961.0781\n",
      "convergence dfGPdfNN Run 8/10, Epoch 796/1000, Training Loss (NLML): -961.0803\n",
      "convergence dfGPdfNN Run 8/10, Epoch 797/1000, Training Loss (NLML): -961.0834\n",
      "convergence dfGPdfNN Run 8/10, Epoch 798/1000, Training Loss (NLML): -961.0826\n",
      "convergence dfGPdfNN Run 8/10, Epoch 799/1000, Training Loss (NLML): -961.0874\n",
      "convergence dfGPdfNN Run 8/10, Epoch 800/1000, Training Loss (NLML): -961.0908\n",
      "convergence dfGPdfNN Run 8/10, Epoch 801/1000, Training Loss (NLML): -961.0894\n",
      "convergence dfGPdfNN Run 8/10, Epoch 802/1000, Training Loss (NLML): -961.0977\n",
      "convergence dfGPdfNN Run 8/10, Epoch 803/1000, Training Loss (NLML): -961.0992\n",
      "convergence dfGPdfNN Run 8/10, Epoch 804/1000, Training Loss (NLML): -961.1010\n",
      "convergence dfGPdfNN Run 8/10, Epoch 805/1000, Training Loss (NLML): -961.0985\n",
      "convergence dfGPdfNN Run 8/10, Epoch 806/1000, Training Loss (NLML): -961.1014\n",
      "convergence dfGPdfNN Run 8/10, Epoch 807/1000, Training Loss (NLML): -961.1215\n",
      "convergence dfGPdfNN Run 8/10, Epoch 808/1000, Training Loss (NLML): -961.1021\n",
      "convergence dfGPdfNN Run 8/10, Epoch 809/1000, Training Loss (NLML): -961.1064\n",
      "convergence dfGPdfNN Run 8/10, Epoch 810/1000, Training Loss (NLML): -961.1240\n",
      "convergence dfGPdfNN Run 8/10, Epoch 811/1000, Training Loss (NLML): -961.1096\n",
      "convergence dfGPdfNN Run 8/10, Epoch 812/1000, Training Loss (NLML): -961.1094\n",
      "convergence dfGPdfNN Run 8/10, Epoch 813/1000, Training Loss (NLML): -961.1116\n",
      "convergence dfGPdfNN Run 8/10, Epoch 814/1000, Training Loss (NLML): -961.1340\n",
      "convergence dfGPdfNN Run 8/10, Epoch 815/1000, Training Loss (NLML): -961.1157\n",
      "convergence dfGPdfNN Run 8/10, Epoch 816/1000, Training Loss (NLML): -961.1210\n",
      "convergence dfGPdfNN Run 8/10, Epoch 817/1000, Training Loss (NLML): -961.1184\n",
      "convergence dfGPdfNN Run 8/10, Epoch 818/1000, Training Loss (NLML): -961.1377\n",
      "convergence dfGPdfNN Run 8/10, Epoch 819/1000, Training Loss (NLML): -961.1229\n",
      "convergence dfGPdfNN Run 8/10, Epoch 820/1000, Training Loss (NLML): -961.1215\n",
      "convergence dfGPdfNN Run 8/10, Epoch 821/1000, Training Loss (NLML): -961.1420\n",
      "convergence dfGPdfNN Run 8/10, Epoch 822/1000, Training Loss (NLML): -961.1438\n",
      "convergence dfGPdfNN Run 8/10, Epoch 823/1000, Training Loss (NLML): -961.1244\n",
      "convergence dfGPdfNN Run 8/10, Epoch 824/1000, Training Loss (NLML): -961.1472\n",
      "convergence dfGPdfNN Run 8/10, Epoch 825/1000, Training Loss (NLML): -961.1467\n",
      "convergence dfGPdfNN Run 8/10, Epoch 826/1000, Training Loss (NLML): -961.1500\n",
      "convergence dfGPdfNN Run 8/10, Epoch 827/1000, Training Loss (NLML): -961.1530\n",
      "convergence dfGPdfNN Run 8/10, Epoch 828/1000, Training Loss (NLML): -961.1545\n",
      "convergence dfGPdfNN Run 8/10, Epoch 829/1000, Training Loss (NLML): -961.1554\n",
      "convergence dfGPdfNN Run 8/10, Epoch 830/1000, Training Loss (NLML): -961.1576\n",
      "convergence dfGPdfNN Run 8/10, Epoch 831/1000, Training Loss (NLML): -961.1571\n",
      "convergence dfGPdfNN Run 8/10, Epoch 832/1000, Training Loss (NLML): -961.1643\n",
      "convergence dfGPdfNN Run 8/10, Epoch 833/1000, Training Loss (NLML): -961.1710\n",
      "convergence dfGPdfNN Run 8/10, Epoch 834/1000, Training Loss (NLML): -961.1779\n",
      "convergence dfGPdfNN Run 8/10, Epoch 835/1000, Training Loss (NLML): -961.1783\n",
      "convergence dfGPdfNN Run 8/10, Epoch 836/1000, Training Loss (NLML): -961.1879\n",
      "convergence dfGPdfNN Run 8/10, Epoch 837/1000, Training Loss (NLML): -961.1919\n",
      "convergence dfGPdfNN Run 8/10, Epoch 838/1000, Training Loss (NLML): -961.1968\n",
      "convergence dfGPdfNN Run 8/10, Epoch 839/1000, Training Loss (NLML): -961.1986\n",
      "convergence dfGPdfNN Run 8/10, Epoch 840/1000, Training Loss (NLML): -961.1936\n",
      "convergence dfGPdfNN Run 8/10, Epoch 841/1000, Training Loss (NLML): -961.1945\n",
      "convergence dfGPdfNN Run 8/10, Epoch 842/1000, Training Loss (NLML): -961.1904\n",
      "convergence dfGPdfNN Run 8/10, Epoch 843/1000, Training Loss (NLML): -961.1886\n",
      "convergence dfGPdfNN Run 8/10, Epoch 844/1000, Training Loss (NLML): -961.1908\n",
      "convergence dfGPdfNN Run 8/10, Epoch 845/1000, Training Loss (NLML): -961.2054\n",
      "convergence dfGPdfNN Run 8/10, Epoch 846/1000, Training Loss (NLML): -961.2063\n",
      "convergence dfGPdfNN Run 8/10, Epoch 847/1000, Training Loss (NLML): -961.1860\n",
      "convergence dfGPdfNN Run 8/10, Epoch 848/1000, Training Loss (NLML): -961.1875\n",
      "convergence dfGPdfNN Run 8/10, Epoch 849/1000, Training Loss (NLML): -961.1779\n",
      "convergence dfGPdfNN Run 8/10, Epoch 850/1000, Training Loss (NLML): -961.1818\n",
      "convergence dfGPdfNN Run 8/10, Epoch 851/1000, Training Loss (NLML): -961.1899\n",
      "convergence dfGPdfNN Run 8/10, Epoch 852/1000, Training Loss (NLML): -961.1901\n",
      "convergence dfGPdfNN Run 8/10, Epoch 853/1000, Training Loss (NLML): -961.1870\n",
      "convergence dfGPdfNN Run 8/10, Epoch 854/1000, Training Loss (NLML): -961.1897\n",
      "convergence dfGPdfNN Run 8/10, Epoch 855/1000, Training Loss (NLML): -961.1833\n",
      "convergence dfGPdfNN Run 8/10, Epoch 856/1000, Training Loss (NLML): -961.1887\n",
      "convergence dfGPdfNN Run 8/10, Epoch 857/1000, Training Loss (NLML): -961.1848\n",
      "convergence dfGPdfNN Run 8/10, Epoch 858/1000, Training Loss (NLML): -961.2034\n",
      "convergence dfGPdfNN Run 8/10, Epoch 859/1000, Training Loss (NLML): -961.1885\n",
      "convergence dfGPdfNN Run 8/10, Epoch 860/1000, Training Loss (NLML): -961.1930\n",
      "convergence dfGPdfNN Run 8/10, Epoch 861/1000, Training Loss (NLML): -961.1982\n",
      "convergence dfGPdfNN Run 8/10, Epoch 862/1000, Training Loss (NLML): -961.2196\n",
      "convergence dfGPdfNN Run 8/10, Epoch 863/1000, Training Loss (NLML): -961.1896\n",
      "convergence dfGPdfNN Run 8/10, Epoch 864/1000, Training Loss (NLML): -961.2173\n",
      "convergence dfGPdfNN Run 8/10, Epoch 865/1000, Training Loss (NLML): -961.2170\n",
      "convergence dfGPdfNN Run 8/10, Epoch 866/1000, Training Loss (NLML): -961.2313\n",
      "convergence dfGPdfNN Run 8/10, Epoch 867/1000, Training Loss (NLML): -961.2260\n",
      "convergence dfGPdfNN Run 8/10, Epoch 868/1000, Training Loss (NLML): -961.2389\n",
      "convergence dfGPdfNN Run 8/10, Epoch 869/1000, Training Loss (NLML): -961.2446\n",
      "convergence dfGPdfNN Run 8/10, Epoch 870/1000, Training Loss (NLML): -961.2411\n",
      "convergence dfGPdfNN Run 8/10, Epoch 871/1000, Training Loss (NLML): -961.2418\n",
      "convergence dfGPdfNN Run 8/10, Epoch 872/1000, Training Loss (NLML): -961.2458\n",
      "convergence dfGPdfNN Run 8/10, Epoch 873/1000, Training Loss (NLML): -961.2377\n",
      "convergence dfGPdfNN Run 8/10, Epoch 874/1000, Training Loss (NLML): -961.2437\n",
      "convergence dfGPdfNN Run 8/10, Epoch 875/1000, Training Loss (NLML): -961.2400\n",
      "convergence dfGPdfNN Run 8/10, Epoch 876/1000, Training Loss (NLML): -961.2288\n",
      "convergence dfGPdfNN Run 8/10, Epoch 877/1000, Training Loss (NLML): -961.2335\n",
      "convergence dfGPdfNN Run 8/10, Epoch 878/1000, Training Loss (NLML): -961.2408\n",
      "convergence dfGPdfNN Run 8/10, Epoch 879/1000, Training Loss (NLML): -961.2371\n",
      "convergence dfGPdfNN Run 8/10, Epoch 880/1000, Training Loss (NLML): -961.2432\n",
      "convergence dfGPdfNN Run 8/10, Epoch 881/1000, Training Loss (NLML): -961.2356\n",
      "convergence dfGPdfNN Run 8/10, Epoch 882/1000, Training Loss (NLML): -961.2432\n",
      "convergence dfGPdfNN Run 8/10, Epoch 883/1000, Training Loss (NLML): -961.2472\n",
      "convergence dfGPdfNN Run 8/10, Epoch 884/1000, Training Loss (NLML): -961.2509\n",
      "convergence dfGPdfNN Run 8/10, Epoch 885/1000, Training Loss (NLML): -961.2513\n",
      "convergence dfGPdfNN Run 8/10, Epoch 886/1000, Training Loss (NLML): -961.2593\n",
      "convergence dfGPdfNN Run 8/10, Epoch 887/1000, Training Loss (NLML): -961.2648\n",
      "convergence dfGPdfNN Run 8/10, Epoch 888/1000, Training Loss (NLML): -961.2708\n",
      "convergence dfGPdfNN Run 8/10, Epoch 889/1000, Training Loss (NLML): -961.2692\n",
      "convergence dfGPdfNN Run 8/10, Epoch 890/1000, Training Loss (NLML): -960.1447\n",
      "convergence dfGPdfNN Run 8/10, Epoch 891/1000, Training Loss (NLML): -961.2695\n",
      "convergence dfGPdfNN Run 8/10, Epoch 892/1000, Training Loss (NLML): -961.2861\n",
      "convergence dfGPdfNN Run 8/10, Epoch 893/1000, Training Loss (NLML): -961.3182\n",
      "convergence dfGPdfNN Run 8/10, Epoch 894/1000, Training Loss (NLML): -961.2969\n",
      "convergence dfGPdfNN Run 8/10, Epoch 895/1000, Training Loss (NLML): -961.2893\n",
      "convergence dfGPdfNN Run 8/10, Epoch 896/1000, Training Loss (NLML): -961.2949\n",
      "convergence dfGPdfNN Run 8/10, Epoch 897/1000, Training Loss (NLML): -961.3088\n",
      "convergence dfGPdfNN Run 8/10, Epoch 898/1000, Training Loss (NLML): -961.2814\n",
      "convergence dfGPdfNN Run 8/10, Epoch 899/1000, Training Loss (NLML): -961.2433\n",
      "convergence dfGPdfNN Run 8/10, Epoch 900/1000, Training Loss (NLML): -961.2589\n",
      "convergence dfGPdfNN Run 8/10, Epoch 901/1000, Training Loss (NLML): -961.2429\n",
      "convergence dfGPdfNN Run 8/10, Epoch 902/1000, Training Loss (NLML): -961.2238\n",
      "convergence dfGPdfNN Run 8/10, Epoch 903/1000, Training Loss (NLML): -961.2267\n",
      "convergence dfGPdfNN Run 8/10, Epoch 904/1000, Training Loss (NLML): -961.2395\n",
      "convergence dfGPdfNN Run 8/10, Epoch 905/1000, Training Loss (NLML): -961.2793\n",
      "convergence dfGPdfNN Run 8/10, Epoch 906/1000, Training Loss (NLML): -961.2729\n",
      "convergence dfGPdfNN Run 8/10, Epoch 907/1000, Training Loss (NLML): -961.2379\n",
      "convergence dfGPdfNN Run 8/10, Epoch 908/1000, Training Loss (NLML): -961.2141\n",
      "convergence dfGPdfNN Run 8/10, Epoch 909/1000, Training Loss (NLML): -961.2034\n",
      "convergence dfGPdfNN Run 8/10, Epoch 910/1000, Training Loss (NLML): -961.2157\n",
      "convergence dfGPdfNN Run 8/10, Epoch 911/1000, Training Loss (NLML): -961.2372\n",
      "convergence dfGPdfNN Run 8/10, Epoch 912/1000, Training Loss (NLML): -961.2345\n",
      "convergence dfGPdfNN Run 8/10, Epoch 913/1000, Training Loss (NLML): -961.2234\n",
      "convergence dfGPdfNN Run 8/10, Epoch 914/1000, Training Loss (NLML): -945.2922\n",
      "convergence dfGPdfNN Run 8/10, Epoch 915/1000, Training Loss (NLML): -961.0172\n",
      "convergence dfGPdfNN Run 8/10, Epoch 916/1000, Training Loss (NLML): -960.9396\n",
      "convergence dfGPdfNN Run 8/10, Epoch 917/1000, Training Loss (NLML): -960.9733\n",
      "convergence dfGPdfNN Run 8/10, Epoch 918/1000, Training Loss (NLML): -960.9099\n",
      "convergence dfGPdfNN Run 8/10, Epoch 919/1000, Training Loss (NLML): -960.8218\n",
      "convergence dfGPdfNN Run 8/10, Epoch 920/1000, Training Loss (NLML): -960.1385\n",
      "convergence dfGPdfNN Run 8/10, Epoch 921/1000, Training Loss (NLML): -960.2765\n",
      "convergence dfGPdfNN Run 8/10, Epoch 922/1000, Training Loss (NLML): -960.3748\n",
      "convergence dfGPdfNN Run 8/10, Epoch 923/1000, Training Loss (NLML): -960.4685\n",
      "convergence dfGPdfNN Run 8/10, Epoch 924/1000, Training Loss (NLML): -960.3497\n",
      "convergence dfGPdfNN Run 8/10, Epoch 925/1000, Training Loss (NLML): -960.5566\n",
      "convergence dfGPdfNN Run 8/10, Epoch 926/1000, Training Loss (NLML): -960.5642\n",
      "convergence dfGPdfNN Run 8/10, Epoch 927/1000, Training Loss (NLML): -960.5094\n",
      "convergence dfGPdfNN Run 8/10, Epoch 928/1000, Training Loss (NLML): -960.4313\n",
      "convergence dfGPdfNN Run 8/10, Epoch 929/1000, Training Loss (NLML): -960.4271\n",
      "convergence dfGPdfNN Run 8/10, Epoch 930/1000, Training Loss (NLML): -960.4886\n",
      "convergence dfGPdfNN Run 8/10, Epoch 931/1000, Training Loss (NLML): -960.6133\n",
      "convergence dfGPdfNN Run 8/10, Epoch 932/1000, Training Loss (NLML): -960.6963\n",
      "convergence dfGPdfNN Run 8/10, Epoch 933/1000, Training Loss (NLML): -960.7596\n",
      "convergence dfGPdfNN Run 8/10, Epoch 934/1000, Training Loss (NLML): -960.7900\n",
      "convergence dfGPdfNN Run 8/10, Epoch 935/1000, Training Loss (NLML): -960.7892\n",
      "convergence dfGPdfNN Run 8/10, Epoch 936/1000, Training Loss (NLML): -960.4631\n",
      "convergence dfGPdfNN Run 8/10, Epoch 937/1000, Training Loss (NLML): -960.7904\n",
      "convergence dfGPdfNN Run 8/10, Epoch 938/1000, Training Loss (NLML): -959.8354\n",
      "convergence dfGPdfNN Run 8/10, Epoch 939/1000, Training Loss (NLML): -959.0811\n",
      "convergence dfGPdfNN Run 8/10, Epoch 940/1000, Training Loss (NLML): -958.4193\n",
      "convergence dfGPdfNN Run 8/10, Epoch 941/1000, Training Loss (NLML): -959.7782\n",
      "convergence dfGPdfNN Run 8/10, Epoch 942/1000, Training Loss (NLML): -960.6954\n",
      "convergence dfGPdfNN Run 8/10, Epoch 943/1000, Training Loss (NLML): -960.6818\n",
      "Early stopping triggered after 943 epochs.\n",
      "\n",
      "--- Training Run 9/10 ---\n",
      "\n",
      "Start Training\n",
      "convergence dfGPdfNN Run 9/10, Epoch 1/1000, Training Loss (NLML): -850.6857\n",
      "convergence dfGPdfNN Run 9/10, Epoch 2/1000, Training Loss (NLML): -860.6410\n",
      "convergence dfGPdfNN Run 9/10, Epoch 3/1000, Training Loss (NLML): -870.0913\n",
      "convergence dfGPdfNN Run 9/10, Epoch 4/1000, Training Loss (NLML): -878.2495\n",
      "convergence dfGPdfNN Run 9/10, Epoch 5/1000, Training Loss (NLML): -881.9772\n",
      "convergence dfGPdfNN Run 9/10, Epoch 6/1000, Training Loss (NLML): -885.0468\n",
      "convergence dfGPdfNN Run 9/10, Epoch 7/1000, Training Loss (NLML): -887.4518\n",
      "convergence dfGPdfNN Run 9/10, Epoch 8/1000, Training Loss (NLML): -889.3878\n",
      "convergence dfGPdfNN Run 9/10, Epoch 9/1000, Training Loss (NLML): -891.2952\n",
      "convergence dfGPdfNN Run 9/10, Epoch 10/1000, Training Loss (NLML): -893.1926\n",
      "convergence dfGPdfNN Run 9/10, Epoch 11/1000, Training Loss (NLML): -895.1665\n",
      "convergence dfGPdfNN Run 9/10, Epoch 12/1000, Training Loss (NLML): -897.2554\n",
      "convergence dfGPdfNN Run 9/10, Epoch 13/1000, Training Loss (NLML): -899.1575\n",
      "convergence dfGPdfNN Run 9/10, Epoch 14/1000, Training Loss (NLML): -901.0878\n",
      "convergence dfGPdfNN Run 9/10, Epoch 15/1000, Training Loss (NLML): -902.8147\n",
      "convergence dfGPdfNN Run 9/10, Epoch 16/1000, Training Loss (NLML): -904.4141\n",
      "convergence dfGPdfNN Run 9/10, Epoch 17/1000, Training Loss (NLML): -905.9965\n",
      "convergence dfGPdfNN Run 9/10, Epoch 18/1000, Training Loss (NLML): -907.4084\n",
      "convergence dfGPdfNN Run 9/10, Epoch 19/1000, Training Loss (NLML): -908.7676\n",
      "convergence dfGPdfNN Run 9/10, Epoch 20/1000, Training Loss (NLML): -910.0321\n",
      "convergence dfGPdfNN Run 9/10, Epoch 21/1000, Training Loss (NLML): -911.2339\n",
      "convergence dfGPdfNN Run 9/10, Epoch 22/1000, Training Loss (NLML): -912.4205\n",
      "convergence dfGPdfNN Run 9/10, Epoch 23/1000, Training Loss (NLML): -913.5645\n",
      "convergence dfGPdfNN Run 9/10, Epoch 24/1000, Training Loss (NLML): -914.6560\n",
      "convergence dfGPdfNN Run 9/10, Epoch 25/1000, Training Loss (NLML): -915.6930\n",
      "convergence dfGPdfNN Run 9/10, Epoch 26/1000, Training Loss (NLML): -916.6946\n",
      "convergence dfGPdfNN Run 9/10, Epoch 27/1000, Training Loss (NLML): -917.6946\n",
      "convergence dfGPdfNN Run 9/10, Epoch 28/1000, Training Loss (NLML): -918.6454\n",
      "convergence dfGPdfNN Run 9/10, Epoch 29/1000, Training Loss (NLML): -919.5392\n",
      "convergence dfGPdfNN Run 9/10, Epoch 30/1000, Training Loss (NLML): -920.3745\n",
      "convergence dfGPdfNN Run 9/10, Epoch 31/1000, Training Loss (NLML): -921.3036\n",
      "convergence dfGPdfNN Run 9/10, Epoch 32/1000, Training Loss (NLML): -922.0952\n",
      "convergence dfGPdfNN Run 9/10, Epoch 33/1000, Training Loss (NLML): -922.9050\n",
      "convergence dfGPdfNN Run 9/10, Epoch 34/1000, Training Loss (NLML): -923.6725\n",
      "convergence dfGPdfNN Run 9/10, Epoch 35/1000, Training Loss (NLML): -924.3905\n",
      "convergence dfGPdfNN Run 9/10, Epoch 36/1000, Training Loss (NLML): -925.0985\n",
      "convergence dfGPdfNN Run 9/10, Epoch 37/1000, Training Loss (NLML): -925.7748\n",
      "convergence dfGPdfNN Run 9/10, Epoch 38/1000, Training Loss (NLML): -926.4310\n",
      "convergence dfGPdfNN Run 9/10, Epoch 39/1000, Training Loss (NLML): -927.0811\n",
      "convergence dfGPdfNN Run 9/10, Epoch 40/1000, Training Loss (NLML): -927.7135\n",
      "convergence dfGPdfNN Run 9/10, Epoch 41/1000, Training Loss (NLML): -928.2946\n",
      "convergence dfGPdfNN Run 9/10, Epoch 42/1000, Training Loss (NLML): -928.8643\n",
      "convergence dfGPdfNN Run 9/10, Epoch 43/1000, Training Loss (NLML): -929.4240\n",
      "convergence dfGPdfNN Run 9/10, Epoch 44/1000, Training Loss (NLML): -929.9557\n",
      "convergence dfGPdfNN Run 9/10, Epoch 45/1000, Training Loss (NLML): -930.4835\n",
      "convergence dfGPdfNN Run 9/10, Epoch 46/1000, Training Loss (NLML): -930.9402\n",
      "convergence dfGPdfNN Run 9/10, Epoch 47/1000, Training Loss (NLML): -931.3898\n",
      "convergence dfGPdfNN Run 9/10, Epoch 48/1000, Training Loss (NLML): -931.9249\n",
      "convergence dfGPdfNN Run 9/10, Epoch 49/1000, Training Loss (NLML): -932.4686\n",
      "convergence dfGPdfNN Run 9/10, Epoch 50/1000, Training Loss (NLML): -932.9667\n",
      "convergence dfGPdfNN Run 9/10, Epoch 51/1000, Training Loss (NLML): -933.4347\n",
      "convergence dfGPdfNN Run 9/10, Epoch 52/1000, Training Loss (NLML): -933.8823\n",
      "convergence dfGPdfNN Run 9/10, Epoch 53/1000, Training Loss (NLML): -934.3341\n",
      "convergence dfGPdfNN Run 9/10, Epoch 54/1000, Training Loss (NLML): -934.7828\n",
      "convergence dfGPdfNN Run 9/10, Epoch 55/1000, Training Loss (NLML): -935.1658\n",
      "convergence dfGPdfNN Run 9/10, Epoch 56/1000, Training Loss (NLML): -935.5955\n",
      "convergence dfGPdfNN Run 9/10, Epoch 57/1000, Training Loss (NLML): -935.9928\n",
      "convergence dfGPdfNN Run 9/10, Epoch 58/1000, Training Loss (NLML): -936.4031\n",
      "convergence dfGPdfNN Run 9/10, Epoch 59/1000, Training Loss (NLML): -936.5995\n",
      "convergence dfGPdfNN Run 9/10, Epoch 60/1000, Training Loss (NLML): -936.8430\n",
      "convergence dfGPdfNN Run 9/10, Epoch 61/1000, Training Loss (NLML): -936.9323\n",
      "convergence dfGPdfNN Run 9/10, Epoch 62/1000, Training Loss (NLML): -937.5284\n",
      "convergence dfGPdfNN Run 9/10, Epoch 63/1000, Training Loss (NLML): -937.3508\n",
      "convergence dfGPdfNN Run 9/10, Epoch 64/1000, Training Loss (NLML): -937.9684\n",
      "convergence dfGPdfNN Run 9/10, Epoch 65/1000, Training Loss (NLML): -938.4236\n",
      "convergence dfGPdfNN Run 9/10, Epoch 66/1000, Training Loss (NLML): -938.8243\n",
      "convergence dfGPdfNN Run 9/10, Epoch 67/1000, Training Loss (NLML): -939.1493\n",
      "convergence dfGPdfNN Run 9/10, Epoch 68/1000, Training Loss (NLML): -939.5244\n",
      "convergence dfGPdfNN Run 9/10, Epoch 69/1000, Training Loss (NLML): -939.9150\n",
      "convergence dfGPdfNN Run 9/10, Epoch 70/1000, Training Loss (NLML): -940.2614\n",
      "convergence dfGPdfNN Run 9/10, Epoch 71/1000, Training Loss (NLML): -940.5912\n",
      "convergence dfGPdfNN Run 9/10, Epoch 72/1000, Training Loss (NLML): -940.8945\n",
      "convergence dfGPdfNN Run 9/10, Epoch 73/1000, Training Loss (NLML): -941.1846\n",
      "convergence dfGPdfNN Run 9/10, Epoch 74/1000, Training Loss (NLML): -941.4771\n",
      "convergence dfGPdfNN Run 9/10, Epoch 75/1000, Training Loss (NLML): -941.7664\n",
      "convergence dfGPdfNN Run 9/10, Epoch 76/1000, Training Loss (NLML): -942.0344\n",
      "convergence dfGPdfNN Run 9/10, Epoch 77/1000, Training Loss (NLML): -942.2986\n",
      "convergence dfGPdfNN Run 9/10, Epoch 78/1000, Training Loss (NLML): -942.5631\n",
      "convergence dfGPdfNN Run 9/10, Epoch 79/1000, Training Loss (NLML): -942.8350\n",
      "convergence dfGPdfNN Run 9/10, Epoch 80/1000, Training Loss (NLML): -943.0934\n",
      "convergence dfGPdfNN Run 9/10, Epoch 81/1000, Training Loss (NLML): -943.3425\n",
      "convergence dfGPdfNN Run 9/10, Epoch 82/1000, Training Loss (NLML): -943.6000\n",
      "convergence dfGPdfNN Run 9/10, Epoch 83/1000, Training Loss (NLML): -943.8353\n",
      "convergence dfGPdfNN Run 9/10, Epoch 84/1000, Training Loss (NLML): -944.0742\n",
      "convergence dfGPdfNN Run 9/10, Epoch 85/1000, Training Loss (NLML): -944.2937\n",
      "convergence dfGPdfNN Run 9/10, Epoch 86/1000, Training Loss (NLML): -944.5531\n",
      "convergence dfGPdfNN Run 9/10, Epoch 87/1000, Training Loss (NLML): -944.7876\n",
      "convergence dfGPdfNN Run 9/10, Epoch 88/1000, Training Loss (NLML): -945.0281\n",
      "convergence dfGPdfNN Run 9/10, Epoch 89/1000, Training Loss (NLML): -945.2645\n",
      "convergence dfGPdfNN Run 9/10, Epoch 90/1000, Training Loss (NLML): -945.4995\n",
      "convergence dfGPdfNN Run 9/10, Epoch 91/1000, Training Loss (NLML): -945.7318\n",
      "convergence dfGPdfNN Run 9/10, Epoch 92/1000, Training Loss (NLML): -945.9598\n",
      "convergence dfGPdfNN Run 9/10, Epoch 93/1000, Training Loss (NLML): -946.1816\n",
      "convergence dfGPdfNN Run 9/10, Epoch 94/1000, Training Loss (NLML): -946.4299\n",
      "convergence dfGPdfNN Run 9/10, Epoch 95/1000, Training Loss (NLML): -946.6479\n",
      "convergence dfGPdfNN Run 9/10, Epoch 96/1000, Training Loss (NLML): -946.8506\n",
      "convergence dfGPdfNN Run 9/10, Epoch 97/1000, Training Loss (NLML): -947.0415\n",
      "convergence dfGPdfNN Run 9/10, Epoch 98/1000, Training Loss (NLML): -947.2460\n",
      "convergence dfGPdfNN Run 9/10, Epoch 99/1000, Training Loss (NLML): -947.4701\n",
      "convergence dfGPdfNN Run 9/10, Epoch 100/1000, Training Loss (NLML): -947.6537\n",
      "convergence dfGPdfNN Run 9/10, Epoch 101/1000, Training Loss (NLML): -947.7634\n",
      "convergence dfGPdfNN Run 9/10, Epoch 102/1000, Training Loss (NLML): -947.9348\n",
      "convergence dfGPdfNN Run 9/10, Epoch 103/1000, Training Loss (NLML): -948.1499\n",
      "convergence dfGPdfNN Run 9/10, Epoch 104/1000, Training Loss (NLML): -948.3815\n",
      "convergence dfGPdfNN Run 9/10, Epoch 105/1000, Training Loss (NLML): -944.5042\n",
      "convergence dfGPdfNN Run 9/10, Epoch 106/1000, Training Loss (NLML): -945.9545\n",
      "convergence dfGPdfNN Run 9/10, Epoch 107/1000, Training Loss (NLML): -946.4425\n",
      "convergence dfGPdfNN Run 9/10, Epoch 108/1000, Training Loss (NLML): -942.5265\n",
      "convergence dfGPdfNN Run 9/10, Epoch 109/1000, Training Loss (NLML): -948.7656\n",
      "convergence dfGPdfNN Run 9/10, Epoch 110/1000, Training Loss (NLML): -948.8314\n",
      "convergence dfGPdfNN Run 9/10, Epoch 111/1000, Training Loss (NLML): -948.8103\n",
      "convergence dfGPdfNN Run 9/10, Epoch 112/1000, Training Loss (NLML): -948.8760\n",
      "convergence dfGPdfNN Run 9/10, Epoch 113/1000, Training Loss (NLML): -948.9857\n",
      "convergence dfGPdfNN Run 9/10, Epoch 114/1000, Training Loss (NLML): -949.1541\n",
      "convergence dfGPdfNN Run 9/10, Epoch 115/1000, Training Loss (NLML): -949.5374\n",
      "convergence dfGPdfNN Run 9/10, Epoch 116/1000, Training Loss (NLML): -949.6887\n",
      "convergence dfGPdfNN Run 9/10, Epoch 117/1000, Training Loss (NLML): -949.8663\n",
      "convergence dfGPdfNN Run 9/10, Epoch 118/1000, Training Loss (NLML): -950.1327\n",
      "convergence dfGPdfNN Run 9/10, Epoch 119/1000, Training Loss (NLML): -950.2354\n",
      "convergence dfGPdfNN Run 9/10, Epoch 120/1000, Training Loss (NLML): -950.0959\n",
      "convergence dfGPdfNN Run 9/10, Epoch 121/1000, Training Loss (NLML): -950.2251\n",
      "convergence dfGPdfNN Run 9/10, Epoch 122/1000, Training Loss (NLML): -950.3478\n",
      "convergence dfGPdfNN Run 9/10, Epoch 123/1000, Training Loss (NLML): -950.4946\n",
      "convergence dfGPdfNN Run 9/10, Epoch 124/1000, Training Loss (NLML): -950.6324\n",
      "convergence dfGPdfNN Run 9/10, Epoch 125/1000, Training Loss (NLML): -950.9204\n",
      "convergence dfGPdfNN Run 9/10, Epoch 126/1000, Training Loss (NLML): -951.1703\n",
      "convergence dfGPdfNN Run 9/10, Epoch 127/1000, Training Loss (NLML): -951.3806\n",
      "convergence dfGPdfNN Run 9/10, Epoch 128/1000, Training Loss (NLML): -951.5966\n",
      "convergence dfGPdfNN Run 9/10, Epoch 129/1000, Training Loss (NLML): -940.1459\n",
      "convergence dfGPdfNN Run 9/10, Epoch 130/1000, Training Loss (NLML): -951.8530\n",
      "convergence dfGPdfNN Run 9/10, Epoch 131/1000, Training Loss (NLML): -951.9501\n",
      "convergence dfGPdfNN Run 9/10, Epoch 132/1000, Training Loss (NLML): -952.1172\n",
      "convergence dfGPdfNN Run 9/10, Epoch 133/1000, Training Loss (NLML): -952.1997\n",
      "convergence dfGPdfNN Run 9/10, Epoch 134/1000, Training Loss (NLML): -952.3286\n",
      "convergence dfGPdfNN Run 9/10, Epoch 135/1000, Training Loss (NLML): -952.4633\n",
      "convergence dfGPdfNN Run 9/10, Epoch 136/1000, Training Loss (NLML): -952.5327\n",
      "convergence dfGPdfNN Run 9/10, Epoch 137/1000, Training Loss (NLML): -952.7529\n",
      "convergence dfGPdfNN Run 9/10, Epoch 138/1000, Training Loss (NLML): -952.9037\n",
      "convergence dfGPdfNN Run 9/10, Epoch 139/1000, Training Loss (NLML): -953.0250\n",
      "convergence dfGPdfNN Run 9/10, Epoch 140/1000, Training Loss (NLML): -953.0607\n",
      "convergence dfGPdfNN Run 9/10, Epoch 141/1000, Training Loss (NLML): -951.1669\n",
      "convergence dfGPdfNN Run 9/10, Epoch 142/1000, Training Loss (NLML): -953.4463\n",
      "convergence dfGPdfNN Run 9/10, Epoch 143/1000, Training Loss (NLML): -953.5563\n",
      "convergence dfGPdfNN Run 9/10, Epoch 144/1000, Training Loss (NLML): -953.5615\n",
      "convergence dfGPdfNN Run 9/10, Epoch 145/1000, Training Loss (NLML): -953.5479\n",
      "convergence dfGPdfNN Run 9/10, Epoch 146/1000, Training Loss (NLML): -950.8877\n",
      "convergence dfGPdfNN Run 9/10, Epoch 147/1000, Training Loss (NLML): -953.6888\n",
      "convergence dfGPdfNN Run 9/10, Epoch 148/1000, Training Loss (NLML): -953.7701\n",
      "convergence dfGPdfNN Run 9/10, Epoch 149/1000, Training Loss (NLML): -953.8186\n",
      "convergence dfGPdfNN Run 9/10, Epoch 150/1000, Training Loss (NLML): -953.9119\n",
      "convergence dfGPdfNN Run 9/10, Epoch 151/1000, Training Loss (NLML): -953.9974\n",
      "convergence dfGPdfNN Run 9/10, Epoch 152/1000, Training Loss (NLML): -954.1324\n",
      "convergence dfGPdfNN Run 9/10, Epoch 153/1000, Training Loss (NLML): -954.2253\n",
      "convergence dfGPdfNN Run 9/10, Epoch 154/1000, Training Loss (NLML): -954.2881\n",
      "convergence dfGPdfNN Run 9/10, Epoch 155/1000, Training Loss (NLML): -904.0782\n",
      "convergence dfGPdfNN Run 9/10, Epoch 156/1000, Training Loss (NLML): -954.3448\n",
      "convergence dfGPdfNN Run 9/10, Epoch 157/1000, Training Loss (NLML): -954.4119\n",
      "convergence dfGPdfNN Run 9/10, Epoch 158/1000, Training Loss (NLML): -954.3544\n",
      "convergence dfGPdfNN Run 9/10, Epoch 159/1000, Training Loss (NLML): -954.1349\n",
      "convergence dfGPdfNN Run 9/10, Epoch 160/1000, Training Loss (NLML): -953.9514\n",
      "convergence dfGPdfNN Run 9/10, Epoch 161/1000, Training Loss (NLML): -953.8346\n",
      "convergence dfGPdfNN Run 9/10, Epoch 162/1000, Training Loss (NLML): -953.9199\n",
      "convergence dfGPdfNN Run 9/10, Epoch 163/1000, Training Loss (NLML): -953.9586\n",
      "convergence dfGPdfNN Run 9/10, Epoch 164/1000, Training Loss (NLML): -953.9806\n",
      "convergence dfGPdfNN Run 9/10, Epoch 165/1000, Training Loss (NLML): -953.8030\n",
      "convergence dfGPdfNN Run 9/10, Epoch 166/1000, Training Loss (NLML): -953.9205\n",
      "convergence dfGPdfNN Run 9/10, Epoch 167/1000, Training Loss (NLML): -954.0505\n",
      "convergence dfGPdfNN Run 9/10, Epoch 168/1000, Training Loss (NLML): -953.7645\n",
      "convergence dfGPdfNN Run 9/10, Epoch 169/1000, Training Loss (NLML): -953.7939\n",
      "convergence dfGPdfNN Run 9/10, Epoch 170/1000, Training Loss (NLML): -954.0137\n",
      "convergence dfGPdfNN Run 9/10, Epoch 171/1000, Training Loss (NLML): -953.8586\n",
      "convergence dfGPdfNN Run 9/10, Epoch 172/1000, Training Loss (NLML): -953.7531\n",
      "convergence dfGPdfNN Run 9/10, Epoch 173/1000, Training Loss (NLML): -953.9072\n",
      "convergence dfGPdfNN Run 9/10, Epoch 174/1000, Training Loss (NLML): -953.9617\n",
      "convergence dfGPdfNN Run 9/10, Epoch 175/1000, Training Loss (NLML): -954.1725\n",
      "convergence dfGPdfNN Run 9/10, Epoch 176/1000, Training Loss (NLML): -954.2853\n",
      "convergence dfGPdfNN Run 9/10, Epoch 177/1000, Training Loss (NLML): -954.4453\n",
      "convergence dfGPdfNN Run 9/10, Epoch 178/1000, Training Loss (NLML): -954.4437\n",
      "convergence dfGPdfNN Run 9/10, Epoch 179/1000, Training Loss (NLML): -954.2516\n",
      "convergence dfGPdfNN Run 9/10, Epoch 180/1000, Training Loss (NLML): -953.7390\n",
      "convergence dfGPdfNN Run 9/10, Epoch 181/1000, Training Loss (NLML): -953.4261\n",
      "convergence dfGPdfNN Run 9/10, Epoch 182/1000, Training Loss (NLML): -954.1543\n",
      "convergence dfGPdfNN Run 9/10, Epoch 183/1000, Training Loss (NLML): -954.2911\n",
      "convergence dfGPdfNN Run 9/10, Epoch 184/1000, Training Loss (NLML): -954.5483\n",
      "convergence dfGPdfNN Run 9/10, Epoch 185/1000, Training Loss (NLML): -954.2379\n",
      "convergence dfGPdfNN Run 9/10, Epoch 186/1000, Training Loss (NLML): -954.4923\n",
      "convergence dfGPdfNN Run 9/10, Epoch 187/1000, Training Loss (NLML): -954.7410\n",
      "convergence dfGPdfNN Run 9/10, Epoch 188/1000, Training Loss (NLML): -954.6991\n",
      "convergence dfGPdfNN Run 9/10, Epoch 189/1000, Training Loss (NLML): -954.8123\n",
      "convergence dfGPdfNN Run 9/10, Epoch 190/1000, Training Loss (NLML): -954.9828\n",
      "convergence dfGPdfNN Run 9/10, Epoch 191/1000, Training Loss (NLML): -955.0542\n",
      "convergence dfGPdfNN Run 9/10, Epoch 192/1000, Training Loss (NLML): -954.6221\n",
      "convergence dfGPdfNN Run 9/10, Epoch 193/1000, Training Loss (NLML): -954.7781\n",
      "convergence dfGPdfNN Run 9/10, Epoch 194/1000, Training Loss (NLML): -954.9348\n",
      "convergence dfGPdfNN Run 9/10, Epoch 195/1000, Training Loss (NLML): -954.7853\n",
      "convergence dfGPdfNN Run 9/10, Epoch 196/1000, Training Loss (NLML): -954.9485\n",
      "convergence dfGPdfNN Run 9/10, Epoch 197/1000, Training Loss (NLML): -955.1798\n",
      "convergence dfGPdfNN Run 9/10, Epoch 198/1000, Training Loss (NLML): -955.2421\n",
      "convergence dfGPdfNN Run 9/10, Epoch 199/1000, Training Loss (NLML): -955.2805\n",
      "convergence dfGPdfNN Run 9/10, Epoch 200/1000, Training Loss (NLML): -955.0675\n",
      "convergence dfGPdfNN Run 9/10, Epoch 201/1000, Training Loss (NLML): -954.7976\n",
      "convergence dfGPdfNN Run 9/10, Epoch 202/1000, Training Loss (NLML): -955.0298\n",
      "convergence dfGPdfNN Run 9/10, Epoch 203/1000, Training Loss (NLML): -955.3765\n",
      "convergence dfGPdfNN Run 9/10, Epoch 204/1000, Training Loss (NLML): -955.6080\n",
      "convergence dfGPdfNN Run 9/10, Epoch 205/1000, Training Loss (NLML): -955.6471\n",
      "convergence dfGPdfNN Run 9/10, Epoch 206/1000, Training Loss (NLML): -954.4723\n",
      "convergence dfGPdfNN Run 9/10, Epoch 207/1000, Training Loss (NLML): -954.8762\n",
      "convergence dfGPdfNN Run 9/10, Epoch 208/1000, Training Loss (NLML): -954.7511\n",
      "convergence dfGPdfNN Run 9/10, Epoch 209/1000, Training Loss (NLML): -937.9642\n",
      "convergence dfGPdfNN Run 9/10, Epoch 210/1000, Training Loss (NLML): -942.7351\n",
      "convergence dfGPdfNN Run 9/10, Epoch 211/1000, Training Loss (NLML): -954.5840\n",
      "convergence dfGPdfNN Run 9/10, Epoch 212/1000, Training Loss (NLML): -954.8497\n",
      "convergence dfGPdfNN Run 9/10, Epoch 213/1000, Training Loss (NLML): -955.2391\n",
      "convergence dfGPdfNN Run 9/10, Epoch 214/1000, Training Loss (NLML): -955.5873\n",
      "convergence dfGPdfNN Run 9/10, Epoch 215/1000, Training Loss (NLML): -956.0186\n",
      "convergence dfGPdfNN Run 9/10, Epoch 216/1000, Training Loss (NLML): -955.9324\n",
      "convergence dfGPdfNN Run 9/10, Epoch 217/1000, Training Loss (NLML): -955.3265\n",
      "convergence dfGPdfNN Run 9/10, Epoch 218/1000, Training Loss (NLML): -954.5758\n",
      "convergence dfGPdfNN Run 9/10, Epoch 219/1000, Training Loss (NLML): -954.4747\n",
      "convergence dfGPdfNN Run 9/10, Epoch 220/1000, Training Loss (NLML): -954.6373\n",
      "convergence dfGPdfNN Run 9/10, Epoch 221/1000, Training Loss (NLML): -955.5448\n",
      "convergence dfGPdfNN Run 9/10, Epoch 222/1000, Training Loss (NLML): -954.8046\n",
      "convergence dfGPdfNN Run 9/10, Epoch 223/1000, Training Loss (NLML): -954.8835\n",
      "convergence dfGPdfNN Run 9/10, Epoch 224/1000, Training Loss (NLML): -954.7888\n",
      "convergence dfGPdfNN Run 9/10, Epoch 225/1000, Training Loss (NLML): -954.8339\n",
      "convergence dfGPdfNN Run 9/10, Epoch 226/1000, Training Loss (NLML): -954.9321\n",
      "convergence dfGPdfNN Run 9/10, Epoch 227/1000, Training Loss (NLML): -955.0685\n",
      "convergence dfGPdfNN Run 9/10, Epoch 228/1000, Training Loss (NLML): -955.1167\n",
      "convergence dfGPdfNN Run 9/10, Epoch 229/1000, Training Loss (NLML): -955.1842\n",
      "convergence dfGPdfNN Run 9/10, Epoch 230/1000, Training Loss (NLML): -955.1349\n",
      "convergence dfGPdfNN Run 9/10, Epoch 231/1000, Training Loss (NLML): -954.5153\n",
      "convergence dfGPdfNN Run 9/10, Epoch 232/1000, Training Loss (NLML): -954.5040\n",
      "convergence dfGPdfNN Run 9/10, Epoch 233/1000, Training Loss (NLML): -955.0986\n",
      "convergence dfGPdfNN Run 9/10, Epoch 234/1000, Training Loss (NLML): -955.5450\n",
      "convergence dfGPdfNN Run 9/10, Epoch 235/1000, Training Loss (NLML): -955.6692\n",
      "convergence dfGPdfNN Run 9/10, Epoch 236/1000, Training Loss (NLML): -955.3097\n",
      "convergence dfGPdfNN Run 9/10, Epoch 237/1000, Training Loss (NLML): -955.4343\n",
      "convergence dfGPdfNN Run 9/10, Epoch 238/1000, Training Loss (NLML): -954.7449\n",
      "convergence dfGPdfNN Run 9/10, Epoch 239/1000, Training Loss (NLML): -954.8488\n",
      "convergence dfGPdfNN Run 9/10, Epoch 240/1000, Training Loss (NLML): -955.8107\n",
      "convergence dfGPdfNN Run 9/10, Epoch 241/1000, Training Loss (NLML): -955.9644\n",
      "convergence dfGPdfNN Run 9/10, Epoch 242/1000, Training Loss (NLML): -956.1259\n",
      "convergence dfGPdfNN Run 9/10, Epoch 243/1000, Training Loss (NLML): -956.2137\n",
      "convergence dfGPdfNN Run 9/10, Epoch 244/1000, Training Loss (NLML): -956.1550\n",
      "convergence dfGPdfNN Run 9/10, Epoch 245/1000, Training Loss (NLML): -955.6122\n",
      "convergence dfGPdfNN Run 9/10, Epoch 246/1000, Training Loss (NLML): -955.6790\n",
      "convergence dfGPdfNN Run 9/10, Epoch 247/1000, Training Loss (NLML): -956.1964\n",
      "convergence dfGPdfNN Run 9/10, Epoch 248/1000, Training Loss (NLML): -956.2780\n",
      "convergence dfGPdfNN Run 9/10, Epoch 249/1000, Training Loss (NLML): -956.3872\n",
      "convergence dfGPdfNN Run 9/10, Epoch 250/1000, Training Loss (NLML): -956.5074\n",
      "convergence dfGPdfNN Run 9/10, Epoch 251/1000, Training Loss (NLML): -956.6409\n",
      "convergence dfGPdfNN Run 9/10, Epoch 252/1000, Training Loss (NLML): -956.4773\n",
      "convergence dfGPdfNN Run 9/10, Epoch 253/1000, Training Loss (NLML): -956.5402\n",
      "convergence dfGPdfNN Run 9/10, Epoch 254/1000, Training Loss (NLML): -956.5391\n",
      "convergence dfGPdfNN Run 9/10, Epoch 255/1000, Training Loss (NLML): -956.6685\n",
      "convergence dfGPdfNN Run 9/10, Epoch 256/1000, Training Loss (NLML): -956.6851\n",
      "convergence dfGPdfNN Run 9/10, Epoch 257/1000, Training Loss (NLML): -956.6606\n",
      "convergence dfGPdfNN Run 9/10, Epoch 258/1000, Training Loss (NLML): -956.7102\n",
      "convergence dfGPdfNN Run 9/10, Epoch 259/1000, Training Loss (NLML): -956.7382\n",
      "convergence dfGPdfNN Run 9/10, Epoch 260/1000, Training Loss (NLML): -956.8409\n",
      "convergence dfGPdfNN Run 9/10, Epoch 261/1000, Training Loss (NLML): -956.8964\n",
      "convergence dfGPdfNN Run 9/10, Epoch 262/1000, Training Loss (NLML): -956.8820\n",
      "convergence dfGPdfNN Run 9/10, Epoch 263/1000, Training Loss (NLML): -956.9202\n",
      "convergence dfGPdfNN Run 9/10, Epoch 264/1000, Training Loss (NLML): -956.9147\n",
      "convergence dfGPdfNN Run 9/10, Epoch 265/1000, Training Loss (NLML): -956.9343\n",
      "convergence dfGPdfNN Run 9/10, Epoch 266/1000, Training Loss (NLML): -957.0629\n",
      "convergence dfGPdfNN Run 9/10, Epoch 267/1000, Training Loss (NLML): -957.1606\n",
      "convergence dfGPdfNN Run 9/10, Epoch 268/1000, Training Loss (NLML): -957.1100\n",
      "convergence dfGPdfNN Run 9/10, Epoch 269/1000, Training Loss (NLML): -957.0615\n",
      "convergence dfGPdfNN Run 9/10, Epoch 270/1000, Training Loss (NLML): -957.1354\n",
      "convergence dfGPdfNN Run 9/10, Epoch 271/1000, Training Loss (NLML): -957.1211\n",
      "convergence dfGPdfNN Run 9/10, Epoch 272/1000, Training Loss (NLML): -957.1478\n",
      "convergence dfGPdfNN Run 9/10, Epoch 273/1000, Training Loss (NLML): -957.1716\n",
      "convergence dfGPdfNN Run 9/10, Epoch 274/1000, Training Loss (NLML): -957.2489\n",
      "convergence dfGPdfNN Run 9/10, Epoch 275/1000, Training Loss (NLML): -957.2850\n",
      "convergence dfGPdfNN Run 9/10, Epoch 276/1000, Training Loss (NLML): -957.3342\n",
      "convergence dfGPdfNN Run 9/10, Epoch 277/1000, Training Loss (NLML): -957.3547\n",
      "convergence dfGPdfNN Run 9/10, Epoch 278/1000, Training Loss (NLML): -957.3948\n",
      "convergence dfGPdfNN Run 9/10, Epoch 279/1000, Training Loss (NLML): -957.4255\n",
      "convergence dfGPdfNN Run 9/10, Epoch 280/1000, Training Loss (NLML): -957.4221\n",
      "convergence dfGPdfNN Run 9/10, Epoch 281/1000, Training Loss (NLML): -957.4457\n",
      "convergence dfGPdfNN Run 9/10, Epoch 282/1000, Training Loss (NLML): -957.4385\n",
      "convergence dfGPdfNN Run 9/10, Epoch 283/1000, Training Loss (NLML): -957.4598\n",
      "convergence dfGPdfNN Run 9/10, Epoch 284/1000, Training Loss (NLML): -957.4065\n",
      "convergence dfGPdfNN Run 9/10, Epoch 285/1000, Training Loss (NLML): -957.3939\n",
      "convergence dfGPdfNN Run 9/10, Epoch 286/1000, Training Loss (NLML): -957.4332\n",
      "convergence dfGPdfNN Run 9/10, Epoch 287/1000, Training Loss (NLML): -957.4497\n",
      "convergence dfGPdfNN Run 9/10, Epoch 288/1000, Training Loss (NLML): -957.4280\n",
      "convergence dfGPdfNN Run 9/10, Epoch 289/1000, Training Loss (NLML): -957.4143\n",
      "convergence dfGPdfNN Run 9/10, Epoch 290/1000, Training Loss (NLML): -957.4218\n",
      "convergence dfGPdfNN Run 9/10, Epoch 291/1000, Training Loss (NLML): -957.4867\n",
      "convergence dfGPdfNN Run 9/10, Epoch 292/1000, Training Loss (NLML): -957.4945\n",
      "convergence dfGPdfNN Run 9/10, Epoch 293/1000, Training Loss (NLML): -957.5377\n",
      "convergence dfGPdfNN Run 9/10, Epoch 294/1000, Training Loss (NLML): -957.5721\n",
      "convergence dfGPdfNN Run 9/10, Epoch 295/1000, Training Loss (NLML): -957.5104\n",
      "convergence dfGPdfNN Run 9/10, Epoch 296/1000, Training Loss (NLML): -957.5387\n",
      "convergence dfGPdfNN Run 9/10, Epoch 297/1000, Training Loss (NLML): -957.5345\n",
      "convergence dfGPdfNN Run 9/10, Epoch 298/1000, Training Loss (NLML): -957.5503\n",
      "convergence dfGPdfNN Run 9/10, Epoch 299/1000, Training Loss (NLML): -957.5834\n",
      "convergence dfGPdfNN Run 9/10, Epoch 300/1000, Training Loss (NLML): -957.3060\n",
      "convergence dfGPdfNN Run 9/10, Epoch 301/1000, Training Loss (NLML): -957.7220\n",
      "convergence dfGPdfNN Run 9/10, Epoch 302/1000, Training Loss (NLML): -957.7213\n",
      "convergence dfGPdfNN Run 9/10, Epoch 303/1000, Training Loss (NLML): -957.7235\n",
      "convergence dfGPdfNN Run 9/10, Epoch 304/1000, Training Loss (NLML): -957.7596\n",
      "convergence dfGPdfNN Run 9/10, Epoch 305/1000, Training Loss (NLML): -957.7991\n",
      "convergence dfGPdfNN Run 9/10, Epoch 306/1000, Training Loss (NLML): -957.7893\n",
      "convergence dfGPdfNN Run 9/10, Epoch 307/1000, Training Loss (NLML): -957.8223\n",
      "convergence dfGPdfNN Run 9/10, Epoch 308/1000, Training Loss (NLML): -957.8572\n",
      "convergence dfGPdfNN Run 9/10, Epoch 309/1000, Training Loss (NLML): -957.8280\n",
      "convergence dfGPdfNN Run 9/10, Epoch 310/1000, Training Loss (NLML): -957.8141\n",
      "convergence dfGPdfNN Run 9/10, Epoch 311/1000, Training Loss (NLML): -957.7955\n",
      "convergence dfGPdfNN Run 9/10, Epoch 312/1000, Training Loss (NLML): -957.7489\n",
      "convergence dfGPdfNN Run 9/10, Epoch 313/1000, Training Loss (NLML): -957.7550\n",
      "convergence dfGPdfNN Run 9/10, Epoch 314/1000, Training Loss (NLML): -957.7435\n",
      "convergence dfGPdfNN Run 9/10, Epoch 315/1000, Training Loss (NLML): -957.7817\n",
      "convergence dfGPdfNN Run 9/10, Epoch 316/1000, Training Loss (NLML): -957.8169\n",
      "convergence dfGPdfNN Run 9/10, Epoch 317/1000, Training Loss (NLML): -957.7987\n",
      "convergence dfGPdfNN Run 9/10, Epoch 318/1000, Training Loss (NLML): -957.8306\n",
      "convergence dfGPdfNN Run 9/10, Epoch 319/1000, Training Loss (NLML): -957.8147\n",
      "convergence dfGPdfNN Run 9/10, Epoch 320/1000, Training Loss (NLML): -957.8436\n",
      "convergence dfGPdfNN Run 9/10, Epoch 321/1000, Training Loss (NLML): -957.8037\n",
      "convergence dfGPdfNN Run 9/10, Epoch 322/1000, Training Loss (NLML): -957.8202\n",
      "convergence dfGPdfNN Run 9/10, Epoch 323/1000, Training Loss (NLML): -957.8376\n",
      "convergence dfGPdfNN Run 9/10, Epoch 324/1000, Training Loss (NLML): -957.8474\n",
      "convergence dfGPdfNN Run 9/10, Epoch 325/1000, Training Loss (NLML): -957.8749\n",
      "convergence dfGPdfNN Run 9/10, Epoch 326/1000, Training Loss (NLML): -957.9135\n",
      "convergence dfGPdfNN Run 9/10, Epoch 327/1000, Training Loss (NLML): -957.9763\n",
      "convergence dfGPdfNN Run 9/10, Epoch 328/1000, Training Loss (NLML): -957.9805\n",
      "convergence dfGPdfNN Run 9/10, Epoch 329/1000, Training Loss (NLML): -957.9850\n",
      "convergence dfGPdfNN Run 9/10, Epoch 330/1000, Training Loss (NLML): -958.0082\n",
      "convergence dfGPdfNN Run 9/10, Epoch 331/1000, Training Loss (NLML): -958.0927\n",
      "convergence dfGPdfNN Run 9/10, Epoch 332/1000, Training Loss (NLML): -958.1110\n",
      "convergence dfGPdfNN Run 9/10, Epoch 333/1000, Training Loss (NLML): -958.1134\n",
      "convergence dfGPdfNN Run 9/10, Epoch 334/1000, Training Loss (NLML): -958.1168\n",
      "convergence dfGPdfNN Run 9/10, Epoch 335/1000, Training Loss (NLML): -958.0785\n",
      "convergence dfGPdfNN Run 9/10, Epoch 336/1000, Training Loss (NLML): -958.1041\n",
      "convergence dfGPdfNN Run 9/10, Epoch 337/1000, Training Loss (NLML): -958.1185\n",
      "convergence dfGPdfNN Run 9/10, Epoch 338/1000, Training Loss (NLML): -958.1172\n",
      "convergence dfGPdfNN Run 9/10, Epoch 339/1000, Training Loss (NLML): -958.0332\n",
      "convergence dfGPdfNN Run 9/10, Epoch 340/1000, Training Loss (NLML): -957.9615\n",
      "convergence dfGPdfNN Run 9/10, Epoch 341/1000, Training Loss (NLML): -957.9402\n",
      "convergence dfGPdfNN Run 9/10, Epoch 342/1000, Training Loss (NLML): -957.9138\n",
      "convergence dfGPdfNN Run 9/10, Epoch 343/1000, Training Loss (NLML): -957.8672\n",
      "convergence dfGPdfNN Run 9/10, Epoch 344/1000, Training Loss (NLML): -957.9401\n",
      "convergence dfGPdfNN Run 9/10, Epoch 345/1000, Training Loss (NLML): -957.9008\n",
      "convergence dfGPdfNN Run 9/10, Epoch 346/1000, Training Loss (NLML): -957.9702\n",
      "convergence dfGPdfNN Run 9/10, Epoch 347/1000, Training Loss (NLML): -958.0204\n",
      "convergence dfGPdfNN Run 9/10, Epoch 348/1000, Training Loss (NLML): -958.0692\n",
      "convergence dfGPdfNN Run 9/10, Epoch 349/1000, Training Loss (NLML): -958.0007\n",
      "convergence dfGPdfNN Run 9/10, Epoch 350/1000, Training Loss (NLML): -958.1084\n",
      "convergence dfGPdfNN Run 9/10, Epoch 351/1000, Training Loss (NLML): -958.1505\n",
      "convergence dfGPdfNN Run 9/10, Epoch 352/1000, Training Loss (NLML): -958.1145\n",
      "convergence dfGPdfNN Run 9/10, Epoch 353/1000, Training Loss (NLML): -958.1488\n",
      "convergence dfGPdfNN Run 9/10, Epoch 354/1000, Training Loss (NLML): -958.1041\n",
      "convergence dfGPdfNN Run 9/10, Epoch 355/1000, Training Loss (NLML): -958.0845\n",
      "convergence dfGPdfNN Run 9/10, Epoch 356/1000, Training Loss (NLML): -958.0129\n",
      "convergence dfGPdfNN Run 9/10, Epoch 357/1000, Training Loss (NLML): -958.0172\n",
      "convergence dfGPdfNN Run 9/10, Epoch 358/1000, Training Loss (NLML): -957.9904\n",
      "convergence dfGPdfNN Run 9/10, Epoch 359/1000, Training Loss (NLML): -958.0562\n",
      "convergence dfGPdfNN Run 9/10, Epoch 360/1000, Training Loss (NLML): -958.1218\n",
      "convergence dfGPdfNN Run 9/10, Epoch 361/1000, Training Loss (NLML): -958.2279\n",
      "convergence dfGPdfNN Run 9/10, Epoch 362/1000, Training Loss (NLML): -958.1736\n",
      "convergence dfGPdfNN Run 9/10, Epoch 363/1000, Training Loss (NLML): -958.1353\n",
      "convergence dfGPdfNN Run 9/10, Epoch 364/1000, Training Loss (NLML): -958.2654\n",
      "convergence dfGPdfNN Run 9/10, Epoch 365/1000, Training Loss (NLML): -958.2643\n",
      "convergence dfGPdfNN Run 9/10, Epoch 366/1000, Training Loss (NLML): -958.2206\n",
      "convergence dfGPdfNN Run 9/10, Epoch 367/1000, Training Loss (NLML): -958.2128\n",
      "convergence dfGPdfNN Run 9/10, Epoch 368/1000, Training Loss (NLML): -958.0841\n",
      "convergence dfGPdfNN Run 9/10, Epoch 369/1000, Training Loss (NLML): -957.9554\n",
      "convergence dfGPdfNN Run 9/10, Epoch 370/1000, Training Loss (NLML): -957.8784\n",
      "convergence dfGPdfNN Run 9/10, Epoch 371/1000, Training Loss (NLML): -957.9318\n",
      "convergence dfGPdfNN Run 9/10, Epoch 372/1000, Training Loss (NLML): -957.9769\n",
      "convergence dfGPdfNN Run 9/10, Epoch 373/1000, Training Loss (NLML): -958.0375\n",
      "convergence dfGPdfNN Run 9/10, Epoch 374/1000, Training Loss (NLML): -958.0983\n",
      "convergence dfGPdfNN Run 9/10, Epoch 375/1000, Training Loss (NLML): -958.1194\n",
      "convergence dfGPdfNN Run 9/10, Epoch 376/1000, Training Loss (NLML): -958.1451\n",
      "convergence dfGPdfNN Run 9/10, Epoch 377/1000, Training Loss (NLML): -958.1462\n",
      "convergence dfGPdfNN Run 9/10, Epoch 378/1000, Training Loss (NLML): -958.1064\n",
      "convergence dfGPdfNN Run 9/10, Epoch 379/1000, Training Loss (NLML): -958.0707\n",
      "convergence dfGPdfNN Run 9/10, Epoch 380/1000, Training Loss (NLML): -958.0192\n",
      "convergence dfGPdfNN Run 9/10, Epoch 381/1000, Training Loss (NLML): -957.9969\n",
      "convergence dfGPdfNN Run 9/10, Epoch 382/1000, Training Loss (NLML): -957.9694\n",
      "convergence dfGPdfNN Run 9/10, Epoch 383/1000, Training Loss (NLML): -957.9891\n",
      "convergence dfGPdfNN Run 9/10, Epoch 384/1000, Training Loss (NLML): -957.9888\n",
      "convergence dfGPdfNN Run 9/10, Epoch 385/1000, Training Loss (NLML): -957.9998\n",
      "convergence dfGPdfNN Run 9/10, Epoch 386/1000, Training Loss (NLML): -958.0831\n",
      "convergence dfGPdfNN Run 9/10, Epoch 387/1000, Training Loss (NLML): -958.1278\n",
      "convergence dfGPdfNN Run 9/10, Epoch 388/1000, Training Loss (NLML): -958.1335\n",
      "convergence dfGPdfNN Run 9/10, Epoch 389/1000, Training Loss (NLML): -958.1219\n",
      "convergence dfGPdfNN Run 9/10, Epoch 390/1000, Training Loss (NLML): -958.1288\n",
      "convergence dfGPdfNN Run 9/10, Epoch 391/1000, Training Loss (NLML): -958.1289\n",
      "convergence dfGPdfNN Run 9/10, Epoch 392/1000, Training Loss (NLML): -958.1270\n",
      "convergence dfGPdfNN Run 9/10, Epoch 393/1000, Training Loss (NLML): -958.1135\n",
      "convergence dfGPdfNN Run 9/10, Epoch 394/1000, Training Loss (NLML): -958.0782\n",
      "convergence dfGPdfNN Run 9/10, Epoch 395/1000, Training Loss (NLML): -958.1252\n",
      "convergence dfGPdfNN Run 9/10, Epoch 396/1000, Training Loss (NLML): -958.1429\n",
      "convergence dfGPdfNN Run 9/10, Epoch 397/1000, Training Loss (NLML): -958.0820\n",
      "convergence dfGPdfNN Run 9/10, Epoch 398/1000, Training Loss (NLML): -958.0575\n",
      "convergence dfGPdfNN Run 9/10, Epoch 399/1000, Training Loss (NLML): -958.0316\n",
      "convergence dfGPdfNN Run 9/10, Epoch 400/1000, Training Loss (NLML): -958.0341\n",
      "convergence dfGPdfNN Run 9/10, Epoch 401/1000, Training Loss (NLML): -957.9995\n",
      "convergence dfGPdfNN Run 9/10, Epoch 402/1000, Training Loss (NLML): -957.9543\n",
      "convergence dfGPdfNN Run 9/10, Epoch 403/1000, Training Loss (NLML): -957.9814\n",
      "convergence dfGPdfNN Run 9/10, Epoch 404/1000, Training Loss (NLML): -958.0116\n",
      "convergence dfGPdfNN Run 9/10, Epoch 405/1000, Training Loss (NLML): -958.0295\n",
      "convergence dfGPdfNN Run 9/10, Epoch 406/1000, Training Loss (NLML): -958.0271\n",
      "convergence dfGPdfNN Run 9/10, Epoch 407/1000, Training Loss (NLML): -958.0010\n",
      "convergence dfGPdfNN Run 9/10, Epoch 408/1000, Training Loss (NLML): -957.9354\n",
      "convergence dfGPdfNN Run 9/10, Epoch 409/1000, Training Loss (NLML): -958.3113\n",
      "convergence dfGPdfNN Run 9/10, Epoch 410/1000, Training Loss (NLML): -958.2372\n",
      "convergence dfGPdfNN Run 9/10, Epoch 411/1000, Training Loss (NLML): -958.2136\n",
      "convergence dfGPdfNN Run 9/10, Epoch 412/1000, Training Loss (NLML): -958.2417\n",
      "convergence dfGPdfNN Run 9/10, Epoch 413/1000, Training Loss (NLML): -958.2916\n",
      "convergence dfGPdfNN Run 9/10, Epoch 414/1000, Training Loss (NLML): -958.2932\n",
      "convergence dfGPdfNN Run 9/10, Epoch 415/1000, Training Loss (NLML): -958.2853\n",
      "convergence dfGPdfNN Run 9/10, Epoch 416/1000, Training Loss (NLML): -958.2806\n",
      "convergence dfGPdfNN Run 9/10, Epoch 417/1000, Training Loss (NLML): -958.2657\n",
      "convergence dfGPdfNN Run 9/10, Epoch 418/1000, Training Loss (NLML): -958.2512\n",
      "convergence dfGPdfNN Run 9/10, Epoch 419/1000, Training Loss (NLML): -958.2628\n",
      "convergence dfGPdfNN Run 9/10, Epoch 420/1000, Training Loss (NLML): -958.2877\n",
      "convergence dfGPdfNN Run 9/10, Epoch 421/1000, Training Loss (NLML): -958.3040\n",
      "convergence dfGPdfNN Run 9/10, Epoch 422/1000, Training Loss (NLML): -957.4103\n",
      "convergence dfGPdfNN Run 9/10, Epoch 423/1000, Training Loss (NLML): -958.4001\n",
      "convergence dfGPdfNN Run 9/10, Epoch 424/1000, Training Loss (NLML): -958.4139\n",
      "convergence dfGPdfNN Run 9/10, Epoch 425/1000, Training Loss (NLML): -958.4501\n",
      "convergence dfGPdfNN Run 9/10, Epoch 426/1000, Training Loss (NLML): -958.4624\n",
      "convergence dfGPdfNN Run 9/10, Epoch 427/1000, Training Loss (NLML): -958.4613\n",
      "convergence dfGPdfNN Run 9/10, Epoch 428/1000, Training Loss (NLML): -958.4623\n",
      "convergence dfGPdfNN Run 9/10, Epoch 429/1000, Training Loss (NLML): -958.4608\n",
      "convergence dfGPdfNN Run 9/10, Epoch 430/1000, Training Loss (NLML): -958.4269\n",
      "convergence dfGPdfNN Run 9/10, Epoch 431/1000, Training Loss (NLML): -958.4252\n",
      "convergence dfGPdfNN Run 9/10, Epoch 432/1000, Training Loss (NLML): -958.4554\n",
      "convergence dfGPdfNN Run 9/10, Epoch 433/1000, Training Loss (NLML): -958.4735\n",
      "convergence dfGPdfNN Run 9/10, Epoch 434/1000, Training Loss (NLML): -958.4769\n",
      "convergence dfGPdfNN Run 9/10, Epoch 435/1000, Training Loss (NLML): -958.4861\n",
      "convergence dfGPdfNN Run 9/10, Epoch 436/1000, Training Loss (NLML): -958.5078\n",
      "convergence dfGPdfNN Run 9/10, Epoch 437/1000, Training Loss (NLML): -958.5157\n",
      "convergence dfGPdfNN Run 9/10, Epoch 438/1000, Training Loss (NLML): -958.5232\n",
      "convergence dfGPdfNN Run 9/10, Epoch 439/1000, Training Loss (NLML): -958.5165\n",
      "convergence dfGPdfNN Run 9/10, Epoch 440/1000, Training Loss (NLML): -958.5267\n",
      "convergence dfGPdfNN Run 9/10, Epoch 441/1000, Training Loss (NLML): -958.5432\n",
      "convergence dfGPdfNN Run 9/10, Epoch 442/1000, Training Loss (NLML): -958.5525\n",
      "convergence dfGPdfNN Run 9/10, Epoch 443/1000, Training Loss (NLML): -958.6348\n",
      "convergence dfGPdfNN Run 9/10, Epoch 444/1000, Training Loss (NLML): -958.6642\n",
      "convergence dfGPdfNN Run 9/10, Epoch 445/1000, Training Loss (NLML): -958.7109\n",
      "convergence dfGPdfNN Run 9/10, Epoch 446/1000, Training Loss (NLML): -958.6932\n",
      "convergence dfGPdfNN Run 9/10, Epoch 447/1000, Training Loss (NLML): -958.7006\n",
      "convergence dfGPdfNN Run 9/10, Epoch 448/1000, Training Loss (NLML): -958.6345\n",
      "convergence dfGPdfNN Run 9/10, Epoch 449/1000, Training Loss (NLML): -958.6393\n",
      "convergence dfGPdfNN Run 9/10, Epoch 450/1000, Training Loss (NLML): -958.7241\n",
      "convergence dfGPdfNN Run 9/10, Epoch 451/1000, Training Loss (NLML): -958.7789\n",
      "convergence dfGPdfNN Run 9/10, Epoch 452/1000, Training Loss (NLML): -958.8320\n",
      "convergence dfGPdfNN Run 9/10, Epoch 453/1000, Training Loss (NLML): -958.7758\n",
      "convergence dfGPdfNN Run 9/10, Epoch 454/1000, Training Loss (NLML): -958.7633\n",
      "convergence dfGPdfNN Run 9/10, Epoch 455/1000, Training Loss (NLML): -958.7719\n",
      "convergence dfGPdfNN Run 9/10, Epoch 456/1000, Training Loss (NLML): -958.7783\n",
      "convergence dfGPdfNN Run 9/10, Epoch 457/1000, Training Loss (NLML): -958.8184\n",
      "convergence dfGPdfNN Run 9/10, Epoch 458/1000, Training Loss (NLML): -958.8547\n",
      "convergence dfGPdfNN Run 9/10, Epoch 459/1000, Training Loss (NLML): -958.8657\n",
      "convergence dfGPdfNN Run 9/10, Epoch 460/1000, Training Loss (NLML): -958.9167\n",
      "convergence dfGPdfNN Run 9/10, Epoch 461/1000, Training Loss (NLML): -958.9258\n",
      "convergence dfGPdfNN Run 9/10, Epoch 462/1000, Training Loss (NLML): -958.8247\n",
      "convergence dfGPdfNN Run 9/10, Epoch 463/1000, Training Loss (NLML): -958.8553\n",
      "convergence dfGPdfNN Run 9/10, Epoch 464/1000, Training Loss (NLML): -958.8585\n",
      "convergence dfGPdfNN Run 9/10, Epoch 465/1000, Training Loss (NLML): -958.8761\n",
      "convergence dfGPdfNN Run 9/10, Epoch 466/1000, Training Loss (NLML): -958.9052\n",
      "convergence dfGPdfNN Run 9/10, Epoch 467/1000, Training Loss (NLML): -958.9568\n",
      "convergence dfGPdfNN Run 9/10, Epoch 468/1000, Training Loss (NLML): -958.9595\n",
      "convergence dfGPdfNN Run 9/10, Epoch 469/1000, Training Loss (NLML): -958.9032\n",
      "convergence dfGPdfNN Run 9/10, Epoch 470/1000, Training Loss (NLML): -958.8914\n",
      "convergence dfGPdfNN Run 9/10, Epoch 471/1000, Training Loss (NLML): -958.8751\n",
      "convergence dfGPdfNN Run 9/10, Epoch 472/1000, Training Loss (NLML): -958.9249\n",
      "convergence dfGPdfNN Run 9/10, Epoch 473/1000, Training Loss (NLML): -958.9899\n",
      "convergence dfGPdfNN Run 9/10, Epoch 474/1000, Training Loss (NLML): -959.0013\n",
      "convergence dfGPdfNN Run 9/10, Epoch 475/1000, Training Loss (NLML): -959.0023\n",
      "convergence dfGPdfNN Run 9/10, Epoch 476/1000, Training Loss (NLML): -958.9371\n",
      "convergence dfGPdfNN Run 9/10, Epoch 477/1000, Training Loss (NLML): -958.8646\n",
      "convergence dfGPdfNN Run 9/10, Epoch 478/1000, Training Loss (NLML): -958.9426\n",
      "convergence dfGPdfNN Run 9/10, Epoch 479/1000, Training Loss (NLML): -958.9880\n",
      "convergence dfGPdfNN Run 9/10, Epoch 480/1000, Training Loss (NLML): -959.0775\n",
      "convergence dfGPdfNN Run 9/10, Epoch 481/1000, Training Loss (NLML): -959.0814\n",
      "convergence dfGPdfNN Run 9/10, Epoch 482/1000, Training Loss (NLML): -959.0049\n",
      "convergence dfGPdfNN Run 9/10, Epoch 483/1000, Training Loss (NLML): -958.9391\n",
      "convergence dfGPdfNN Run 9/10, Epoch 484/1000, Training Loss (NLML): -958.9701\n",
      "convergence dfGPdfNN Run 9/10, Epoch 485/1000, Training Loss (NLML): -959.0442\n",
      "convergence dfGPdfNN Run 9/10, Epoch 486/1000, Training Loss (NLML): -959.1014\n",
      "convergence dfGPdfNN Run 9/10, Epoch 487/1000, Training Loss (NLML): -959.1167\n",
      "convergence dfGPdfNN Run 9/10, Epoch 488/1000, Training Loss (NLML): -959.1189\n",
      "convergence dfGPdfNN Run 9/10, Epoch 489/1000, Training Loss (NLML): -959.1145\n",
      "convergence dfGPdfNN Run 9/10, Epoch 490/1000, Training Loss (NLML): -959.0771\n",
      "convergence dfGPdfNN Run 9/10, Epoch 491/1000, Training Loss (NLML): -959.0823\n",
      "convergence dfGPdfNN Run 9/10, Epoch 492/1000, Training Loss (NLML): -959.2101\n",
      "convergence dfGPdfNN Run 9/10, Epoch 493/1000, Training Loss (NLML): -959.1730\n",
      "convergence dfGPdfNN Run 9/10, Epoch 494/1000, Training Loss (NLML): -959.1573\n",
      "convergence dfGPdfNN Run 9/10, Epoch 495/1000, Training Loss (NLML): -959.1426\n",
      "convergence dfGPdfNN Run 9/10, Epoch 496/1000, Training Loss (NLML): -959.1716\n",
      "convergence dfGPdfNN Run 9/10, Epoch 497/1000, Training Loss (NLML): -959.1437\n",
      "convergence dfGPdfNN Run 9/10, Epoch 498/1000, Training Loss (NLML): -959.1370\n",
      "convergence dfGPdfNN Run 9/10, Epoch 499/1000, Training Loss (NLML): -959.1736\n",
      "convergence dfGPdfNN Run 9/10, Epoch 500/1000, Training Loss (NLML): -959.1241\n",
      "convergence dfGPdfNN Run 9/10, Epoch 501/1000, Training Loss (NLML): -959.2200\n",
      "convergence dfGPdfNN Run 9/10, Epoch 502/1000, Training Loss (NLML): -959.2471\n",
      "convergence dfGPdfNN Run 9/10, Epoch 503/1000, Training Loss (NLML): -959.2588\n",
      "convergence dfGPdfNN Run 9/10, Epoch 504/1000, Training Loss (NLML): -959.2762\n",
      "convergence dfGPdfNN Run 9/10, Epoch 505/1000, Training Loss (NLML): -959.2751\n",
      "convergence dfGPdfNN Run 9/10, Epoch 506/1000, Training Loss (NLML): -959.2456\n",
      "convergence dfGPdfNN Run 9/10, Epoch 507/1000, Training Loss (NLML): -959.3136\n",
      "convergence dfGPdfNN Run 9/10, Epoch 508/1000, Training Loss (NLML): -959.3510\n",
      "convergence dfGPdfNN Run 9/10, Epoch 509/1000, Training Loss (NLML): -959.3862\n",
      "convergence dfGPdfNN Run 9/10, Epoch 510/1000, Training Loss (NLML): -959.3937\n",
      "convergence dfGPdfNN Run 9/10, Epoch 511/1000, Training Loss (NLML): -959.3428\n",
      "convergence dfGPdfNN Run 9/10, Epoch 512/1000, Training Loss (NLML): -959.2606\n",
      "convergence dfGPdfNN Run 9/10, Epoch 513/1000, Training Loss (NLML): -959.2947\n",
      "convergence dfGPdfNN Run 9/10, Epoch 514/1000, Training Loss (NLML): -959.2744\n",
      "convergence dfGPdfNN Run 9/10, Epoch 515/1000, Training Loss (NLML): -959.1556\n",
      "convergence dfGPdfNN Run 9/10, Epoch 516/1000, Training Loss (NLML): -959.1627\n",
      "convergence dfGPdfNN Run 9/10, Epoch 517/1000, Training Loss (NLML): -959.2014\n",
      "convergence dfGPdfNN Run 9/10, Epoch 518/1000, Training Loss (NLML): -959.3290\n",
      "convergence dfGPdfNN Run 9/10, Epoch 519/1000, Training Loss (NLML): -959.3624\n",
      "convergence dfGPdfNN Run 9/10, Epoch 520/1000, Training Loss (NLML): -959.3693\n",
      "convergence dfGPdfNN Run 9/10, Epoch 521/1000, Training Loss (NLML): -959.2482\n",
      "convergence dfGPdfNN Run 9/10, Epoch 522/1000, Training Loss (NLML): -959.2057\n",
      "convergence dfGPdfNN Run 9/10, Epoch 523/1000, Training Loss (NLML): -959.2882\n",
      "convergence dfGPdfNN Run 9/10, Epoch 524/1000, Training Loss (NLML): -959.3629\n",
      "convergence dfGPdfNN Run 9/10, Epoch 525/1000, Training Loss (NLML): -959.3379\n",
      "convergence dfGPdfNN Run 9/10, Epoch 526/1000, Training Loss (NLML): -959.3408\n",
      "convergence dfGPdfNN Run 9/10, Epoch 527/1000, Training Loss (NLML): -959.2958\n",
      "convergence dfGPdfNN Run 9/10, Epoch 528/1000, Training Loss (NLML): -959.3356\n",
      "convergence dfGPdfNN Run 9/10, Epoch 529/1000, Training Loss (NLML): -959.3712\n",
      "convergence dfGPdfNN Run 9/10, Epoch 530/1000, Training Loss (NLML): -959.4708\n",
      "convergence dfGPdfNN Run 9/10, Epoch 531/1000, Training Loss (NLML): -959.4806\n",
      "convergence dfGPdfNN Run 9/10, Epoch 532/1000, Training Loss (NLML): -959.4084\n",
      "convergence dfGPdfNN Run 9/10, Epoch 533/1000, Training Loss (NLML): -959.3359\n",
      "convergence dfGPdfNN Run 9/10, Epoch 534/1000, Training Loss (NLML): -959.4202\n",
      "convergence dfGPdfNN Run 9/10, Epoch 535/1000, Training Loss (NLML): -959.4485\n",
      "convergence dfGPdfNN Run 9/10, Epoch 536/1000, Training Loss (NLML): -959.4344\n",
      "convergence dfGPdfNN Run 9/10, Epoch 537/1000, Training Loss (NLML): -959.4496\n",
      "convergence dfGPdfNN Run 9/10, Epoch 538/1000, Training Loss (NLML): -959.4502\n",
      "convergence dfGPdfNN Run 9/10, Epoch 539/1000, Training Loss (NLML): -959.4563\n",
      "convergence dfGPdfNN Run 9/10, Epoch 540/1000, Training Loss (NLML): -959.4181\n",
      "convergence dfGPdfNN Run 9/10, Epoch 541/1000, Training Loss (NLML): -959.3867\n",
      "convergence dfGPdfNN Run 9/10, Epoch 542/1000, Training Loss (NLML): -959.3829\n",
      "convergence dfGPdfNN Run 9/10, Epoch 543/1000, Training Loss (NLML): -959.4286\n",
      "convergence dfGPdfNN Run 9/10, Epoch 544/1000, Training Loss (NLML): -959.4736\n",
      "convergence dfGPdfNN Run 9/10, Epoch 545/1000, Training Loss (NLML): -959.4808\n",
      "convergence dfGPdfNN Run 9/10, Epoch 546/1000, Training Loss (NLML): -959.4890\n",
      "convergence dfGPdfNN Run 9/10, Epoch 547/1000, Training Loss (NLML): -959.4640\n",
      "convergence dfGPdfNN Run 9/10, Epoch 548/1000, Training Loss (NLML): -959.3546\n",
      "convergence dfGPdfNN Run 9/10, Epoch 549/1000, Training Loss (NLML): -959.3947\n",
      "convergence dfGPdfNN Run 9/10, Epoch 550/1000, Training Loss (NLML): -959.4305\n",
      "convergence dfGPdfNN Run 9/10, Epoch 551/1000, Training Loss (NLML): -959.5078\n",
      "convergence dfGPdfNN Run 9/10, Epoch 552/1000, Training Loss (NLML): -959.5276\n",
      "convergence dfGPdfNN Run 9/10, Epoch 553/1000, Training Loss (NLML): -959.5316\n",
      "convergence dfGPdfNN Run 9/10, Epoch 554/1000, Training Loss (NLML): -959.4808\n",
      "convergence dfGPdfNN Run 9/10, Epoch 555/1000, Training Loss (NLML): -959.5226\n",
      "convergence dfGPdfNN Run 9/10, Epoch 556/1000, Training Loss (NLML): -959.5801\n",
      "convergence dfGPdfNN Run 9/10, Epoch 557/1000, Training Loss (NLML): -959.5756\n",
      "convergence dfGPdfNN Run 9/10, Epoch 558/1000, Training Loss (NLML): -959.5802\n",
      "convergence dfGPdfNN Run 9/10, Epoch 559/1000, Training Loss (NLML): -959.5221\n",
      "convergence dfGPdfNN Run 9/10, Epoch 560/1000, Training Loss (NLML): -959.5304\n",
      "convergence dfGPdfNN Run 9/10, Epoch 561/1000, Training Loss (NLML): -959.5341\n",
      "convergence dfGPdfNN Run 9/10, Epoch 562/1000, Training Loss (NLML): -959.5576\n",
      "convergence dfGPdfNN Run 9/10, Epoch 563/1000, Training Loss (NLML): -959.5005\n",
      "convergence dfGPdfNN Run 9/10, Epoch 564/1000, Training Loss (NLML): -959.5006\n",
      "convergence dfGPdfNN Run 9/10, Epoch 565/1000, Training Loss (NLML): -959.4999\n",
      "convergence dfGPdfNN Run 9/10, Epoch 566/1000, Training Loss (NLML): -959.5050\n",
      "convergence dfGPdfNN Run 9/10, Epoch 567/1000, Training Loss (NLML): -959.5372\n",
      "convergence dfGPdfNN Run 9/10, Epoch 568/1000, Training Loss (NLML): -959.5808\n",
      "convergence dfGPdfNN Run 9/10, Epoch 569/1000, Training Loss (NLML): -959.5562\n",
      "convergence dfGPdfNN Run 9/10, Epoch 570/1000, Training Loss (NLML): -959.5488\n",
      "convergence dfGPdfNN Run 9/10, Epoch 571/1000, Training Loss (NLML): -959.5146\n",
      "convergence dfGPdfNN Run 9/10, Epoch 572/1000, Training Loss (NLML): -959.5164\n",
      "convergence dfGPdfNN Run 9/10, Epoch 573/1000, Training Loss (NLML): -959.5736\n",
      "convergence dfGPdfNN Run 9/10, Epoch 574/1000, Training Loss (NLML): -959.6233\n",
      "convergence dfGPdfNN Run 9/10, Epoch 575/1000, Training Loss (NLML): -959.5820\n",
      "convergence dfGPdfNN Run 9/10, Epoch 576/1000, Training Loss (NLML): -959.5859\n",
      "convergence dfGPdfNN Run 9/10, Epoch 577/1000, Training Loss (NLML): -959.5992\n",
      "convergence dfGPdfNN Run 9/10, Epoch 578/1000, Training Loss (NLML): -959.5771\n",
      "convergence dfGPdfNN Run 9/10, Epoch 579/1000, Training Loss (NLML): -959.6149\n",
      "convergence dfGPdfNN Run 9/10, Epoch 580/1000, Training Loss (NLML): -959.6830\n",
      "convergence dfGPdfNN Run 9/10, Epoch 581/1000, Training Loss (NLML): -959.6652\n",
      "convergence dfGPdfNN Run 9/10, Epoch 582/1000, Training Loss (NLML): -959.6410\n",
      "convergence dfGPdfNN Run 9/10, Epoch 583/1000, Training Loss (NLML): -959.5468\n",
      "convergence dfGPdfNN Run 9/10, Epoch 584/1000, Training Loss (NLML): -959.5062\n",
      "convergence dfGPdfNN Run 9/10, Epoch 585/1000, Training Loss (NLML): -959.4901\n",
      "convergence dfGPdfNN Run 9/10, Epoch 586/1000, Training Loss (NLML): -959.6199\n",
      "convergence dfGPdfNN Run 9/10, Epoch 587/1000, Training Loss (NLML): -959.6537\n",
      "convergence dfGPdfNN Run 9/10, Epoch 588/1000, Training Loss (NLML): -959.6683\n",
      "convergence dfGPdfNN Run 9/10, Epoch 589/1000, Training Loss (NLML): -959.6587\n",
      "convergence dfGPdfNN Run 9/10, Epoch 590/1000, Training Loss (NLML): -959.6134\n",
      "convergence dfGPdfNN Run 9/10, Epoch 591/1000, Training Loss (NLML): -959.5251\n",
      "convergence dfGPdfNN Run 9/10, Epoch 592/1000, Training Loss (NLML): -959.6016\n",
      "convergence dfGPdfNN Run 9/10, Epoch 593/1000, Training Loss (NLML): -959.6398\n",
      "convergence dfGPdfNN Run 9/10, Epoch 594/1000, Training Loss (NLML): -959.6897\n",
      "convergence dfGPdfNN Run 9/10, Epoch 595/1000, Training Loss (NLML): -959.7137\n",
      "convergence dfGPdfNN Run 9/10, Epoch 596/1000, Training Loss (NLML): -959.7126\n",
      "convergence dfGPdfNN Run 9/10, Epoch 597/1000, Training Loss (NLML): -959.6483\n",
      "convergence dfGPdfNN Run 9/10, Epoch 598/1000, Training Loss (NLML): -959.6384\n",
      "convergence dfGPdfNN Run 9/10, Epoch 599/1000, Training Loss (NLML): -959.5992\n",
      "convergence dfGPdfNN Run 9/10, Epoch 600/1000, Training Loss (NLML): -959.5289\n",
      "convergence dfGPdfNN Run 9/10, Epoch 601/1000, Training Loss (NLML): -959.5579\n",
      "convergence dfGPdfNN Run 9/10, Epoch 602/1000, Training Loss (NLML): -959.5834\n",
      "convergence dfGPdfNN Run 9/10, Epoch 603/1000, Training Loss (NLML): -959.6875\n",
      "convergence dfGPdfNN Run 9/10, Epoch 604/1000, Training Loss (NLML): -959.7344\n",
      "convergence dfGPdfNN Run 9/10, Epoch 605/1000, Training Loss (NLML): -959.7589\n",
      "convergence dfGPdfNN Run 9/10, Epoch 606/1000, Training Loss (NLML): -959.7264\n",
      "convergence dfGPdfNN Run 9/10, Epoch 607/1000, Training Loss (NLML): -959.6798\n",
      "convergence dfGPdfNN Run 9/10, Epoch 608/1000, Training Loss (NLML): -959.6023\n",
      "convergence dfGPdfNN Run 9/10, Epoch 609/1000, Training Loss (NLML): -959.7002\n",
      "convergence dfGPdfNN Run 9/10, Epoch 610/1000, Training Loss (NLML): -959.8154\n",
      "convergence dfGPdfNN Run 9/10, Epoch 611/1000, Training Loss (NLML): -959.8177\n",
      "convergence dfGPdfNN Run 9/10, Epoch 612/1000, Training Loss (NLML): -959.7596\n",
      "convergence dfGPdfNN Run 9/10, Epoch 613/1000, Training Loss (NLML): -959.7351\n",
      "convergence dfGPdfNN Run 9/10, Epoch 614/1000, Training Loss (NLML): -959.6663\n",
      "convergence dfGPdfNN Run 9/10, Epoch 615/1000, Training Loss (NLML): -959.6761\n",
      "convergence dfGPdfNN Run 9/10, Epoch 616/1000, Training Loss (NLML): -959.7413\n",
      "convergence dfGPdfNN Run 9/10, Epoch 617/1000, Training Loss (NLML): -959.7709\n",
      "convergence dfGPdfNN Run 9/10, Epoch 618/1000, Training Loss (NLML): -959.7579\n",
      "convergence dfGPdfNN Run 9/10, Epoch 619/1000, Training Loss (NLML): -959.7754\n",
      "convergence dfGPdfNN Run 9/10, Epoch 620/1000, Training Loss (NLML): -959.8077\n",
      "convergence dfGPdfNN Run 9/10, Epoch 621/1000, Training Loss (NLML): -959.9008\n",
      "convergence dfGPdfNN Run 9/10, Epoch 622/1000, Training Loss (NLML): -959.9166\n",
      "convergence dfGPdfNN Run 9/10, Epoch 623/1000, Training Loss (NLML): -959.9066\n",
      "convergence dfGPdfNN Run 9/10, Epoch 624/1000, Training Loss (NLML): -959.7896\n",
      "convergence dfGPdfNN Run 9/10, Epoch 625/1000, Training Loss (NLML): -959.7883\n",
      "convergence dfGPdfNN Run 9/10, Epoch 626/1000, Training Loss (NLML): -959.8369\n",
      "convergence dfGPdfNN Run 9/10, Epoch 627/1000, Training Loss (NLML): -959.8375\n",
      "convergence dfGPdfNN Run 9/10, Epoch 628/1000, Training Loss (NLML): -959.7944\n",
      "convergence dfGPdfNN Run 9/10, Epoch 629/1000, Training Loss (NLML): -959.7974\n",
      "convergence dfGPdfNN Run 9/10, Epoch 630/1000, Training Loss (NLML): -959.8031\n",
      "convergence dfGPdfNN Run 9/10, Epoch 631/1000, Training Loss (NLML): -959.8363\n",
      "convergence dfGPdfNN Run 9/10, Epoch 632/1000, Training Loss (NLML): -959.9229\n",
      "convergence dfGPdfNN Run 9/10, Epoch 633/1000, Training Loss (NLML): -959.9749\n",
      "convergence dfGPdfNN Run 9/10, Epoch 634/1000, Training Loss (NLML): -959.9642\n",
      "convergence dfGPdfNN Run 9/10, Epoch 635/1000, Training Loss (NLML): -959.9193\n",
      "convergence dfGPdfNN Run 9/10, Epoch 636/1000, Training Loss (NLML): -959.8004\n",
      "convergence dfGPdfNN Run 9/10, Epoch 637/1000, Training Loss (NLML): -959.8573\n",
      "convergence dfGPdfNN Run 9/10, Epoch 638/1000, Training Loss (NLML): -959.9303\n",
      "convergence dfGPdfNN Run 9/10, Epoch 639/1000, Training Loss (NLML): -959.8911\n",
      "convergence dfGPdfNN Run 9/10, Epoch 640/1000, Training Loss (NLML): -959.8365\n",
      "convergence dfGPdfNN Run 9/10, Epoch 641/1000, Training Loss (NLML): -959.8372\n",
      "convergence dfGPdfNN Run 9/10, Epoch 642/1000, Training Loss (NLML): -959.8400\n",
      "convergence dfGPdfNN Run 9/10, Epoch 643/1000, Training Loss (NLML): -959.8787\n",
      "convergence dfGPdfNN Run 9/10, Epoch 644/1000, Training Loss (NLML): -959.9077\n",
      "convergence dfGPdfNN Run 9/10, Epoch 645/1000, Training Loss (NLML): -959.9880\n",
      "convergence dfGPdfNN Run 9/10, Epoch 646/1000, Training Loss (NLML): -960.0239\n",
      "convergence dfGPdfNN Run 9/10, Epoch 647/1000, Training Loss (NLML): -960.0052\n",
      "convergence dfGPdfNN Run 9/10, Epoch 648/1000, Training Loss (NLML): -959.9022\n",
      "convergence dfGPdfNN Run 9/10, Epoch 649/1000, Training Loss (NLML): -959.8975\n",
      "convergence dfGPdfNN Run 9/10, Epoch 650/1000, Training Loss (NLML): -959.9335\n",
      "convergence dfGPdfNN Run 9/10, Epoch 651/1000, Training Loss (NLML): -959.9602\n",
      "convergence dfGPdfNN Run 9/10, Epoch 652/1000, Training Loss (NLML): -959.9592\n",
      "convergence dfGPdfNN Run 9/10, Epoch 653/1000, Training Loss (NLML): -959.8683\n",
      "convergence dfGPdfNN Run 9/10, Epoch 654/1000, Training Loss (NLML): -959.8514\n",
      "convergence dfGPdfNN Run 9/10, Epoch 655/1000, Training Loss (NLML): -959.8336\n",
      "convergence dfGPdfNN Run 9/10, Epoch 656/1000, Training Loss (NLML): -959.8867\n",
      "convergence dfGPdfNN Run 9/10, Epoch 657/1000, Training Loss (NLML): -959.9115\n",
      "convergence dfGPdfNN Run 9/10, Epoch 658/1000, Training Loss (NLML): -959.9807\n",
      "convergence dfGPdfNN Run 9/10, Epoch 659/1000, Training Loss (NLML): -960.0319\n",
      "convergence dfGPdfNN Run 9/10, Epoch 660/1000, Training Loss (NLML): -960.0521\n",
      "convergence dfGPdfNN Run 9/10, Epoch 661/1000, Training Loss (NLML): -960.0955\n",
      "convergence dfGPdfNN Run 9/10, Epoch 662/1000, Training Loss (NLML): -960.1394\n",
      "convergence dfGPdfNN Run 9/10, Epoch 663/1000, Training Loss (NLML): -960.1189\n",
      "convergence dfGPdfNN Run 9/10, Epoch 664/1000, Training Loss (NLML): -960.0417\n",
      "convergence dfGPdfNN Run 9/10, Epoch 665/1000, Training Loss (NLML): -959.9432\n",
      "convergence dfGPdfNN Run 9/10, Epoch 666/1000, Training Loss (NLML): -959.9855\n",
      "convergence dfGPdfNN Run 9/10, Epoch 667/1000, Training Loss (NLML): -960.0442\n",
      "convergence dfGPdfNN Run 9/10, Epoch 668/1000, Training Loss (NLML): -960.0892\n",
      "convergence dfGPdfNN Run 9/10, Epoch 669/1000, Training Loss (NLML): -960.1890\n",
      "convergence dfGPdfNN Run 9/10, Epoch 670/1000, Training Loss (NLML): -960.0994\n",
      "convergence dfGPdfNN Run 9/10, Epoch 671/1000, Training Loss (NLML): -959.8679\n",
      "convergence dfGPdfNN Run 9/10, Epoch 672/1000, Training Loss (NLML): -959.8829\n",
      "convergence dfGPdfNN Run 9/10, Epoch 673/1000, Training Loss (NLML): -959.8948\n",
      "convergence dfGPdfNN Run 9/10, Epoch 674/1000, Training Loss (NLML): -959.9351\n",
      "convergence dfGPdfNN Run 9/10, Epoch 675/1000, Training Loss (NLML): -960.0295\n",
      "convergence dfGPdfNN Run 9/10, Epoch 676/1000, Training Loss (NLML): -959.9977\n",
      "convergence dfGPdfNN Run 9/10, Epoch 677/1000, Training Loss (NLML): -960.1121\n",
      "convergence dfGPdfNN Run 9/10, Epoch 678/1000, Training Loss (NLML): -960.0988\n",
      "convergence dfGPdfNN Run 9/10, Epoch 679/1000, Training Loss (NLML): -960.0542\n",
      "convergence dfGPdfNN Run 9/10, Epoch 680/1000, Training Loss (NLML): -960.0725\n",
      "convergence dfGPdfNN Run 9/10, Epoch 681/1000, Training Loss (NLML): -960.1023\n",
      "convergence dfGPdfNN Run 9/10, Epoch 682/1000, Training Loss (NLML): -960.1251\n",
      "convergence dfGPdfNN Run 9/10, Epoch 683/1000, Training Loss (NLML): -960.1744\n",
      "convergence dfGPdfNN Run 9/10, Epoch 684/1000, Training Loss (NLML): -960.1704\n",
      "convergence dfGPdfNN Run 9/10, Epoch 685/1000, Training Loss (NLML): -960.1584\n",
      "convergence dfGPdfNN Run 9/10, Epoch 686/1000, Training Loss (NLML): -960.1454\n",
      "convergence dfGPdfNN Run 9/10, Epoch 687/1000, Training Loss (NLML): -960.1517\n",
      "convergence dfGPdfNN Run 9/10, Epoch 688/1000, Training Loss (NLML): -960.1671\n",
      "convergence dfGPdfNN Run 9/10, Epoch 689/1000, Training Loss (NLML): -960.2100\n",
      "convergence dfGPdfNN Run 9/10, Epoch 690/1000, Training Loss (NLML): -960.2177\n",
      "convergence dfGPdfNN Run 9/10, Epoch 691/1000, Training Loss (NLML): -960.2208\n",
      "convergence dfGPdfNN Run 9/10, Epoch 692/1000, Training Loss (NLML): -960.2050\n",
      "convergence dfGPdfNN Run 9/10, Epoch 693/1000, Training Loss (NLML): -960.2015\n",
      "convergence dfGPdfNN Run 9/10, Epoch 694/1000, Training Loss (NLML): -960.2129\n",
      "convergence dfGPdfNN Run 9/10, Epoch 695/1000, Training Loss (NLML): -960.2236\n",
      "convergence dfGPdfNN Run 9/10, Epoch 696/1000, Training Loss (NLML): -960.2318\n",
      "convergence dfGPdfNN Run 9/10, Epoch 697/1000, Training Loss (NLML): -960.2378\n",
      "convergence dfGPdfNN Run 9/10, Epoch 698/1000, Training Loss (NLML): -960.2343\n",
      "convergence dfGPdfNN Run 9/10, Epoch 699/1000, Training Loss (NLML): -960.2166\n",
      "convergence dfGPdfNN Run 9/10, Epoch 700/1000, Training Loss (NLML): -960.2172\n",
      "convergence dfGPdfNN Run 9/10, Epoch 701/1000, Training Loss (NLML): -960.2230\n",
      "convergence dfGPdfNN Run 9/10, Epoch 702/1000, Training Loss (NLML): -960.2284\n",
      "convergence dfGPdfNN Run 9/10, Epoch 703/1000, Training Loss (NLML): -960.2314\n",
      "convergence dfGPdfNN Run 9/10, Epoch 704/1000, Training Loss (NLML): -960.2130\n",
      "convergence dfGPdfNN Run 9/10, Epoch 705/1000, Training Loss (NLML): -960.1877\n",
      "convergence dfGPdfNN Run 9/10, Epoch 706/1000, Training Loss (NLML): -960.1772\n",
      "convergence dfGPdfNN Run 9/10, Epoch 707/1000, Training Loss (NLML): -960.1814\n",
      "convergence dfGPdfNN Run 9/10, Epoch 708/1000, Training Loss (NLML): -960.2106\n",
      "convergence dfGPdfNN Run 9/10, Epoch 709/1000, Training Loss (NLML): -960.1951\n",
      "convergence dfGPdfNN Run 9/10, Epoch 710/1000, Training Loss (NLML): -960.1993\n",
      "convergence dfGPdfNN Run 9/10, Epoch 711/1000, Training Loss (NLML): -960.2074\n",
      "convergence dfGPdfNN Run 9/10, Epoch 712/1000, Training Loss (NLML): -960.1477\n",
      "convergence dfGPdfNN Run 9/10, Epoch 713/1000, Training Loss (NLML): -960.1313\n",
      "convergence dfGPdfNN Run 9/10, Epoch 714/1000, Training Loss (NLML): -960.1288\n",
      "convergence dfGPdfNN Run 9/10, Epoch 715/1000, Training Loss (NLML): -960.1515\n",
      "convergence dfGPdfNN Run 9/10, Epoch 716/1000, Training Loss (NLML): -960.1061\n",
      "convergence dfGPdfNN Run 9/10, Epoch 717/1000, Training Loss (NLML): -960.1108\n",
      "convergence dfGPdfNN Run 9/10, Epoch 718/1000, Training Loss (NLML): -960.1162\n",
      "convergence dfGPdfNN Run 9/10, Epoch 719/1000, Training Loss (NLML): -960.1244\n",
      "convergence dfGPdfNN Run 9/10, Epoch 720/1000, Training Loss (NLML): -960.1292\n",
      "convergence dfGPdfNN Run 9/10, Epoch 721/1000, Training Loss (NLML): -959.9821\n",
      "convergence dfGPdfNN Run 9/10, Epoch 722/1000, Training Loss (NLML): -959.9637\n",
      "convergence dfGPdfNN Run 9/10, Epoch 723/1000, Training Loss (NLML): -959.9840\n",
      "convergence dfGPdfNN Run 9/10, Epoch 724/1000, Training Loss (NLML): -960.0099\n",
      "convergence dfGPdfNN Run 9/10, Epoch 725/1000, Training Loss (NLML): -960.0358\n",
      "convergence dfGPdfNN Run 9/10, Epoch 726/1000, Training Loss (NLML): -960.0530\n",
      "convergence dfGPdfNN Run 9/10, Epoch 727/1000, Training Loss (NLML): -960.0869\n",
      "convergence dfGPdfNN Run 9/10, Epoch 728/1000, Training Loss (NLML): -960.0946\n",
      "convergence dfGPdfNN Run 9/10, Epoch 729/1000, Training Loss (NLML): -960.0974\n",
      "convergence dfGPdfNN Run 9/10, Epoch 730/1000, Training Loss (NLML): -960.0760\n",
      "convergence dfGPdfNN Run 9/10, Epoch 731/1000, Training Loss (NLML): -960.0664\n",
      "convergence dfGPdfNN Run 9/10, Epoch 732/1000, Training Loss (NLML): -960.0743\n",
      "convergence dfGPdfNN Run 9/10, Epoch 733/1000, Training Loss (NLML): -960.0884\n",
      "convergence dfGPdfNN Run 9/10, Epoch 734/1000, Training Loss (NLML): -960.0708\n",
      "convergence dfGPdfNN Run 9/10, Epoch 735/1000, Training Loss (NLML): -960.0743\n",
      "convergence dfGPdfNN Run 9/10, Epoch 736/1000, Training Loss (NLML): -960.0776\n",
      "convergence dfGPdfNN Run 9/10, Epoch 737/1000, Training Loss (NLML): -960.0822\n",
      "convergence dfGPdfNN Run 9/10, Epoch 738/1000, Training Loss (NLML): -960.0851\n",
      "convergence dfGPdfNN Run 9/10, Epoch 739/1000, Training Loss (NLML): -960.0956\n",
      "convergence dfGPdfNN Run 9/10, Epoch 740/1000, Training Loss (NLML): -960.0887\n",
      "convergence dfGPdfNN Run 9/10, Epoch 741/1000, Training Loss (NLML): -960.0940\n",
      "convergence dfGPdfNN Run 9/10, Epoch 742/1000, Training Loss (NLML): -960.1040\n",
      "convergence dfGPdfNN Run 9/10, Epoch 743/1000, Training Loss (NLML): -960.1071\n",
      "convergence dfGPdfNN Run 9/10, Epoch 744/1000, Training Loss (NLML): -960.1227\n",
      "convergence dfGPdfNN Run 9/10, Epoch 745/1000, Training Loss (NLML): -960.1414\n",
      "convergence dfGPdfNN Run 9/10, Epoch 746/1000, Training Loss (NLML): -960.1605\n",
      "convergence dfGPdfNN Run 9/10, Epoch 747/1000, Training Loss (NLML): -960.1731\n",
      "Early stopping triggered after 747 epochs.\n",
      "\n",
      "--- Training Run 10/10 ---\n",
      "\n",
      "Start Training\n",
      "convergence dfGPdfNN Run 10/10, Epoch 1/1000, Training Loss (NLML): -863.7656\n",
      "convergence dfGPdfNN Run 10/10, Epoch 2/1000, Training Loss (NLML): -873.5072\n",
      "convergence dfGPdfNN Run 10/10, Epoch 3/1000, Training Loss (NLML): -879.6776\n",
      "convergence dfGPdfNN Run 10/10, Epoch 4/1000, Training Loss (NLML): -883.2446\n",
      "convergence dfGPdfNN Run 10/10, Epoch 5/1000, Training Loss (NLML): -886.3529\n",
      "convergence dfGPdfNN Run 10/10, Epoch 6/1000, Training Loss (NLML): -888.3539\n",
      "convergence dfGPdfNN Run 10/10, Epoch 7/1000, Training Loss (NLML): -891.1956\n",
      "convergence dfGPdfNN Run 10/10, Epoch 8/1000, Training Loss (NLML): -893.8679\n",
      "convergence dfGPdfNN Run 10/10, Epoch 9/1000, Training Loss (NLML): -895.9878\n",
      "convergence dfGPdfNN Run 10/10, Epoch 10/1000, Training Loss (NLML): -898.3131\n",
      "convergence dfGPdfNN Run 10/10, Epoch 11/1000, Training Loss (NLML): -900.1975\n",
      "convergence dfGPdfNN Run 10/10, Epoch 12/1000, Training Loss (NLML): -902.0211\n",
      "convergence dfGPdfNN Run 10/10, Epoch 13/1000, Training Loss (NLML): -903.8209\n",
      "convergence dfGPdfNN Run 10/10, Epoch 14/1000, Training Loss (NLML): -905.4860\n",
      "convergence dfGPdfNN Run 10/10, Epoch 15/1000, Training Loss (NLML): -906.9609\n",
      "convergence dfGPdfNN Run 10/10, Epoch 16/1000, Training Loss (NLML): -908.2709\n",
      "convergence dfGPdfNN Run 10/10, Epoch 17/1000, Training Loss (NLML): -909.7222\n",
      "convergence dfGPdfNN Run 10/10, Epoch 18/1000, Training Loss (NLML): -910.9832\n",
      "convergence dfGPdfNN Run 10/10, Epoch 19/1000, Training Loss (NLML): -912.3777\n",
      "convergence dfGPdfNN Run 10/10, Epoch 20/1000, Training Loss (NLML): -913.7439\n",
      "convergence dfGPdfNN Run 10/10, Epoch 21/1000, Training Loss (NLML): -914.9447\n",
      "convergence dfGPdfNN Run 10/10, Epoch 22/1000, Training Loss (NLML): -915.8541\n",
      "convergence dfGPdfNN Run 10/10, Epoch 23/1000, Training Loss (NLML): -916.9781\n",
      "convergence dfGPdfNN Run 10/10, Epoch 24/1000, Training Loss (NLML): -918.0626\n",
      "convergence dfGPdfNN Run 10/10, Epoch 25/1000, Training Loss (NLML): -919.0985\n",
      "convergence dfGPdfNN Run 10/10, Epoch 26/1000, Training Loss (NLML): -920.0940\n",
      "convergence dfGPdfNN Run 10/10, Epoch 27/1000, Training Loss (NLML): -921.0411\n",
      "convergence dfGPdfNN Run 10/10, Epoch 28/1000, Training Loss (NLML): -921.9946\n",
      "convergence dfGPdfNN Run 10/10, Epoch 29/1000, Training Loss (NLML): -922.9135\n",
      "convergence dfGPdfNN Run 10/10, Epoch 30/1000, Training Loss (NLML): -923.7518\n",
      "convergence dfGPdfNN Run 10/10, Epoch 31/1000, Training Loss (NLML): -924.5542\n",
      "convergence dfGPdfNN Run 10/10, Epoch 32/1000, Training Loss (NLML): -925.3232\n",
      "convergence dfGPdfNN Run 10/10, Epoch 33/1000, Training Loss (NLML): -926.0785\n",
      "convergence dfGPdfNN Run 10/10, Epoch 34/1000, Training Loss (NLML): -926.7753\n",
      "convergence dfGPdfNN Run 10/10, Epoch 35/1000, Training Loss (NLML): -927.4740\n",
      "convergence dfGPdfNN Run 10/10, Epoch 36/1000, Training Loss (NLML): -928.1323\n",
      "convergence dfGPdfNN Run 10/10, Epoch 37/1000, Training Loss (NLML): -928.7977\n",
      "convergence dfGPdfNN Run 10/10, Epoch 38/1000, Training Loss (NLML): -929.4542\n",
      "convergence dfGPdfNN Run 10/10, Epoch 39/1000, Training Loss (NLML): -929.4094\n",
      "convergence dfGPdfNN Run 10/10, Epoch 40/1000, Training Loss (NLML): -930.6852\n",
      "convergence dfGPdfNN Run 10/10, Epoch 41/1000, Training Loss (NLML): -931.2411\n",
      "convergence dfGPdfNN Run 10/10, Epoch 42/1000, Training Loss (NLML): -931.8016\n",
      "convergence dfGPdfNN Run 10/10, Epoch 43/1000, Training Loss (NLML): -932.3350\n",
      "convergence dfGPdfNN Run 10/10, Epoch 44/1000, Training Loss (NLML): -932.8572\n",
      "convergence dfGPdfNN Run 10/10, Epoch 45/1000, Training Loss (NLML): -933.3630\n",
      "convergence dfGPdfNN Run 10/10, Epoch 46/1000, Training Loss (NLML): -933.8713\n",
      "convergence dfGPdfNN Run 10/10, Epoch 47/1000, Training Loss (NLML): -934.3696\n",
      "convergence dfGPdfNN Run 10/10, Epoch 48/1000, Training Loss (NLML): -934.8500\n",
      "convergence dfGPdfNN Run 10/10, Epoch 49/1000, Training Loss (NLML): -935.3181\n",
      "convergence dfGPdfNN Run 10/10, Epoch 50/1000, Training Loss (NLML): -935.2140\n",
      "convergence dfGPdfNN Run 10/10, Epoch 51/1000, Training Loss (NLML): -935.6967\n",
      "convergence dfGPdfNN Run 10/10, Epoch 52/1000, Training Loss (NLML): -936.6471\n",
      "convergence dfGPdfNN Run 10/10, Epoch 53/1000, Training Loss (NLML): -936.1927\n",
      "convergence dfGPdfNN Run 10/10, Epoch 54/1000, Training Loss (NLML): -937.4700\n",
      "convergence dfGPdfNN Run 10/10, Epoch 55/1000, Training Loss (NLML): -937.8749\n",
      "convergence dfGPdfNN Run 10/10, Epoch 56/1000, Training Loss (NLML): -938.2632\n",
      "convergence dfGPdfNN Run 10/10, Epoch 57/1000, Training Loss (NLML): -938.6417\n",
      "convergence dfGPdfNN Run 10/10, Epoch 58/1000, Training Loss (NLML): -939.0231\n",
      "convergence dfGPdfNN Run 10/10, Epoch 59/1000, Training Loss (NLML): -939.3944\n",
      "convergence dfGPdfNN Run 10/10, Epoch 60/1000, Training Loss (NLML): -939.7715\n",
      "convergence dfGPdfNN Run 10/10, Epoch 61/1000, Training Loss (NLML): -940.1351\n",
      "convergence dfGPdfNN Run 10/10, Epoch 62/1000, Training Loss (NLML): -940.4921\n",
      "convergence dfGPdfNN Run 10/10, Epoch 63/1000, Training Loss (NLML): -940.8413\n",
      "convergence dfGPdfNN Run 10/10, Epoch 64/1000, Training Loss (NLML): -940.5608\n",
      "convergence dfGPdfNN Run 10/10, Epoch 65/1000, Training Loss (NLML): -941.4849\n",
      "convergence dfGPdfNN Run 10/10, Epoch 66/1000, Training Loss (NLML): -941.8168\n",
      "convergence dfGPdfNN Run 10/10, Epoch 67/1000, Training Loss (NLML): -942.1051\n",
      "convergence dfGPdfNN Run 10/10, Epoch 68/1000, Training Loss (NLML): -942.3687\n",
      "convergence dfGPdfNN Run 10/10, Epoch 69/1000, Training Loss (NLML): -942.6598\n",
      "convergence dfGPdfNN Run 10/10, Epoch 70/1000, Training Loss (NLML): -942.9916\n",
      "convergence dfGPdfNN Run 10/10, Epoch 71/1000, Training Loss (NLML): -943.3210\n",
      "convergence dfGPdfNN Run 10/10, Epoch 72/1000, Training Loss (NLML): -943.5498\n",
      "convergence dfGPdfNN Run 10/10, Epoch 73/1000, Training Loss (NLML): -943.8563\n",
      "convergence dfGPdfNN Run 10/10, Epoch 74/1000, Training Loss (NLML): -944.1165\n",
      "convergence dfGPdfNN Run 10/10, Epoch 75/1000, Training Loss (NLML): -944.3568\n",
      "convergence dfGPdfNN Run 10/10, Epoch 76/1000, Training Loss (NLML): -944.5851\n",
      "convergence dfGPdfNN Run 10/10, Epoch 77/1000, Training Loss (NLML): -944.8268\n",
      "convergence dfGPdfNN Run 10/10, Epoch 78/1000, Training Loss (NLML): -945.1135\n",
      "convergence dfGPdfNN Run 10/10, Epoch 79/1000, Training Loss (NLML): -945.4152\n",
      "convergence dfGPdfNN Run 10/10, Epoch 80/1000, Training Loss (NLML): -945.7179\n",
      "convergence dfGPdfNN Run 10/10, Epoch 81/1000, Training Loss (NLML): -945.9360\n",
      "convergence dfGPdfNN Run 10/10, Epoch 82/1000, Training Loss (NLML): -946.1537\n",
      "convergence dfGPdfNN Run 10/10, Epoch 83/1000, Training Loss (NLML): -946.3784\n",
      "convergence dfGPdfNN Run 10/10, Epoch 84/1000, Training Loss (NLML): -946.5878\n",
      "convergence dfGPdfNN Run 10/10, Epoch 85/1000, Training Loss (NLML): -946.8138\n",
      "convergence dfGPdfNN Run 10/10, Epoch 86/1000, Training Loss (NLML): -947.0468\n",
      "convergence dfGPdfNN Run 10/10, Epoch 87/1000, Training Loss (NLML): -947.2406\n",
      "convergence dfGPdfNN Run 10/10, Epoch 88/1000, Training Loss (NLML): -947.4553\n",
      "convergence dfGPdfNN Run 10/10, Epoch 89/1000, Training Loss (NLML): -947.6759\n",
      "convergence dfGPdfNN Run 10/10, Epoch 90/1000, Training Loss (NLML): -947.9028\n",
      "convergence dfGPdfNN Run 10/10, Epoch 91/1000, Training Loss (NLML): -948.1111\n",
      "convergence dfGPdfNN Run 10/10, Epoch 92/1000, Training Loss (NLML): -948.3041\n",
      "convergence dfGPdfNN Run 10/10, Epoch 93/1000, Training Loss (NLML): -948.5005\n",
      "convergence dfGPdfNN Run 10/10, Epoch 94/1000, Training Loss (NLML): -948.7045\n",
      "convergence dfGPdfNN Run 10/10, Epoch 95/1000, Training Loss (NLML): -948.8950\n",
      "convergence dfGPdfNN Run 10/10, Epoch 96/1000, Training Loss (NLML): -949.0803\n",
      "convergence dfGPdfNN Run 10/10, Epoch 97/1000, Training Loss (NLML): -949.2659\n",
      "convergence dfGPdfNN Run 10/10, Epoch 98/1000, Training Loss (NLML): -949.4406\n",
      "convergence dfGPdfNN Run 10/10, Epoch 99/1000, Training Loss (NLML): -949.6163\n",
      "convergence dfGPdfNN Run 10/10, Epoch 100/1000, Training Loss (NLML): -949.7858\n",
      "convergence dfGPdfNN Run 10/10, Epoch 101/1000, Training Loss (NLML): -949.9476\n",
      "convergence dfGPdfNN Run 10/10, Epoch 102/1000, Training Loss (NLML): -950.1199\n",
      "convergence dfGPdfNN Run 10/10, Epoch 103/1000, Training Loss (NLML): -950.2897\n",
      "convergence dfGPdfNN Run 10/10, Epoch 104/1000, Training Loss (NLML): -950.4478\n",
      "convergence dfGPdfNN Run 10/10, Epoch 105/1000, Training Loss (NLML): -950.5833\n",
      "convergence dfGPdfNN Run 10/10, Epoch 106/1000, Training Loss (NLML): -950.7317\n",
      "convergence dfGPdfNN Run 10/10, Epoch 107/1000, Training Loss (NLML): -950.8649\n",
      "convergence dfGPdfNN Run 10/10, Epoch 108/1000, Training Loss (NLML): -951.0072\n",
      "convergence dfGPdfNN Run 10/10, Epoch 109/1000, Training Loss (NLML): -951.1470\n",
      "convergence dfGPdfNN Run 10/10, Epoch 110/1000, Training Loss (NLML): -951.2927\n",
      "convergence dfGPdfNN Run 10/10, Epoch 111/1000, Training Loss (NLML): -951.4220\n",
      "convergence dfGPdfNN Run 10/10, Epoch 112/1000, Training Loss (NLML): -951.5430\n",
      "convergence dfGPdfNN Run 10/10, Epoch 113/1000, Training Loss (NLML): -951.6716\n",
      "convergence dfGPdfNN Run 10/10, Epoch 114/1000, Training Loss (NLML): -951.7938\n",
      "convergence dfGPdfNN Run 10/10, Epoch 115/1000, Training Loss (NLML): -951.9126\n",
      "convergence dfGPdfNN Run 10/10, Epoch 116/1000, Training Loss (NLML): -952.0265\n",
      "convergence dfGPdfNN Run 10/10, Epoch 117/1000, Training Loss (NLML): -952.1499\n",
      "convergence dfGPdfNN Run 10/10, Epoch 118/1000, Training Loss (NLML): -952.2627\n",
      "convergence dfGPdfNN Run 10/10, Epoch 119/1000, Training Loss (NLML): -952.3608\n",
      "convergence dfGPdfNN Run 10/10, Epoch 120/1000, Training Loss (NLML): -952.4548\n",
      "convergence dfGPdfNN Run 10/10, Epoch 121/1000, Training Loss (NLML): -952.5378\n",
      "convergence dfGPdfNN Run 10/10, Epoch 122/1000, Training Loss (NLML): -952.5725\n",
      "convergence dfGPdfNN Run 10/10, Epoch 123/1000, Training Loss (NLML): -952.6196\n",
      "convergence dfGPdfNN Run 10/10, Epoch 124/1000, Training Loss (NLML): -952.6359\n",
      "convergence dfGPdfNN Run 10/10, Epoch 125/1000, Training Loss (NLML): -952.5961\n",
      "convergence dfGPdfNN Run 10/10, Epoch 126/1000, Training Loss (NLML): -952.6691\n",
      "convergence dfGPdfNN Run 10/10, Epoch 127/1000, Training Loss (NLML): -952.7479\n",
      "convergence dfGPdfNN Run 10/10, Epoch 128/1000, Training Loss (NLML): -952.7982\n",
      "convergence dfGPdfNN Run 10/10, Epoch 129/1000, Training Loss (NLML): -952.8535\n",
      "convergence dfGPdfNN Run 10/10, Epoch 130/1000, Training Loss (NLML): -952.8914\n",
      "convergence dfGPdfNN Run 10/10, Epoch 131/1000, Training Loss (NLML): -952.9244\n",
      "convergence dfGPdfNN Run 10/10, Epoch 132/1000, Training Loss (NLML): -952.9969\n",
      "convergence dfGPdfNN Run 10/10, Epoch 133/1000, Training Loss (NLML): -953.0486\n",
      "convergence dfGPdfNN Run 10/10, Epoch 134/1000, Training Loss (NLML): -953.1423\n",
      "convergence dfGPdfNN Run 10/10, Epoch 135/1000, Training Loss (NLML): -953.2095\n",
      "convergence dfGPdfNN Run 10/10, Epoch 136/1000, Training Loss (NLML): -953.2528\n",
      "convergence dfGPdfNN Run 10/10, Epoch 137/1000, Training Loss (NLML): -953.3451\n",
      "convergence dfGPdfNN Run 10/10, Epoch 138/1000, Training Loss (NLML): -953.4155\n",
      "convergence dfGPdfNN Run 10/10, Epoch 139/1000, Training Loss (NLML): -953.5074\n",
      "convergence dfGPdfNN Run 10/10, Epoch 140/1000, Training Loss (NLML): -953.5889\n",
      "convergence dfGPdfNN Run 10/10, Epoch 141/1000, Training Loss (NLML): -953.6464\n",
      "convergence dfGPdfNN Run 10/10, Epoch 142/1000, Training Loss (NLML): -953.6844\n",
      "convergence dfGPdfNN Run 10/10, Epoch 143/1000, Training Loss (NLML): -953.6566\n",
      "convergence dfGPdfNN Run 10/10, Epoch 144/1000, Training Loss (NLML): -953.6589\n",
      "convergence dfGPdfNN Run 10/10, Epoch 145/1000, Training Loss (NLML): -953.6472\n",
      "convergence dfGPdfNN Run 10/10, Epoch 146/1000, Training Loss (NLML): -953.5760\n",
      "convergence dfGPdfNN Run 10/10, Epoch 147/1000, Training Loss (NLML): -953.5846\n",
      "convergence dfGPdfNN Run 10/10, Epoch 148/1000, Training Loss (NLML): -953.5974\n",
      "convergence dfGPdfNN Run 10/10, Epoch 149/1000, Training Loss (NLML): -953.6238\n",
      "convergence dfGPdfNN Run 10/10, Epoch 150/1000, Training Loss (NLML): -953.6956\n",
      "convergence dfGPdfNN Run 10/10, Epoch 151/1000, Training Loss (NLML): -953.7802\n",
      "convergence dfGPdfNN Run 10/10, Epoch 152/1000, Training Loss (NLML): -953.8522\n",
      "convergence dfGPdfNN Run 10/10, Epoch 153/1000, Training Loss (NLML): -953.9423\n",
      "convergence dfGPdfNN Run 10/10, Epoch 154/1000, Training Loss (NLML): -954.0421\n",
      "convergence dfGPdfNN Run 10/10, Epoch 155/1000, Training Loss (NLML): -954.2058\n",
      "convergence dfGPdfNN Run 10/10, Epoch 156/1000, Training Loss (NLML): -954.2156\n",
      "convergence dfGPdfNN Run 10/10, Epoch 157/1000, Training Loss (NLML): -954.2939\n",
      "convergence dfGPdfNN Run 10/10, Epoch 158/1000, Training Loss (NLML): -954.3735\n",
      "convergence dfGPdfNN Run 10/10, Epoch 159/1000, Training Loss (NLML): -954.4001\n",
      "convergence dfGPdfNN Run 10/10, Epoch 160/1000, Training Loss (NLML): -954.4945\n",
      "convergence dfGPdfNN Run 10/10, Epoch 161/1000, Training Loss (NLML): -954.4558\n",
      "convergence dfGPdfNN Run 10/10, Epoch 162/1000, Training Loss (NLML): -954.5365\n",
      "convergence dfGPdfNN Run 10/10, Epoch 163/1000, Training Loss (NLML): -954.5724\n",
      "convergence dfGPdfNN Run 10/10, Epoch 164/1000, Training Loss (NLML): -954.4056\n",
      "convergence dfGPdfNN Run 10/10, Epoch 165/1000, Training Loss (NLML): -954.2163\n",
      "convergence dfGPdfNN Run 10/10, Epoch 166/1000, Training Loss (NLML): -954.0654\n",
      "convergence dfGPdfNN Run 10/10, Epoch 167/1000, Training Loss (NLML): -954.0596\n",
      "convergence dfGPdfNN Run 10/10, Epoch 168/1000, Training Loss (NLML): -953.9827\n",
      "convergence dfGPdfNN Run 10/10, Epoch 169/1000, Training Loss (NLML): -954.1053\n",
      "convergence dfGPdfNN Run 10/10, Epoch 170/1000, Training Loss (NLML): -954.1100\n",
      "convergence dfGPdfNN Run 10/10, Epoch 171/1000, Training Loss (NLML): -918.8597\n",
      "convergence dfGPdfNN Run 10/10, Epoch 172/1000, Training Loss (NLML): -954.1677\n",
      "convergence dfGPdfNN Run 10/10, Epoch 173/1000, Training Loss (NLML): -954.0903\n",
      "convergence dfGPdfNN Run 10/10, Epoch 174/1000, Training Loss (NLML): -954.0261\n",
      "convergence dfGPdfNN Run 10/10, Epoch 175/1000, Training Loss (NLML): -953.8811\n",
      "convergence dfGPdfNN Run 10/10, Epoch 176/1000, Training Loss (NLML): -953.7418\n",
      "convergence dfGPdfNN Run 10/10, Epoch 177/1000, Training Loss (NLML): -953.8483\n",
      "convergence dfGPdfNN Run 10/10, Epoch 178/1000, Training Loss (NLML): -953.9185\n",
      "convergence dfGPdfNN Run 10/10, Epoch 179/1000, Training Loss (NLML): -954.0664\n",
      "convergence dfGPdfNN Run 10/10, Epoch 180/1000, Training Loss (NLML): -954.1854\n",
      "convergence dfGPdfNN Run 10/10, Epoch 181/1000, Training Loss (NLML): -954.2938\n",
      "convergence dfGPdfNN Run 10/10, Epoch 182/1000, Training Loss (NLML): -954.2931\n",
      "convergence dfGPdfNN Run 10/10, Epoch 183/1000, Training Loss (NLML): -954.2129\n",
      "convergence dfGPdfNN Run 10/10, Epoch 184/1000, Training Loss (NLML): -954.0662\n",
      "convergence dfGPdfNN Run 10/10, Epoch 185/1000, Training Loss (NLML): -954.1354\n",
      "convergence dfGPdfNN Run 10/10, Epoch 186/1000, Training Loss (NLML): -954.2220\n",
      "convergence dfGPdfNN Run 10/10, Epoch 187/1000, Training Loss (NLML): -954.2643\n",
      "convergence dfGPdfNN Run 10/10, Epoch 188/1000, Training Loss (NLML): -954.4442\n",
      "convergence dfGPdfNN Run 10/10, Epoch 189/1000, Training Loss (NLML): -954.6676\n",
      "convergence dfGPdfNN Run 10/10, Epoch 190/1000, Training Loss (NLML): -954.5228\n",
      "convergence dfGPdfNN Run 10/10, Epoch 191/1000, Training Loss (NLML): -954.4935\n",
      "convergence dfGPdfNN Run 10/10, Epoch 192/1000, Training Loss (NLML): -954.3898\n",
      "convergence dfGPdfNN Run 10/10, Epoch 193/1000, Training Loss (NLML): -954.2944\n",
      "convergence dfGPdfNN Run 10/10, Epoch 194/1000, Training Loss (NLML): -954.2714\n",
      "convergence dfGPdfNN Run 10/10, Epoch 195/1000, Training Loss (NLML): -954.3491\n",
      "convergence dfGPdfNN Run 10/10, Epoch 196/1000, Training Loss (NLML): -954.5015\n",
      "convergence dfGPdfNN Run 10/10, Epoch 197/1000, Training Loss (NLML): -954.8309\n",
      "convergence dfGPdfNN Run 10/10, Epoch 198/1000, Training Loss (NLML): -954.9785\n",
      "convergence dfGPdfNN Run 10/10, Epoch 199/1000, Training Loss (NLML): -955.0264\n",
      "convergence dfGPdfNN Run 10/10, Epoch 200/1000, Training Loss (NLML): -955.1714\n",
      "convergence dfGPdfNN Run 10/10, Epoch 201/1000, Training Loss (NLML): -955.1846\n",
      "convergence dfGPdfNN Run 10/10, Epoch 202/1000, Training Loss (NLML): -955.2686\n",
      "convergence dfGPdfNN Run 10/10, Epoch 203/1000, Training Loss (NLML): -955.3029\n",
      "convergence dfGPdfNN Run 10/10, Epoch 204/1000, Training Loss (NLML): -955.1270\n",
      "convergence dfGPdfNN Run 10/10, Epoch 205/1000, Training Loss (NLML): -955.1469\n",
      "convergence dfGPdfNN Run 10/10, Epoch 206/1000, Training Loss (NLML): -955.3661\n",
      "convergence dfGPdfNN Run 10/10, Epoch 207/1000, Training Loss (NLML): -955.5150\n",
      "convergence dfGPdfNN Run 10/10, Epoch 208/1000, Training Loss (NLML): -955.5033\n",
      "convergence dfGPdfNN Run 10/10, Epoch 209/1000, Training Loss (NLML): -955.4750\n",
      "convergence dfGPdfNN Run 10/10, Epoch 210/1000, Training Loss (NLML): -955.5789\n",
      "convergence dfGPdfNN Run 10/10, Epoch 211/1000, Training Loss (NLML): -955.5682\n",
      "convergence dfGPdfNN Run 10/10, Epoch 212/1000, Training Loss (NLML): -955.4442\n",
      "convergence dfGPdfNN Run 10/10, Epoch 213/1000, Training Loss (NLML): -955.4158\n",
      "convergence dfGPdfNN Run 10/10, Epoch 214/1000, Training Loss (NLML): -955.5265\n",
      "convergence dfGPdfNN Run 10/10, Epoch 215/1000, Training Loss (NLML): -955.4194\n",
      "convergence dfGPdfNN Run 10/10, Epoch 216/1000, Training Loss (NLML): -955.2717\n",
      "convergence dfGPdfNN Run 10/10, Epoch 217/1000, Training Loss (NLML): -955.1953\n",
      "convergence dfGPdfNN Run 10/10, Epoch 218/1000, Training Loss (NLML): -955.0918\n",
      "convergence dfGPdfNN Run 10/10, Epoch 219/1000, Training Loss (NLML): -955.0332\n",
      "convergence dfGPdfNN Run 10/10, Epoch 220/1000, Training Loss (NLML): -954.9849\n",
      "convergence dfGPdfNN Run 10/10, Epoch 221/1000, Training Loss (NLML): -955.0251\n",
      "convergence dfGPdfNN Run 10/10, Epoch 222/1000, Training Loss (NLML): -955.0538\n",
      "convergence dfGPdfNN Run 10/10, Epoch 223/1000, Training Loss (NLML): -955.0039\n",
      "convergence dfGPdfNN Run 10/10, Epoch 224/1000, Training Loss (NLML): -955.0211\n",
      "convergence dfGPdfNN Run 10/10, Epoch 225/1000, Training Loss (NLML): -955.0488\n",
      "convergence dfGPdfNN Run 10/10, Epoch 226/1000, Training Loss (NLML): -955.0483\n",
      "convergence dfGPdfNN Run 10/10, Epoch 227/1000, Training Loss (NLML): -955.0851\n",
      "convergence dfGPdfNN Run 10/10, Epoch 228/1000, Training Loss (NLML): -955.1110\n",
      "convergence dfGPdfNN Run 10/10, Epoch 229/1000, Training Loss (NLML): -955.0841\n",
      "convergence dfGPdfNN Run 10/10, Epoch 230/1000, Training Loss (NLML): -955.0680\n",
      "convergence dfGPdfNN Run 10/10, Epoch 231/1000, Training Loss (NLML): -955.0876\n",
      "convergence dfGPdfNN Run 10/10, Epoch 232/1000, Training Loss (NLML): -955.0986\n",
      "convergence dfGPdfNN Run 10/10, Epoch 233/1000, Training Loss (NLML): -955.1133\n",
      "convergence dfGPdfNN Run 10/10, Epoch 234/1000, Training Loss (NLML): -955.1371\n",
      "convergence dfGPdfNN Run 10/10, Epoch 235/1000, Training Loss (NLML): -955.1705\n",
      "convergence dfGPdfNN Run 10/10, Epoch 236/1000, Training Loss (NLML): -955.2411\n",
      "convergence dfGPdfNN Run 10/10, Epoch 237/1000, Training Loss (NLML): -955.2673\n",
      "convergence dfGPdfNN Run 10/10, Epoch 238/1000, Training Loss (NLML): -955.3032\n",
      "convergence dfGPdfNN Run 10/10, Epoch 239/1000, Training Loss (NLML): -955.4050\n",
      "convergence dfGPdfNN Run 10/10, Epoch 240/1000, Training Loss (NLML): -955.4388\n",
      "convergence dfGPdfNN Run 10/10, Epoch 241/1000, Training Loss (NLML): -955.4741\n",
      "convergence dfGPdfNN Run 10/10, Epoch 242/1000, Training Loss (NLML): -955.4948\n",
      "convergence dfGPdfNN Run 10/10, Epoch 243/1000, Training Loss (NLML): -955.5516\n",
      "convergence dfGPdfNN Run 10/10, Epoch 244/1000, Training Loss (NLML): -955.5784\n",
      "convergence dfGPdfNN Run 10/10, Epoch 245/1000, Training Loss (NLML): -955.5717\n",
      "convergence dfGPdfNN Run 10/10, Epoch 246/1000, Training Loss (NLML): -955.5254\n",
      "convergence dfGPdfNN Run 10/10, Epoch 247/1000, Training Loss (NLML): -955.4574\n",
      "convergence dfGPdfNN Run 10/10, Epoch 248/1000, Training Loss (NLML): -955.5570\n",
      "convergence dfGPdfNN Run 10/10, Epoch 249/1000, Training Loss (NLML): -955.5834\n",
      "convergence dfGPdfNN Run 10/10, Epoch 250/1000, Training Loss (NLML): -955.5997\n",
      "convergence dfGPdfNN Run 10/10, Epoch 251/1000, Training Loss (NLML): -955.6482\n",
      "convergence dfGPdfNN Run 10/10, Epoch 252/1000, Training Loss (NLML): -955.6796\n",
      "convergence dfGPdfNN Run 10/10, Epoch 253/1000, Training Loss (NLML): -955.7295\n",
      "convergence dfGPdfNN Run 10/10, Epoch 254/1000, Training Loss (NLML): -955.7380\n",
      "convergence dfGPdfNN Run 10/10, Epoch 255/1000, Training Loss (NLML): -955.7427\n",
      "convergence dfGPdfNN Run 10/10, Epoch 256/1000, Training Loss (NLML): -955.7191\n",
      "convergence dfGPdfNN Run 10/10, Epoch 257/1000, Training Loss (NLML): -955.6696\n",
      "convergence dfGPdfNN Run 10/10, Epoch 258/1000, Training Loss (NLML): -955.6383\n",
      "convergence dfGPdfNN Run 10/10, Epoch 259/1000, Training Loss (NLML): -955.6252\n",
      "convergence dfGPdfNN Run 10/10, Epoch 260/1000, Training Loss (NLML): -955.5812\n",
      "convergence dfGPdfNN Run 10/10, Epoch 261/1000, Training Loss (NLML): -955.6237\n",
      "convergence dfGPdfNN Run 10/10, Epoch 262/1000, Training Loss (NLML): -955.6854\n",
      "convergence dfGPdfNN Run 10/10, Epoch 263/1000, Training Loss (NLML): -955.8143\n",
      "convergence dfGPdfNN Run 10/10, Epoch 264/1000, Training Loss (NLML): -955.8474\n",
      "convergence dfGPdfNN Run 10/10, Epoch 265/1000, Training Loss (NLML): -955.8317\n",
      "convergence dfGPdfNN Run 10/10, Epoch 266/1000, Training Loss (NLML): -955.7598\n",
      "convergence dfGPdfNN Run 10/10, Epoch 267/1000, Training Loss (NLML): -955.7375\n",
      "convergence dfGPdfNN Run 10/10, Epoch 268/1000, Training Loss (NLML): -955.8158\n",
      "convergence dfGPdfNN Run 10/10, Epoch 269/1000, Training Loss (NLML): -955.8364\n",
      "convergence dfGPdfNN Run 10/10, Epoch 270/1000, Training Loss (NLML): -955.8765\n",
      "convergence dfGPdfNN Run 10/10, Epoch 271/1000, Training Loss (NLML): -955.9318\n",
      "convergence dfGPdfNN Run 10/10, Epoch 272/1000, Training Loss (NLML): -955.9493\n",
      "convergence dfGPdfNN Run 10/10, Epoch 273/1000, Training Loss (NLML): -955.9567\n",
      "convergence dfGPdfNN Run 10/10, Epoch 274/1000, Training Loss (NLML): -955.9668\n",
      "convergence dfGPdfNN Run 10/10, Epoch 275/1000, Training Loss (NLML): -955.9958\n",
      "convergence dfGPdfNN Run 10/10, Epoch 276/1000, Training Loss (NLML): -956.0341\n",
      "convergence dfGPdfNN Run 10/10, Epoch 277/1000, Training Loss (NLML): -956.0538\n",
      "convergence dfGPdfNN Run 10/10, Epoch 278/1000, Training Loss (NLML): -956.0596\n",
      "convergence dfGPdfNN Run 10/10, Epoch 279/1000, Training Loss (NLML): -956.0839\n",
      "convergence dfGPdfNN Run 10/10, Epoch 280/1000, Training Loss (NLML): -956.1001\n",
      "convergence dfGPdfNN Run 10/10, Epoch 281/1000, Training Loss (NLML): -956.1244\n",
      "convergence dfGPdfNN Run 10/10, Epoch 282/1000, Training Loss (NLML): -956.1648\n",
      "convergence dfGPdfNN Run 10/10, Epoch 283/1000, Training Loss (NLML): -956.1803\n",
      "convergence dfGPdfNN Run 10/10, Epoch 284/1000, Training Loss (NLML): -956.2000\n",
      "convergence dfGPdfNN Run 10/10, Epoch 285/1000, Training Loss (NLML): -956.1798\n",
      "convergence dfGPdfNN Run 10/10, Epoch 286/1000, Training Loss (NLML): -956.1984\n",
      "convergence dfGPdfNN Run 10/10, Epoch 287/1000, Training Loss (NLML): -956.2198\n",
      "convergence dfGPdfNN Run 10/10, Epoch 288/1000, Training Loss (NLML): -956.2378\n",
      "convergence dfGPdfNN Run 10/10, Epoch 289/1000, Training Loss (NLML): -956.2539\n",
      "convergence dfGPdfNN Run 10/10, Epoch 290/1000, Training Loss (NLML): -956.3029\n",
      "convergence dfGPdfNN Run 10/10, Epoch 291/1000, Training Loss (NLML): -956.3545\n",
      "convergence dfGPdfNN Run 10/10, Epoch 292/1000, Training Loss (NLML): -956.3723\n",
      "convergence dfGPdfNN Run 10/10, Epoch 293/1000, Training Loss (NLML): -956.3926\n",
      "convergence dfGPdfNN Run 10/10, Epoch 294/1000, Training Loss (NLML): -956.4207\n",
      "convergence dfGPdfNN Run 10/10, Epoch 295/1000, Training Loss (NLML): -956.4392\n",
      "convergence dfGPdfNN Run 10/10, Epoch 296/1000, Training Loss (NLML): -956.4515\n",
      "convergence dfGPdfNN Run 10/10, Epoch 297/1000, Training Loss (NLML): -956.4678\n",
      "convergence dfGPdfNN Run 10/10, Epoch 298/1000, Training Loss (NLML): -956.4845\n",
      "convergence dfGPdfNN Run 10/10, Epoch 299/1000, Training Loss (NLML): -956.4991\n",
      "convergence dfGPdfNN Run 10/10, Epoch 300/1000, Training Loss (NLML): -956.5044\n",
      "convergence dfGPdfNN Run 10/10, Epoch 301/1000, Training Loss (NLML): -956.5535\n",
      "convergence dfGPdfNN Run 10/10, Epoch 302/1000, Training Loss (NLML): -956.5598\n",
      "convergence dfGPdfNN Run 10/10, Epoch 303/1000, Training Loss (NLML): -956.5983\n",
      "convergence dfGPdfNN Run 10/10, Epoch 304/1000, Training Loss (NLML): -956.6157\n",
      "convergence dfGPdfNN Run 10/10, Epoch 305/1000, Training Loss (NLML): -956.6307\n",
      "convergence dfGPdfNN Run 10/10, Epoch 306/1000, Training Loss (NLML): -956.6481\n",
      "convergence dfGPdfNN Run 10/10, Epoch 307/1000, Training Loss (NLML): -956.6643\n",
      "convergence dfGPdfNN Run 10/10, Epoch 308/1000, Training Loss (NLML): -956.6881\n",
      "convergence dfGPdfNN Run 10/10, Epoch 309/1000, Training Loss (NLML): -956.6774\n",
      "convergence dfGPdfNN Run 10/10, Epoch 310/1000, Training Loss (NLML): -956.6830\n",
      "convergence dfGPdfNN Run 10/10, Epoch 311/1000, Training Loss (NLML): -956.7064\n",
      "convergence dfGPdfNN Run 10/10, Epoch 312/1000, Training Loss (NLML): -956.7235\n",
      "convergence dfGPdfNN Run 10/10, Epoch 313/1000, Training Loss (NLML): -956.7744\n",
      "convergence dfGPdfNN Run 10/10, Epoch 314/1000, Training Loss (NLML): -956.7900\n",
      "convergence dfGPdfNN Run 10/10, Epoch 315/1000, Training Loss (NLML): -956.8046\n",
      "convergence dfGPdfNN Run 10/10, Epoch 316/1000, Training Loss (NLML): -956.8143\n",
      "convergence dfGPdfNN Run 10/10, Epoch 317/1000, Training Loss (NLML): -956.8281\n",
      "convergence dfGPdfNN Run 10/10, Epoch 318/1000, Training Loss (NLML): -956.8439\n",
      "convergence dfGPdfNN Run 10/10, Epoch 319/1000, Training Loss (NLML): -956.8546\n",
      "convergence dfGPdfNN Run 10/10, Epoch 320/1000, Training Loss (NLML): -956.8684\n",
      "convergence dfGPdfNN Run 10/10, Epoch 321/1000, Training Loss (NLML): -956.8807\n",
      "convergence dfGPdfNN Run 10/10, Epoch 322/1000, Training Loss (NLML): -956.8949\n",
      "convergence dfGPdfNN Run 10/10, Epoch 323/1000, Training Loss (NLML): -956.9091\n",
      "convergence dfGPdfNN Run 10/10, Epoch 324/1000, Training Loss (NLML): -956.9382\n",
      "convergence dfGPdfNN Run 10/10, Epoch 325/1000, Training Loss (NLML): -956.9526\n",
      "convergence dfGPdfNN Run 10/10, Epoch 326/1000, Training Loss (NLML): -956.9656\n",
      "convergence dfGPdfNN Run 10/10, Epoch 327/1000, Training Loss (NLML): -956.9807\n",
      "convergence dfGPdfNN Run 10/10, Epoch 328/1000, Training Loss (NLML): -956.9943\n",
      "convergence dfGPdfNN Run 10/10, Epoch 329/1000, Training Loss (NLML): -956.9741\n",
      "convergence dfGPdfNN Run 10/10, Epoch 330/1000, Training Loss (NLML): -956.9534\n",
      "convergence dfGPdfNN Run 10/10, Epoch 331/1000, Training Loss (NLML): -956.9576\n",
      "convergence dfGPdfNN Run 10/10, Epoch 332/1000, Training Loss (NLML): -956.9741\n",
      "convergence dfGPdfNN Run 10/10, Epoch 333/1000, Training Loss (NLML): -956.9921\n",
      "convergence dfGPdfNN Run 10/10, Epoch 334/1000, Training Loss (NLML): -957.0062\n",
      "convergence dfGPdfNN Run 10/10, Epoch 335/1000, Training Loss (NLML): -956.9783\n",
      "convergence dfGPdfNN Run 10/10, Epoch 336/1000, Training Loss (NLML): -956.9601\n",
      "convergence dfGPdfNN Run 10/10, Epoch 337/1000, Training Loss (NLML): -956.9751\n",
      "convergence dfGPdfNN Run 10/10, Epoch 338/1000, Training Loss (NLML): -956.9910\n",
      "convergence dfGPdfNN Run 10/10, Epoch 339/1000, Training Loss (NLML): -956.9996\n",
      "convergence dfGPdfNN Run 10/10, Epoch 340/1000, Training Loss (NLML): -957.0122\n",
      "convergence dfGPdfNN Run 10/10, Epoch 341/1000, Training Loss (NLML): -957.0551\n",
      "convergence dfGPdfNN Run 10/10, Epoch 342/1000, Training Loss (NLML): -957.0697\n",
      "convergence dfGPdfNN Run 10/10, Epoch 343/1000, Training Loss (NLML): -957.0836\n",
      "convergence dfGPdfNN Run 10/10, Epoch 344/1000, Training Loss (NLML): -957.0865\n",
      "convergence dfGPdfNN Run 10/10, Epoch 345/1000, Training Loss (NLML): -957.0719\n",
      "convergence dfGPdfNN Run 10/10, Epoch 346/1000, Training Loss (NLML): -957.0864\n",
      "convergence dfGPdfNN Run 10/10, Epoch 347/1000, Training Loss (NLML): -957.0951\n",
      "convergence dfGPdfNN Run 10/10, Epoch 348/1000, Training Loss (NLML): -957.1014\n",
      "convergence dfGPdfNN Run 10/10, Epoch 349/1000, Training Loss (NLML): -957.1097\n",
      "convergence dfGPdfNN Run 10/10, Epoch 350/1000, Training Loss (NLML): -957.1252\n",
      "convergence dfGPdfNN Run 10/10, Epoch 351/1000, Training Loss (NLML): -957.1389\n",
      "convergence dfGPdfNN Run 10/10, Epoch 352/1000, Training Loss (NLML): -957.1575\n",
      "convergence dfGPdfNN Run 10/10, Epoch 353/1000, Training Loss (NLML): -957.1750\n",
      "convergence dfGPdfNN Run 10/10, Epoch 354/1000, Training Loss (NLML): -957.1967\n",
      "convergence dfGPdfNN Run 10/10, Epoch 355/1000, Training Loss (NLML): -957.2233\n",
      "convergence dfGPdfNN Run 10/10, Epoch 356/1000, Training Loss (NLML): -957.2609\n",
      "convergence dfGPdfNN Run 10/10, Epoch 357/1000, Training Loss (NLML): -957.2917\n",
      "convergence dfGPdfNN Run 10/10, Epoch 358/1000, Training Loss (NLML): -957.3337\n",
      "convergence dfGPdfNN Run 10/10, Epoch 359/1000, Training Loss (NLML): -957.3531\n",
      "convergence dfGPdfNN Run 10/10, Epoch 360/1000, Training Loss (NLML): -957.3334\n",
      "convergence dfGPdfNN Run 10/10, Epoch 361/1000, Training Loss (NLML): -957.3076\n",
      "convergence dfGPdfNN Run 10/10, Epoch 362/1000, Training Loss (NLML): -957.2570\n",
      "convergence dfGPdfNN Run 10/10, Epoch 363/1000, Training Loss (NLML): -957.3042\n",
      "convergence dfGPdfNN Run 10/10, Epoch 364/1000, Training Loss (NLML): -957.3040\n",
      "convergence dfGPdfNN Run 10/10, Epoch 365/1000, Training Loss (NLML): -957.2601\n",
      "convergence dfGPdfNN Run 10/10, Epoch 366/1000, Training Loss (NLML): -957.1542\n",
      "convergence dfGPdfNN Run 10/10, Epoch 367/1000, Training Loss (NLML): -957.2798\n",
      "convergence dfGPdfNN Run 10/10, Epoch 368/1000, Training Loss (NLML): -957.4921\n",
      "convergence dfGPdfNN Run 10/10, Epoch 369/1000, Training Loss (NLML): -957.5422\n",
      "convergence dfGPdfNN Run 10/10, Epoch 370/1000, Training Loss (NLML): -957.5643\n",
      "convergence dfGPdfNN Run 10/10, Epoch 371/1000, Training Loss (NLML): -957.5729\n",
      "convergence dfGPdfNN Run 10/10, Epoch 372/1000, Training Loss (NLML): -957.5753\n",
      "convergence dfGPdfNN Run 10/10, Epoch 373/1000, Training Loss (NLML): -957.4969\n",
      "convergence dfGPdfNN Run 10/10, Epoch 374/1000, Training Loss (NLML): -957.4252\n",
      "convergence dfGPdfNN Run 10/10, Epoch 375/1000, Training Loss (NLML): -957.3971\n",
      "convergence dfGPdfNN Run 10/10, Epoch 376/1000, Training Loss (NLML): -957.3846\n",
      "convergence dfGPdfNN Run 10/10, Epoch 377/1000, Training Loss (NLML): -957.3871\n",
      "convergence dfGPdfNN Run 10/10, Epoch 378/1000, Training Loss (NLML): -957.4402\n",
      "convergence dfGPdfNN Run 10/10, Epoch 379/1000, Training Loss (NLML): -957.4780\n",
      "convergence dfGPdfNN Run 10/10, Epoch 380/1000, Training Loss (NLML): -957.5278\n",
      "convergence dfGPdfNN Run 10/10, Epoch 381/1000, Training Loss (NLML): -957.5565\n",
      "convergence dfGPdfNN Run 10/10, Epoch 382/1000, Training Loss (NLML): -957.5684\n",
      "convergence dfGPdfNN Run 10/10, Epoch 383/1000, Training Loss (NLML): -957.5845\n",
      "convergence dfGPdfNN Run 10/10, Epoch 384/1000, Training Loss (NLML): -957.5929\n",
      "convergence dfGPdfNN Run 10/10, Epoch 385/1000, Training Loss (NLML): -957.6124\n",
      "convergence dfGPdfNN Run 10/10, Epoch 386/1000, Training Loss (NLML): -957.6777\n",
      "convergence dfGPdfNN Run 10/10, Epoch 387/1000, Training Loss (NLML): -957.6902\n",
      "convergence dfGPdfNN Run 10/10, Epoch 388/1000, Training Loss (NLML): -957.7042\n",
      "convergence dfGPdfNN Run 10/10, Epoch 389/1000, Training Loss (NLML): -957.7184\n",
      "convergence dfGPdfNN Run 10/10, Epoch 390/1000, Training Loss (NLML): -957.6693\n",
      "convergence dfGPdfNN Run 10/10, Epoch 391/1000, Training Loss (NLML): -957.6753\n",
      "convergence dfGPdfNN Run 10/10, Epoch 392/1000, Training Loss (NLML): -957.6827\n",
      "convergence dfGPdfNN Run 10/10, Epoch 393/1000, Training Loss (NLML): -957.6919\n",
      "convergence dfGPdfNN Run 10/10, Epoch 394/1000, Training Loss (NLML): -957.6663\n",
      "convergence dfGPdfNN Run 10/10, Epoch 395/1000, Training Loss (NLML): -957.6787\n",
      "convergence dfGPdfNN Run 10/10, Epoch 396/1000, Training Loss (NLML): -957.6512\n",
      "convergence dfGPdfNN Run 10/10, Epoch 397/1000, Training Loss (NLML): -957.6539\n",
      "convergence dfGPdfNN Run 10/10, Epoch 398/1000, Training Loss (NLML): -957.6685\n",
      "convergence dfGPdfNN Run 10/10, Epoch 399/1000, Training Loss (NLML): -957.7144\n",
      "convergence dfGPdfNN Run 10/10, Epoch 400/1000, Training Loss (NLML): -957.7357\n",
      "convergence dfGPdfNN Run 10/10, Epoch 401/1000, Training Loss (NLML): -957.7461\n",
      "convergence dfGPdfNN Run 10/10, Epoch 402/1000, Training Loss (NLML): -957.7571\n",
      "convergence dfGPdfNN Run 10/10, Epoch 403/1000, Training Loss (NLML): -957.7722\n",
      "convergence dfGPdfNN Run 10/10, Epoch 404/1000, Training Loss (NLML): -957.7825\n",
      "convergence dfGPdfNN Run 10/10, Epoch 405/1000, Training Loss (NLML): -957.7948\n",
      "convergence dfGPdfNN Run 10/10, Epoch 406/1000, Training Loss (NLML): -957.8099\n",
      "convergence dfGPdfNN Run 10/10, Epoch 407/1000, Training Loss (NLML): -957.8214\n",
      "convergence dfGPdfNN Run 10/10, Epoch 408/1000, Training Loss (NLML): -957.8326\n",
      "convergence dfGPdfNN Run 10/10, Epoch 409/1000, Training Loss (NLML): -957.8422\n",
      "convergence dfGPdfNN Run 10/10, Epoch 410/1000, Training Loss (NLML): -957.8528\n",
      "convergence dfGPdfNN Run 10/10, Epoch 411/1000, Training Loss (NLML): -957.8585\n",
      "convergence dfGPdfNN Run 10/10, Epoch 412/1000, Training Loss (NLML): -957.8706\n",
      "convergence dfGPdfNN Run 10/10, Epoch 413/1000, Training Loss (NLML): -957.8802\n",
      "convergence dfGPdfNN Run 10/10, Epoch 414/1000, Training Loss (NLML): -957.8917\n",
      "convergence dfGPdfNN Run 10/10, Epoch 415/1000, Training Loss (NLML): -957.9020\n",
      "convergence dfGPdfNN Run 10/10, Epoch 416/1000, Training Loss (NLML): -957.9066\n",
      "convergence dfGPdfNN Run 10/10, Epoch 417/1000, Training Loss (NLML): -957.9478\n",
      "convergence dfGPdfNN Run 10/10, Epoch 418/1000, Training Loss (NLML): -957.9565\n",
      "convergence dfGPdfNN Run 10/10, Epoch 419/1000, Training Loss (NLML): -957.9657\n",
      "convergence dfGPdfNN Run 10/10, Epoch 420/1000, Training Loss (NLML): -957.9771\n",
      "convergence dfGPdfNN Run 10/10, Epoch 421/1000, Training Loss (NLML): -957.9879\n",
      "convergence dfGPdfNN Run 10/10, Epoch 422/1000, Training Loss (NLML): -958.0013\n",
      "convergence dfGPdfNN Run 10/10, Epoch 423/1000, Training Loss (NLML): -958.0125\n",
      "convergence dfGPdfNN Run 10/10, Epoch 424/1000, Training Loss (NLML): -958.0214\n",
      "convergence dfGPdfNN Run 10/10, Epoch 425/1000, Training Loss (NLML): -958.0327\n",
      "convergence dfGPdfNN Run 10/10, Epoch 426/1000, Training Loss (NLML): -958.0436\n",
      "convergence dfGPdfNN Run 10/10, Epoch 427/1000, Training Loss (NLML): -958.0518\n",
      "convergence dfGPdfNN Run 10/10, Epoch 428/1000, Training Loss (NLML): -958.0619\n",
      "convergence dfGPdfNN Run 10/10, Epoch 429/1000, Training Loss (NLML): -958.0717\n",
      "convergence dfGPdfNN Run 10/10, Epoch 430/1000, Training Loss (NLML): -958.0817\n",
      "convergence dfGPdfNN Run 10/10, Epoch 431/1000, Training Loss (NLML): -958.0920\n",
      "convergence dfGPdfNN Run 10/10, Epoch 432/1000, Training Loss (NLML): -958.1016\n",
      "convergence dfGPdfNN Run 10/10, Epoch 433/1000, Training Loss (NLML): -958.1115\n",
      "convergence dfGPdfNN Run 10/10, Epoch 434/1000, Training Loss (NLML): -958.1212\n",
      "convergence dfGPdfNN Run 10/10, Epoch 435/1000, Training Loss (NLML): -958.1318\n",
      "convergence dfGPdfNN Run 10/10, Epoch 436/1000, Training Loss (NLML): -958.1393\n",
      "convergence dfGPdfNN Run 10/10, Epoch 437/1000, Training Loss (NLML): -958.1487\n",
      "convergence dfGPdfNN Run 10/10, Epoch 438/1000, Training Loss (NLML): -958.1592\n",
      "convergence dfGPdfNN Run 10/10, Epoch 439/1000, Training Loss (NLML): -958.1688\n",
      "convergence dfGPdfNN Run 10/10, Epoch 440/1000, Training Loss (NLML): -958.1770\n",
      "convergence dfGPdfNN Run 10/10, Epoch 441/1000, Training Loss (NLML): -958.1868\n",
      "convergence dfGPdfNN Run 10/10, Epoch 442/1000, Training Loss (NLML): -958.1951\n",
      "convergence dfGPdfNN Run 10/10, Epoch 443/1000, Training Loss (NLML): -958.2051\n",
      "convergence dfGPdfNN Run 10/10, Epoch 444/1000, Training Loss (NLML): -958.2152\n",
      "convergence dfGPdfNN Run 10/10, Epoch 445/1000, Training Loss (NLML): -958.2225\n",
      "convergence dfGPdfNN Run 10/10, Epoch 446/1000, Training Loss (NLML): -958.2325\n",
      "convergence dfGPdfNN Run 10/10, Epoch 447/1000, Training Loss (NLML): -958.2399\n",
      "convergence dfGPdfNN Run 10/10, Epoch 448/1000, Training Loss (NLML): -958.2493\n",
      "convergence dfGPdfNN Run 10/10, Epoch 449/1000, Training Loss (NLML): -958.2609\n",
      "convergence dfGPdfNN Run 10/10, Epoch 450/1000, Training Loss (NLML): -958.2677\n",
      "convergence dfGPdfNN Run 10/10, Epoch 451/1000, Training Loss (NLML): -958.2769\n",
      "convergence dfGPdfNN Run 10/10, Epoch 452/1000, Training Loss (NLML): -958.2850\n",
      "convergence dfGPdfNN Run 10/10, Epoch 453/1000, Training Loss (NLML): -958.2948\n",
      "convergence dfGPdfNN Run 10/10, Epoch 454/1000, Training Loss (NLML): -958.3031\n",
      "convergence dfGPdfNN Run 10/10, Epoch 455/1000, Training Loss (NLML): -958.3124\n",
      "convergence dfGPdfNN Run 10/10, Epoch 456/1000, Training Loss (NLML): -958.3213\n",
      "convergence dfGPdfNN Run 10/10, Epoch 457/1000, Training Loss (NLML): -958.3291\n",
      "convergence dfGPdfNN Run 10/10, Epoch 458/1000, Training Loss (NLML): -958.3373\n",
      "convergence dfGPdfNN Run 10/10, Epoch 459/1000, Training Loss (NLML): -958.3457\n",
      "convergence dfGPdfNN Run 10/10, Epoch 460/1000, Training Loss (NLML): -958.3561\n",
      "convergence dfGPdfNN Run 10/10, Epoch 461/1000, Training Loss (NLML): -958.3595\n",
      "convergence dfGPdfNN Run 10/10, Epoch 462/1000, Training Loss (NLML): -958.3673\n",
      "convergence dfGPdfNN Run 10/10, Epoch 463/1000, Training Loss (NLML): -958.3767\n",
      "convergence dfGPdfNN Run 10/10, Epoch 464/1000, Training Loss (NLML): -958.3851\n",
      "convergence dfGPdfNN Run 10/10, Epoch 465/1000, Training Loss (NLML): -958.3950\n",
      "convergence dfGPdfNN Run 10/10, Epoch 466/1000, Training Loss (NLML): -958.3977\n",
      "convergence dfGPdfNN Run 10/10, Epoch 467/1000, Training Loss (NLML): -958.4094\n",
      "convergence dfGPdfNN Run 10/10, Epoch 468/1000, Training Loss (NLML): -958.4183\n",
      "convergence dfGPdfNN Run 10/10, Epoch 469/1000, Training Loss (NLML): -958.4274\n",
      "convergence dfGPdfNN Run 10/10, Epoch 470/1000, Training Loss (NLML): -958.4365\n",
      "convergence dfGPdfNN Run 10/10, Epoch 471/1000, Training Loss (NLML): -958.4432\n",
      "convergence dfGPdfNN Run 10/10, Epoch 472/1000, Training Loss (NLML): -958.4529\n",
      "convergence dfGPdfNN Run 10/10, Epoch 473/1000, Training Loss (NLML): -958.4620\n",
      "convergence dfGPdfNN Run 10/10, Epoch 474/1000, Training Loss (NLML): -958.4694\n",
      "convergence dfGPdfNN Run 10/10, Epoch 475/1000, Training Loss (NLML): -958.4772\n",
      "convergence dfGPdfNN Run 10/10, Epoch 476/1000, Training Loss (NLML): -958.4865\n",
      "convergence dfGPdfNN Run 10/10, Epoch 477/1000, Training Loss (NLML): -958.4922\n",
      "convergence dfGPdfNN Run 10/10, Epoch 478/1000, Training Loss (NLML): -958.5010\n",
      "convergence dfGPdfNN Run 10/10, Epoch 479/1000, Training Loss (NLML): -958.5088\n",
      "convergence dfGPdfNN Run 10/10, Epoch 480/1000, Training Loss (NLML): -958.5186\n",
      "convergence dfGPdfNN Run 10/10, Epoch 481/1000, Training Loss (NLML): -958.5254\n",
      "convergence dfGPdfNN Run 10/10, Epoch 482/1000, Training Loss (NLML): -958.5333\n",
      "convergence dfGPdfNN Run 10/10, Epoch 483/1000, Training Loss (NLML): -958.5411\n",
      "convergence dfGPdfNN Run 10/10, Epoch 484/1000, Training Loss (NLML): -958.5503\n",
      "convergence dfGPdfNN Run 10/10, Epoch 485/1000, Training Loss (NLML): -958.5583\n",
      "convergence dfGPdfNN Run 10/10, Epoch 486/1000, Training Loss (NLML): -958.5647\n",
      "convergence dfGPdfNN Run 10/10, Epoch 487/1000, Training Loss (NLML): -958.5720\n",
      "convergence dfGPdfNN Run 10/10, Epoch 488/1000, Training Loss (NLML): -958.5809\n",
      "convergence dfGPdfNN Run 10/10, Epoch 489/1000, Training Loss (NLML): -958.5879\n",
      "convergence dfGPdfNN Run 10/10, Epoch 490/1000, Training Loss (NLML): -958.5962\n",
      "convergence dfGPdfNN Run 10/10, Epoch 491/1000, Training Loss (NLML): -958.6027\n",
      "convergence dfGPdfNN Run 10/10, Epoch 492/1000, Training Loss (NLML): -958.6105\n",
      "convergence dfGPdfNN Run 10/10, Epoch 493/1000, Training Loss (NLML): -958.6188\n",
      "convergence dfGPdfNN Run 10/10, Epoch 494/1000, Training Loss (NLML): -958.6270\n",
      "convergence dfGPdfNN Run 10/10, Epoch 495/1000, Training Loss (NLML): -958.6337\n",
      "convergence dfGPdfNN Run 10/10, Epoch 496/1000, Training Loss (NLML): -958.6405\n",
      "convergence dfGPdfNN Run 10/10, Epoch 497/1000, Training Loss (NLML): -958.6492\n",
      "convergence dfGPdfNN Run 10/10, Epoch 498/1000, Training Loss (NLML): -958.6552\n",
      "convergence dfGPdfNN Run 10/10, Epoch 499/1000, Training Loss (NLML): -958.6650\n",
      "convergence dfGPdfNN Run 10/10, Epoch 500/1000, Training Loss (NLML): -958.6707\n",
      "convergence dfGPdfNN Run 10/10, Epoch 501/1000, Training Loss (NLML): -958.6781\n",
      "convergence dfGPdfNN Run 10/10, Epoch 502/1000, Training Loss (NLML): -958.6854\n",
      "convergence dfGPdfNN Run 10/10, Epoch 503/1000, Training Loss (NLML): -958.6923\n",
      "convergence dfGPdfNN Run 10/10, Epoch 504/1000, Training Loss (NLML): -958.7008\n",
      "convergence dfGPdfNN Run 10/10, Epoch 505/1000, Training Loss (NLML): -958.7090\n",
      "convergence dfGPdfNN Run 10/10, Epoch 506/1000, Training Loss (NLML): -958.7140\n",
      "convergence dfGPdfNN Run 10/10, Epoch 507/1000, Training Loss (NLML): -958.7224\n",
      "convergence dfGPdfNN Run 10/10, Epoch 508/1000, Training Loss (NLML): -958.7292\n",
      "convergence dfGPdfNN Run 10/10, Epoch 509/1000, Training Loss (NLML): -958.7369\n",
      "convergence dfGPdfNN Run 10/10, Epoch 510/1000, Training Loss (NLML): -958.7429\n",
      "convergence dfGPdfNN Run 10/10, Epoch 511/1000, Training Loss (NLML): -958.7506\n",
      "convergence dfGPdfNN Run 10/10, Epoch 512/1000, Training Loss (NLML): -958.7576\n",
      "convergence dfGPdfNN Run 10/10, Epoch 513/1000, Training Loss (NLML): -958.7645\n",
      "convergence dfGPdfNN Run 10/10, Epoch 514/1000, Training Loss (NLML): -958.7703\n",
      "convergence dfGPdfNN Run 10/10, Epoch 515/1000, Training Loss (NLML): -958.7781\n",
      "convergence dfGPdfNN Run 10/10, Epoch 516/1000, Training Loss (NLML): -958.7849\n",
      "convergence dfGPdfNN Run 10/10, Epoch 517/1000, Training Loss (NLML): -958.7931\n",
      "convergence dfGPdfNN Run 10/10, Epoch 518/1000, Training Loss (NLML): -958.7983\n",
      "convergence dfGPdfNN Run 10/10, Epoch 519/1000, Training Loss (NLML): -958.8062\n",
      "convergence dfGPdfNN Run 10/10, Epoch 520/1000, Training Loss (NLML): -958.8136\n",
      "convergence dfGPdfNN Run 10/10, Epoch 521/1000, Training Loss (NLML): -958.8199\n",
      "convergence dfGPdfNN Run 10/10, Epoch 522/1000, Training Loss (NLML): -958.8275\n",
      "convergence dfGPdfNN Run 10/10, Epoch 523/1000, Training Loss (NLML): -958.8347\n",
      "convergence dfGPdfNN Run 10/10, Epoch 524/1000, Training Loss (NLML): -958.8413\n",
      "convergence dfGPdfNN Run 10/10, Epoch 525/1000, Training Loss (NLML): -958.8483\n",
      "convergence dfGPdfNN Run 10/10, Epoch 526/1000, Training Loss (NLML): -958.8536\n",
      "convergence dfGPdfNN Run 10/10, Epoch 527/1000, Training Loss (NLML): -958.8616\n",
      "convergence dfGPdfNN Run 10/10, Epoch 528/1000, Training Loss (NLML): -958.8676\n",
      "convergence dfGPdfNN Run 10/10, Epoch 529/1000, Training Loss (NLML): -958.8750\n",
      "convergence dfGPdfNN Run 10/10, Epoch 530/1000, Training Loss (NLML): -958.8813\n",
      "convergence dfGPdfNN Run 10/10, Epoch 531/1000, Training Loss (NLML): -958.8868\n",
      "convergence dfGPdfNN Run 10/10, Epoch 532/1000, Training Loss (NLML): -958.8943\n",
      "convergence dfGPdfNN Run 10/10, Epoch 533/1000, Training Loss (NLML): -958.9010\n",
      "convergence dfGPdfNN Run 10/10, Epoch 534/1000, Training Loss (NLML): -958.9125\n",
      "convergence dfGPdfNN Run 10/10, Epoch 535/1000, Training Loss (NLML): -958.9137\n",
      "convergence dfGPdfNN Run 10/10, Epoch 536/1000, Training Loss (NLML): -958.9208\n",
      "convergence dfGPdfNN Run 10/10, Epoch 537/1000, Training Loss (NLML): -958.9260\n",
      "convergence dfGPdfNN Run 10/10, Epoch 538/1000, Training Loss (NLML): -958.9324\n",
      "convergence dfGPdfNN Run 10/10, Epoch 539/1000, Training Loss (NLML): -958.9393\n",
      "convergence dfGPdfNN Run 10/10, Epoch 540/1000, Training Loss (NLML): -958.9485\n",
      "convergence dfGPdfNN Run 10/10, Epoch 541/1000, Training Loss (NLML): -958.9526\n",
      "convergence dfGPdfNN Run 10/10, Epoch 542/1000, Training Loss (NLML): -958.9608\n",
      "convergence dfGPdfNN Run 10/10, Epoch 543/1000, Training Loss (NLML): -958.9657\n",
      "convergence dfGPdfNN Run 10/10, Epoch 544/1000, Training Loss (NLML): -958.9708\n",
      "convergence dfGPdfNN Run 10/10, Epoch 545/1000, Training Loss (NLML): -958.9773\n",
      "convergence dfGPdfNN Run 10/10, Epoch 546/1000, Training Loss (NLML): -958.9841\n",
      "convergence dfGPdfNN Run 10/10, Epoch 547/1000, Training Loss (NLML): -958.9913\n",
      "convergence dfGPdfNN Run 10/10, Epoch 548/1000, Training Loss (NLML): -958.9963\n",
      "convergence dfGPdfNN Run 10/10, Epoch 549/1000, Training Loss (NLML): -959.0035\n",
      "convergence dfGPdfNN Run 10/10, Epoch 550/1000, Training Loss (NLML): -959.0095\n",
      "convergence dfGPdfNN Run 10/10, Epoch 551/1000, Training Loss (NLML): -959.0159\n",
      "convergence dfGPdfNN Run 10/10, Epoch 552/1000, Training Loss (NLML): -959.0220\n",
      "convergence dfGPdfNN Run 10/10, Epoch 553/1000, Training Loss (NLML): -959.0278\n",
      "convergence dfGPdfNN Run 10/10, Epoch 554/1000, Training Loss (NLML): -959.0344\n",
      "convergence dfGPdfNN Run 10/10, Epoch 555/1000, Training Loss (NLML): -959.0465\n",
      "convergence dfGPdfNN Run 10/10, Epoch 556/1000, Training Loss (NLML): -959.0481\n",
      "convergence dfGPdfNN Run 10/10, Epoch 557/1000, Training Loss (NLML): -959.0553\n",
      "convergence dfGPdfNN Run 10/10, Epoch 558/1000, Training Loss (NLML): -959.0603\n",
      "convergence dfGPdfNN Run 10/10, Epoch 559/1000, Training Loss (NLML): -959.0660\n",
      "convergence dfGPdfNN Run 10/10, Epoch 560/1000, Training Loss (NLML): -959.0732\n",
      "convergence dfGPdfNN Run 10/10, Epoch 561/1000, Training Loss (NLML): -959.0784\n",
      "convergence dfGPdfNN Run 10/10, Epoch 562/1000, Training Loss (NLML): -959.0840\n",
      "convergence dfGPdfNN Run 10/10, Epoch 563/1000, Training Loss (NLML): -959.0898\n",
      "convergence dfGPdfNN Run 10/10, Epoch 564/1000, Training Loss (NLML): -959.0963\n",
      "convergence dfGPdfNN Run 10/10, Epoch 565/1000, Training Loss (NLML): -959.1017\n",
      "convergence dfGPdfNN Run 10/10, Epoch 566/1000, Training Loss (NLML): -959.1125\n",
      "convergence dfGPdfNN Run 10/10, Epoch 567/1000, Training Loss (NLML): -959.1144\n",
      "convergence dfGPdfNN Run 10/10, Epoch 568/1000, Training Loss (NLML): -959.1256\n",
      "convergence dfGPdfNN Run 10/10, Epoch 569/1000, Training Loss (NLML): -959.1289\n",
      "convergence dfGPdfNN Run 10/10, Epoch 570/1000, Training Loss (NLML): -959.1354\n",
      "convergence dfGPdfNN Run 10/10, Epoch 571/1000, Training Loss (NLML): -959.1422\n",
      "convergence dfGPdfNN Run 10/10, Epoch 572/1000, Training Loss (NLML): -959.1266\n",
      "convergence dfGPdfNN Run 10/10, Epoch 573/1000, Training Loss (NLML): -959.1335\n",
      "convergence dfGPdfNN Run 10/10, Epoch 574/1000, Training Loss (NLML): -959.1191\n",
      "convergence dfGPdfNN Run 10/10, Epoch 575/1000, Training Loss (NLML): -959.1244\n",
      "convergence dfGPdfNN Run 10/10, Epoch 576/1000, Training Loss (NLML): -959.1331\n",
      "convergence dfGPdfNN Run 10/10, Epoch 577/1000, Training Loss (NLML): -959.1368\n",
      "convergence dfGPdfNN Run 10/10, Epoch 578/1000, Training Loss (NLML): -959.1428\n",
      "convergence dfGPdfNN Run 10/10, Epoch 579/1000, Training Loss (NLML): -959.1477\n",
      "convergence dfGPdfNN Run 10/10, Epoch 580/1000, Training Loss (NLML): -959.1544\n",
      "convergence dfGPdfNN Run 10/10, Epoch 581/1000, Training Loss (NLML): -959.1605\n",
      "convergence dfGPdfNN Run 10/10, Epoch 582/1000, Training Loss (NLML): -959.1680\n",
      "convergence dfGPdfNN Run 10/10, Epoch 583/1000, Training Loss (NLML): -959.1725\n",
      "convergence dfGPdfNN Run 10/10, Epoch 584/1000, Training Loss (NLML): -959.1650\n",
      "convergence dfGPdfNN Run 10/10, Epoch 585/1000, Training Loss (NLML): -959.1880\n",
      "convergence dfGPdfNN Run 10/10, Epoch 586/1000, Training Loss (NLML): -959.1954\n",
      "convergence dfGPdfNN Run 10/10, Epoch 587/1000, Training Loss (NLML): -959.2020\n",
      "convergence dfGPdfNN Run 10/10, Epoch 588/1000, Training Loss (NLML): -959.2078\n",
      "convergence dfGPdfNN Run 10/10, Epoch 589/1000, Training Loss (NLML): -959.2142\n",
      "convergence dfGPdfNN Run 10/10, Epoch 590/1000, Training Loss (NLML): -959.2197\n",
      "convergence dfGPdfNN Run 10/10, Epoch 591/1000, Training Loss (NLML): -959.2212\n",
      "convergence dfGPdfNN Run 10/10, Epoch 592/1000, Training Loss (NLML): -959.2281\n",
      "convergence dfGPdfNN Run 10/10, Epoch 593/1000, Training Loss (NLML): -959.2327\n",
      "convergence dfGPdfNN Run 10/10, Epoch 594/1000, Training Loss (NLML): -959.2389\n",
      "convergence dfGPdfNN Run 10/10, Epoch 595/1000, Training Loss (NLML): -959.2437\n",
      "convergence dfGPdfNN Run 10/10, Epoch 596/1000, Training Loss (NLML): -959.2496\n",
      "convergence dfGPdfNN Run 10/10, Epoch 597/1000, Training Loss (NLML): -959.2550\n",
      "convergence dfGPdfNN Run 10/10, Epoch 598/1000, Training Loss (NLML): -959.2616\n",
      "convergence dfGPdfNN Run 10/10, Epoch 599/1000, Training Loss (NLML): -959.2688\n",
      "convergence dfGPdfNN Run 10/10, Epoch 600/1000, Training Loss (NLML): -959.2734\n",
      "convergence dfGPdfNN Run 10/10, Epoch 601/1000, Training Loss (NLML): -959.2780\n",
      "convergence dfGPdfNN Run 10/10, Epoch 602/1000, Training Loss (NLML): -959.2826\n",
      "convergence dfGPdfNN Run 10/10, Epoch 603/1000, Training Loss (NLML): -959.2875\n",
      "convergence dfGPdfNN Run 10/10, Epoch 604/1000, Training Loss (NLML): -959.2944\n",
      "convergence dfGPdfNN Run 10/10, Epoch 605/1000, Training Loss (NLML): -959.2990\n",
      "convergence dfGPdfNN Run 10/10, Epoch 606/1000, Training Loss (NLML): -959.3055\n",
      "convergence dfGPdfNN Run 10/10, Epoch 607/1000, Training Loss (NLML): -959.2946\n",
      "convergence dfGPdfNN Run 10/10, Epoch 608/1000, Training Loss (NLML): -959.2976\n",
      "convergence dfGPdfNN Run 10/10, Epoch 609/1000, Training Loss (NLML): -959.3082\n",
      "convergence dfGPdfNN Run 10/10, Epoch 610/1000, Training Loss (NLML): -959.3307\n",
      "convergence dfGPdfNN Run 10/10, Epoch 611/1000, Training Loss (NLML): -959.3301\n",
      "convergence dfGPdfNN Run 10/10, Epoch 612/1000, Training Loss (NLML): -959.3361\n",
      "convergence dfGPdfNN Run 10/10, Epoch 613/1000, Training Loss (NLML): -959.3424\n",
      "convergence dfGPdfNN Run 10/10, Epoch 614/1000, Training Loss (NLML): -959.3467\n",
      "convergence dfGPdfNN Run 10/10, Epoch 615/1000, Training Loss (NLML): -959.3507\n",
      "convergence dfGPdfNN Run 10/10, Epoch 616/1000, Training Loss (NLML): -959.3555\n",
      "convergence dfGPdfNN Run 10/10, Epoch 617/1000, Training Loss (NLML): -959.3597\n",
      "convergence dfGPdfNN Run 10/10, Epoch 618/1000, Training Loss (NLML): -959.3655\n",
      "convergence dfGPdfNN Run 10/10, Epoch 619/1000, Training Loss (NLML): -959.3711\n",
      "convergence dfGPdfNN Run 10/10, Epoch 620/1000, Training Loss (NLML): -959.3811\n",
      "convergence dfGPdfNN Run 10/10, Epoch 621/1000, Training Loss (NLML): -959.3849\n",
      "convergence dfGPdfNN Run 10/10, Epoch 622/1000, Training Loss (NLML): -959.3865\n",
      "convergence dfGPdfNN Run 10/10, Epoch 623/1000, Training Loss (NLML): -959.3918\n",
      "convergence dfGPdfNN Run 10/10, Epoch 624/1000, Training Loss (NLML): -959.3966\n",
      "convergence dfGPdfNN Run 10/10, Epoch 625/1000, Training Loss (NLML): -959.4031\n",
      "convergence dfGPdfNN Run 10/10, Epoch 626/1000, Training Loss (NLML): -959.4066\n",
      "convergence dfGPdfNN Run 10/10, Epoch 627/1000, Training Loss (NLML): -959.4122\n",
      "convergence dfGPdfNN Run 10/10, Epoch 628/1000, Training Loss (NLML): -959.4171\n",
      "convergence dfGPdfNN Run 10/10, Epoch 629/1000, Training Loss (NLML): -959.4198\n",
      "convergence dfGPdfNN Run 10/10, Epoch 630/1000, Training Loss (NLML): -959.4297\n",
      "convergence dfGPdfNN Run 10/10, Epoch 631/1000, Training Loss (NLML): -959.4358\n",
      "convergence dfGPdfNN Run 10/10, Epoch 632/1000, Training Loss (NLML): -959.4351\n",
      "convergence dfGPdfNN Run 10/10, Epoch 633/1000, Training Loss (NLML): -959.4413\n",
      "convergence dfGPdfNN Run 10/10, Epoch 634/1000, Training Loss (NLML): -959.4437\n",
      "convergence dfGPdfNN Run 10/10, Epoch 635/1000, Training Loss (NLML): -959.4484\n",
      "convergence dfGPdfNN Run 10/10, Epoch 636/1000, Training Loss (NLML): -959.4547\n",
      "convergence dfGPdfNN Run 10/10, Epoch 637/1000, Training Loss (NLML): -959.4594\n",
      "convergence dfGPdfNN Run 10/10, Epoch 638/1000, Training Loss (NLML): -959.4630\n",
      "convergence dfGPdfNN Run 10/10, Epoch 639/1000, Training Loss (NLML): -959.4678\n",
      "convergence dfGPdfNN Run 10/10, Epoch 640/1000, Training Loss (NLML): -959.4722\n",
      "convergence dfGPdfNN Run 10/10, Epoch 641/1000, Training Loss (NLML): -959.4773\n",
      "convergence dfGPdfNN Run 10/10, Epoch 642/1000, Training Loss (NLML): -959.4823\n",
      "convergence dfGPdfNN Run 10/10, Epoch 643/1000, Training Loss (NLML): -959.4871\n",
      "convergence dfGPdfNN Run 10/10, Epoch 644/1000, Training Loss (NLML): -959.4924\n",
      "convergence dfGPdfNN Run 10/10, Epoch 645/1000, Training Loss (NLML): -959.4960\n",
      "convergence dfGPdfNN Run 10/10, Epoch 646/1000, Training Loss (NLML): -959.5012\n",
      "convergence dfGPdfNN Run 10/10, Epoch 647/1000, Training Loss (NLML): -959.5068\n",
      "convergence dfGPdfNN Run 10/10, Epoch 648/1000, Training Loss (NLML): -959.5099\n",
      "convergence dfGPdfNN Run 10/10, Epoch 649/1000, Training Loss (NLML): -959.5126\n",
      "convergence dfGPdfNN Run 10/10, Epoch 650/1000, Training Loss (NLML): -959.5201\n",
      "convergence dfGPdfNN Run 10/10, Epoch 651/1000, Training Loss (NLML): -959.5227\n",
      "convergence dfGPdfNN Run 10/10, Epoch 652/1000, Training Loss (NLML): -959.5283\n",
      "convergence dfGPdfNN Run 10/10, Epoch 653/1000, Training Loss (NLML): -959.5339\n",
      "convergence dfGPdfNN Run 10/10, Epoch 654/1000, Training Loss (NLML): -959.5392\n",
      "convergence dfGPdfNN Run 10/10, Epoch 655/1000, Training Loss (NLML): -959.5414\n",
      "convergence dfGPdfNN Run 10/10, Epoch 656/1000, Training Loss (NLML): -959.5476\n",
      "convergence dfGPdfNN Run 10/10, Epoch 657/1000, Training Loss (NLML): -959.5519\n",
      "convergence dfGPdfNN Run 10/10, Epoch 658/1000, Training Loss (NLML): -959.5564\n",
      "convergence dfGPdfNN Run 10/10, Epoch 659/1000, Training Loss (NLML): -959.5599\n",
      "convergence dfGPdfNN Run 10/10, Epoch 660/1000, Training Loss (NLML): -959.5677\n",
      "convergence dfGPdfNN Run 10/10, Epoch 661/1000, Training Loss (NLML): -959.5707\n",
      "convergence dfGPdfNN Run 10/10, Epoch 662/1000, Training Loss (NLML): -959.5742\n",
      "convergence dfGPdfNN Run 10/10, Epoch 663/1000, Training Loss (NLML): -959.5790\n",
      "convergence dfGPdfNN Run 10/10, Epoch 664/1000, Training Loss (NLML): -959.5834\n",
      "convergence dfGPdfNN Run 10/10, Epoch 665/1000, Training Loss (NLML): -959.5869\n",
      "convergence dfGPdfNN Run 10/10, Epoch 666/1000, Training Loss (NLML): -959.5914\n",
      "convergence dfGPdfNN Run 10/10, Epoch 667/1000, Training Loss (NLML): -959.5970\n",
      "convergence dfGPdfNN Run 10/10, Epoch 668/1000, Training Loss (NLML): -959.6018\n",
      "convergence dfGPdfNN Run 10/10, Epoch 669/1000, Training Loss (NLML): -959.6063\n",
      "convergence dfGPdfNN Run 10/10, Epoch 670/1000, Training Loss (NLML): -959.6097\n",
      "convergence dfGPdfNN Run 10/10, Epoch 671/1000, Training Loss (NLML): -959.6163\n",
      "convergence dfGPdfNN Run 10/10, Epoch 672/1000, Training Loss (NLML): -959.6188\n",
      "convergence dfGPdfNN Run 10/10, Epoch 673/1000, Training Loss (NLML): -959.6237\n",
      "convergence dfGPdfNN Run 10/10, Epoch 674/1000, Training Loss (NLML): -959.6272\n",
      "convergence dfGPdfNN Run 10/10, Epoch 675/1000, Training Loss (NLML): -959.6328\n",
      "convergence dfGPdfNN Run 10/10, Epoch 676/1000, Training Loss (NLML): -959.6367\n",
      "convergence dfGPdfNN Run 10/10, Epoch 677/1000, Training Loss (NLML): -959.6405\n",
      "convergence dfGPdfNN Run 10/10, Epoch 678/1000, Training Loss (NLML): -959.6462\n",
      "convergence dfGPdfNN Run 10/10, Epoch 679/1000, Training Loss (NLML): -959.6497\n",
      "convergence dfGPdfNN Run 10/10, Epoch 680/1000, Training Loss (NLML): -959.6536\n",
      "convergence dfGPdfNN Run 10/10, Epoch 681/1000, Training Loss (NLML): -959.6588\n",
      "convergence dfGPdfNN Run 10/10, Epoch 682/1000, Training Loss (NLML): -959.6622\n",
      "convergence dfGPdfNN Run 10/10, Epoch 683/1000, Training Loss (NLML): -959.6650\n",
      "convergence dfGPdfNN Run 10/10, Epoch 684/1000, Training Loss (NLML): -959.6705\n",
      "convergence dfGPdfNN Run 10/10, Epoch 685/1000, Training Loss (NLML): -959.6761\n",
      "convergence dfGPdfNN Run 10/10, Epoch 686/1000, Training Loss (NLML): -959.6808\n",
      "convergence dfGPdfNN Run 10/10, Epoch 687/1000, Training Loss (NLML): -959.6841\n",
      "convergence dfGPdfNN Run 10/10, Epoch 688/1000, Training Loss (NLML): -959.6881\n",
      "convergence dfGPdfNN Run 10/10, Epoch 689/1000, Training Loss (NLML): -959.6903\n",
      "convergence dfGPdfNN Run 10/10, Epoch 690/1000, Training Loss (NLML): -959.6951\n",
      "convergence dfGPdfNN Run 10/10, Epoch 691/1000, Training Loss (NLML): -959.7006\n",
      "convergence dfGPdfNN Run 10/10, Epoch 692/1000, Training Loss (NLML): -959.7050\n",
      "convergence dfGPdfNN Run 10/10, Epoch 693/1000, Training Loss (NLML): -959.7079\n",
      "convergence dfGPdfNN Run 10/10, Epoch 694/1000, Training Loss (NLML): -959.7135\n",
      "convergence dfGPdfNN Run 10/10, Epoch 695/1000, Training Loss (NLML): -959.7177\n",
      "convergence dfGPdfNN Run 10/10, Epoch 696/1000, Training Loss (NLML): -959.7218\n",
      "convergence dfGPdfNN Run 10/10, Epoch 697/1000, Training Loss (NLML): -959.7258\n",
      "convergence dfGPdfNN Run 10/10, Epoch 698/1000, Training Loss (NLML): -959.7292\n",
      "convergence dfGPdfNN Run 10/10, Epoch 699/1000, Training Loss (NLML): -959.7343\n",
      "convergence dfGPdfNN Run 10/10, Epoch 700/1000, Training Loss (NLML): -959.7358\n",
      "convergence dfGPdfNN Run 10/10, Epoch 701/1000, Training Loss (NLML): -959.7432\n",
      "convergence dfGPdfNN Run 10/10, Epoch 702/1000, Training Loss (NLML): -959.7446\n",
      "convergence dfGPdfNN Run 10/10, Epoch 703/1000, Training Loss (NLML): -959.7496\n",
      "convergence dfGPdfNN Run 10/10, Epoch 704/1000, Training Loss (NLML): -959.7532\n",
      "convergence dfGPdfNN Run 10/10, Epoch 705/1000, Training Loss (NLML): -959.7588\n",
      "convergence dfGPdfNN Run 10/10, Epoch 706/1000, Training Loss (NLML): -959.7610\n",
      "convergence dfGPdfNN Run 10/10, Epoch 707/1000, Training Loss (NLML): -959.7654\n",
      "convergence dfGPdfNN Run 10/10, Epoch 708/1000, Training Loss (NLML): -959.7683\n",
      "convergence dfGPdfNN Run 10/10, Epoch 709/1000, Training Loss (NLML): -959.7686\n",
      "convergence dfGPdfNN Run 10/10, Epoch 710/1000, Training Loss (NLML): -959.7743\n",
      "convergence dfGPdfNN Run 10/10, Epoch 711/1000, Training Loss (NLML): -959.7783\n",
      "convergence dfGPdfNN Run 10/10, Epoch 712/1000, Training Loss (NLML): -959.7812\n",
      "convergence dfGPdfNN Run 10/10, Epoch 713/1000, Training Loss (NLML): -959.7859\n",
      "convergence dfGPdfNN Run 10/10, Epoch 714/1000, Training Loss (NLML): -959.7881\n",
      "convergence dfGPdfNN Run 10/10, Epoch 715/1000, Training Loss (NLML): -959.7947\n",
      "convergence dfGPdfNN Run 10/10, Epoch 716/1000, Training Loss (NLML): -959.7991\n",
      "convergence dfGPdfNN Run 10/10, Epoch 717/1000, Training Loss (NLML): -959.8011\n",
      "convergence dfGPdfNN Run 10/10, Epoch 718/1000, Training Loss (NLML): -959.8053\n",
      "convergence dfGPdfNN Run 10/10, Epoch 719/1000, Training Loss (NLML): -959.8087\n",
      "convergence dfGPdfNN Run 10/10, Epoch 720/1000, Training Loss (NLML): -959.8123\n",
      "convergence dfGPdfNN Run 10/10, Epoch 721/1000, Training Loss (NLML): -959.8169\n",
      "convergence dfGPdfNN Run 10/10, Epoch 722/1000, Training Loss (NLML): -959.8217\n",
      "convergence dfGPdfNN Run 10/10, Epoch 723/1000, Training Loss (NLML): -959.8245\n",
      "convergence dfGPdfNN Run 10/10, Epoch 724/1000, Training Loss (NLML): -959.8274\n",
      "convergence dfGPdfNN Run 10/10, Epoch 725/1000, Training Loss (NLML): -959.8323\n",
      "convergence dfGPdfNN Run 10/10, Epoch 726/1000, Training Loss (NLML): -959.8368\n",
      "convergence dfGPdfNN Run 10/10, Epoch 727/1000, Training Loss (NLML): -959.8403\n",
      "convergence dfGPdfNN Run 10/10, Epoch 728/1000, Training Loss (NLML): -959.8457\n",
      "convergence dfGPdfNN Run 10/10, Epoch 729/1000, Training Loss (NLML): -959.8466\n",
      "convergence dfGPdfNN Run 10/10, Epoch 730/1000, Training Loss (NLML): -959.8522\n",
      "convergence dfGPdfNN Run 10/10, Epoch 731/1000, Training Loss (NLML): -959.8546\n",
      "convergence dfGPdfNN Run 10/10, Epoch 732/1000, Training Loss (NLML): -959.8579\n",
      "convergence dfGPdfNN Run 10/10, Epoch 733/1000, Training Loss (NLML): -959.8613\n",
      "convergence dfGPdfNN Run 10/10, Epoch 734/1000, Training Loss (NLML): -959.8656\n",
      "convergence dfGPdfNN Run 10/10, Epoch 735/1000, Training Loss (NLML): -959.8694\n",
      "convergence dfGPdfNN Run 10/10, Epoch 736/1000, Training Loss (NLML): -959.8732\n",
      "convergence dfGPdfNN Run 10/10, Epoch 737/1000, Training Loss (NLML): -959.8794\n",
      "convergence dfGPdfNN Run 10/10, Epoch 738/1000, Training Loss (NLML): -959.8813\n",
      "convergence dfGPdfNN Run 10/10, Epoch 739/1000, Training Loss (NLML): -959.8853\n",
      "convergence dfGPdfNN Run 10/10, Epoch 740/1000, Training Loss (NLML): -959.8889\n",
      "convergence dfGPdfNN Run 10/10, Epoch 741/1000, Training Loss (NLML): -959.8933\n",
      "convergence dfGPdfNN Run 10/10, Epoch 742/1000, Training Loss (NLML): -959.8964\n",
      "convergence dfGPdfNN Run 10/10, Epoch 743/1000, Training Loss (NLML): -959.9009\n",
      "convergence dfGPdfNN Run 10/10, Epoch 744/1000, Training Loss (NLML): -959.9032\n",
      "convergence dfGPdfNN Run 10/10, Epoch 745/1000, Training Loss (NLML): -959.9064\n",
      "convergence dfGPdfNN Run 10/10, Epoch 746/1000, Training Loss (NLML): -959.9119\n",
      "convergence dfGPdfNN Run 10/10, Epoch 747/1000, Training Loss (NLML): -959.9153\n",
      "convergence dfGPdfNN Run 10/10, Epoch 748/1000, Training Loss (NLML): -959.9193\n",
      "convergence dfGPdfNN Run 10/10, Epoch 749/1000, Training Loss (NLML): -959.9216\n",
      "convergence dfGPdfNN Run 10/10, Epoch 750/1000, Training Loss (NLML): -959.9270\n",
      "convergence dfGPdfNN Run 10/10, Epoch 751/1000, Training Loss (NLML): -959.9279\n",
      "convergence dfGPdfNN Run 10/10, Epoch 752/1000, Training Loss (NLML): -959.9335\n",
      "convergence dfGPdfNN Run 10/10, Epoch 753/1000, Training Loss (NLML): -959.9352\n",
      "convergence dfGPdfNN Run 10/10, Epoch 754/1000, Training Loss (NLML): -959.9397\n",
      "convergence dfGPdfNN Run 10/10, Epoch 755/1000, Training Loss (NLML): -959.9424\n",
      "convergence dfGPdfNN Run 10/10, Epoch 756/1000, Training Loss (NLML): -959.9478\n",
      "convergence dfGPdfNN Run 10/10, Epoch 757/1000, Training Loss (NLML): -959.9513\n",
      "convergence dfGPdfNN Run 10/10, Epoch 758/1000, Training Loss (NLML): -959.9540\n",
      "convergence dfGPdfNN Run 10/10, Epoch 759/1000, Training Loss (NLML): -959.9578\n",
      "convergence dfGPdfNN Run 10/10, Epoch 760/1000, Training Loss (NLML): -959.9607\n",
      "convergence dfGPdfNN Run 10/10, Epoch 761/1000, Training Loss (NLML): -959.9630\n",
      "convergence dfGPdfNN Run 10/10, Epoch 762/1000, Training Loss (NLML): -959.9673\n",
      "convergence dfGPdfNN Run 10/10, Epoch 763/1000, Training Loss (NLML): -959.9720\n",
      "convergence dfGPdfNN Run 10/10, Epoch 764/1000, Training Loss (NLML): -959.9735\n",
      "convergence dfGPdfNN Run 10/10, Epoch 765/1000, Training Loss (NLML): -959.9784\n",
      "convergence dfGPdfNN Run 10/10, Epoch 766/1000, Training Loss (NLML): -959.9816\n",
      "convergence dfGPdfNN Run 10/10, Epoch 767/1000, Training Loss (NLML): -959.9857\n",
      "convergence dfGPdfNN Run 10/10, Epoch 768/1000, Training Loss (NLML): -959.9897\n",
      "convergence dfGPdfNN Run 10/10, Epoch 769/1000, Training Loss (NLML): -959.9928\n",
      "convergence dfGPdfNN Run 10/10, Epoch 770/1000, Training Loss (NLML): -959.9971\n",
      "convergence dfGPdfNN Run 10/10, Epoch 771/1000, Training Loss (NLML): -959.9989\n",
      "convergence dfGPdfNN Run 10/10, Epoch 772/1000, Training Loss (NLML): -960.0022\n",
      "convergence dfGPdfNN Run 10/10, Epoch 773/1000, Training Loss (NLML): -960.0046\n",
      "convergence dfGPdfNN Run 10/10, Epoch 774/1000, Training Loss (NLML): -960.0090\n",
      "convergence dfGPdfNN Run 10/10, Epoch 775/1000, Training Loss (NLML): -960.0122\n",
      "convergence dfGPdfNN Run 10/10, Epoch 776/1000, Training Loss (NLML): -960.0167\n",
      "convergence dfGPdfNN Run 10/10, Epoch 777/1000, Training Loss (NLML): -960.0200\n",
      "convergence dfGPdfNN Run 10/10, Epoch 778/1000, Training Loss (NLML): -960.0228\n",
      "convergence dfGPdfNN Run 10/10, Epoch 779/1000, Training Loss (NLML): -960.0258\n",
      "convergence dfGPdfNN Run 10/10, Epoch 780/1000, Training Loss (NLML): -960.0297\n",
      "convergence dfGPdfNN Run 10/10, Epoch 781/1000, Training Loss (NLML): -960.0319\n",
      "convergence dfGPdfNN Run 10/10, Epoch 782/1000, Training Loss (NLML): -960.0360\n",
      "convergence dfGPdfNN Run 10/10, Epoch 783/1000, Training Loss (NLML): -960.0400\n",
      "convergence dfGPdfNN Run 10/10, Epoch 784/1000, Training Loss (NLML): -960.0421\n",
      "convergence dfGPdfNN Run 10/10, Epoch 785/1000, Training Loss (NLML): -960.0459\n",
      "convergence dfGPdfNN Run 10/10, Epoch 786/1000, Training Loss (NLML): -960.0494\n",
      "convergence dfGPdfNN Run 10/10, Epoch 787/1000, Training Loss (NLML): -960.0531\n",
      "convergence dfGPdfNN Run 10/10, Epoch 788/1000, Training Loss (NLML): -960.0575\n",
      "convergence dfGPdfNN Run 10/10, Epoch 789/1000, Training Loss (NLML): -960.0597\n",
      "convergence dfGPdfNN Run 10/10, Epoch 790/1000, Training Loss (NLML): -960.0618\n",
      "convergence dfGPdfNN Run 10/10, Epoch 791/1000, Training Loss (NLML): -960.0645\n",
      "convergence dfGPdfNN Run 10/10, Epoch 792/1000, Training Loss (NLML): -960.0682\n",
      "convergence dfGPdfNN Run 10/10, Epoch 793/1000, Training Loss (NLML): -960.0706\n",
      "convergence dfGPdfNN Run 10/10, Epoch 794/1000, Training Loss (NLML): -960.0736\n",
      "convergence dfGPdfNN Run 10/10, Epoch 795/1000, Training Loss (NLML): -960.0781\n",
      "convergence dfGPdfNN Run 10/10, Epoch 796/1000, Training Loss (NLML): -960.0814\n",
      "convergence dfGPdfNN Run 10/10, Epoch 797/1000, Training Loss (NLML): -960.0845\n",
      "convergence dfGPdfNN Run 10/10, Epoch 798/1000, Training Loss (NLML): -960.0886\n",
      "convergence dfGPdfNN Run 10/10, Epoch 799/1000, Training Loss (NLML): -960.0916\n",
      "convergence dfGPdfNN Run 10/10, Epoch 800/1000, Training Loss (NLML): -960.0958\n",
      "convergence dfGPdfNN Run 10/10, Epoch 801/1000, Training Loss (NLML): -960.0979\n",
      "convergence dfGPdfNN Run 10/10, Epoch 802/1000, Training Loss (NLML): -960.0992\n",
      "convergence dfGPdfNN Run 10/10, Epoch 803/1000, Training Loss (NLML): -960.1051\n",
      "convergence dfGPdfNN Run 10/10, Epoch 804/1000, Training Loss (NLML): -960.1086\n",
      "convergence dfGPdfNN Run 10/10, Epoch 805/1000, Training Loss (NLML): -960.1122\n",
      "convergence dfGPdfNN Run 10/10, Epoch 806/1000, Training Loss (NLML): -960.1135\n",
      "convergence dfGPdfNN Run 10/10, Epoch 807/1000, Training Loss (NLML): -960.1168\n",
      "convergence dfGPdfNN Run 10/10, Epoch 808/1000, Training Loss (NLML): -960.1208\n",
      "convergence dfGPdfNN Run 10/10, Epoch 809/1000, Training Loss (NLML): -960.1229\n",
      "convergence dfGPdfNN Run 10/10, Epoch 810/1000, Training Loss (NLML): -960.1256\n",
      "convergence dfGPdfNN Run 10/10, Epoch 811/1000, Training Loss (NLML): -960.1302\n",
      "convergence dfGPdfNN Run 10/10, Epoch 812/1000, Training Loss (NLML): -960.1326\n",
      "convergence dfGPdfNN Run 10/10, Epoch 813/1000, Training Loss (NLML): -960.1344\n",
      "convergence dfGPdfNN Run 10/10, Epoch 814/1000, Training Loss (NLML): -960.1376\n",
      "convergence dfGPdfNN Run 10/10, Epoch 815/1000, Training Loss (NLML): -960.1403\n",
      "convergence dfGPdfNN Run 10/10, Epoch 816/1000, Training Loss (NLML): -960.1434\n",
      "convergence dfGPdfNN Run 10/10, Epoch 817/1000, Training Loss (NLML): -960.1487\n",
      "convergence dfGPdfNN Run 10/10, Epoch 818/1000, Training Loss (NLML): -960.1515\n",
      "convergence dfGPdfNN Run 10/10, Epoch 819/1000, Training Loss (NLML): -960.1553\n",
      "convergence dfGPdfNN Run 10/10, Epoch 820/1000, Training Loss (NLML): -960.1559\n",
      "convergence dfGPdfNN Run 10/10, Epoch 821/1000, Training Loss (NLML): -960.1603\n",
      "convergence dfGPdfNN Run 10/10, Epoch 822/1000, Training Loss (NLML): -960.1625\n",
      "convergence dfGPdfNN Run 10/10, Epoch 823/1000, Training Loss (NLML): -960.1660\n",
      "convergence dfGPdfNN Run 10/10, Epoch 824/1000, Training Loss (NLML): -960.1700\n",
      "convergence dfGPdfNN Run 10/10, Epoch 825/1000, Training Loss (NLML): -960.1707\n",
      "convergence dfGPdfNN Run 10/10, Epoch 826/1000, Training Loss (NLML): -960.1741\n",
      "convergence dfGPdfNN Run 10/10, Epoch 827/1000, Training Loss (NLML): -960.1777\n",
      "convergence dfGPdfNN Run 10/10, Epoch 828/1000, Training Loss (NLML): -960.1813\n",
      "convergence dfGPdfNN Run 10/10, Epoch 829/1000, Training Loss (NLML): -960.1818\n",
      "convergence dfGPdfNN Run 10/10, Epoch 830/1000, Training Loss (NLML): -960.1876\n",
      "convergence dfGPdfNN Run 10/10, Epoch 831/1000, Training Loss (NLML): -960.1880\n",
      "convergence dfGPdfNN Run 10/10, Epoch 832/1000, Training Loss (NLML): -960.1920\n",
      "convergence dfGPdfNN Run 10/10, Epoch 833/1000, Training Loss (NLML): -960.1945\n",
      "convergence dfGPdfNN Run 10/10, Epoch 834/1000, Training Loss (NLML): -960.1976\n",
      "convergence dfGPdfNN Run 10/10, Epoch 835/1000, Training Loss (NLML): -960.2010\n",
      "convergence dfGPdfNN Run 10/10, Epoch 836/1000, Training Loss (NLML): -960.2053\n",
      "convergence dfGPdfNN Run 10/10, Epoch 837/1000, Training Loss (NLML): -960.2081\n",
      "convergence dfGPdfNN Run 10/10, Epoch 838/1000, Training Loss (NLML): -960.2096\n",
      "convergence dfGPdfNN Run 10/10, Epoch 839/1000, Training Loss (NLML): -960.2147\n",
      "convergence dfGPdfNN Run 10/10, Epoch 840/1000, Training Loss (NLML): -960.2174\n",
      "convergence dfGPdfNN Run 10/10, Epoch 841/1000, Training Loss (NLML): -960.2208\n",
      "convergence dfGPdfNN Run 10/10, Epoch 842/1000, Training Loss (NLML): -960.2217\n",
      "convergence dfGPdfNN Run 10/10, Epoch 843/1000, Training Loss (NLML): -960.2257\n",
      "convergence dfGPdfNN Run 10/10, Epoch 844/1000, Training Loss (NLML): -960.2284\n",
      "convergence dfGPdfNN Run 10/10, Epoch 845/1000, Training Loss (NLML): -960.2316\n",
      "convergence dfGPdfNN Run 10/10, Epoch 846/1000, Training Loss (NLML): -960.2350\n",
      "convergence dfGPdfNN Run 10/10, Epoch 847/1000, Training Loss (NLML): -960.2362\n",
      "convergence dfGPdfNN Run 10/10, Epoch 848/1000, Training Loss (NLML): -960.2385\n",
      "convergence dfGPdfNN Run 10/10, Epoch 849/1000, Training Loss (NLML): -960.2443\n",
      "convergence dfGPdfNN Run 10/10, Epoch 850/1000, Training Loss (NLML): -960.2443\n",
      "convergence dfGPdfNN Run 10/10, Epoch 851/1000, Training Loss (NLML): -960.2479\n",
      "convergence dfGPdfNN Run 10/10, Epoch 852/1000, Training Loss (NLML): -960.2513\n",
      "convergence dfGPdfNN Run 10/10, Epoch 853/1000, Training Loss (NLML): -960.2534\n",
      "convergence dfGPdfNN Run 10/10, Epoch 854/1000, Training Loss (NLML): -960.2554\n",
      "convergence dfGPdfNN Run 10/10, Epoch 855/1000, Training Loss (NLML): -960.2598\n",
      "convergence dfGPdfNN Run 10/10, Epoch 856/1000, Training Loss (NLML): -960.2635\n",
      "convergence dfGPdfNN Run 10/10, Epoch 857/1000, Training Loss (NLML): -960.2646\n",
      "convergence dfGPdfNN Run 10/10, Epoch 858/1000, Training Loss (NLML): -960.2673\n",
      "convergence dfGPdfNN Run 10/10, Epoch 859/1000, Training Loss (NLML): -960.2688\n",
      "convergence dfGPdfNN Run 10/10, Epoch 860/1000, Training Loss (NLML): -960.2748\n",
      "convergence dfGPdfNN Run 10/10, Epoch 861/1000, Training Loss (NLML): -960.2766\n",
      "convergence dfGPdfNN Run 10/10, Epoch 862/1000, Training Loss (NLML): -960.2792\n",
      "convergence dfGPdfNN Run 10/10, Epoch 863/1000, Training Loss (NLML): -960.2804\n",
      "convergence dfGPdfNN Run 10/10, Epoch 864/1000, Training Loss (NLML): -960.2847\n",
      "convergence dfGPdfNN Run 10/10, Epoch 865/1000, Training Loss (NLML): -960.2877\n",
      "convergence dfGPdfNN Run 10/10, Epoch 866/1000, Training Loss (NLML): -960.2905\n",
      "convergence dfGPdfNN Run 10/10, Epoch 867/1000, Training Loss (NLML): -960.2933\n",
      "convergence dfGPdfNN Run 10/10, Epoch 868/1000, Training Loss (NLML): -960.2975\n",
      "convergence dfGPdfNN Run 10/10, Epoch 869/1000, Training Loss (NLML): -960.2971\n",
      "convergence dfGPdfNN Run 10/10, Epoch 870/1000, Training Loss (NLML): -960.3018\n",
      "convergence dfGPdfNN Run 10/10, Epoch 871/1000, Training Loss (NLML): -960.3035\n",
      "convergence dfGPdfNN Run 10/10, Epoch 872/1000, Training Loss (NLML): -960.3057\n",
      "convergence dfGPdfNN Run 10/10, Epoch 873/1000, Training Loss (NLML): -960.3105\n",
      "convergence dfGPdfNN Run 10/10, Epoch 874/1000, Training Loss (NLML): -960.3123\n",
      "convergence dfGPdfNN Run 10/10, Epoch 875/1000, Training Loss (NLML): -960.3152\n",
      "convergence dfGPdfNN Run 10/10, Epoch 876/1000, Training Loss (NLML): -960.3180\n",
      "convergence dfGPdfNN Run 10/10, Epoch 877/1000, Training Loss (NLML): -960.3197\n",
      "convergence dfGPdfNN Run 10/10, Epoch 878/1000, Training Loss (NLML): -960.3221\n",
      "convergence dfGPdfNN Run 10/10, Epoch 879/1000, Training Loss (NLML): -960.3257\n",
      "convergence dfGPdfNN Run 10/10, Epoch 880/1000, Training Loss (NLML): -960.3292\n",
      "convergence dfGPdfNN Run 10/10, Epoch 881/1000, Training Loss (NLML): -960.3307\n",
      "convergence dfGPdfNN Run 10/10, Epoch 882/1000, Training Loss (NLML): -960.3345\n",
      "convergence dfGPdfNN Run 10/10, Epoch 883/1000, Training Loss (NLML): -960.3353\n",
      "convergence dfGPdfNN Run 10/10, Epoch 884/1000, Training Loss (NLML): -960.3391\n",
      "convergence dfGPdfNN Run 10/10, Epoch 885/1000, Training Loss (NLML): -960.3412\n",
      "convergence dfGPdfNN Run 10/10, Epoch 886/1000, Training Loss (NLML): -960.3440\n",
      "convergence dfGPdfNN Run 10/10, Epoch 887/1000, Training Loss (NLML): -960.3473\n",
      "convergence dfGPdfNN Run 10/10, Epoch 888/1000, Training Loss (NLML): -960.3494\n",
      "convergence dfGPdfNN Run 10/10, Epoch 889/1000, Training Loss (NLML): -960.3525\n",
      "convergence dfGPdfNN Run 10/10, Epoch 890/1000, Training Loss (NLML): -960.3547\n",
      "convergence dfGPdfNN Run 10/10, Epoch 891/1000, Training Loss (NLML): -960.3580\n",
      "convergence dfGPdfNN Run 10/10, Epoch 892/1000, Training Loss (NLML): -960.3600\n",
      "convergence dfGPdfNN Run 10/10, Epoch 893/1000, Training Loss (NLML): -960.3613\n",
      "convergence dfGPdfNN Run 10/10, Epoch 894/1000, Training Loss (NLML): -960.3651\n",
      "convergence dfGPdfNN Run 10/10, Epoch 895/1000, Training Loss (NLML): -960.3685\n",
      "convergence dfGPdfNN Run 10/10, Epoch 896/1000, Training Loss (NLML): -960.3695\n",
      "convergence dfGPdfNN Run 10/10, Epoch 897/1000, Training Loss (NLML): -960.3723\n",
      "convergence dfGPdfNN Run 10/10, Epoch 898/1000, Training Loss (NLML): -960.3750\n",
      "convergence dfGPdfNN Run 10/10, Epoch 899/1000, Training Loss (NLML): -960.3782\n",
      "convergence dfGPdfNN Run 10/10, Epoch 900/1000, Training Loss (NLML): -960.3820\n",
      "convergence dfGPdfNN Run 10/10, Epoch 901/1000, Training Loss (NLML): -960.3827\n",
      "convergence dfGPdfNN Run 10/10, Epoch 902/1000, Training Loss (NLML): -960.3845\n",
      "convergence dfGPdfNN Run 10/10, Epoch 903/1000, Training Loss (NLML): -960.3876\n",
      "convergence dfGPdfNN Run 10/10, Epoch 904/1000, Training Loss (NLML): -960.3892\n",
      "convergence dfGPdfNN Run 10/10, Epoch 905/1000, Training Loss (NLML): -960.3940\n",
      "convergence dfGPdfNN Run 10/10, Epoch 906/1000, Training Loss (NLML): -960.3942\n",
      "convergence dfGPdfNN Run 10/10, Epoch 907/1000, Training Loss (NLML): -960.4044\n",
      "convergence dfGPdfNN Run 10/10, Epoch 908/1000, Training Loss (NLML): -960.4014\n",
      "convergence dfGPdfNN Run 10/10, Epoch 909/1000, Training Loss (NLML): -960.4110\n",
      "convergence dfGPdfNN Run 10/10, Epoch 910/1000, Training Loss (NLML): -960.4121\n",
      "convergence dfGPdfNN Run 10/10, Epoch 911/1000, Training Loss (NLML): -960.4144\n",
      "convergence dfGPdfNN Run 10/10, Epoch 912/1000, Training Loss (NLML): -960.4187\n",
      "convergence dfGPdfNN Run 10/10, Epoch 913/1000, Training Loss (NLML): -960.4186\n",
      "convergence dfGPdfNN Run 10/10, Epoch 914/1000, Training Loss (NLML): -960.4192\n",
      "convergence dfGPdfNN Run 10/10, Epoch 915/1000, Training Loss (NLML): -960.4231\n",
      "convergence dfGPdfNN Run 10/10, Epoch 916/1000, Training Loss (NLML): -960.4265\n",
      "convergence dfGPdfNN Run 10/10, Epoch 917/1000, Training Loss (NLML): -960.4287\n",
      "convergence dfGPdfNN Run 10/10, Epoch 918/1000, Training Loss (NLML): -960.4299\n",
      "convergence dfGPdfNN Run 10/10, Epoch 919/1000, Training Loss (NLML): -960.4340\n",
      "convergence dfGPdfNN Run 10/10, Epoch 920/1000, Training Loss (NLML): -960.4355\n",
      "convergence dfGPdfNN Run 10/10, Epoch 921/1000, Training Loss (NLML): -960.4371\n",
      "convergence dfGPdfNN Run 10/10, Epoch 922/1000, Training Loss (NLML): -960.4294\n",
      "convergence dfGPdfNN Run 10/10, Epoch 923/1000, Training Loss (NLML): -960.4338\n",
      "convergence dfGPdfNN Run 10/10, Epoch 924/1000, Training Loss (NLML): -960.4344\n",
      "convergence dfGPdfNN Run 10/10, Epoch 925/1000, Training Loss (NLML): -960.4297\n",
      "convergence dfGPdfNN Run 10/10, Epoch 926/1000, Training Loss (NLML): -960.4323\n",
      "convergence dfGPdfNN Run 10/10, Epoch 927/1000, Training Loss (NLML): -960.4344\n",
      "convergence dfGPdfNN Run 10/10, Epoch 928/1000, Training Loss (NLML): -960.4354\n",
      "convergence dfGPdfNN Run 10/10, Epoch 929/1000, Training Loss (NLML): -960.4395\n",
      "convergence dfGPdfNN Run 10/10, Epoch 930/1000, Training Loss (NLML): -960.4418\n",
      "convergence dfGPdfNN Run 10/10, Epoch 931/1000, Training Loss (NLML): -960.4420\n",
      "convergence dfGPdfNN Run 10/10, Epoch 932/1000, Training Loss (NLML): -960.4474\n",
      "convergence dfGPdfNN Run 10/10, Epoch 933/1000, Training Loss (NLML): -960.4524\n",
      "convergence dfGPdfNN Run 10/10, Epoch 934/1000, Training Loss (NLML): -960.4564\n",
      "convergence dfGPdfNN Run 10/10, Epoch 935/1000, Training Loss (NLML): -960.4583\n",
      "convergence dfGPdfNN Run 10/10, Epoch 936/1000, Training Loss (NLML): -960.4567\n",
      "convergence dfGPdfNN Run 10/10, Epoch 937/1000, Training Loss (NLML): -960.4587\n",
      "convergence dfGPdfNN Run 10/10, Epoch 938/1000, Training Loss (NLML): -960.4524\n",
      "convergence dfGPdfNN Run 10/10, Epoch 939/1000, Training Loss (NLML): -960.4556\n",
      "convergence dfGPdfNN Run 10/10, Epoch 940/1000, Training Loss (NLML): -960.4580\n",
      "convergence dfGPdfNN Run 10/10, Epoch 941/1000, Training Loss (NLML): -960.4672\n",
      "convergence dfGPdfNN Run 10/10, Epoch 942/1000, Training Loss (NLML): -960.4725\n",
      "convergence dfGPdfNN Run 10/10, Epoch 943/1000, Training Loss (NLML): -960.4742\n",
      "convergence dfGPdfNN Run 10/10, Epoch 944/1000, Training Loss (NLML): -960.4672\n",
      "convergence dfGPdfNN Run 10/10, Epoch 945/1000, Training Loss (NLML): -960.4680\n",
      "convergence dfGPdfNN Run 10/10, Epoch 946/1000, Training Loss (NLML): -960.4686\n",
      "convergence dfGPdfNN Run 10/10, Epoch 947/1000, Training Loss (NLML): -960.4720\n",
      "convergence dfGPdfNN Run 10/10, Epoch 948/1000, Training Loss (NLML): -960.4768\n",
      "convergence dfGPdfNN Run 10/10, Epoch 949/1000, Training Loss (NLML): -960.4781\n",
      "convergence dfGPdfNN Run 10/10, Epoch 950/1000, Training Loss (NLML): -960.4819\n",
      "convergence dfGPdfNN Run 10/10, Epoch 951/1000, Training Loss (NLML): -960.4821\n",
      "convergence dfGPdfNN Run 10/10, Epoch 952/1000, Training Loss (NLML): -960.4868\n",
      "convergence dfGPdfNN Run 10/10, Epoch 953/1000, Training Loss (NLML): -960.4874\n",
      "convergence dfGPdfNN Run 10/10, Epoch 954/1000, Training Loss (NLML): -960.4916\n",
      "convergence dfGPdfNN Run 10/10, Epoch 955/1000, Training Loss (NLML): -960.4951\n",
      "convergence dfGPdfNN Run 10/10, Epoch 956/1000, Training Loss (NLML): -960.4939\n",
      "convergence dfGPdfNN Run 10/10, Epoch 957/1000, Training Loss (NLML): -960.5065\n",
      "convergence dfGPdfNN Run 10/10, Epoch 958/1000, Training Loss (NLML): -960.5063\n",
      "convergence dfGPdfNN Run 10/10, Epoch 959/1000, Training Loss (NLML): -960.5092\n",
      "convergence dfGPdfNN Run 10/10, Epoch 960/1000, Training Loss (NLML): -960.5121\n",
      "convergence dfGPdfNN Run 10/10, Epoch 961/1000, Training Loss (NLML): -960.5142\n",
      "convergence dfGPdfNN Run 10/10, Epoch 962/1000, Training Loss (NLML): -960.5173\n",
      "convergence dfGPdfNN Run 10/10, Epoch 963/1000, Training Loss (NLML): -960.5236\n",
      "convergence dfGPdfNN Run 10/10, Epoch 964/1000, Training Loss (NLML): -960.5220\n",
      "convergence dfGPdfNN Run 10/10, Epoch 965/1000, Training Loss (NLML): -960.5258\n",
      "convergence dfGPdfNN Run 10/10, Epoch 966/1000, Training Loss (NLML): -960.5286\n",
      "convergence dfGPdfNN Run 10/10, Epoch 967/1000, Training Loss (NLML): -960.5293\n",
      "convergence dfGPdfNN Run 10/10, Epoch 968/1000, Training Loss (NLML): -960.5316\n",
      "convergence dfGPdfNN Run 10/10, Epoch 969/1000, Training Loss (NLML): -960.5336\n",
      "convergence dfGPdfNN Run 10/10, Epoch 970/1000, Training Loss (NLML): -960.5350\n",
      "convergence dfGPdfNN Run 10/10, Epoch 971/1000, Training Loss (NLML): -960.5475\n",
      "convergence dfGPdfNN Run 10/10, Epoch 972/1000, Training Loss (NLML): -960.5505\n",
      "convergence dfGPdfNN Run 10/10, Epoch 973/1000, Training Loss (NLML): -960.5436\n",
      "convergence dfGPdfNN Run 10/10, Epoch 974/1000, Training Loss (NLML): -960.5444\n",
      "convergence dfGPdfNN Run 10/10, Epoch 975/1000, Training Loss (NLML): -960.5455\n",
      "convergence dfGPdfNN Run 10/10, Epoch 976/1000, Training Loss (NLML): -960.5496\n",
      "convergence dfGPdfNN Run 10/10, Epoch 977/1000, Training Loss (NLML): -960.5526\n",
      "convergence dfGPdfNN Run 10/10, Epoch 978/1000, Training Loss (NLML): -960.5549\n",
      "convergence dfGPdfNN Run 10/10, Epoch 979/1000, Training Loss (NLML): -960.5581\n",
      "convergence dfGPdfNN Run 10/10, Epoch 980/1000, Training Loss (NLML): -960.5569\n",
      "convergence dfGPdfNN Run 10/10, Epoch 981/1000, Training Loss (NLML): -960.5619\n",
      "convergence dfGPdfNN Run 10/10, Epoch 982/1000, Training Loss (NLML): -960.5641\n",
      "convergence dfGPdfNN Run 10/10, Epoch 983/1000, Training Loss (NLML): -960.5652\n",
      "convergence dfGPdfNN Run 10/10, Epoch 984/1000, Training Loss (NLML): -960.5663\n",
      "convergence dfGPdfNN Run 10/10, Epoch 985/1000, Training Loss (NLML): -960.5702\n",
      "convergence dfGPdfNN Run 10/10, Epoch 986/1000, Training Loss (NLML): -960.5725\n",
      "convergence dfGPdfNN Run 10/10, Epoch 987/1000, Training Loss (NLML): -960.5741\n",
      "convergence dfGPdfNN Run 10/10, Epoch 988/1000, Training Loss (NLML): -960.5756\n",
      "convergence dfGPdfNN Run 10/10, Epoch 989/1000, Training Loss (NLML): -960.5778\n",
      "convergence dfGPdfNN Run 10/10, Epoch 990/1000, Training Loss (NLML): -960.5824\n",
      "convergence dfGPdfNN Run 10/10, Epoch 991/1000, Training Loss (NLML): -960.5828\n",
      "convergence dfGPdfNN Run 10/10, Epoch 992/1000, Training Loss (NLML): -960.5873\n",
      "convergence dfGPdfNN Run 10/10, Epoch 993/1000, Training Loss (NLML): -960.5847\n",
      "convergence dfGPdfNN Run 10/10, Epoch 994/1000, Training Loss (NLML): -960.5886\n",
      "convergence dfGPdfNN Run 10/10, Epoch 995/1000, Training Loss (NLML): -960.5901\n",
      "convergence dfGPdfNN Run 10/10, Epoch 996/1000, Training Loss (NLML): -960.5941\n",
      "convergence dfGPdfNN Run 10/10, Epoch 997/1000, Training Loss (NLML): -960.5955\n",
      "convergence dfGPdfNN Run 10/10, Epoch 998/1000, Training Loss (NLML): -960.5995\n",
      "convergence dfGPdfNN Run 10/10, Epoch 999/1000, Training Loss (NLML): -960.6000\n",
      "convergence dfGPdfNN Run 10/10, Epoch 1000/1000, Training Loss (NLML): -960.6033\n",
      "\n",
      "Results saved to results/dfGPdfNN/convergence_dfGPdfNN_metrics_per_run.csv\n",
      "\n",
      "Mean & Std saved to results/dfGPdfNN/convergence_dfGPdfNN_metrics_summary.csv\n"
     ]
    }
   ],
   "source": [
    "from GP_models import GP_predict\n",
    "from NN_models import dfNN_for_vmap\n",
    "from simulate import simulate_convergence, simulate_branching, simulate_ridge, simulate_merge, simulate_deflection\n",
    "from metrics import compute_RMSE, compute_MAE, compute_NLL, compute_NLL_full\n",
    "from utils import set_seed\n",
    "\n",
    "# Global file for training configs\n",
    "from configs import PATIENCE, GP_MAX_NUM_EPOCHS, NUM_RUNS, GP_LEARNING_RATE, WEIGHT_DECAY, N_SIDE, DFGPDFNN_RESULTS_DIR, SIGMA_F_RANGE, L_RANGE\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "### TIMING ###\n",
    "import time\n",
    "start_time = time.time()  # Start timing after imports\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu' # for now\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "# stick to covention to keep df lower case\n",
    "model_name = \"dfGPdfNN\"\n",
    "\n",
    "#########################\n",
    "### x_train & y_train ###\n",
    "#########################\n",
    "\n",
    "# Import all simulation functions\n",
    "from simulate import (\n",
    "    simulate_convergence,\n",
    "    simulate_branching,\n",
    "    simulate_merge,\n",
    "    simulate_deflection,\n",
    "    simulate_ridge,\n",
    ")\n",
    "\n",
    "# Define simulations as a dictionary with names as keys to function objects\n",
    "simulations = {\n",
    "    \"convergence\": simulate_convergence,\n",
    "    \"branching\": simulate_branching,\n",
    "    \"merge\": simulate_merge,\n",
    "    \"deflection\": simulate_deflection,\n",
    "    \"ridge\": simulate_ridge,\n",
    "}\n",
    "\n",
    "# Load training inputs\n",
    "x_train = torch.load(\"data/sim_data/x_train_lines_discretised_0to1.pt\", weights_only = False).float()\n",
    "\n",
    "# Storage dictionaries\n",
    "y_train_dict = {}\n",
    "\n",
    "# Make y_train_dict: Iterate over all simulation functions\n",
    "for sim_name, sim_func in simulations.items():\n",
    "\n",
    "    # Generate training observations\n",
    "    y_train = sim_func(x_train)\n",
    "    y_train_dict[sim_name] = y_train  # Store training outputs\n",
    "\n",
    "    # Print details\n",
    "    print(f\"=== {sim_name.upper()} ===\")\n",
    "    print(f\"Training inputs shape: {x_train.shape}\")\n",
    "    print(f\"Training observations shape: {y_train.shape}\")\n",
    "    print(f\"Training inputs dtype: {x_train.dtype}\")\n",
    "    print()\n",
    "\n",
    "#######################\n",
    "### x_test & y_test ###\n",
    "#######################\n",
    "\n",
    "print(\"=== Generating test data ===\")\n",
    "\n",
    "# Choose discretisation that is good for simulations and also for quiver plotting\n",
    "N_SIDE = N_SIDE\n",
    "\n",
    "side_array = torch.linspace(start = 0.0, end = 1.0, steps = N_SIDE)\n",
    "XX, YY = torch.meshgrid(side_array, side_array, indexing = \"xy\")\n",
    "x_test_grid = torch.cat([XX.unsqueeze(-1), YY.unsqueeze(-1)], dim = -1)\n",
    "# long format\n",
    "x_test = x_test_grid.reshape(-1, 2)\n",
    "\n",
    "# Storage dictionaries\n",
    "y_test_dict = {}\n",
    "\n",
    "# Make y_test_dict: Iterate over all simulation functions\n",
    "for sim_name, sim_func in simulations.items():\n",
    "\n",
    "    # Generate test observations\n",
    "    y_test = sim_func(x_test)\n",
    "    y_test_dict[sim_name] = y_test  # Store test outputs\n",
    "\n",
    "    # Print details\n",
    "    print(f\"=== {sim_name.upper()} ===\")\n",
    "    print(f\"Test inputs shape: {x_test.shape}\")\n",
    "    print(f\"Test observations shape: {y_test.shape}\")\n",
    "    print(f\"Test inputs dtype: {x_test.dtype}\")\n",
    "    print()\n",
    "\n",
    "    # visualise_v_quiver(y_test, x_test, title_string = name)\n",
    "\n",
    "#####################\n",
    "### Training loop ###\n",
    "#####################\n",
    "\n",
    "# Early stopping parameters\n",
    "PATIENCE = PATIENCE\n",
    "MAX_NUM_EPOCHS = GP_MAX_NUM_EPOCHS\n",
    "\n",
    "# Number of training runs for mean and std of metrics\n",
    "NUM_RUNS = NUM_RUNS\n",
    "LEARNING_RATE = GP_LEARNING_RATE\n",
    "WEIGHT_DECAY = WEIGHT_DECAY\n",
    "\n",
    "# Pass in all the training data\n",
    "# BATCH_SIZE = BATCH_SIZE\n",
    "\n",
    "# Ensure the results folder exists\n",
    "RESULTS_DIR = DFGPDFNN_RESULTS_DIR\n",
    "os.makedirs(RESULTS_DIR, exist_ok = True)\n",
    "\n",
    "### LOOP OVER SIMULATIONS ###\n",
    "for sim_name, sim_func in simulations.items():\n",
    "    print(f\"\\nTraining for {sim_name.upper()}...\")\n",
    "\n",
    "    # Store metrics for the current simulation\n",
    "    simulation_results = []\n",
    "\n",
    "    # x_train is the same, select y_train\n",
    "    x_train = x_train.to(device)\n",
    "\n",
    "    y_train = y_train_dict[sim_name].to(device)\n",
    "    # select the correct y_test (PREVIOUS ERROR)\n",
    "    y_test = y_test_dict[sim_name].to(device)\n",
    "\n",
    "    ### LOOP OVER RUNS ###\n",
    "    for run in range(NUM_RUNS):\n",
    "        print(f\"\\n--- Training Run {run + 1}/{NUM_RUNS} ---\")\n",
    "\n",
    "        # Sample from uniform distributions to initialise hyperparameters\n",
    "        sigma_n = torch.tensor([0.05], requires_grad = False).to(device) # no optimisation for noise, no sampling\n",
    "        # nn.Parameter avoids the leaf problem\n",
    "        sigma_f = nn.Parameter(torch.empty(1, device = device).uniform_( * SIGMA_F_RANGE)) # Trainable\n",
    "        l = nn.Parameter(torch.empty(2, device = device).uniform_( * L_RANGE)) # Trainable\n",
    "        \n",
    "        # We do not need to \"initialse\" the GP model but the mean model\n",
    "        # Initialise fresh model\n",
    "        dfNN_mean_model = dfNN_for_vmap()\n",
    "        dfNN_mean_model.to(device)\n",
    "        # We don't need a criterion either\n",
    "\n",
    "        # Define optimizer (e.g., AdamW)\n",
    "        optimizer = optim.AdamW(list(dfNN_mean_model.parameters()) + [sigma_f, l], lr = LEARNING_RATE, weight_decay = WEIGHT_DECAY)\n",
    "\n",
    "        # Initialise tensors to store losses over epochs (for convergence plot)\n",
    "        epoch_train_NLML_losses = torch.zeros(MAX_NUM_EPOCHS)\n",
    "        epoch_train_RMSE_losses = torch.zeros(MAX_NUM_EPOCHS)\n",
    "        epoch_test_RMSE_losses = torch.zeros(MAX_NUM_EPOCHS)\n",
    "\n",
    "        epoch_sigma_f = torch.zeros(MAX_NUM_EPOCHS)\n",
    "        epoch_l1 = torch.zeros(MAX_NUM_EPOCHS)\n",
    "        epoch_l2 = torch.zeros(MAX_NUM_EPOCHS)\n",
    "\n",
    "        # Early stopping variables\n",
    "        best_loss = float('inf')\n",
    "        epochs_no_improve = 0\n",
    "\n",
    "        ### LOOP OVER EPOCHS ###\n",
    "        print(\"\\nStart Training\")\n",
    "        for epoch in range(MAX_NUM_EPOCHS):\n",
    "            \n",
    "            dfNN_mean_model.train()\n",
    "\n",
    "            # No batching - full epoch pass in one\n",
    "            if run == 0:\n",
    "                mean_pred_train, _, lml_train = GP_predict(\n",
    "                        x_train,\n",
    "                        y_train,\n",
    "                        x_train, # have predictions for training data again\n",
    "                        [sigma_n, sigma_f, l], # initial hyperparameters\n",
    "                        dfNN_mean_model, # vmap?\n",
    "                        divergence_free_bool = True)\n",
    "                \n",
    "                loss = - lml_train\n",
    "                # Backpropagation\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Compute test loss for loss convergence plot\n",
    "                mean_pred_test, _, _ = GP_predict(\n",
    "                        x_train,\n",
    "                        y_train,\n",
    "                        x_test.to(device), # have predictions for training data again\n",
    "                        [sigma_n, sigma_f, l], # initial hyperparameters\n",
    "                        dfNN_mean_model,\n",
    "                        divergence_free_bool = True)\n",
    "                \n",
    "                train_RMSE = compute_RMSE(y_train, mean_pred_train)\n",
    "                test_RMSE = compute_RMSE(y_test, mean_pred_test)\n",
    "\n",
    "                epoch_train_NLML_losses[epoch] = - lml_train\n",
    "                epoch_train_RMSE_losses[epoch] = train_RMSE\n",
    "                # epoch_test_NLML_losses[epoch] =  # train NLML\n",
    "                epoch_test_RMSE_losses[epoch] = test_RMSE\n",
    "\n",
    "                epoch_sigma_f[epoch] = sigma_f\n",
    "                epoch_l1[epoch] = l[0]\n",
    "                epoch_l2[epoch] = l[1]\n",
    "\n",
    "                print(f\"{sim_name} {model_name} Run {run + 1}/{NUM_RUNS}, Epoch {epoch + 1}/{MAX_NUM_EPOCHS}, Training Loss (NLML): {loss:.4f}, (RMSE): {train_RMSE:.4f}\")\n",
    "            \n",
    "            else:\n",
    "                # Save compute after run 1\n",
    "                _, _, lml_train = GP_predict(\n",
    "                        x_train,\n",
    "                        y_train,\n",
    "                        x_train[0:2], # have predictions for training data again\n",
    "                        [sigma_n, sigma_f, l], # initial hyperparameters\n",
    "                        dfNN_mean_model,\n",
    "                        divergence_free_bool = True)\n",
    "                \n",
    "                loss = - lml_train\n",
    "                # Backpropagation\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                print(f\"{sim_name} {model_name} Run {run + 1}/{NUM_RUNS}, Epoch {epoch + 1}/{MAX_NUM_EPOCHS}, Training Loss (NLML): {loss:.4f}\")\n",
    "\n",
    "            # Early stopping check\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                epochs_no_improve = 0  # Reset counter\n",
    "                # best_model_state = dfGP_model.state_dict()  # Save best model\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "\n",
    "            if epochs_no_improve >= PATIENCE:\n",
    "                print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
    "                break\n",
    "\n",
    "        ################\n",
    "        ### EVALUATE ###\n",
    "        ################\n",
    "\n",
    "        dfNN_mean_model.eval()\n",
    "\n",
    "        # Now HPs should be tuned\n",
    "        # Evaluate the trained model after all epochs are finished/early stopping\n",
    "\n",
    "        mean_pred_test, covar_pred_test, _ = GP_predict(\n",
    "                     x_train,\n",
    "                     y_train,\n",
    "                     x_test.to(device),\n",
    "                     [sigma_n, sigma_f, l], # optimal hypers\n",
    "                     dfNN_mean_model,\n",
    "                     divergence_free_bool = True)\n",
    "\n",
    "        # Only save things for one run\n",
    "        if run == 0:\n",
    "            #(1) Save predictions from first run so we can visualise them later\n",
    "            torch.save(mean_pred_test, f\"{RESULTS_DIR}/{sim_name}_{model_name}_test_mean_predictions.pt\")\n",
    "            torch.save(covar_pred_test, f\"{RESULTS_DIR}/{sim_name}_{model_name}_test_covar_predictions.pt\")\n",
    "\n",
    "            #(2) Save best hyperparameters for run 1\n",
    "            # Stack tensors into a single tensor\n",
    "            best_hypers_tensor = torch.cat([\n",
    "                sigma_n.reshape(-1),  # Ensure 1D shape\n",
    "                sigma_f.reshape(-1),\n",
    "                l.reshape(-1)\n",
    "            ])\n",
    "\n",
    "            # Save the tensor\n",
    "            torch.save(best_hypers_tensor, f\"{RESULTS_DIR}/{sim_name}_{model_name}_best_hypers.pt\")\n",
    "\n",
    "            #(3) Save loss over epochs\n",
    "            df_losses = pd.DataFrame({\n",
    "                'Epoch': list(range(epoch_train_NLML_losses.shape[0])), # pythonic\n",
    "                'Train Loss NLML': epoch_train_NLML_losses.tolist(),\n",
    "                'Train Loss RMSE': epoch_train_RMSE_losses.tolist(),\n",
    "                'Test Loss RMSE': epoch_test_RMSE_losses.tolist(),\n",
    "                'Sigma_f': epoch_sigma_f.tolist(),\n",
    "                'l1': epoch_l1.tolist(),\n",
    "                'l2': epoch_l2.tolist()\n",
    "                })\n",
    "            \n",
    "            df_losses.to_csv(f\"{RESULTS_DIR}/{sim_name}_{model_name}_losses_over_epochs.csv\", index = False)\n",
    "\n",
    "        mean_pred_train, covar_pred_train, _ = GP_predict(\n",
    "                     x_train,\n",
    "                     y_train,\n",
    "                     x_train,\n",
    "                     [sigma_n, sigma_f, l], # optimal hypers\n",
    "                     dfNN_mean_model,\n",
    "                     divergence_free_bool = True)\n",
    "\n",
    "        ### Divergence\n",
    "        # Need wrapper function for functional divergence\n",
    "        def apply_GP(input):\n",
    "            mean, _, _ = GP_predict(\n",
    "                x_train,\n",
    "                y_train,\n",
    "                input,\n",
    "                [sigma_n, sigma_f, l], # optimal hypers\n",
    "                dfNN_mean_model,\n",
    "                divergence_free_bool = True)\n",
    "            return mean\n",
    "        \n",
    "\n",
    "        # functional div test\n",
    "        jac_autograd_test = torch.autograd.functional.jacobian(apply_GP, \n",
    "                                        x_test.to(device))\n",
    "        jac_autograd_test = torch.einsum(\"bobi -> boi\", jac_autograd_test) # batch out batch in\n",
    "        dfGPdfNN_test_div = torch.diagonal(jac_autograd_test, dim1 = -2, dim2 = -1).sum().item()\n",
    "\n",
    "        # functional div train\n",
    "        jac_autograd_train = torch.autograd.functional.jacobian(apply_GP, \n",
    "                                        x_train.to(device))\n",
    "        jac_autograd_train = torch.einsum(\"bobi -> boi\", jac_autograd_train) # batch out batch in\n",
    "        dfGPdfNN_train_div = torch.diagonal(jac_autograd_train, dim1 = -2, dim2 = -1).sum().item()\n",
    "\n",
    "        # Compute metrics (convert tensors to float) for every run's tuned model\n",
    "        dfGPdfNN_train_RMSE = compute_RMSE(y_train, mean_pred_train).item()\n",
    "        dfGPdfNN_train_MAE = compute_MAE(y_train, mean_pred_train).item()\n",
    "        dfGPdfNN_train_NLL = compute_NLL_full(y_train, mean_pred_train, covar_pred_train).item()\n",
    "\n",
    "        dfGPdfNN_test_RMSE = compute_RMSE(y_test, mean_pred_test).item()\n",
    "        dfGPdfNN_test_MAE = compute_MAE(y_test, mean_pred_test).item()\n",
    "        # full has cuased issues\n",
    "        dfGPdfNN_test_NLL = compute_NLL(y_test, mean_pred_test, covar_pred_test).item()\n",
    "\n",
    "        simulation_results.append([\n",
    "            run + 1,\n",
    "            dfGPdfNN_train_RMSE, dfGPdfNN_train_MAE, dfGPdfNN_train_NLL, dfGPdfNN_train_div,\n",
    "            dfGPdfNN_test_RMSE, dfGPdfNN_test_MAE, dfGPdfNN_test_NLL, dfGPdfNN_test_div\n",
    "        ])\n",
    "\n",
    "    ### FINISH LOOP OVER RUNS ###\n",
    "    # Convert results to a Pandas DataFrame\n",
    "    df = pd.DataFrame(\n",
    "        simulation_results, \n",
    "        columns = [\"Run\", \n",
    "                   \"Train RMSE\", \"Train MAE\", \"Train NLL\", \"Train Divergence\",\n",
    "                   \"Test RMSE\", \"Test MAE\", \"Test NLL\", \"Test Divergence\"])\n",
    "\n",
    "    # Compute mean and standard deviation for each metric\n",
    "    mean_std_df = df.iloc[:, 1:].agg([\"mean\", \"std\"])  # Exclude \"Run\" column\n",
    "\n",
    "    # Save results to CSV\n",
    "    results_file = os.path.join(RESULTS_DIR, f\"{sim_name}_{model_name}_metrics_per_run.csv\")\n",
    "    df.to_csv(results_file, index = False)\n",
    "    print(f\"\\nResults saved to {results_file}\")\n",
    "\n",
    "    # Save mean and standard deviation to CSV\n",
    "    mean_std_file = os.path.join(RESULTS_DIR, f\"{sim_name}_{model_name}_metrics_summary.csv\")\n",
    "    mean_std_df.to_csv(mean_std_file)\n",
    "    print(f\"\\nMean & Std saved to {mean_std_file}\")\n",
    "    # Only train for one simulation for now\n",
    "\n",
    "### End timing ###\n",
    "end_time = time.time()  # End timing\n",
    "elapsed_time = end_time - start_time  # Compute elapsed time\n",
    "\n",
    "print(f\"Elapsed wall time: {elapsed_time:.4f} seconds\")\n",
    "\n",
    "# Define full path for the file\n",
    "wall_time_path = os.path.join(RESULTS_DIR, \"wall_time.txt\")\n",
    "\n",
    "# Save to the correct folder\n",
    "with open(wall_time_path, \"w\") as f:\n",
    "    f.write(f\"Elapsed wall time: {elapsed_time:.4f} seconds\\n\")\n",
    "\n",
    "print(f\"Wall time saved to {wall_time_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 4090'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wall_time_path = os.path.join(RESULTS_DIR, model_name + \"_run_\" \"wall_time.txt\")\n",
    "gpu_name = torch.cuda.get_device_name(0)\n",
    "gpu_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  2.6897, -0.4457,  0.8486],\n",
       "        [-2.6897,  0.0000, -1.0157, -0.8175],\n",
       "        [ 0.4457,  1.0157,  0.0000, -1.0308],\n",
       "        [-0.8486,  0.8175,  1.0308,  0.0000]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "A = torch.randn(4, 4)\n",
    "A - A.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABcYAAAHqCAYAAAAu1Z3mAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzdd1wT9/8H8FfY24UCKiBuECe4sO6toHVb69a2jrpbW+tCa90DW6vWVmtbtdq6ql93na3iQty46oAqiCCyBYH8/uCXlEiABJLcXfJ6Ph55SC6X5J0Id5/P6z73OZlcLpeDiIiIiIiIiIiIiMhEmAldABERERERERERERGRITEYJyIiIiIiIiIiIiKTwmCciIiIiIiIiIiIiEwKg3EiIiIiIiIiIiIiMikMxomIiIiIiIiIiIjIpDAYJyIiIiIiIiIiIiKTwmCciIiIiIiIiIiIiEwKg3EiIiIiIiIiIiIiMikMxomIiIiIiIiIiIjIpDAYJyIiIiIiIiIiIiKTwmCciIiIiIiIiIiIiEwKg3ES3IULF9CrVy94eHjA2toaLi4uaN68OaZNm6aX99u8eTNkMhkeP36sl9dX917qbp988olOatLmucHBwZDJZFq/BxERkZhcv34do0aNQrVq1WBrawtbW1vUqFEDH330ES5fvmyQGtTtUw3Vxjh37hyCg4Px6tWrItcVU1tEk3ZIrVq1Cqw3783BwQHJycla10pEZIoU22lD7SPffl997RcN2bdX0Od+ivtAIsOzELoAMm0HDhxAjx490KZNGyxduhRubm6Ijo7G5cuXsX37dqxYsULoEnXixx9/RO3atVWWVaxYUflz9+7dERoaCjc3N0OXRkREJCnfffcdPv74Y9SqVQuTJk1CnTp1IJPJEBERgV9//RWNGzfGgwcPUK1aNYPXZqj9+blz5zBv3jwMHz4cpUuX1ug5UmmLjBo1Cp999hk++uijfPUCQFxcHL766isMHDgQjo6OAlRIRERiIcS+S5/7Ke4DiQyPwTgJaunSpfDy8sKRI0dgYfHfr+PAgQOxdOlSASvTXFpaGuzs7Apdx9fXF/7+/gU+Xr58eZQvX17XpRERERmVs2fPYty4cejevTt27twJKysr5WPt2rXD+PHj8fvvv8PW1rbA19Bkv11cYt6fS6UtMnz4cMyaNQsWFhaYPHlyvsdXrVoFAPjggw8MXBkREYmNEPsufe6nuA8kMjxOpUKCio+Ph7Ozs0oormBmpvrreefOHbz33ntwcXGBtbU1PDw8MHToUGRkZAAAHjx4gBEjRqBGjRqws7NDpUqVEBQUhBs3bhRZh6bPVZwCfOXKFfTt2xdlypTRyYi0gk4Bu3//PgYNGoQKFSrA2toa3t7e+PbbbzV6zQMHDqBBgwawtraGl5cXli9frtHzevfujcqVK+dbnpWVhQYNGqBjx44avQ4REZGuLVy4EObm5vjuu+9UQvG8+vXrpxwJXdh+W5t2g6b71JLszxW13rp1C++99x5KlSoFFxcXjBw5EomJiSrrffrppwAALy8v5WnVp06d0ug7LIiu2yLFbYdUqFABQUFB2LZtm7KNl9emTZtQt25dNG3aFADbLUREuqJtnzg8PBy9e/eGk5MTSpUqhcGDB+PFixc6eQ+g6P4/kH/fpe1nKGqfq462+yltcB9IZHgMxklQzZs3x4ULFzBx4kRcuHABb968UbvetWvX0LhxY5w/fx7z58/HoUOHsGjRImRkZCAzMxMA8OzZM5QrVw6LFy/G4cOH8e2338LCwgJNmzbF3bt3C61D2+f27t0b1atXx++//47169cX+Tmzs7ORlZWlcivK7du30bhxY9y8eRMrVqzA//73P3Tv3h0TJ07EvHnzCn3u8ePH0bNnTzg6OmL79u1YtmwZfvvtN/z4449Fvm+rVq3w9OlTPHnyRGX5ypUrcefOHaxdu7bI1yAiItK17OxsnDx5Ev7+/lqfMq1uv63pvr8k+1RA+/15nz59ULNmTezatQuff/45tm3bhilTpigfHz16NCZMmAAA2L17N0JDQxEaGopGjRoVWoch2yIl/c4++OADJCQkYM+ePSrLL1y4gJs3b2L06NHKZWy3EBHphrZ94l69eqF69erYuXMngoODsXfvXnTu3LnAPr0276FJ/18Xn6GofW5BtNlPaYv7QCIDkxMJKC4uTv7OO+/IAcgByC0tLeUBAQHyRYsWyZOTk5XrtWvXTl66dGl5bGysxq+dlZUlz8zMlNeoUUM+ZcoU5fIff/xRDkD+6NEjrZ87d+5cOQD5nDlzNKpB8V7qbm/evCm0ps6dO8srV64sT0xMVHnNjz/+WG5jYyN/+fJlgc9t2rSpvGLFivL09HTlsqSkJHnZsmXlRf3Zh4WFyQHIt23bplz28OFDuZ2dnXz+/PkafW4iIiJdi4mJkQOQDxw4MN9jWVlZ8jdv3ihvOTk5crlcu/12Qft+bfapJdmfK2pdunSpynrjxo2T29jYKD+TXC6XL1u2rMi2zNs1GbItUpJ2iFwul2dnZ8s9PDzkHTp0UFn+4YcfqryvXM52CxGRJhTb6UuXLmn8nKL6xHmXyeVy+datW+UA5Fu2bMn3vgXtrwp6D037/8V9fW32uepos596W2hoqByAfO7cuSV+be4DiUqOI8ZJUOXKlcNff/2FS5cuYfHixejZsyfu3buHGTNmoG7duoiLi0NaWhpOnz6N/v37Fzp/WFZWFhYuXAgfHx9YWVnBwsICVlZWuH//PiIiIgqtQ9vn9unTR6vP+fPPP+PSpUsqN3XTxyi8fv0ax48fR69evWBnZ6cyuqtbt254/fo1zp8/r/a5qampuHTpEnr37g0bGxvlckdHRwQFBRVZa4MGDeDk5ISzZ88ql40dOxbu7u747LPPtPjUREREhuHn5wdLS0vl7e2Ld6vbb2uy7y/pPrU4+/MePXqo3K9Xrx5ev36N2NhYjb8PdQzVFinpdwbkTqc3cuRIHD9+XDkKLi0tDdu3b0efPn1QpkwZ5bpstxAR6Ya2feL3339f5X7//v1hYWGBkydPlug9NO3/6+IzFHefq81+SlvcBxIZFoNxEgV/f3989tln+P333/Hs2TNMmTIFjx8/xtKlS5GQkIDs7Gy1c2flNXXqVMyePRvvvvsu9u/fjwsXLuDSpUuoX78+0tPTdfpcbU/h9vb2hr+/v8qtMPHx8cjKysI333yj0tG3tLREt27dAORekVqdhIQE5OTkwNXVNd9j6pa9zczMDAEBATh37hwAYOvWrThy5AjWr19f4HyuRERE+ubs7AxbW9t8pwsDwLZt23Dp0iXs27dP7XPV7bc12feXdJ9anP15uXLlVO5bW1sDQJFtmaIYqi1S0u9MYeTIkZDJZNi8eTMA4Pfff0dSUlK+C46x3UJEpBva9onf3qZbWFigXLlyiI+PL9F7aNr/18VnKMk+V9P91Nv8/f3x4sULTJ8+vcSvzX0gUckVPEyESCCWlpaYO3cuVq1ahZs3b6Js2bIwNzfHv//+W+jztmzZgqFDh2LhwoUqy+Pi4lC6dGmdPlcmk2n0WYqrTJkyMDc3x5AhQzB+/Hi163h5eRX4XJlMhpiYmHyPqVumTqtWrTB79mxERkZi6tSpGDZsGNq0aaNx/URERLpmbm6Odu3a4ejRo4iOjlYJu318fAAg34UjFdTttzXZ95d0n1qS/bnQilu7LtohAODu7o7OnTvjxx9/xJw5c7Bx40bUqFEDrVu3zrcu2y1ERCWnbZ84JiYGlSpVUt7PyspCfHx8vrBZ2/fQtP+vi89QEtrsp/KysLCAs7Ozzl6b+0CikuGIcRJUdHS02uWK05wqVqwIW1tbtG7dGr///nuBo6SB3E6v4givwoEDB/D06dMi6yjJc/XBzs4Obdu2RXh4OOrVq5dvhJe/v3+BDQ57e3s0adIEu3fvxuvXr5XLk5OTsX//fo3ev1WrVsjOzkZgYCCys7OxfPlynXwuIiKikpgxYways7MxZsyYQi/upQlN9v0l3aeWZH9eGF2NIi9McWvXRTtEYfTo0Xjy5AnWr1+Pv/76q8CLmbHdQkRUctr2ibdu3apy/7fffkNWVlahoawm76Fp/18Xn6GkNN1P6fO1uQ8kKhmOGCdBde7cGZUrV0ZQUBBq166NnJwcXL16FStWrICDgwMmTZoEIPeqyu+88w6aNm2Kzz//HNWrV8fz58+xb98+fPfdd3B0dERgYCA2b96M2rVro169eggLC8OyZcs0OgWrJM/Vl9WrV+Odd95By5YtMXbsWFSpUgXJycl48OAB9u/fjxMnThT43C+//BJdunRBx44dMW3aNGRnZ2PJkiWwt7fHy5cvi3zvxo0bw9bWFjdu3MCmTZuKPKJNRERkCC1atMC3336LCRMmoFGjRvjwww9Rp04dmJmZITo6Grt27QIAODk5Fflamu77S7pPLcn+vCB169ZVvvawYcNgaWmJWrVqwdHRUevX0kftJf3OFIKCguDi4oIpU6bA0tISw4cPV7se2y1ERJo5ceKE2rOrunXrpnWfePfu3bCwsEDHjh1x69YtzJ49G/Xr10f//v0LfH9N30OT/n9JXl9XNN1P6fO1uQ8kKhkG4ySoWbNm4Y8//sCqVasQHR2NjIwMuLm5oUOHDpgxYwa8vb0BAPXr18fFixcxd+5czJgxA8nJyXB1dUW7du2Uc2etXr0alpaWWLRoEVJSUtCoUSPs3r0bs2bNKrKOkjxXX3x8fHDlyhV8+eWXmDVrFmJjY1G6dGnUqFFDObdnQTp27Ii9e/di1qxZGDBgAFxdXTFu3Dikp6dj3rx5Rb63mZkZypQpA39/f53u3ImIiEpqzJgxaN68OVavXo1Vq1bh2bNnkMlkqFy5MgICAnD8+HG0a9euyNfRdN9f0n1qSfbnBWnTpg1mzJiBn376Cd9//z1ycnJw8uRJnZ86XdzaS/qdKVhaWmLEiBFYtmwZevfujQoVKqhdj+0WIiLNFHRBxkePHmndJ969ezeCg4Oxbt06yGQyBAUFISQkpNC5rTV9D036/yV5fV3RdD+lz9fmPpCoZGRyuVwudBFEJC7Lly/HzJkzcfXqVeXBCSIiIiIxYruFiMhwgoODMW/ePLx48YKjk0WA+0CikuGIcSICAKSlpeHatWu4dOkSZs6cia+++oo7ViIiIhIltluIiMhUcR9IpDsMxokIAHD06FH06tULrq6u+OKLL/DJJ58IXRIRERGRWmy3EBGRqeI+kEh3OJUKEREREREREREREZkUM22fcObMGQQFBaFixYqQyWTYu3dvkc85ffo0/Pz8YGNjg6pVq2L9+vXFqZWIiEhr+tpv7dq1Cz4+PrC2toaPjw/27Nmjh+qJiIiI8mO/nIiIpObp06cYPHgwypUrBzs7OzRo0ABhYWHKx+VyOYKDg1GxYkXY2tqiTZs2uHXrll5r0joYT01NRf369bFmzRqN1n/06BG6deuGli1bIjw8HF988QUmTpyIXbt2aV0sERGRtvSx3woNDcWAAQMwZMgQXLt2DUOGDEH//v1x4cIFfX0MIiIiIiX2y4mISEoSEhLQokULWFpa4tChQ7h9+zZWrFiB0qVLK9dZunQpVq5ciTVr1uDSpUtwdXVFx44dkZycrLe6SjSVikwmw549e/Duu+8WuM5nn32Gffv2ISIiQrlszJgxuHbtGkJDQ4v71kRERFrT1X5rwIABSEpKwqFDh5TrdOnSBWXKlMGvv/6qt/qJiIiI3sZ+ORERid3nn3+Os2fP4q+//lL7uFwuR8WKFTF58mR89tlnAICMjAy4uLhgyZIl+Oijj/RSl94vvhkaGopOnTqpLOvcuTM2btyIN2/ewNLSMt9zMjIykJGRobyfk5ODly9foly5cpDJZPoumYjI5MjlciQnJ6NixYowM9P6ZKICvX79GpmZmTp7PSC31rf3BdbW1rC2ttbJ62uy3woNDcWUKVPyrRMSEqKTGoxRTk4Onj17BkdHR+7LiYhIUvTVTjKk4vTLAfbNiYgMSZ/7G6H75vv27UPnzp3Rr18/nD59GpUqVcK4cePwwQcfAMg9sykmJkZlX2VtbY3WrVvj3Llz0g3GY2Ji4OLiorLMxcUFWVlZiIuLg5ubW77nLFq0CPPmzdN3aURE9JaoqChUrlxZJ6/1+vVrVHZyQvybNzp5PQUHBwekpKSoLJs7dy6Cg4N18vqa7LcKWicmJkYnNRijZ8+ewd3dXegyiIiIik2X7SRDK06/HGDfnIhICLre34ihb/7w4UOsW7cOU6dOxRdffIGLFy9i4sSJsLa2xtChQ5V9aXX7qidPnui07rz0HowDyHf0QDF7S0FHmGfMmIGpU6cq7ycmJsLDwwMHTl+HvYOjRu95L7aM1nXevpdR9Ep53L0WpfV7iF2t+iULLXxqFm/EZs0KCRqv65l5X+N17R5d1Wi91KvXNX5NTTw+HVH0ShJRpbV3iZ5v36CeVuuneTXQeN0nVjW0em1DbBcUpLZ9eJOZgj+3tIGjo2bbWE1kZmYi/s0b7PdvAHtzc528Zmp2NoIuX0VUVBScnJyUy3U1WlxBk/2WunU4cqpgit+tt//viEqiYsWKSE1NRb9+/fDDDz8IXY5Gqlativj4eAQGBmLr1q1Cl0NEGkhKSoK7u7tO20lC0LZfDhTcN3/ww3w42tmofY6u+1baknJfrKR9LwVt+2B5adMfA7TrkxmyP1YYsfbVSprHFETbnEabfAbQLqNRR9PcpjiE3h5pKzkjE/VX/6bz/Y0Y+uY5OTnw9/fHwoULAQANGzbErVu3sG7dOgwdOlS5nqH72XoPxl1dXfONoIuNjYWFhQXKlSun9jkFDbu3d3CEg4NmnWnbVO073da22m1wLa0ctH4PsbO2LVlYYWtfvHDKwSFb43UdM+01Xtfezlaj9cysrTR+TU04WBjkmJNBOJbwu3HQ8P9AwdxB8/9fByvtfl+Ls13wawjcvKN9Y0yq2wd97HDszc11/jfh5OSkt3BVk/1WQeu8fXSb/qP43dLn/x2ZHltbW6SmpiI7O1syv1dpaWkAgNKlS0umZiLKJeUD4MXplwMF980d7WzgVEA73ymgKVLCwktWcAlIuS9W0r6XgrZ9sLy06Y8B2vXJDJHTaEKsfbWS5jEF0Tan0SafAbTLaNTRNLcpDl1nPYair/2NkH1zNzc3+Pj4qCzz9vZWXgTa1dUVQO4ZTnnPYtJ3P1vvE6Q1b94cx44dU1l29OhR+Pv7FziPGZGhOfg1FLoEIhIJTfZbBa0TEBBgsDqJ6L8RKa9fvxa4Es1kZ2cjPT0dQO6pp0REhsJ+ORERCalFixa4e/euyrJ79+7B09MTAODl5QVXV1eVfVVmZiZOnz6t13621sF4SkoKrl69iqtXrwLInRz96tWriIyMBJB7qlXeIfBjxozBkydPMHXqVERERGDTpk3YuHEjPvnkE918AiIiokLoY781adIkHD16FEuWLMGdO3ewZMkS/Pnnn5g8ebIhPxqRybOxyT2NXyrBuGK0OADY25dsdBURmTb2y4mISEqmTJmC8+fPY+HChXjw4AG2bduGDRs2YPz48QByR8lPnjwZCxcuxJ49e3Dz5k0MHz4cdnZ2GDRokN7q0nr8/OXLl9G2bVvlfcV8Y8OGDcPmzZsRHR2t3BkDuYn/wYMHMWXKFHz77beoWLEivv76a/Tp00cH5RMRERVOH/utgIAAbN++HbNmzcLs2bNRrVo17NixA02bNjXcByMiZTCekaH706z1Ie/FiThinIhKgv1yIiKSksaNG2PPnj2YMWMG5s+fDy8vL4SEhOD9999XrjN9+nSkp6dj3LhxSEhIQNOmTXH06FG9XuND62C8TZs2yot0qLN58+Z8y1q3bo0rV65o+1Yl4u2SgIjn2l/YgYiIjIu+9lt9+/ZF3759S1oekSikpqZKcgSz1EaMp6amKn9mME5EJSGVfjkREZFCYGAgAgMDC3xcJpMhODgYwcHBBqtJ73OME5kir3Z1hC5BJ4zlcxARUeEOHjyI0NBQocvQmtTmGM87YlyKByKIiIiIiIwJg3EiIiIiE5eRkYEPP/wQb968EboUrUhtxDinUiEiIiIiEg8G46Tk3chT6BKIiIhIAJmZmbh58yZWrlwpdClakdoc43mnUuGIcSIi/eBZr0REpCkG40REREQmLjMzEwAwb948PHz4UOBqNMcR40RERKQtDgokIgUG40REREQmThGMK64CX9gF3cREynOMMxgnIiIiIhIWg3EiIiIiE6cIxgHgyJEj2LFjh4DVaE5qI8Y5lQoRERERkXgwGCciIiIycW/P0T158mQkJCQIVI3mFMF4VlYWsrOzBa6maBwxTkREROroa3oX39rWenldImPBYJyI1OJFa0qG89YRkZTkHTEOAM+fP8eMGTMEqkZzimAckMYFOPMG41IbMS6Xy3Hs2DGhyyAioiI4+DUUugQiIslgME5ERGRizpw5g6CgIFSsWBEymQx79+4t8jmnT5+Gn58fbGxsULVqVaxfv17/hZLBvB2MA8B3332Hc+fOCVCN5hRzjAPSmE4l71QqUhoxnpaWhsGDB+PChQtCl0JEREREpDMMxomIiExMamoq6tevjzVr1mi0/qNHj9CtWze0bNkS4eHh+OKLLzBx4kTs2rVLz5WSoagLxgHgo48+wps3bwxcjebyjhiXQjCed8S4nZ2dgJVoLjIyEu+88w62bduGXr16CV0OEREREZHOWAhdABERERlW165d0bVrV43XX79+PTw8PBASEgIA8Pb2xuXLl7F8+XL06dNHT1WSIakLxgMDAxEREYGVK1fis88+E6Cqokk1GLezs4OZmfjHp/z111/o06cPXrx4gRo1asDHx0fokoiIiIiIdEb8LXKSDF7UgYjIOIWGhqJTp04qyzp37ozLly+LejQxaS4zMxN2dnYYMGCActnHH3+MBw8eYMKECQJWVjipzTGumEpFCtOoxMfHY9OmTXjx4gUAoFevXpDJZAJXRVQ0uVyO2NhYocsgIiIiCeCI8RLwbuSJiCtPhC6DiDTk7ZKAiOdlhC6DSHJiYmLg4uKisszFxQVZWVmIi4uDm5tbvudkZGSoBJVJSUk6q+fu3bs4c+YMAODdd99F+fLldfba+vDzzz8jLS0N1apVQ8eOHYUuR61SpUrh5MmTKFeuHO7du4dOnTqhSpUqAMQ95Yevry9GjhwJGxsbODk5CV1Okbp06QIXFxfY2toKXUqRypUrh8WLF+Pff//FkydP0Lt3b6FLKtSjR49w5swZvHz5EgMHDlS7XRKTP/74A8+fP0fZsmXRt29focspVEJCAlavXo3q1aujZcuW8PQU7wXGc3JyMGHCBDx8+BAHDx7kwRwBOfg1REpYuNBlEBERFYrBOJGeeLWrg0cnbgldhqSkVmskdAlEVIC3wwW5XK52ucKiRYswb948vdRy9uxZfPjhhwCABg0aiD4YX758OW7cuIEmTZqINhj/6quvYGlpCQC4cuWKwNVo5ujRo+jUqRPat28vdCkaGzNmjNAlaMXFxQXHjh3DxYsX4e/vL3Q5hQoNDcXw4cMBAA0bNhR9ML58+XL8/fff8Pb2Fn0wHhERodyeb9iwAR988IHAFamXlZWFUaNGYe/evQgICEBCQgLKli0rdFlEREQkYpxKhYiIiArl6uqKmJgYlWWxsbGwsLBAuXLl1D5nxowZSExMVN6ioqIMUaooRUdHA4CogzpFKC4V0dHRmDNnjtBlmIwmTZqIfk70vNui+Ph4ASvRTHZ2NgDA3Nxc4EqK9uDBA+XPNWrUELCSwiUmJmL+/Pl49eoVDh06xFDcxHm1qyN0CUREJAEcMU5E+bAhSUR5NW/eHPv371dZdvToUfj7+xcYqFpbW8PamteeyMzMRFxcHACgYsWKAldjPM6ePYsLFy7g7t27qFWrltDlkAjkDUFfvnwpYCWakWowXr16dQErKVy5cuUKPFhLREREpI64h34QERGRzqWkpODq1au4evUqgNy5ea9evYrIyEgAuaO9hw4dqlx/zJgxePLkCaZOnYqIiAhs2rQJGzduxCeffCJE+ZKSd6S9mEeMS83Zs2cB5M7fLmU5OTm4c+eO0GUYhbyBKINx3bp//z6A3Ivd8gAfERERGRMG4wQg90KiRERkGi5fvoyGDRuiYcOGAICpU6eiYcOGyqkpoqOjlSE5AHh5eeHgwYM4deoUGjRogC+//BJff/01+vTpI0j9UvLs2TPlzwyUdOfvv/8GAPzyyy/IyckRuJriO378OHbv3i10GUYh74hxTqWiW4oR49WqVRP9lDpERERE2uBUKkRERCamTZs2yotnqrN58+Z8y1q3bi2ZizKKiWJ+cYAjxnUlJSUF4eHhAICoqCicPHlSUhfgzGv9+vVCl2A0SpUqBXNzc2RnZ3PEuA7J5XLliHExzy9OREREVBw85E9ERESkJxwxrnsXL15UhoqAdKdTefbsGf744w9cvnxZ6FKMgkwmQ5kyZQBwxLguvXz5EomJiQDEPb84ERERUXEwGCfBebskCF0CERGRXnDEuO4pplFR2LVrF1JSUgSqpvg2bdqE7OxsREZGKi/QSiWjmGdcCiPGs7KyAAAWFuI+gVcxWhxgME5ERETGh8E4ERERkZ4oRoybm5ujfPnyAldjHBQX3lRITU3Frl27BKqmeLKzs7Fhwwbl/bCwMAGrMR6KecY5Ylx3FPOLAwzGiYiIyPgwGCciKoJvbWuhSyAiiVKMGHd1deVF63QgKysL586dy7dcatOpHD58GFFRUcr7nE5FN6Q0YlzMwXhWVhYWL16MrKwslWCcc4wTERGRsWEPjej/Ofg1FLoEUfBqV0foEoiIjIZixDjnF9eNGzduqJ025eTJk4iMjBSgouJ5+6KbHDGuG3lHjBd2gWExEHMwbmFhge+//x6tWrXC6dOnAQDW1tawtrbGhg0bcPDgQYErJCIiItINBuN5cFQoERER6ZJixDjnF9cNxTQqdnZ2ymW1a9eGXC7HL7/8IlRZWomMjMwXLDIY1w3FiPHMzEykpaUJXE3hxByMA0CtWrUQGhqKU6dOAQDevHmDSpUqYdKkSfD39xe2OCIiIiIdYTBOpEemPvqao/CJyJRlZmbixYsXADhiXFeuXbuGxYsXY/v27cplCxYswL1791CmTBkBK9PcDz/8gJycHJVlkZGRyt8VKj7FiHFA/POMSyEYzysnJwfZ2dkYMWIEKlSoIFBVRNox9b4YEREVTdyXQScikjDvRp6IuPJE6DKISCDPnz9X/swR47oREhICe3t7XLx4Ubns1atXqFGjhiTmP37z5g1++OEHtY+FhYWhS5cuBq7IuOQNxl++fAkPDw8Bqymc1IJxADAzM8PUqVMFqIbINDDIJyIyPI4YJyIiItIDxfziAEeM64q9vT0AoHTp0splr169EqaYYti/fz+io6NRqlSpfI/xApwlp5hKBeCI8ZJSF4z36dMH1atXF6AaIiLj5t3IU+gSiEwWR4wTERER6VBSUhKcnJxUgnGOGNctqQbjz549w/79+1G6dGm0bNkSABAcHIzr169znnEdeHvEuJgpgnELC3F2x2rWrJlv2aeffipAJURERET6wxHjRERERDo0YMAA/Pzzz/lGjCclJSkvHkklk3fEtZSC8Y8//hiBgYGIi4tTLvP398euXbswY8YMASszDlIaMZ6VlQVAvCPGK1asCAcHB+X9tm3bonHjxgJWRFLE6w0REZHYMRgnIiIi0iEPDw8MGzYMM2fOVC4LDg6Gm5sb7ty5I2BlxsPa2hq2trYApBWMK8TExCh/dnV1BQA0adJEqHKMhhRHjIs1GJfJZCqjxj/77DMBqyEiIiLSDwbjRKTEC74QEZVco0aNAACJiYnKZfv27YOlpSUGDhwoVFlGRzGdirEE41RyUhoxLvZgHPhvnvF69eqhU6dOAldDREREpHsMxokXeiAiItKhhg3Vnzo+fPhw5cUjqeSMJRivUKGCgJUYFwcHB+Wc3RwxXnKKYHz69OmQyWQCV0NEpsq3trXQJZCepVZrJHQJZMIYjJcQQ+Vc3FkRERHlqlu3rtqwa8yYMQJUY7yMIRh3dnaGpaWlwNUYD5lMphw1LuYR43K5HHK5HID4g3EPDw/0799f6FKIiIiI9ILBOBEREZEO2drawsfHR2VZu3btULt2bYEqMk7GEIxzGhXdU8wzrhgxnpqaiqdPnwpZUj6K0eKAuIPxmjVrYurUqTx4Q0REREaLwTgRERGRjinmGVcYO3asQJUYLwbjpHDr1i3MnDkTkZGRKiPGf/vtN9SuXVtlvn8xkEowXrt2bYwaNUroMoiIiIj0hsE4ERERkY7lDcbd3NzQs2dPAasxTopgPCUlBVlZWcIWowW5XM5gXMd8fHywY8cOeHl54cqVKwCAO3fuYMCAAUhJSRHd2RpSCcbt7Ozg4OAgdBlEJeLVro7QJZBIcVpcIgIYjBPpHRtjRESmJ+8FOD/44ANORaAHimAcgOhGBBcmMTERGRkZABiM64pMJsPw4cORk5ODtLQ0lceaNm0KMzNxdXnyHshRXCyUiIiIiAxPXK1EIhIMA3wiIt1p0KABgNzRoB988IGwxRipvMG4lKZTUYwWBxiM69KwYcMgk8nyLW/evLkA1RROKiPGiYiIiIwdg3EiIiIiHXN0dETNmjXRo0cPVK5cWehyjBKDccrL3d0dHTt2zLecwTgRERERFYTBuA5wbioiIiJ6W6NGjXjRTT1iME5vGzFihMp9mUyGpk2bClRNwRiMExEREYkDg3EdYThuHBz8Gha9EpEWuG0gMl0DBw5E+/bthS7DaDEYp7e9++67Kr8XderUQalSpYQrqAAMxonIVHm7JAhdguiIrb/I/yMyNQzGTZzYNsIkDM4vTsZu7dq18PLygo2NDfz8/PDXX38VuO7w4cMhk8ny3erU+e/vZPPmzWrXef36tSE+DklEjx49RHfRP2PCYJzeZmNjg0GDBinvi3EaFYDBOJkWMQw8Yl+HiEh8Fi1aBJlMhsmTJyuXqeuLN2vWTK91sLemQwyZqSBsjEmfb21roUugYtqxYwcmT56MmTNnIjw8HC1btkTXrl0RGRmpdv3Vq1cjOjpaeYuKikLZsmXRr18/lfWcnJxU1ouOjoaNjY0hPhJJhLoLAYrJ+fPnVQI6qZF6MG5paYkyZcoIXI3xyTudCoNxIlJgf4yISDwuXbqEDRs2oF69evke69Kli0of++DBg3qthcH4W0oafpliOM7AUNrYSCRjt3LlSowaNQqjR4+Gt7c3QkJC4O7ujnXr1qldv1SpUnB1dVXeLl++jISEBLVz1+ZdjyM/SWru37+P4cOHIysrS+hSiiVvMJ6QIJ3TfhXBuIuLC88o0AM/Pz/UrVsXAINxIpIOXfXJSjpCP7VaI53UIRVC5zdCvz+REFJSUvD+++/j+++/VztIxNraWqWPXbZsWb3WY9StcaHmRuLGTTxMbccuJmI4bZKMV1JSksotIyND7XqZmZkICwtDp06dVJZ36tQJ586d0+i9Nm7ciA4dOsDTU3XbnpKSAk9PT1SuXBmBgYEIDw8v3ochEkhAQAC2bNmCgQMHIjMzU+hytJZ37mgpjhjnwTT9kMlkGDFiBMqUKYOaNWsKXY5aDMaJhMEBQdLFwXhUXMwl9E/TvrnC+PHj0b17d3To0EHt46dOnUKFChVQs2ZNfPDBB4iNjdVH2UoWen11E+bdyBMRV54IXUahGOCr5+DXEClhug+4vNrVwaMTt3T+uiXBxiEZQpXW3nC0ttLJayVnZAIXwuDu7q6yfO7cuQgODs63flxcHLKzs+Hi4qKy3MXFRWWe34JER0fj0KFD2LZtm8ry2rVrY/Pmzahbty6SkpKwevVqtGjRAteuXUONGjW0/2BEAqhatSoqVKiAXbt2oXfv3ti5c6ekpgOytraGra0t0tPTGYyTisGDB+P06dOiHZGf9ywNCwt2x4gMSYx9MjJNzGNICEL2zQFg+/btuHLlCi5duqT28a5du6Jfv37w9PTEo0ePMHv2bLRr1w5hYWGwttbPATK2xPRICuE4kanxdklAxHPDzunKbYHuRUVFwcnJSXm/qJ3k23M9y+VyjeZ/3rx5M0qXLo13331XZXmzZs1ULgLSokULNGrUCN988w2+/vprDT4BkfBkMhkCAgKwd+9eHDhwAIGBgfjjjz9gb28vdGkaK126tKSC8ezsbLx48QIAg3F9Kl++PObPny90GfncunULPj4+akeMR0VFoXLlyqK/NgFRcehr4FFxMRwnU8GR9mQImvbNo6KiMGnSJBw9erTAwTgDBgxQ/uzr6wt/f394enriwIED6N27t24L/3/iHEZhRHgUkMSKo8VJypycnFRuBe18nZ2dYW5unm90eGxsbL5R5G+Ty+XYtGkThgwZAiurwo+qm5mZoXHjxrh//752H4S0snLlSrUj/e/cuYNNmzYJUJH0BQQEKH8+fvw4OnfujMTERAEr0o5innGpBOMvXrxATk4OAAbj+qbuYk5C27lzJ/z9/XH48GHlsocPH2L48OEYOXIkQ3EiIgEwsyEqGU375mFhYYiNjYWfnx8sLCxgYWGB06dP4+uvv4aFhYXKwAEFNzc3eHp66rWfzWCciIiMlpWVFfz8/HDs2DGV5ceOHVMJBNU5ffo0Hjx4gFGjRhX5PnK5HFevXoWbm1uJ6qXC3bt3D15eXvjuu++Uy6ZOnQofHx+8fv1awMqk6+2/g7Nnz6JDhw54+fKlQBVpR2rBeN4DOwzGTU/Xrl1x5coVTJ8+Xbls1apV+OmnnxAYGChgZUSmR0yDhMRUCxkGw3gyRe3bt8eNGzdw9epV5c3f3x/vv/8+rl69qva6K/Hx8YiKitJrP5vBuAGIcaOnq5p4ao52xNLoEUsdUsPfd2maOnUqfvjhB2zatAkRERGYMmUKIiMjMWbMGADAjBkzMHTo0HzP27hxI5o2bQpfX998j82bNw9HjhzBw4cPcfXqVYwaNQpXr15VvibpR1BQEF6/fo2LFy8ql/3999+Qy+UMlYrJz88PlpaWKssuX76MNm3a4Pnz5wJVpTkG48JISUnBzp07kZycLHQpkuLv74/y5curfeztKbuISP/YJyIiMhxHR0f4+vqq3Ozt7VGuXDn4+voiJSUFn3zyCUJDQ/H48WOcOnUKQUFBcHZ2Rq9evfRWF4NxAxFjOE4F45WLDS+1WiOhSyAjNWDAAISEhGD+/Plo0KABzpw5g4MHD8LTM3e7HB0djcjISJXnJCYmYteuXQWOFn/16hU+/PBDeHt7o1OnTnj69CnOnDmDJk2a6P3zmLJ27drB1tY23/L69evDw8NDgIqkz8bGBn5+fvmW37hxA61bt8a///4rQFWaYzBuOGlpadi5cyf69euHChUq4MmTJ3B0dBS6LEkxMzND586d8y1v2LChcp9EZKzYv6KSMpZBSsyGiNQzNzfHjRs30LNnT9SsWRPDhg1DzZo1ERoaqtc2Jy++qYZvbWvcvJOh89flBfiITBf//oU1btw4jBs3Tu1jmzdvzresVKlSSEtLK/D1Vq1ahVWrVumqPNKQra0tOnbsiH379qksDwoKEqgi4xAQEIDz58+rLKtTpw48PDywdu1aLFiwAGZm4hxLoQjGU1NT8ebNm3yj38VGasH469evcfjwYezYsQP79+9HamoqAKBv376YOnWqwNVJU7du3bBlyxaVZRwtTiQcY7oQJw8+FB/7asZNbBcApv+cOnVK+bOtrS2OHDli8BrE2csxYjw6qMrbJUHoEgxO6FP2hH5/IqKSUDdlCoPxklE3337Dhg1x8OBBLFy4ULShOPBfMA5AEhcNzRuMF3UBYKGlpaVh2LBh6NWrF7Zv364MxWvVqoVNmzbxQpHF1KlTp3x/UwzGdSs+Ph4PHz4UugySEPaPiITHM8hJKOLt6RgxocNxod+fjB9HKxCRvrwdjLu4uMDf31+gaoyDIhjv3bs3mjVrBgDYsmULIiIihCxLI3mDcSlMp6IIxh0cHODg4CBwNYWzs7PDmjVrUKlSJeUye3t77N69m1OolEC5cuVUpt2qWrUq6tatK2BFxiM7Oxtr165FjRo1GIyTZDCUNy3MYojEh8G4QLhBFD9jDHfZ8CIiqXNzc0Pt2rWV91u3bi3qEc0A8MEHH2DMmDHYuXOn0KWo5ebmhqZNm2L9+vVYtWoV6tSpg8OHD8Pb21vo0orUtm1bLF++HD/88APKlSsndDlFGjt2LNasWYN58+YJXYpGFBdDcnNzA5B7UWIfHx+Bq1Lvzp07mDlzJvr164ebN28KXU6hunXrpvy5YsWKoh99HxERgaZNm2L+/Pl48kScUw2cOXMGfn5+GD9+PBISEjBs2DDI5XKhy6K3iLl/xX4SEZFp4hzjApL6PFbGcvELIRjTXHZERIZWs2ZN3LlzBwBEP1o8KysLW7duRXp6Ol68eIG+ffsKXZJae/bsQfny5VG+fHlcv35d9AcbFBo3bozGjRsLXYbGWrdujdatWwtdhsbMzMywb98+rF+/Ho8ePcKAAQOELqlA//77LxYuXAggd2oSX19fgSsqWNeuXTFnzhwAwLNnzwSupmj/+9//cPHiRVy8eBGtWrUS1YVC//33X0yfPh2//vqrynKxHsAhcWMfzbRJPZ8houJhME5kIjgKQjf0dXFeItJcgwYNlBfgbNq0qcDVFO727dtIT08HAFEHuIoRwQAkE4qTYVhZWSEoKAiVK1cWupRC1axZU/nzvXv3BKykaI0aNYKFhQWysrJUpgMSqwMHDgDIvTB1ixYtBK5GlYuLC9asWYNFixYhOTkZSUlJSE5ORnJyMhISElC2bFmhSyQiAsBZA4jEisG4wAx9VJIbYyLhcBQCkW54eHgof7a1tRWwkqJdvnxZ+bPYR7cTFcTLy0voEopUuXJl2NjY4PXr16IPxs3MzGBvb4/ExETY2NgIXU6hEhMT8ffffwPIvXCopaWlwBWpsrS0RNmyZRmAk85w1Li4cZCS5niGP5FmOCRIBBhWmyZDjuDmaHEiMiZin483r0uXLil/9vPzE7ASIuNmZmaGGjVqABD/iHHgv4N6VlZWAldSuKNHjyI7OxsA0L17d4GrIWMg5nnGFQzVd9Ll+0jhezVlzHyEx78RKgiD8QIY+ugaN5TixI0nERGVhCIYr169OsqUKSNwNUTGrVatWgByg3GxX3jRwiL3xF1ra3GP6FNMowLkzo1OZCo4sMg0MZchMj1GH4x7uyQIXYLGuBEmfWCjjohIGBkZGbh+/ToAcc8vTmQsFPOMp6SkIDo6WuBqCpeVlQVA3CPGc3JycOjQIQC527AKFSoIXBERERGRbhl9MC41+gzHdfnaUpqvKrVaI6FLMClSGGUvpQNmRCRd169fx5s3bwAwGCcyBCldgDMzMxOAuEeMX758GbGxsQA4jQqZJg4wIl3gAEgicWMwLkLccJoOfTe22JgTH/59E5mOvPOL88KbupGRkSH6KTJIOFIKxjMyci8eJ7YR48+fP0dMTAwA1WlUGIyTqWJ/ikwJB5CRKWIwTkRkYN6NPBmQE5mAy5cvA8i9KGCjRjx7SReuX7+Ow4cPC10GiZSUgnHFiHGxBeOxsbF455138OjRI2Uw7uLiwm0Y6ZQUzjDVN1MO3Esavur77HVd9tPY5yMSPwbjIsUNqHhIteFmyo0tfdNVY0wRkOe9EZHxUIwY9/Hxgb29vcDVGId79+5hyZIlQpdBIlWuXDmULVsWAHD37l2BqylYTk6OcpolsU2lYmZmhn/++QcBAQEICwsDAHTr1g0JCQk4efIkEhMTBa6QyPDYryIiMl4MxkWMRypNAxtalJe6sJx/v0TSk5qaitu3bwPgNCq6dO/ePZw+fRoXLlwQuhS9uH//vtAlSJ5i1LiYR4wrQnFAfCPGzcxyu4eK6VQA4Ndff4WzszOWL18OJycnoUojEhT7bET6p+/rw0l10CPpF4NxkWMgRsXBhpvxYVhOJC3h4eHIyckBwAtv6pIi7Fy6dKnAleheWFgYNmzYIHQZklerVi0AwMOHD1UCaDFRzC8OiHPE+Ntev34NR0dHrF+/HjKZTICqiKgwugr79B1Kmhr214ikgcF4IfQ9d5WmxLZBFcv3QsaDjbDi4ehyIvHKe+FNBuO6o5geY8+ePaKeKkNbcrkcn376KV68eCF0KZKnGDGelZWFx48fC1tMARTziwPiGzFeUPC9dOlSuLu7G7gaMmZSHLnJwUemg30qEop9g3pCl2ByGIxLBDfMxk2XjSw22IhhOZEwYmNjlYGX4sKblpaWqFePDVxdkMvlyhHjcrkcy5cvF7gi3Tl48CBOnjzJYFwHpHABTqmNGG/VqhU+/PBDAaohEh9d9LXYXyN942BG6XHwayjJA4bGgMG4hBQ33GIoVnLcQJHUcXQ5kf7dv38fbdu2xbNnz5QjxuvVqye64EuqoqOjkZqaqrz/888/Izo6WsCKdCMrKwuffvopACAuLk7gaqRPCsG4mEeMvx2M29jY4Pvvv1cbmBMRCUXswS/7WaQpBuLCYwtHYoxpA+vtkiB0CUZH6NEH3KBLjyIgr1Wfp0cTlZSLiwvOnTuHRo0aKS+i6OHhgZ9++gkjR45k6FlCb4ecmZmZCAkJEaYYHdq0aRMiIiIAMBjXherVqyt/Fut0O3mDcbEdOHs7AJ83b57KwQYiEr7PRUTSx0BcPBiME4kEG1jSIvZRCkRkeC4uLgCA58+fK5ft2bMHw4cPh62tLZydnYUqzSioG/27fv16JCYmClCNbqSkpGDOnDnK+wzGS87Ozk45F7ZYR4znnUpFbCPG884x7ufnh6lTpwpYDRk7KYdC7LsZP56xT/rAQFx8GIxLEDe0pA4bZ0REwnJwcICNjU2+5fb29pg9e7YAFRkXdSFnUlIS1q9fL0A1urF8+XKVAylJSUkqoSkVj2KEs1iDcSlMpWJhYYGNGzfCwsJC4IqIxIv9LyLpESqUZiAuXsUKxteuXQsvLy/Y2NjAz88Pf/31V6Hrb926FfXr14ednR3c3NwwYsQIxMfHF6tgyiVUOM5RskREROrJZDLlqPG8pk2bBldXVwEqMi4FTYsREhIiyTD52bNnWLZsWb7lbCOXnCIYf/r0KVJSUgSuJj8pXHzz888/R/369QWuhjTBvrm06DpMZ9BGJG4MxMVP62B8x44dmDx5MmbOnInw8HC0bNkSXbt2RWRkpNr1//77bwwdOhSjRo3CrVu38Pvvv+PSpUsYPXp0iYs3dZqE4xxdrjti3phxtAIRkTi8HYyXL18e06ZNE6ga43Lv3j1UqFABtWrVAgA4OTnh0KFD6NatGw4cOCBwddqbO3cu0tLS8i3ndColp/gdAYAHDx4gMjJSVPPRi33EuLe3N2bNmiV0KaQB9s2Fx34Y5cX8hRQYiEuH1sH4ypUrMWrUKIwePRre3t4ICQmBu7s71q1bp3b98+fPo0qVKpg4cSK8vLzwzjvv4KOPPsLly5dLXLwhiH2ENDe8xoUNKyIiaXs7GJ89ezacnJwEqsZ4vHnzBvXq1cONGzfQrVs3ALnTjjRp0gQbN25E7969Ba5QO7du3cKmTZvUPsZgvPiWL1+OP/74QznHOACsWLEC9erVw9OnTwWsTJWYR4wrplARW12knjH0zY0hOGIfThhiz2rINDEQlx6tgvHMzEyEhYWhU6dOKss7deqEc+fOqX1OQEAA/v33Xxw8eBByuRzPnz/Hzp070b179wLfJyMjA0lJSSo3KhjDcdPGhljxeLskCF0CERmhChUqKH/28vLCRx99JGA1xsPc3By//fYbKlSogBo1aiiX379/X8Cqiu+LL75AkyZN0LVr13yPvXjxQoCKjEOlSpXw7rvvYsCAAcplW7ZsQWJiosrvjdDEPGK8fPnyaN68udBlkAbYNyfSP22yFuYypo2BuHRpFYzHxcUhOzs732goFxcXxMTEqH1OQEAAtm7digEDBsDKygqurq4oXbo0vvnmmwLfZ9GiRShVqpTylnfUR3GYQgDGjTAREZHw8raRvvrqK9GFXlJlZmYGmUwGAJIPxnNycrBlyxaEhoaiSZMmyuUbN26Eq6srR4yXQL9+/VC1alVkZWXle0yswThHZlNxSbVvbqw4WInINDEQl75iXXxT0TFRkMvl+ZYp3L59GxMnTsScOXMQFhaGw4cP49GjRxgzZkyBrz9jxgwkJiYqb1FRUcUp0+QxLJcmbRpVbIAJi6fvEdHbFAFFw4YNVUatku5IPRg3MzODo6MjAODChQsAgNKlS2P48OG4fPkyPD3ZfisuCwsLfPrpp2ofE1MwnncqFR48o5Ji31w82DcjMZB6HzW1WiOhS9AIA3HjYaHNys7OzjA3N893BDo2NjbfkWqFRYsWoUWLFspGar169WBvb4+WLVtiwYIFcHNzy/cca2trjp4oBu9Gnoi48kRvry/lDWxqtUaw/+dKiV/Hwa8hUsLCdVCR8eFOgYjov2B88eLFMDMr1vgDKoK7uzusra2RkZEhyWBcQS6XK4PxJk2awMzMDJUqVUKlSpUErkzahg8fjuDgYDx//ly5zNbWFhUrVhSwKlVinkqFpIN9c+lhcG68OChRWkqS6zD3MD5a9disrKzg5+eHY8eOqSw/duwYAgIC1D4nLS0tX8fQ3NwcQG6HgHSLG2TTwYYVEZH4VKhQAe3bt0fHjh2FLsVomZmZoVq1agCkOWJc4f79+0hIyJ3ur2nTpgJXYzxsbGwwZcoUlWXVq1cX1YEqMV98k6TDmPrmxhQ0sY9mfJixkIIxbavoP1q3EKdOnYoffvgBmzZtQkREBKZMmYLIyEjl6VczZszA0KFDlesHBQVh9+7dWLduHR4+fIizZ89i4sSJaNKkiahGbhgTbriljw0qIiJpcnV1xeLFiws8jZ10QzEtxv379yU70EIxWhwAmjVrJmAlxmfMmDFwcnJS3hfTNCoAR4yT7rBvLk6G6ssxpJP2We0kHZw2xbhpNZUKAAwYMADx8fGYP38+oqOj4evri4MHDyrnQ4yOjkZkZKRy/eHDhyM5ORlr1qzBtGnTULp0abRr1w5LlizR3aegfBiOGzcG50RE4lS7dm2G4gagCDoTExMRFxeH8uXLC1yR9s6fP6/8Oe9FOKnkSpUqhXHjxmHx4sUAxBeMc8Q46Qr75uLl1a4OHp24JXQZZADMXowXw3DToHUwDgDjxo3DuHHj1D62efPmfMsmTJiACRMmFOetRMG3tjVu3skoekUiIiIyaQzFDePtC3BKMRhXjBivVq0anJ2dBa7G+EyaNAmrVq1CRkaG6IJxjhgnXTK1vjkR6Ye3S4LQJYgGA3HTIp7J9ogkwlAbyYJGhRvbaHGpXHWayNisXbsWXl5esLGxgZ+fH/76669C19+6dSvq168POzs7uLm5YcSIEYiPjzdQtUSq3g7GpSY9PR3Xrl0DwGlU9MXV1RUjR44EwBHjRFJgjEFU3n6bsfXhTE1Bo8I5Wty4cMoU08RgnDTCubuIiIzHjh07MHnyZMycORPh4eFo2bIlunbtqnK6dV5///03hg4dilGjRuHWrVv4/fffcenSJYwePdrAlRPlknowfuXKFWRlZQHghTf16ZNPPoGZmZnognGOGCcyHQzE1ePoZBITBuKmjcE4EVEJ8KARSdHKlSsxatQojB49Gt7e3ggJCYG7uzvWrVundv3z58+jSpUqmDhxIry8vPDOO+/go48+wuXLlw1cOVGuihUrwtbWFgDw4MEDgavRXt4LbzIY15+qVati9OjRcHV1FboUpKen4+zZswBUg3Fra2vk5OTg8ePHAlVGRPrGcFy/hOqPcbS4tCnCcAbixGCcBMEjxJp5uxEl1kYVdyZE0pGZmYmwsDB06tRJZXmnTp1w7tw5tc8JCAjAv//+i4MHD0Iul+P58+fYuXMnunfvboiSlb744gvcvHkz3/JTp04hJCTEoLVIyYULFxAaGgq5XC50KTpjZmaG6tWrA5DmiHFFMG5tbY0GDRoIW4yRW7RokSjm/re1tcXw4cPRo0cP3LlzR7l869atqFOnjsrFWIlMFfsUmuN3ZXh5g3CxhuIctEWkPZMJxksaxJryBsaUP3tBhGiIiDUUlzIeoCFTFBcXh+zsbLi4uKgsd3FxQUxMjNrnBAQEYOvWrRgwYACsrKzg6uqK0qVL45tvvinwfTIyMpCUlKRyK6m0tDTUq1cPixcvVi4bMmQI2rZti1KlSpX49Y1V/fr18eGHH8LHxwdLly5FdHS00CXphGJ6jPv370su9FeEoA0bNuRUGnpWtmxZoUtQ6tSpE/bv349Dhw4pl3344Yd49OiRwQ80EhHlJfXrPjGzMC5S/30kaTGZYJxI1wx5EU6G4uLGhhhJ0dsjKOVyeYGjKm/fvo2JEydizpw5CAsLw+HDh/Ho0SOMGTOmwNdftGgRSpUqpby5u7uXuOagoCDI5XKVEcJ3796FTCZDt27dSvz6xsrGxgbbt2/H48eP8dlnn8Hd3R2BgYHYtWuXypQOUqMIxpOTkxEbGytwNZqLiYlRzufPaVRMy9tn6ih06dIFjo6OBq6GSJw4EprEzLuRp2hHixNR8TAYp0Ix8CMiMi7Ozs4wNzfPNzo8NjY23yhyhUWLFqFFixb49NNPUa9ePXTu3Blr167Fpk2bChx9PGPGDCQmJipvUVFRJa69ZcuWcHJyyre8SZMmBdZOuerUqYOVK1cCALKzs3HgwAH07dsXlSpVwuTJk3Ht2jWBK9SeVC/AmXd+8WbNmglYCRla27ZtYW5unm95nz59BKiGiKSKBw/UY3ZBRMXBYJxMhj5Ox2GjhN8BkdRYWVnBz88Px44dU1l+7NgxBAQEqH1OWloazMxUmwyKcKegKSysra3h5OSkctNF7V26dMm3PCgoqMSvbQrGjBmDnj17qiyLi4vD6tWr0aBBAzRq1AjffPMN4uPjBapQO1INxvPOJc0R46bFyckp38EQS0tLbsOI3sL+BRERGQqDcS2Y2hFIU/u8ZHjGNHcY/15ISqZOnYoffvgBmzZtQkREBKZMmYLIyEjl1CgzZszA0KFDlesHBQVh9+7dWLduHR4+fIizZ89i4sSJaNKkCSpWrGjQ2gMDA/MtY6ikGZlMhh9++KHA/7Pw8HCsW7cOJ0+eNHBlxSPVYFwxYrx8+fKoUqWKsMWQwb09nUrHjh1RunRpYYohEjGG4+oZ0/eij+s9mXKfrCSfXYzX3jKmrIDEzULoAoikzsGvIVLCwoUug4rJ2yUBEc/LCF0GkUENGDAA8fHxmD9/PqKjo+Hr64uDBw/C0zN3zsTo6GjlHMgAMHz4cCQnJ2PNmjWYNm0aSpcujXbt2mHJkiUGr71bt24wMzNDTk4OAMDDwwN169Y1eB1S5ezsjC1btqB9+/b5Rvu3a9cOR44cgYWFNJqHrq6ucHBwQEpKimSC8ezsbFy6dAlA7jQqBc3rLyaLFi3C/v37kZOTo9HN19cX33zzDSpVqiR06aLUqVMnzJ07V3m/b9++AlZjnJKSkpCQkKDcpxGRafGtbY2bdzKELoOIJMKkRoyL8SgYGQdjOnJPZIzWrl0LLy8v2NjYwM/PD3/99VeB6546dQoymSzf7c6dOyrr7dq1Cz4+PrC2toaPjw/27Nmj74+hU+PGjcPjx4+RkZGBsLAwtGrVSvnY5s2bcerUKZX1J0yYgFu3biEtLQ3Pnj3Dli1bBAm+ypUrpzLlS2BgoCTCRTFp27YtPv/883zLT5w4gR49ekhmKhWZTIbq1asDkM6I8du3byMlJQWAdKZRmTx5MmxsbHDhwgVcunQJYWFhCA8Px7Vr13Djxg3cunULERERePDgAfr3749ff/2VoXgh/P39lSPELSws8k1vRMV3+/ZtjB8/HpUrV0ZGBkMxY8A+lip+H0QkZevWrUO9evWU02w2b94chw4dUj4ul8sRHByMihUrwtbWFm3atMGtW7f0XpdJBeOkOX2egsQDFMaDjTNVpnzqnpjt2LEDkydPxsyZMxEeHo6WLVuia9euKiOi1bl79y6io6OVt7zTNoSGhmLAgAEYMmQIrl27hiFDhqB///4qF9Uj/ck7dQqnUSmeefPmoUmTJgCAKlWqoHz58gCAQ4cOoVGjRrh48aKQ5WlM8Xf54MGDAue7F5O82wgpBONyuRy3b99Gw4aF7+99fHwQGhqKBQsWwNqa+8LCWFhYoH379gByz9IoW7aswBVJW1ZWFnbt2oV27dqhTp06WLt2LerWrYuaNWsKXRoRCcjU+mXG+nk5nYpxqVy5MhYvXozLly/j8uXLaNeuHXr27KkMv5cuXYqVK1dizZo1uHTpElxdXdGxY0ckJyfrtS4G41oy1g0OlZyphcSm9nlJulauXIlRo0Zh9OjR8Pb2RkhICNzd3bFu3bpCn1ehQgW4uroqb4qLTQJASEgIOnbsiBkzZqB27dqYMWMG2rdvj5CQED1/GgL+m2fc3t4ebdq0EbYYDTx58kR0oa2lpSW2bdsGR0dHBAYGIjw8HC1atAAAREZGYtq0aaKrWR1FMJ6amoro6GiBqymaIhiXyWRo3LixwNUULS0tDe+88w5Wrlyp9nEzMzN8/vnnCAsLE83nycjIQGZmptBlFEoxz7hUplHJyMhAaGio0GWoeP78ORYsWAAvLy/07dtX5doIN27cQNu2bQWsjnSJfY5c+vweGD5KGzMqkoqgoCB069YNNWvWRM2aNfHVV1/BwcEB58+fh1wuR0hICGbOnInevXvD19cXP/30E9LS0rBt2za91sVgnPIx5g0rd/q6wQYqCS0pKUnlVtAp05mZmQgLC8t3sbNOnTrh3Llzhb5Hw4YN4ebmhvbt2+e7GGFoaGi+1+zcuXORr0m68ejRI9jY2MDW1haPHz8WupxCJScno0GDBmjWrBmOHTsmdDkqqlWrhrVr16Jhw4aoVKkSTp48ialTp6JcuXLYsmWLJKaoee+99/DHH3/g9u3bqFChgtDlFGn16tU4c+YMNmzYgFKlSgldTpHs7e3RoUMHAICDg4PKY7Vr18a5c+ewaNEi2NjYCFGeisjISIwcORIuLi7YvXu30OUUqmPHjpDJZNi4cSM+/vhjocsp0rx589CiRQtMnjwZaWlpgtUhl8tx7tw5DBo0CO7u7pg9ezb+/ffffOslJyfnm/6MpI19DyoOY841FEzhM5L4ado3zys7Oxvbt29HamoqmjdvjkePHiEmJkalj21tbY3WrVvrvY8tjasrEUmEKVyIU1cNUzEdpNDlBTh5sZf87BvUg4OdrU5eKyctHQDg7u6usnzu3LkIDg7Ot35cXByys7Ph4uKistzFxQUxMTFq38PNzQ0bNmyAn58fMjIy8Msvv6B9+/Y4deqUch7umJgYrV6TdOv58+d4/fo1Xr9+rfdT60pq/fr1ePXqFS5evIj79++jY8eOQpekYvDgwco5ry0tLbFixQp8/vnnyqlVxM7X1xe+vr5Cl6ExOzs7tGzZEi1bthS6FI3NmTMHs2fPxpMnT9C/f3+YmZlh2rRpmD9/vigCcQU7Ozv8/PPPyo7WwIEDhS6pQF5eXihdujQuXLgAMzNxj1O6cOEClixZArlcjj179mD+/PmC1JGeno7Fixdj06ZNasPwvGxsbDBq1CjI5XJJHOAjKgoPDBSfMffNdBWKc6pb0yJk3xzIPaurefPmeP36NRwcHLBnzx74+Pgow291fewnT57opN6CMBgvBm5cqTDGHI6zUUZiERUVBScnJ+X9oua0fbtjXFhnuVatWqhVq5byfvPmzREVFYXly5erXKBSm9ck0/T69WvlFBQVKlTAiBEjBK5IvbdHAkslFCfDUEyRcvjwYdSsWRObN29G8+bNBa4qP2dnZ3Ts2BGHDx/GoUOH8OrVK+VFLsXI1tYWCQkJKvsysUlPT8fw4cORk5MDANi4caNg9dra2mLevHmYN28e4uLicO3aNVy9elV5i4iIQHZ2NoDcbW/t2rW5TzYyxtzHMmW6HKBkSkwpt0mt1gj2/1wRugwqhDZ981q1auHq1at49eoVdu3ahWHDhuH06dPKx4XoY4t7iIIe8GgYUfEwFNecKTVUhKK4krXiVtDO19nZGebm5vlGcsfGxuY7Gl2YZs2a4f79+8r7rq6uJX5NMn4//fST8vdkypQpsLXVzegMIiE0b94cV69eFWUorqAYJZ6ZmYm9e/cKW0wRFGdqiHlKndmzZyunJBk7dqxyWh2hOTs7o3379pg2bRp++eUX3LhxAykpKbh06RK+//57jB8/Hjt37hT9GUVEmmAfrOSMrW9mbJ+HpE/TvjkAWFlZoXr16vD398eiRYtQv359rF69Gq6urgAgSB/b5IJxKhg3sLpjbA0YY/s8ZDqsrKzg5+eXb27nY8eOISAgQOPXCQ8Ph5ubm/J+8+bN873m0aNHtXpNMm5ZWVlYsmQJgNzG4tixYwWuiKhkOnbsKPqDO++++y6srKwAANu3bxe4moLl5OQoQ1sxBeOKkeEA8PfffyvPeKlatSqWLl0qVFkasbGxgb+/P0aPHo01a9Zg7969cHR0FLos0jH2ScjUMbMhYyOXy5GRkQEvLy+4urqq9LEzMzNx+vRpvfexOZUKERWKDVCSuqlTp2LIkCHw9/dH8+bNsWHDBkRGRmLMmDEAgBkzZuDp06f4+eefAQAhISGoUqUK6tSpg8zMTGzZsgW7du3Crl27lK85adIktGrVCkuWLEHPnj3xxx9/4M8//8Tff/8tyGck8fntt9/w6NEjAMDHH38sqvCLyFiVKlUK3bp1w969e/Hnn3/ixYsXopwaKDk5GXK5HIC4gvHp06dj2bJlSEtLw4gRI5Q1/vjjj/mmXCIikhJjmA7XlENxTqdiHL744gt07doV7u7uSE5Oxvbt23Hq1CkcPnwYMpkMkydPxsKFC1GjRg3UqFEDCxcuhJ2dHQYNGqTXuhiMF5MxbFjzMtRGVgxT2Rhqo2oM8+CZUiiu6/ntjG0bIWUDBgxAfHw85s+fj+joaPj6+uLgwYPw9PQEAERHRyMyMlK5fmZmJj755BM8ffoUtra2qFOnDg4cOIBu3bop1wkICMD27dsxa9YszJ49G9WqVcOOHTvQtGlTg38+Ep+cnBwsWrQIQO4oxkmTJglcEZHpGDhwIPbu3Yvs7Gzs2rVLeRBUTJKSkpQ/iyUYj4qKwooVKxAQEIBTp07hwYMHAP47EEwkFsbQx9KUKfXFqHCmHIqT8Xj+/DmGDBmC6OholCpVCvXq1cPhw4fRsWNHALkH6NPT0zFu3DgkJCSgadOmOHr0qN7PAGMwTkRq6bMhllqtkd5em0idcePGYdy4cWof27x5s8r96dOnY/r06UW+Zt++fdG3b19dlEdG5sCBA7h58yYAYPTo0ahQoYLAFRGZjsDAQNjZ2SEtLQ3bt28XZTCemJio/FksF9/ct28fgNy5xGNjYwFAOVqLSGxMKRzXN1Pql0l14JI+Q3ExDFwk07Fx48ZCH5fJZAgODkZwcLBhCvp/JjnHOP/4VfHoo/5I9Si/VOsmIhKaXC5Xjha3sLDAJ598InBFRKbF3t4ePXr0AACcOXMGT58+Fbii/PIG42IZMa64WKkiFAeApUuXin5eeSJjZUr9MUPmM1LLPqRWrz6Z0kEcMiyTDMZ1hRsp0oTUGjVSq1fMuI0gMg0JCQn4999/AeQGcaGhoQCA999/XzllDxEZzsCBAwHkHqj6/fffBa4mP7EF469evcKpU6fyLe/VqxccHR3x66+/Gr4ooiKwz0LGjn1JIsNgME4Gw5H64scGJhGR9m7evIl+/fohMzNTOe2ATCbDZ599JnBlRKapS5cuysB5+/btAleTn9iC8YMHDyIrKyvfcltbW2zatAnvvfeeAFURmS72yfRLCoGzFGokMhYMxk0cN7iGIYXGjRRq1DcevCGi4rh79y7Onz+Pnj174ujRowByR1p6e3sLXBmRabK2tkavXr0AABcuXMCjR48ErkiV2C6+qZhGJS93d3ecPXsW/fv3N3xBRBpi/6VkODWFODGjITIsBuNkktgIUMVGpf6wYUNk/O7evQsAOHz4sHJZ27Ztcfr0aeW0KkRkWIrpVABgx44dAlaSn5hGjGdkZODQoUMqy1q0aIFLly6hYUO2D0n8jK0fY2yfR6zE2kcTa11iwRyH9IHBeAlJecNlyNo5Ele8jRxD18WdGREZG0UwnteECRPQoUMHvHr1yvAFERHatWsHZ2dnAKrTqYjhbzJvMO7k5CRgJcCJEyeQkpKivD9q1CicOHECLi4uAlZFRKR/Us5ydIEZDVEukw3GdbkR8K1tbfIbVdKM2MJxsdVDRCRF6oJxANi4cSO6du1q4GqICAAsLS3Rt29fAMC1a9cQERGBEydO4NNPPxW4MnGNGFdMo2Jubo6vv/4a33//PaysrAStiUhbxtKnMZbPURwMaRnUEwnFZINxfZBSQC6VOkl/TLnhZWj8eyMyXm/evMHDhw/zLV+yZAmGDh0qQEVEpJB3OpXPPvsMgYGBiI+PF7CiXIpg3MbGRtAQOicnB/v27UOZMmVw5MgRTJgwATKZTLB6iMhweBZvLrH008RShxTwd5d0jcG4HkgpICfDE0MgLYYaxIqjFYhIGw8fPkRWVpbKskmTJoliVCqRqdqxYwcGDRqEw4cPw8LCAgCwf/9+pKenIy0tTeDq/rv4ptCjxS9evIiyZcvi0qVLaN++vaC1EJWUg19DSfdxpFw7lQyzIyJhMRjXI7EG5IauiUGjuLDRRUSkO29PozJw4ECsXLmSoy6JBNS7d288ePAAixcvznfgSgzBuGLEuNDBeGZmJkJDQ1GtWjVB6yDSJakH5GR4QmY2YsyLiEwNg3EDEGtATsIRqrHGRqJwuA0gMk55g/H27dtj8+bNMDNj84pISJaWlti2bRvs7e3zPZaamipARarEEoy3atVK8It/EukLA3ISO/YPicSBPTcDEkNALvT7i4mpzU0lhoahqX3nRGT8FMF4w4YNsXv3blhbcz9LJAbVq1fHmjVr8i0X04hxhtJE+ieFgNzQ9bFPlp+hcxKhcxmpn9XP32HSJZMOxoXaGIghICfhGbIBJPbGIBGRVN29exdVq1bFoUOHGHLpmRgumkjSMmzYMAwYMEBlmZiCcaFHjBOZEikE5CQsQ2U0zIKIxMWkg3GhGTogF2IDLPUjkfpmiMYZG4Da09fvLRtBRMbn1atXOHLkCFxcXIQuxehNmzYNb968EboMkhCZTIb169fDw8NDuUzoYFwul4vm4ptEpkhsAbmYahGaKWQH7A8SiQ+DcRHgCHLSFza0iIj05/Xr19i8eTOqV68udCkmISwsDMuWLRO6DIO4cOGC0CUYjdKlS2PLli3Kuf+FDsbT09OVFwRlME4kHLEF5IbCKSgKp89chpmPbvF3mXSFwbiIMCA3TfpqkJliQ4+IyJBsbGzg5+cndBkmZd68ebh9+7bQZejV8ePHsXz5cqHLMCotW7bErFmzAOQG43K53OA1ZGRkAPhvGhWAwTiRGAgZkLO/ZjqY8xCJF4NxEdJHQM5pVEwLG1nipfj7ZuOISHuhoaFqA62srCxcvHhRgIrIkORyOTIzMzFy5EhkZ2cLXY5e3L59G3369EGlSpWELsXozJ49G82bNweQe7aHoX311VeYN28eYmJilMtKlSqFmJgYhISECBLWE9F/THUEOeVnDFkMEWnO5INxMYe3DM/0Tyyn3+iyESbWBp1YvmsxyRuSa3ojMmXLly9HixYtcPbsWeWy33//HT4+Pjh06JCAlZEhXbhwAatXrxa6DJ17/vw5unfvjsTERHh5eQldjtGxsLDA1q1b4eTkJMh0Ko0aNUJwcDDatGmjXPb999/Dw8MDt27dgkwmM3hNRJSfoQJysfbZKJeu+l1i7L+JOQMjEoKF0AVQ0RQb05t3Mkr0fBI3B7+GSAkLL/FrkG54uyQg4nkZocvIpzh/z8XddhCJTWBgIEaOHInQ0FDlMsWc00FBQUKVRQaSd0TtzJkzERQUhBo1aghYke6kpaWhR48eePz4MQAwGNcTLy8vrFu3DqmpqShXrpxB3/udd94BAOWFNwEgIiICADBkyBCD1kKkC2leDeD0/K7QZeiNol9V0v6ZmHCgkmExh9G/1GqNYP/PFaHLIIljMC4hJQ3IybgxFKeCaNIoS09lw43Er3v37pDJZPmmHKhYsSIaNuQ20JS8fv0ao0ePxsmTJ5UXVZSqnJwcDBkyRGU6IAbj+jNo0CDlxS8NydnZGd7e3sowXKFKlSrK0JyIxEcfATn7bQUT0+Ak39rWHJxIZAKk3ZMwUdpMqSDUBpmn5xRPcRtJbFwRkSmoUKECmjZtmm95YGAgpyEwAW8fEDlz5gzWrVsnUDW68/nnn2P37t0qyxiM65eFhTBjg9QF4IMHD5b8wR0yXaY0AplzkJOmGIoTSQtbYRLGOYcJYChORKZF3ZQpnEbFdH322WfK6Uek6LvvvlNOB6Tg7OwMBwcHgSoifWrZsmW+ZZxGhUhaShqQs+8mLdrmLcxnDM+UDtCRfjAYNwIFbXy5UZYmbRpLbFgRkakJDAxUuW9ra4v27dsLVA0Z0tsjxgEgNTUVH3zwgdrHxO7w4cMYP358vuUcLW683g7GmzRpgpo1awpUDZFumGooJbUR5Kb6/2RIzF+IpInBOIxj2g+OHi8+MTYSNGlksSGmf8awbSAyNnXr1oWHh4fyfocOHWBraytgRSSUChUqYNGiRShfvjxOnz4tdDlauX79Ovr374/s7Ox8jzEYN16enp6oXLmy8v7QoUMFrIaIdEFqATlpT5OcRSpZDPu3RPkxGDcyioBcyA0zN7b6x8YXEZkqmUymMnXK2yPIyXjJ5XK0aNEC/fv3BwDExsZiyJAh2LZtG9q0aSNscVq6du0avvzyS0yfPj3fYwzGjZdMJlOOGrewsMCAAQMErohIN6Q6CEaXNAnI2YeTrsLyFamE4kSkHoNxIpEqqOHEBhURmToG46apS5cuOHr0KAYNGqRcduDAAQErKr4hQ4Zg0qRJSEpKUi7z9/cHwGDc2CmC8W7dusHZ2VngaohI18Q4glxqBy6kNNCOobg4SO13nMSFwTiRhIitkUVEJITWrVvD3t4efn5+qFixotDlkIGsWrUKdnZ26NChA6ytczui//vf/wSuqvji4uKwefNmAEDDhg1x+vRpNGrUiMG4kVME47zoJhkbBlOq3g7I2Y+TvrdDcIbiRMaBwTjplJSO7koBG1NERPnZ2NigU6dOKiPHyfjJZDIAgL29Pdq1awcA+PPPP5Geni5kWcW2fv16vH79GgAwdepU2NnZYe/evfD19RW4MtInHx8fVKtWjWe7EJkIMY4gp5JjKE5kPBiM/z8GuiRmbEwJh9sGInEKCgpiMK5GVlYW9uzZg//97384e/Ysbt26hWfPniEtLQ1yuVzo8nRGESqmp6fjxIkTAlejvYyMDKxZswYAULFiReW86e7u7jwLwsiZmZlh9erVsLGxEboUIp3jqHEydkJfz40Kxu0PFZeF0AWQ8ZBygJharRHs/7kidBlqST0U5w6KiPShV69eKFWqlNBliI6FhQVq1aqFvn37IiIiQuUxKysrlC5dGmXKlFH519XVFfPmzYOTk5NAVWuve/fuGD9+PIDc6VS6d+8ucEXa+fXXX/H8+XMAwMSJE2FlZSVwRWRIUvt9JSLpYl+MFKSc1xDpE0eM58ENRfHxuyNjxt9vIvEpXbq0cmoNUuXj44NLly7lm8M4MzMTsbGxuHv3Li5cuIDDhw9j//796N69u6RCcQDw9PREvXr1AOQG41IaDS+Xy7Fy5UoAgJ2dHT788EOBK9LO69evsWbNGuzfvx937txBZmam0CURkYgwiBUPKf9fsP9FxSHl33kSDoNxKjFj2WlxI6p7xvSdGsvvORGZBnt7e/z000/44YcfCp2yoVevXpKd01oxncq///6L69evC1yN5o4fP44bN24AAEaOHIkyZcoIXJF2bGxskJSUhB49esDb2xu2traoWrUqOnfujPHjxyMkJAQHDhzAvXv38ObNG6HLJROQkJCATZs2SfZ6A8YotVojo+oHSBG/f8qLfVmigjEYfws3GKaNDQgiIjIWMpkMo0aNwoULF1CjRg2162zZsgWVK1dGYGAgfv/9d+XFIKUg78UL9+/fL2Al2lGMFpfJZJg0aZLA1RTPhAkT4O3tDQDIycnBo0ePcPToUaxduxZTpkxBYGAgatWqBVtbW9SoUQPffPMNsrOzBa6ajMnr16+xa9cu9O7dG66urti3bx9sbW2FLovewr4VlYS3SwLzGdIatzukLQbjVCLGuKPihpQKYoy/70Rk/OrVq4ewsDAMHDhQZbm5uTkAIDs7GwcOHED//v3h5uaGsWPH4urVqwJUqp0mTZrA2dkZQO50KlJw+/ZtHDp0CADQs2dPVK9eXeCKinb06FGsXr0a48ePR4cOHeDh4QEnJ6d8c9i/zczMDP369cPOnTsxYcIE5e8bUXHl5OTg5MmTGD16NFxdXdG3b1/s2bMHmZmZmDhxIpKSkvDvv/8iIiICt27dErpc+n/sWxmesX3n7IOVjCl+f8b2N0D6xWBcDVPccBSHMX9P3JCWnLF+h8b8e09EupeVlYXJkyfj8ePHgtbh6OiIbdu2Ye3atcoLPa5btw47d+5EUFCQMrR89eoV1q9fj+PHjwtZrkbMzc2VFzF89uwZ0tLSBK6oaFeuXFFObTNt2jSBq9HM2LFjMXnyZKxduxbHjx9HVFRUoetbWVnho48+wr179/Drr7+ifv36Bqo0d/72X375BY8ePTLYexaXlM7O2L9/PwIDAwWZGkcul+PatWuYPn06PD090a5dO2zcuBGJiYkq67Vv3x6lSpWCu7s7fHx80KdPH4PXSgUz1n6BGBnrd80+GGnLWP8WSPcYjFOxmMKOiRtSIiJx2rJlC8qWLYuyZcviypUrQpdToOzsbAwZMgSrV69G69at8c8//whaj0wmw9ixYxEaGoqqVavixYsX6NOnD/bt24enT59i5cqVqF+/PszNzfH+++8LWqumpk+fjvDwcDx58gR2dnZCl1OkwYMHIzIyEuvWrUOLFi2ELkcjNWvWVP5csWJFtG3bFh999BFGjx6tsp6joyOmT5+Ox48fY/369ahWrZpB63z9+jUGDhyIoUOHYvDgwcjKyjLo+2urWbNmKF26dL4zOYT0dvCdnp6Ojz/+GD169MCBAwcQHBxssFpiYmKwePFi1K1bFw0aNMCyZcvw77//avz8pKQkPVZHxcG+lf4Z+3dsChmErvE7IyqahdAFiJW3SwIinkvrYkiGYkob19RqjWD/j3hDF7EyhUYZtw9EwsnMzERCQu6+SKzzFmdnZ2PEiBHYvn07gNxpJSwtLQWuKlejRo1w5coV/P3338plLi4umDJlCqZMmYJHjx7B1dVVwAo15+PjI3QJWitfvjzGjBkjdBkaW7hwIRYuXIgaNWrAwcFBufyLL74AkPt5Jk+ejHHjxqF06dICVQlYW1srR2GfO3cOCxcuxJw5cwSrpyiRkZFITExERkaG0KUAAJ4+fYoVK1Yo58C/ceMG3nvvPeWUJBYWFga9UGyFChXQvXt3ODg44MyZMzh9+jRiY2MLXD8wMBC1a9eGo6MjHB0dUa5cOYPVSppj30p/jL3/pcB+GGmD2xzSBINx0oopheIK3JgSEVFRMjMzlVOU5OTk4MMPP8Qvv/wCAHB3d8eJEyfg4eEhZIkqSpUqpZyG5G1eXl4GrobErGHDhvmWyeVyhIWFYc2aNRg5cqQoLnook8nwww8/oF69eoiJicH8+fPRqVMnNGvWTOjS8klOTlYe3PP09BS4mtyR4gMGDEBaWhrkcjm+/fZbfPLJJ8rQvkaNGti2bRv8/f0NVpOZmRnq1q2LunXr4uOPP4ZcLsfdu3dx+vRpZVD+9OlT5fp2dnZYtmyZweqj4mPfSvdMJRRXYDhO2uA2h4rCqVQKYYohcGFM+ftIrdbI5BocxWUq35Mp/z0QkaqMjAxMmDABQG5gOG7cOGzatAlA7tQTJ06cYNhMRkUul+N///sfxo8fL4pQXKF8+fL48ccfAeSetfH+++8jOTlZ4Kryi4yMVP4shgNmn3/+Oc6ePYs7d+4gKCgIEyZMUIbiI0eOxJUrVwwaiqsjk8lQu3ZtfPTRR9i6dSuioqLw4MEDbNq0CcOGDcOlS5dw9uxZQWskzZlKf8EQTPW7ZF+saPyOiDTDYJxIC6ba8CD12NggIgDYsGEDNm7ciNjYWEycOBHfffcdAMDV1RUnTpxA9erVBa6QSLfENDXQ27p06YKJEycCAB4+fIhJkyYpH8vJyRHF9EtPnjxR/iz0iPFdu3Ypp09JT0/HgQMHAOSeVbJjxw5s3LhRZQodsZDJZKhWrRpGjBiBzZs34+HDh2jUiO10KeHAo5Iz9e/P2yWB/THSiKn/rVDhGIwXgRvaXPwe/sONKhERKaSmpmLBggXIzs5Gx44dsWbNGgC5I1ePHz+OWrVqCVwhkelZsmQJfH19AQA//vgjdu7cCQDYuXMnzpw5I2RpAMQTjN+7dw8jRozIt7xmzZq4du0a+vfvL0BVxSemsxdIc+xbUUkxq8iP30l+3NZQQRiMa8DUNyqm/vnV4UZVPVP8Xvj3QWTavv76a+UF4a5fvw4AKFeuHI4fPy7JC0MSGQMbGxts27YN1tbWAIAPP/wQjx8/xqxZs7Bjxw6BqxPHVCppaWno27ev2qlm7t27h7FjxyI8PFyAysgUmWIfoqT4nalin4w0wb8bUofBOBWKO5iCcaNKCvw7Eb+1a9fCy8sLNjY28PPzw19//VXgurt370bHjh1Rvnx5ODk5oXnz5jhy5IjKOps3b4ZMJst3e/36tb4/ColIQkICli5dmm95w4YNERMTgzdv3ghQFREBQN26dbF48WIAuX+rTZs2xf3797Fr1y5kZWUJWptixLiNjQ3Kly9v8PeXy+UYO3Ysbty4ofZxW1tbODk5ITIyEnK53MDVkali30pz/K7UY58sF78HErMzZ84gKCgIFStWhEwmw969e1UeHz58eL4+tr4vpM5gXEOmuHExxc+sLTZK/sPvgsRqx44dmDx5MmbOnInw8HC0bNkSXbt2VRmxl9eZM2fQsWNHHDx4EGFhYWjbti2CgoLyjZxzcnJCdHS0ys3GxsYQH4lEYtmyZXj16lW+5X/++Sd69OiBr7/+mqESkYAmTJiAgIAAAFCe2REXF4dTp04JWNV/I8Y9PDwgk8kM/v7ff/89fv75Z5VllpaW6NGjB7Zt24bY2Fhs374dPXv2FKQ+Ml3sTxSN31HhmGFQUYT6G1JcV6GoW5pXA0HqM5TU1FTUr19fOf2kOl26dFHpYx88eFCvNVno9dVJsrhD0VxqtUaw/+eK0GWQwLxdEhDxvIzQZZAaK1euxKhRozB69GgAQEhICI4cOYJ169Zh0aJF+dYPCQlRub9w4UL88ccf2L9/Pxo2bKhcLpPJ4OrqqtfaSbxiYmKwevXqfMtlMhmGDh2KL7/8Eu7u7gJURkTJyckYMmQI/vzzT6SmpuZ7fMeOHejQoYMAleVSjBgXYn7xsLAwTJgwAQBgbm6O9u3bY+DAgXj33XdRpgzbMSQ89q0KxlBcM+yXUVF0tZ3h36T2unbtiq5duxa6jrW1tUH72QzGtWAqG1iG4toz9QYcdwi5TGUbIQZJSUkq962trZVzyeaVmZmJsLAwfP755yrLO3XqhHPnzmn0Xjk5OUhOTkbZsmVVlqekpMDT0xPZ2dlo0KABvvzyS5XgnIzbwoULkZaWprKsffv2WL58ORo0aCBMUUQEAHB0dMT333+PgQMH4sSJE/ke3717N9auXQtLS0uD1ZSYmIhSpUrhzZs3ePbsGQDDB+MvX75E//790bRpUwwcOBB9+/ZFhQoVDFoDkSZMvW+lDvtb2jHVfhmznJLh31nxaNo319SpU6dQoUIFlC5dGq1bt8ZXX32l1/YKg3FSwQ1p8Sk2omzEEalK82oAcwd73bxWSu7Iv7dH4s6dOxfBwcH51o+Li0N2djZcXFxUlru4uCAmJkaj91yxYgVSU1PRv39/5bLatWtj8+bNqFu3LpKSkrB69Wq0aNEC165dQ40aNbT8VKSpffv2oVu3bvmWp6Sk4OzZs+jcubNB6nj8+DHWr1+vvF+nTh0sW7YMXbp04bQDRCJRvnx5HDlyBLNnz1bONa7w8uVLHD9+HF26dDFYPSEhIUhNTcWIESOQk5MD4L8Lb965cwc1a9aEmZl+Z7mMjIzEqVOneDYLSQLD8f8wrCseRbZhigE5Fc1U/66E7JtromvXrujXrx88PT3x6NEjzJ49G+3atUNYWFiJwvbCcI5xLTE4pqKY2gbW1D5vUbiNMIyoqCgkJiYqbzNmzCh0/bfDSrlcrlGA+euvvyI4OBg7duxQOUrdrFkzDB48GPXr10fLli3x22+/oWbNmvjmm2+K94FIIzt27EDt2rVx4MAB5bJ169ahSpUquHr1qsHqmDdvHt68eQNXV1d8//33uHr1Krp27cpQnEhkLCwssGjRIuzduxdOTk4qj/32228GrcXHxwfLli3DO++8o1x24cIF+Pn5Ydq0aXoPxQGgQYMGDMVJUhRz7hKVhKn0z0zlc5L4aNs3L8yAAQPQvXt3+Pr6IigoCIcOHcK9e/dU+n+6xmCclLgh1R024Ewb/5b0z8nJSeVW0NFjZ2dnmJub5xsdHhsbm28U+dt27NiBUaNG4bfffityLlozMzM0btwY9+/f1+6DkFYCAwPxzz//YPfu3cplP/74I+Lj4xEYGGiQGiIiIrB7924EBwfj/v37GD16NCwseAIekZj17NkTly9fhq+vr3LZnj17kJmZabAa6tatCyB3tLrCgQMHcOXKFUyaNMlgdRBJkSn3rUz5s+sS+2dE+qNp37w43Nzc4Onpqdd+NoPxYjDGjaoxfiahmUIjxhQ+I0mblZUV/Pz8cOzYMZXlx44dQ0BAQIHP+/XXXzF8+HBs27YN3bt3L/J95HI5rl69Cjc3txLXTAXr0qULzM3N8y338vKCj4+PQWp4+PAh7ty5g7lz58LBwcEg70lEJVejRg2cP38egwYNAgC8evUq375Bn6pXr662o+jt7Y2OHTsarA4iqTLFfocpfmZ9MtbMw9slwWg/G1F8fDyioqL02s9mME7ciOqRMTZmFKc0GuNn0yX+XYnH1KlT8cMPP2DTpk2IiIjAlClTEBkZiTFjxgAAZsyYgaFDhyrX//XXXzF06FCsWLECzZo1Q0xMDGJiYpCYmKhcZ968eThy5AgePnyIq1evYtSoUbh69aryNUk/ypQpg5YtW+ZbHhQUZLBpTLp3784DIEQSZW9vjy1btuCbb76BhYUFduzYYbD3trCwgLe3d77lkyZN4jRMRBoypf6HKX1WQzKWPpoiDDeWz0OmIyUlBVevXlVOg/no0SNcvXoVkZGRSElJwSeffILQ0FA8fvwYp06dQlBQEJydndGrVy+91cRgvJiMZQNkLJ9DzIylUcMwXHv8+xKHAQMGICQkBPPnz0eDBg1w5swZHDx4EJ6engCA6OhoREZGKtf/7rvvkJWVhfHjx8PNzU15y3uq+6tXr/Dhhx/C29sbnTp1wtOnT3HmzBk0adLE4J/P1KibMiUoKEiASohIimQyGT7++GOcPn0aly9fxuvXrw323nmncgFyD/YNGTLEYO9PZAxMoT9iCp9RSFLuozEMJ6m7fPkyGjZsiIYNGwLIHcTWsGFDzJkzB+bm5rhx4wZ69uyJmjVrYtiwYahZsyZCQ0Ph6Oiot5qKFYyvXbsWXl5esLGxgZ+fH/76669C18/IyMDMmTPh6ekJa2trVKtWDZs2bSpWwaQ73KAajlQbNxwdTsZi3LhxePz4MTIyMhAWFoZWrVopH9u8eTNOnTqlvH/q1CnI5fJ8t82bNyvXWbVqFZ48eYKMjAzExsbiyJEjaN68uQE/kel6OwR3dHRU+f8kItJEQEAATp48iRcvXhjsPRXzjCt8+OGHsLOzM9j7k3Eyxb65MfdNjPmziYnUshAG4mQs2rRpU2Bf29bWFkeOHEFsbCwyMzPx5MkTbN68We8XDtf6alE7duzA5MmTsXbtWrRo0QLfffcdunbtitu3b8PDw0Ptc/r374/nz59j48aNqF69OmJjY5GVlVXi4oXm7ZKAiOdlhC6jWLhRNTxFI8f+nysCV1I0Nsh0R8rbCSIxqlmzJmrWrIl79+4BADp37gwrKyuBqyIiKSrqIsy6ljcYNzc3x/jx4w36/mR8TLlvnlqtkST6VdpgH8ywFJmIWPtqzGyIDEPrYHzlypUYNWoURo8eDQAICQnBkSNHsG7dOixatCjf+ocPH8bp06fx8OFDlC1bFgBQpUqVklVNJGFibsSxMUZEUhAUFIQVK1YofyYikoK8U6n06dNH7yOgyPiZet9czP0qbbEfJhyxDWRiIE5kWFpNpZKZmYmwsDB06tRJZXmnTp1w7tw5tc/Zt28f/P39sXTpUlSqVAk1a9bEJ598gvT09OJXLSJSvOiBlGo1VmJq+HC6FP3j3xyRbinmGTczM0O3bt0EroaISDOVK1dGqVKlAEDluhVExcG+eS72Y0gXhO6vSTFXIjIWWgXjcXFxyM7OznfaoYuLC2JiYtQ+5+HDh/j7779x8+ZN7NmzByEhIdi5c2ehpw5mZGQgKSlJ5SYFeTdmYt2oibEmUyV0A46NSMPi3x6JjZTnJG3RogVKly6N5s2bw9nZWZAaiIi0JZPJULduXfj7+/O6FFRi7JurknK/Rsq1GxMh+mtizY2ITInWU6kAuY26vORyeb5lCjk5OZDJZNi6datyhMTKlSvRt29ffPvtt7C1tc33nEWLFmHevHnFKU101G3khDpNhxtc8TH06X9sdBERIP05SS0tLdG1a1fUr19fkPcnIiouX19fvPPOOwX2nYi0xb75f9T1dcQ+1Qr7Z+JiiGlVmMsQiYtWI8adnZ1hbm6e7wh0bGxsgRevcXNzQ6VKlZQ7XgDw9vaGXC7Hv//+q/Y5M2bMQGJiovIWFRWlTZmiJ8TIcm58xcsQjSGODhcH/h2SWOSdk9Tb2xshISFwd3fHunXr1K6vmJP04MGD6NChA6pUqYImTZogICDAwJX/JygoSDmlChGRVHTu3Bn9+vUTugwyAuybaybvtJFi6w+JrR7Kpa8+G0eHE4mTVsG4lZUV/Pz8cOzYMZXlx44dK7Bz3KJFCzx79gwpKSnKZffu3YOZmRkqV66s9jnW1tZwcnJSuRk7fYbl3PiKn74aRWJsAJo6sU+3RMbPWOYk7dmzJ3x8fAR7fyKi4ujZsyesrKyELoOMAPvmxfN2UC5UX4l9NHHTVV+N/T4i8dN6KpWpU6diyJAhyrnxNmzYgMjISIwZMwZA7hHlp0+f4ueffwYADBo0CF9++SVGjBiBefPmIS4uDp9++ilGjhyp9lQt+s/bG8/inNLDDbB06GpaFTaypEXIv1ExXX2dDKckc5La2Nhgz549iIuLw7hx4/Dy5csC5xnPyMhARkaG8r6u5yS1s7PT6esRERkCp1AhXWLfXDcMPQUL+2vSUdypVZjDEEmH1sH4gAEDEB8fj/nz5yM6Ohq+vr44ePAgPD09AQDR0dGIjIxUru/g4IBjx45hwoQJ8Pf3R7ly5dC/f38sWLBAd5/CRGg7Xzk3xtKjaCQVpyHGBhZpK+82IiVFnBdSIv3hnKRERETSxr65/ugrLGefTXq0CceZwRBJT7Euvjlu3DiMGzdO7WObN2/Ot6x27dr5TvEi3RDTxT1JdzQdPc6GFRFpSx9zktaoUSPfc2bMmIGpU6cq7yclJcHd3V1Hn4KIiIgA9s0NSYoX9yTdKCwcZxhOJG1azTFO0sA5rIxDYaE35w4nouLinKREREREuqHNnOXsv0nb2xkLcxci41CsEeNEZBhvjxxnY4qIdIFzkhIRERHpB/tsxotBOJHxYTBOJHJsWBGRrnFOUiIiIiIiIjJ1DMaJiIhMEOckJSIiIiIiIlPGOcaJiIiIiIiIiIiIyKQwGCciIiIiIiIiIiIik8JgnIiIiIiIiIiIiIhMCoNxIiIiIhK1PXv24NSpU0KXYVCPHj1CTk6O0GUQERERERktBuNEREREJGpt27ZFr1690KNHD0RERAhdjl7I5XJcu3YNc+fORd26dbF7926YmbGpTkRERESkL2xtExEREZGolS5dGp999hn279+PunXrYty4cYiNjRW6rBLLycnBuXPn8Mknn6BatWpo0KAB5s+fj/r162Pq1KlCl0dEREREZNQYjBMRERGZgMzMTNy4cQNbt27FZ599hp07dwpdklYmTJgAFxcXZGdnY926dahevToWLlyItLQ0oUvTyps3b3Ds2DGMHTsWlSpVQosWLbBixQo8evQIANCoUSN8//33kMlkAldatGfPniEmJkboMoiIiIiIisVC6AKIiIiISHfkcjmeP3+O69evq9xu376NN2/eAAA8PDwwa9YsgSvVjr29PWbNmoUJEyYAAJKTkzFz5kysXbsWX331FYYMGSLqqUciIiKwePFi7Nu3D69evVK7Tvny5bFnzx7Y2toatrhisrKyQvXq1VGxYkW0adMGbdq0QevWreHi4iJ0aWTkoqOj8ccffyA8PBzr1q0T9d8+ERERiRdbEEREREQ68urVK0HmwH769CmmT5+ODh06wMXFBW5ubujcuTM+/fRT/PLLL7h27ZoyFAeAfv364dGjR0hISIBcLjd4vcX1wQcfwNPTU2XZ06dPMXz4cAwePBgZGRkCVVa02rVro1u3bihdurTaxy0sLLBz5054eHgYtrAScHZ2xsyZMxEREYF169ZhwIABcHV1hY+PD8aNG4fff/9d0Clvbt++jSdPngj2/lKTlpaGrVu3Cl1GgR48eIBly5YhICAAlSpVwtixY9GgQQOG4kRERFRsHDFOREREpAOHDx/G6NGj8fPPP8Pb21u5/OHDh3BycoKzs7Pe3rtSpUoYP3481q5di/Dw8CLXX7FiBVasWAEgdyS2u7u7yu29995DrVq19FZvcVlbW2Pu3LkYOXKkyvJdu3ahd+/eAlWlGZlMhnfffRf79+/H48eP8z2+evVqtGrVyvCFFWHbtm2IiopCSkoKUlNTkZKSovLzy5cv8z0nIiJCGZYDgI+Pj3JEeZs2bVC+fHm91fvkyRN4eHhAJpMhPj4ebdu2xd69e9G8eXO9vae25HK5qKbKycnJwZYtWzBz5kx06tQJ77//vsrjCQkJuHz5Mjp06GDQuhUXpN2zZw/27NmDGzduqDxuZWWFcuXKYffu3cjKylK52dnZYeDAgQarlciUVcm8g8dWtYUugwRQJfOO0CUQlRiDcZKcx1a1uQE2cWx4EZmu7OxsmJub49mzZ7C0tET58uWVy4SSnJyMadOm4fvvvwcAeHt748qVK9i7dy/27t2LGzduYPny5Zg2bZpe6/D09MSSJUswd+5cbNu2Dd988w2uX79e5PNSU1Nx584d3Lnz3761WbNmogzGAWDIkCFYsmQJ7t69C5lMBnNzc0H//7VhZWWlNkgePXo0xo4dK0BFRVu5ciXCwsJK9Bp3796Fh4cHmjVrBmtrax1Vlt/r16/h7+8PV1dXTJw4Eb6+voiNjUXbtm2xcePGfIGvELKzszF16lR8/PHHMDc3R9WqVQWt5+TJk5g2bZrygFqrVq1w/fp1nD9/HqGhoTh//rxy2/D48eN8Z2zoWnZ2NkJDQ5VhuGLufXUyMzMxYMAAtY95eXkxGCcyIIbjpoeZDBkLBuMkSQzHTRcbXESmbePGjbh37x7Wr18PDw8P+Pn5YfTo0WjdurUg9Zw4cQIjR45Uma6hadOmiIqKUllv7969eg/GFezs7DB69GiMGjUKZ86cwTfffIM9e/YgJycHQO6UHT/99BNevHiBqKgoREVFITIyElFRUYiOjkZOTg7c3d0NUmtxWFhY4Msvv0T//v1x6NAhZGVloXv37kKXpRGZTIaNGzdixIgRyMrKwvHjx9G8eXOsWbNGVCOI83JwcFC7zN7eXvlvQQdgWrRogffeew/9+vVDhQoV9F0qtm/fjri4OMTFxeHDDz9UztWekZGBwYMH486dO5g3b56gU2+8fPkSX3/9NX7//XdER0fj/PnzqFq1KpydnQ36OxAREYHp06fjf//7n8ryDz74QGXapbxCQ0P1Hoxfu3YNO3fuxOHDhwsNxYuSlZWlw6qISBMMx00HsxgyJgzGSbIYjpseNrSIqF27dvjoo48A5AY7T58+xcaNGw1eR2pqKj777DN8++23+R7LG4pXrFgRPXv2RK9evQxZHoDcELZ169Zo3bo1IiMjsW7dOmzYsAEvX76EpaUlJk2alO85WVlZePbsGdzc3Axerzb69OmDjz76CJ07dxa6FK25ubnh8OHDWLBgASIiIrBr1y69jqIuqR9//BFyuVwZhNva2qoEy8ePH0eHDh2U9+vWrYtBgwZh4MCBqFKlikFrbdmyJSZOnIgff/wRycnJSE9PV3l8wYIFuHPnDn766SfY2dkZtDaFFy9eAMi9eCQABAQEYNKkSVi5cqVB3j82NhbBwcHYsGEDsrOz8z2eNxQ3MzND3bp10axZMzRr1swgU/00atQIjRo1AgA8evQIhw8fxuHDh3H8+HGkpqaqrGtubo4tW7agbNmysLCwULnZ2NjovVYiyo/huHFj/kLGiME4SRrDcdPAxhURKVSvXh21a9dWntrfpUsXWFlZGbSGv//+G8OHD8c///yj9nE7OztMmjQJ7777Lvz9/UVxYTgPDw8sWrQIc+bMwbZt23D9+nX069cv33oWFhaSuPijmZkZ1qxZI3QZJeLj44Pdu3eL/iCEl5dXoY+vWrUKVapUwaBBg/Dee+/B19fXQJXlV61aNaxevRpffvklpk2bhh9++CHfOjt37sTjx4/xxx9/oGLFigavURGMK7zzzjtYsmSJ3t83PT0dISEhWLRoEZKTkwtcz9bWFrNmzULz5s3h7+8PR0dHvddWEC8vL4wdOxZjx45FRkYGzp49i0OHDuHw4cO4efMmsrOzce/ePcyZM0ewGomITAVzFzJWDMZJ8hiOGzeG4kT0tqCgIGUwHhQUZLD3TU9Px6xZs7Bq1SrI5fIC10tLS0PZsmXRpEkTg9WmKVtbW4waNUroMnTCwkLazdhevXqJdvoUTWVlZWHWrFlo2rSpqD7LiRMnsG3btgIfv3z5Mho3boz9+/crRycbSt5gvGLFitixYwcsLS31+p5//vknxowZU+DBvLzS09NhYWGBtm3b6rUmbVlbW6Ndu3Zo164dli1bhqioKBw5cgTnzp1DWlqaYGcAEFF+HDVufJi3kDGTdo+C6P8xHDdObFARkTqBgYFYtmwZzMzM0LVrV4O856VLlzBy5Ejcu3cP1tbWygs+Km5mZmYq99etW4datWoZNLgnaRFTkFxcFhYWaNasmdBlqHj58iXOnTuHwMBAREZGIjIyEtHR0fkOZj179gzvvPMOtmzZgt69exusvri4OAC5393vv/8OV1dXvb9nhw4dcP/+fSQnJyM+Ph7x8fGIi4sr8OfffvsN7du3h5+fn95rKy53d3eMHj0ao0ePFroUIlKD4bhxYMZCpoDBOBkNhuPGg40oIipMQEAAypQpA19fX5QrV84g79m4cWPcuHHDIO9FRMVXtmxZLF26VGXZmzdv8PTpU+WFZhUXm42MjMT8+fPx/PlzjBkzxiAHKxQjxletWoWAgAC9v5+CTCaDk5MTnJycipweh4hIFxiOSxuzFTIVDMbJqDAclz42noioKBYWFujWrRvq168vdClEJAGWlpaoUqWKwS8Gqs6LFy8wePBgjB8/XuhSiIj0juG4NDFTIVMi/NWgiHSMO17p4v8dEWkqKCgIgYGBQpdBRKSVihUr4rvvvjOKqXSIiDTBkFU6qmTe4f8XmRyOGCejxJHj0sJAnIi01aNHD9jY2AhdBhGRVqZNm6b3i20SEYkNR46LH/MTMlUcMU5EgmIDiYiKw9bWliMuiUhyGIoTkali8Cpe/L8hU8ZgnIwWA1fx4/8RERERERGRaWAAKy6cOoWIwTgZOQav4vTYqjb/b4iIiIiIiEwMg1hx4P8DUS4G42T0GMCKC/8/iIiIiIiITBdDWWHx+yf6D4NxMgkMY8WB/w9ERERERETEcFYY/N6JVDEYJ5PBUFY4nDqFiIiIiIiI8mJIazicT5xIPQbjZFIYzhoev3MiIiIiIiJSh2Gt/vE7JioYg3EyOQxqDYffNRERERERERWGwa3+8LslKhyDcTJJDGz1i1OnkNisXbsWXl5esLGxgZ+fH/76669C1z99+jT8/PxgY2ODqlWrYv369fnW2bVrF3x8fGBtbQ0fHx/s2bNHX+UTERERERk1Bri6xalTSKy07ZvrG4NxMlkMbvWD3yuJzY4dOzB58mTMnDkT4eHhaNmyJbp27YrIyEi16z969AjdunVDy5YtER4eji+++AITJ07Erl27lOuEhoZiwIABGDJkCK5du4YhQ4agf//+uHDhgqE+FhERERGRUWGQqxv8HkmstO2bGwKDcTJpDHF1i98nidHKlSsxatQojB49Gt7e3ggJCYG7uzvWrVundv3169fDw8MDISEh8Pb2xujRozFy5EgsX75cuU5ISAg6duyIGTNmoHbt2pgxYwbat2+PkJAQA30qIiIiIiLjw1C3ZPj9kZhp2zc3BAbjZPIY5uoGv0cypKSkJJVbRkaG2vUyMzMRFhaGTp06qSzv1KkTzp07p/Y5oaGh+dbv3LkzLl++jDdv3hS6TkGvSUREREREmmG4qz1OnUJC0Wff3BAsBHtnI8CNTvGJLUR9bFWb/5/FJLb/S4B/m8WRnJmqt9d+YlUDDlZOOnmtFKskAIC7u7vK8rlz5yI4ODjf+nFxccjOzoaLi4vKchcXF8TExKh9j5iYGLXrZ2VlIS4uDm5ubgWuU9BrEhERERGR5tinI9I9qfXNDYHBeDFwA11yVTLviC5QFVs9RFSwqKgoODn9t0O3trYudH2ZTKZyXy6X51tW1PpvL9f2NYmIiIiMhS7DFRIOsw0yBGYtmlOEzVKi7765vjEY1wJ3GkRE4uDk5KSy8y2Is7MzzM3N8x2Bjo2NzXekWsHV1VXt+hYWFihXrlyh6xT0mkREREREYsMzp4mopPTZNzcEzjGuIe4sdI/fKekDf68oLysrK/j5+eHYsWMqy48dO4aAgAC1z2nevHm+9Y8ePQp/f39YWloWuk5Br0lERERERGRqOFqcFIrTNzcEjhgvAkM2/RLjlCpEZFymTp2KIUOGwN/fH82bN8eGDRsQGRmJMWPGAABmzJiBp0+f4ueffwYAjBkzBmvWrMHUqVPxwQcfIDQ0FBs3bsSvv/6qfM1JkyahVatWWLJkCXr27Ik//vgDf/75J/7++29BPiMRERERUXFw1DgRGUpRfXMhMBgvBHcORETSN2DAAMTHx2P+/PmIjo6Gr68vDh48CE9PTwBAdHQ0IiMjlet7eXnh4MGDmDJlCr799ltUrFgRX3/9Nfr06aNcJyAgANu3b8esWbMwe/ZsVKtWDTt27EDTpk0N/vmIiIiIiEqC4TgRGUJRfXMhMBhXgzsEw+KocdIV/u1SQcaNG4dx48apfWzz5s35lrVu3RpXrlwp9DX79u2Lvn376qI8IiIiIiIio8Kch9QprG8uBM4xnkeVzDsM1gTC752IiIiIiIhIGAwxicgUMRj/fwxmiYiIiIiIiMhUMRwnIlNj8sE4R4mLB/8fqCT4+0NEREREREQkPB5kIakw2WCcgbg48f+EiIiIiIiISBgMNInIlJhkMM7wVdz4/0NEREREREQkDIbjRGQqTCoY5yhxIuPEv2siIiIiIiLdYThORKbAZIJxBmfSwv8vIiIqzMOHD5GRkSF0GUREWnn48KHQJRAREekVD6qQlBh9MM5R4tLF/zciIirIrl27cOrUKaHLICLSyvLlyxEWFiZ0GUREGmHASUTGzmiDcQbiRERExmv//v343//+J3QZRCRROTk5iI6OxsWLF3H8+HHI5XKDvK+DgwP69OmD+Ph4g7wfEVFJMRwnImNmIXQB+sBA3HhUybzDHTEVin/vRKYnPj4eZ8+eRWRkJL7++mvIZDK9v2d4eDiuXr0Kc3NzmJubw8zMTPmzupuZmRnc3Nzg7e2t99qIKL+kpCRERkYiMjISUVFRKv9GRkbi33//xZs3b1CjRg3s37/fINsRAChfvjyePHmC999/HwcOHIC5ublB3rcocrkcKSkpiIuLQ3x8PMqXLw9PT0+hyyIiIolhfkNSY1TBOAMy48RwnIiI8jp8+DBycnLw5MkT3Lx5E3Xr1tX7e9atWxeHDh1CcHAw3rx5U+T6bm5uOH36tN7rIiJVL1++RNeuXXHx4sUi123bti127tyJsmXLGqCyXOXLlwcAHDlyBMHBwfjyyy/1/p4XL15ERESEMvSOj49X+7Ni2+bp6YlLly7pvS5diIuLg4ODA2xsbIQuhcioPbaqzbyFiIySUQTj3EATERGZjv3796v8bIhg3MLCAl988QUCAwMxbNgwXL16tcB17ezscPToUdSoUUPvdRXXw4cPUbVqVaHLMGkxMTGoUKECzMykO7NhVlYWPv/8c3Tt2hVt2rQRxejnsmXL4vDhw+jbty9OnDhR4HoffPABvv32W1haWhqwuv+CcQBYsGABmjRpgqCgIL2+Z61atbB3716sXLmyyIsW29raYu/evSp1ikl2djYuX76MQ4cO4fDhw8jKytLoIAgRlRzDcSIyRtJtif8/bphNA/+fSR3+XhCZnjdv3uDw4cPK+3lDckOoV68eLly4gDlz5hQYAqalpaF9+/b44IMPcPDgQbx+/dqgNRYkMzMT27ZtQ7NmzbBmzRqhyymx+/fvC11Ciezbtw/z5s0TuowSsbCwgIWFBTp06IDKlStj8uTJuHjxosHm61YnLS0Nu3btQmxsrNrHzczMsGrVKnz33XcGD8UBwNnZWeX+kCFD8ODBA72+Z6lSpbBw4ULcu3cPgwcPLnTd9PR09O7dG4MGDcLXX3+NixcvIjMzU6/1FeX58+f4+eef8d5776FChQpo1qwZ5s2bhwsXLmDevHmSPrhERGRMeKY/SZFMLmTLVUNJSUkoVaoUToU9goODEwAGYqaKG1rKi9sB3UlOSYVX805ITEyEk5OTTl5T3ba7pFJSktDGz0undZJhKH4fSvp/d/LkSbRv3x5yuRzm5ubIyclRjrw1tMuXL2PYsGG4fft2oes5ODjgvffew4YNGwxUmaqYmBh89913WL9+PWJiYgDkzpneoEEDALn/N1FRUSq3yZMnG3R6CW3Fx8ejefPmuH37NszNzQ02P7SuPH78GGvXrsWyZcuwe/du9OrVS+iSCrRu3TpERkbCwcEBDg4OsLe3V/n51atX6N27t8pzqlWrhvfeew+DBg0y6Dz7v/32G8aOHYuXL1+qfdzR0RG//vorunfvbrCa3vbw4UNUq1ZNeb9fv36oXbs25s2bZ7Df47CwMEybNk3j6Z6sra3h5+eHefPmoUOHDnquLvcA6Pnz53H48GEcOnQI4eHhatczNzdHYGAgLC0tlQdpFD87Oztj4cKFOqtJV/swY6CP9h1JC/tgVBDmNSWnr/4u++YFk9xUKtwIExERma43b97g+vXrCA4OhpeXF7p3746nT58KEoz7+/sjLCwMwcHBWLZsGXJycgAAs2fPxoEDB3DlyhUAQEpKCtLS0gxe38WLF/H111/jt99+U5kX3cLCAjNmzFCG4ElJSfmeGxgYiCZNmhiyXK0sXboU9+/fx7Bhw5CVlYUtW7YIMvq3OA4cOICePXsqp6oYOnQozp8/jzp16ghcmXpbt27F2bNntXrOP//8gwULFmDBggVo0KAB3nvvPQwcOBAeHh56qjKXl5eXMhR3cHBAly5dsHPnTgC582b/73//g6+vr15rKEr58uXh6OiI7t2748WLF1iwYAFq1qxp0Br8/Pxw8uRJ7N+/H9OnT8fdu3eVj3Xt2hVxcXEIDw9HVlYWACAjIwPnzp0zyMjsy5cvY8KECTh//nyR62ZnZ+OPP/5Q+5inp6dOg3Ei+g+nVCEiYyKpEeOPQo/C0cFe6HJIYDwKSQAPkukaR4yTvhn7aLvQ0FAMHz4c9+7dw+3bt+Ht7Y3IyEj88ccf2Lt3Lz7++GODjArOzMzE77//rpwCobh27tyJPn366LAy3YmOjka1atWQnp6uXBYSEoJJkyYJWJVmXr9+jWrVquHZs2cqy6tVq4ZLly6hTJkyAlVWsAYNGuDatWs6ea0ePXpg1apVep3f/v3330fTpk2Vf4+NGzdGQEAA9uzZI8gBtLfJ5XL8/fffeOedd0RxpsObN2+wYcMGBAcHIy4uDkuXLsWnn36K9PR0XLlyBefPn8f58+dx8eJF3Lx5E46OjgapKyIiAnv27MGePXtw+fJlteuYmZnB2dkZWVlZ+W5VqlTBP//8o7N6jH0fpg2OGCeAfTHKjzmNbnDEuOFJbsQ4UZXMO9zoEhGRqDRv3hzh4eGYOXMm7t69C29vb3h4eGDChAmYMGGC3t//+fPnWLduHb777jvldCmFqVChAqpUqQJ3d3e1N1dXV73XXFxfffWVSigO5I5Q/vvvv9G8eXNRXACyINbW1hg+fHi+kaz//PMPBg4ciIMHD4qu/gsXLiA1NRUpKSnKW977169fx+LFi9U+t3Tp0mjVqhXatm2LNm3aoG7dunr/fFu3blX+nJiYiMGDB+P777+HjY2NXt9XUzKZDC1bthS6DCVLS0uMHz8egwcPxuLFi5UjtW1tbdGiRQu0aNFCkLq8vb3h7e2NL774AlFRUdi7dy92796NM2fOKM/OkclkCA0NVXugRbEOEekHR40TkbFgME5ERESkA3Z2dli1apUgF6orV64cBgwYAG9vb1y/fl15i4yMVLv+iBEjCgwzxezRo0dq52rftm0batasiaZNm4ouWFa4c+cOPv74Yxw/flzt40ePHsUXX3yBJUuWGLiywllbW8Pa2rrAOed37dql/LlUqVIqQXi9evUE/f9o3Lgx2rVrJ4qR2WJXqlQpLFq0CHFxcUKXko+7u7vyIGNcXBz279+PPXv24OjRo1iyZAm+++67fM/hBTmJ9I/hOBEZAwbjRERERDpkZWVl8Pe0sLBQjrAcMGCAcvmrV69w48YNXLt2TRmW37hxA8uXL0e/fv3g5+dn8FpLYt68eSrzpVtbW2Py5MmYMWMGSpUqJWBlRatRowbmzJmDunXrYvfu3WoPWixdulQ5J7cU3LhxAxkZGVi+fDnatm2L+vXri+rAhJRP6xWKs7Oz0CUUytnZGSNGjMCIESOQkpKCEydOICcnh0E4ERERFQuDcSIiIiIjVbp0abRs2VJl6oacnBw8fPhQkAuClkRERAR++eUX5f33338fX331FTw9PQWsSnPm5uZo1aoVWrVqhZUrV+LKlSvYs2cPdu/ejYiICOV6o0aNQu3atdGwYUMBq9VM3bp1sX//fqHLIBPl4OCAHj16CF0GERERSRgPrRMRERGZEDMzM1SvXh316tUTuhStzJkzBzk5OWjVqhUuXbqELVu2SCYUf5tMJoOfnx8WLFiA27dv4/bt2/jqq6/g7++P9PR0vPvuu3jx4oXQZRIRERERGTUG40REREQkamFhYbh58yb++OMPnDp1Cv7+/kKXpFOKiwxeunQJjx8/xtSpU7F27VpeQJCIiIiISI84lQoRERERiZqlpSWuX78OS0tLoUvRO09PT0yaNEnoMoiIiIiIjB6DcSIiIiISNalN+0JEREREROLHqVSIiIiIiIiIiIiIyKQwGCciIiIiIiIiIiIik8JgnIiIiIiIiIiIiIhMCoNxIiIiE7R27Vp4eXnBxsYGfn5++OuvvzR63tmzZ2FhYYEGDRrot0AiIiIiIhGx/+cK7P+5InQZRKRDDMaJiIhMzI4dOzB58mTMnDkT4eHhaNmyJbp27YrIyMhCn5eYmIihQ4eiffv2BqqUiIiIiEh4eQNxBuRExsNC6AJId9RtmFOrNRKgEtKliOdllD97uyQIWAkRGYuVK1di1KhRGD16NAAgJCQER44cwbp167Bo0aICn/fRRx9h0KBBMDc3x969ew1ULRERERGRcAoKwfMuZ/ZCJE0MxiVK06OT9v9c4QZawvKG4nnvMyAnouLKzMxEWFgYPv/8c5XlnTp1wrlz5wp83o8//oh//vkHW7ZswYIFC/RdJhEREZHkvN1/U2D/Tbq0yV4ABuREUlOsYHzt2rVYtmwZoqOjUadOHYSEhKBly5ZFPu/s2bNo3bo1fH19cfXq1eK8tUkq6Sk6DMelqaBG1duPsZFFQMm3E9lp6TqqhMQuLi4O2dnZcHFxUVnu4uKCmJgYtc+5f/8+Pv/8c/z111+wsNCs6ZCRkYGMjAzl/aSkpOIXTURERGqxby6cwvpr/9fefcc3Vf3/A3+lu4yWUaFU9l76AcoqiAyhbDeCKAIiQ0S2CIIyHIBsZYoURCqCIHxll72HdDCk7EJBWhCELkrn/f3RX2JD0jZJk5w7Xs/HIw/JzU3yzrW595xXzj3X3LrstymLrf0rjiInUharg3H9vKSLFy9Gy5YtsWzZMnTu3BkXLlxAxYoV83xe7nlJ7969W6ii1YzzVBFgfSMLYECuFPyOk1zodDqj+5IkmSwDgKysLPTu3RtTp05FzZo1LX796dOnY+rUqYWuk4iIiMxj39x5rOmf5fca7LMpg736bBxFTiR/Vl98M/e8pHXq1MH8+fNRoUIFLFmyJN/n6eclDQoKsrlYtdFfsCH3zZHvRcpga6Mr+m5JuzTYyHH4PSQ58PPzg6urq8no8Hv37pmMIgeApKQknD59GsOGDYObmxvc3Nwwbdo0nDlzBm5ubti3b5/Z95kwYQISEhIMt1u3btn1c1y9ehVPnjyx62sSETnao0ePRJdAKsK+uWPo+1W5b/Z8bZI3R/TZnJH5EJFtrArG9fOSBgcHGy23dF7SyZMn21alCjgzBM+vBpI3e41EYEguP/z+kVx4eHggMDAQu3fvNlq+e/dutGjRwmR9Hx8fnDt3DlFRUYbbkCFDUKtWLURFRaFZs2Zm38fT0xM+Pj5GN3vasGED9u/fb9fXJCJytGXLltn9h0LSJvbN7cORIXh+70ny5Iw+GwNyInmxaioVzktqGTnv5DjfuHw5ooHEaVaIyJzRo0ejT58+aNy4MYKCgvDDDz8gNjYWQ4YMAZAz2vvvv//G6tWr4eLigvr16xs9v0yZMvDy8jJZ7kxbt25FbGwsOnfuLKwGIiJrRUZG4tGjR5g+fbroUkjh2De3npwCaU6rIj/OznE4FzmRPNh08U3OS2pMzkG4OQzH5cXZoxLYAHM+pe0jSP169uyJBw8eYNq0aYiLi0P9+vWxfft2VKpUCQAQFxeH2NhYwVXm7f79+zh+/DhiY2OxaNEis20QIiI5OnfuHHbv3o3PP/8cRYoUEV0OqQD75ubJKQQn+RPdX+Nc5ETiWDWVilrmJbUXJZ8Co9S61UZEg43TrDgXv2skV0OHDsWNGzeQlpaG8PBwvPjii4bHVq1ahQMHDuT53ClTpiAqKsrxReZh+/btyM7Oxq1bt3D27FlhdRARWSMtLQ2XLl3Cv//+izVr1oguhxSOfXNTSptSUil1qpncMh251UOkBVYF42qZl5RycIcrluiGkJIajUREuW3ZssXsv4mI5OzSpUvIysoCACxYsACSJAmuiJSMfXNjSu3XKLVuNZBzHsKLdRI5j9VTqahhXlJ74A6KCkNODSBOs+I43E8Q2V96ejp27dpluL9lyxZMmjRJYEVEpGR5TTvhCOfOnTP8+8KFC9izZw86dOjglPcmdWLfPIec+la24HzjzqekfhqnWSFyLKuDcaXPS0rGON+488m54caLddqPkhpbREpy6NAhJCUlGe6fOnUK8fHx8Pf3F1gVESlRSEgIWrdujWrVqjnl/c6fP290f8GCBQzGqVDYN5d338oaDMedR6n9NF6sk8gxrJpKRU/J85Lag1J3pHlR2+eRM6U03JQ2Px8RaYe5qVO2bdsmoBIiUqonT55g0KBBmDNnjtNCccB4xDiQs++6fPmy096f1EnLfXO19VXU9nnkSC3ZB6dZIbIfm4JxUh/uVB1PqQ0dBuTW4/dJuR4+fIg+ffrA19cXvr6+6NOnDx49epTn+hkZGfj000/x3HPPoWjRoggICMB7772HO3fuGK3Xpk0b6HQ6o1uvXr0c/GnUR5Iks8H41q1bnVZDRkaG096LiOzv5s2beOGFF7B8+XK89dZbTn3vp4NxAPj++++dWgORGqi5f6LWzyUHauyjcS5yUrOvv/4aLVq0QJEiRVCiRAmz6zzdx9bpdFi6dKlV78Ng3Epq3uGo+bOJpJaGm1o+h6Pxe6RsvXv3RlRUFHbu3ImdO3ciKioKffr0yXP9x48fIyIiAp9//jkiIiLw+++/4/Lly3j55ZdN1h04cCDi4uIMt2XLljnyo6hSdHQ0YmJiTJaHhYXhyZMnTqlh1apVmDt3LtLS0pzyfkRkP2FhYWjUqBHCw8MBwKnBeGJiotkpLVauXJnvD7BEZIz9EbKFFvpoWviMpC3p6eno0aMHPvzww3zXW7lypVE/u2/fvla9j9VzjJO6cc5x+1Jjw40X6yS1io6Oxs6dO3HixAk0a9YMALB8+XIEBQXh0qVLqFWrlslzfH19sXv3bqNl33//PZo2bYrY2FhUrFjRsLxIkSKcB7uQzI0WB3J+oNi/fz86d+7s8BreeecdVKtWDQsXLsSMGTPQo0cPp124j4hsk52djenTp+Pzzz+HJEkAgOeeew516tRxWg1Pzy+ul5KSgpCQEIwePdpptRAplRr7VuZwvnH7YVhMpFxTp04FkDMwKT8lSpQoVD+bI8atoJWdqlY+p6NppeFG/+F3x3kSExONbvYYvXv8+HH4+voaQnEAaN68OXx9fXHs2DGLXychIQE6nc7kdK/Q0FD4+fmhXr16GDt2rNEFJMkyx44dw1dffYV58+YZlm3evBnDhw/Hnj17nFJDkSJF8PnnnyMmJgY9e/ZEUFAQjhw54pT3JiLrPXr0CK+++iomTZpkCMUB544WB3KCcXd3d6NRTF9++SWWLl2KqKgoZGdnO7UeIqXRWt9Ka5/XEbTYN9PiZybxHNE3t8awYcPg5+eHJk2aYOnSpVa3qThinMziyPHC0UpDhqMZyBKX75WEd4qPXV4rNcUVAFChQgWj5ZMnT8aUKVMK9drx8fEoU6aMyfIyZcogPj7eotd48uQJxo8fj969e8PH57/P/M4776BKlSrw9/fH+fPnMWHCBJw5c8ZktDnl75dffkHRokUREhJiWBYQEIAFCxYgJSXFaXV88MEHmDVrFm7cuIGTJ0+iVatWeO211zBjxgzUrFnTaXUQUf7Onj2L119/HdeuXTN5zNnBuKenJyIiIuDv74+ffvoJAJCUlIRJkyZh8ODBTqlh//79OHToEHr16mX2LCgiudJK3+pp7GvZjgExkXlK6Ztb6ssvv8RLL70Eb29v7N27F2PGjMH9+/cxadIki1+DI8YtxB0rWUqrDTet4z7CuW7duoWEhATDbcKECXmuO2XKFLMX5ch9O336NACYnRJDkiSLpsrIyMhAr169kJ2djcWLFxs9NnDgQLRv3x7169dHr169sGHDBuzZswcREfy7sUbRokVteszePDw8DKf26W3atAn16tXDxx9/jH/++cdptRCReWvWrEHz5s3NhuINGjRw+o9Yffv2Rf369VG6dGl4e3sDgNk5xx2pTZs2uHLlCmrXro2GDRti5syZuHHjhlNrILKW1vtWWv/8ttB6v0zrn5+cz1F9c0tMmjQJQUFBaNCgAcaMGYNp06Zh1qxZVtXPEeOUJ44at54WGy4cycDGhwg+Pj5GI7LzM2zYMPTq1SvfdSpXroyzZ8/i7t27Jo/9888/KFu2bL7Pz8jIwFtvvYWYmBjs27evwNoaNWoEd3d3XLlyBY0acT+rRO+88w5mzpyJCxcuGJZlZmZi4cKFWL16NSZMmIARI0YYAjAicp6kpCQkJydj0KBB2LZtG65evWr0uLNHi+em0+lQqVIlXLx4ETdv3nT6ey9btgxRUVGG2/jx4xEUFIRevXqhR48eKFeunFNrIsqPFvtW5rC/ZTn2y8galuxj+N0rmCP65rZq3rw5EhMTcffu3QL78HoMxi2g5Z0rw3HLseFGJE9+fn7w8/MrcL2goCAkJCTg1KlTaNq0KQDg5MmTSEhIQIsWLfJ8nj4Uv3LlCvbv34/SpUsX+F5//fUXMjIyGEAomKurK7766iu8/vrrJo8lJiZi6dKl8Pb2xvDhw3lxTiInK168OIYMGYL4+Hj88ssvJo+LDMYBCAvGgZyzazZu3IgmTZoYrnVx/PhxHD9+HCNHjkSbNm3Qq1cvvPHGGxYdz4gchX0rYwzHC6bl3OZpzHEKZuk+xpp9Eb+jBbO0b26ryMhIeHl5mVzvKz+cSqUA3LlyG1iCDTft4vdDPerUqYNOnTph4MCBOHHiBE6cOIGBAweiW7duRnOx1q5dG5s2bQKQM0L4zTffxOnTpxEaGoqsrCzEx8cjPj4e6enpAIBr165h2rRpOH36NG7cuIHt27ejR48eaNiwIVq2bCnks5J9vPrqq2jcuLHJ8rfffhsxMTEYMWIEQ3EiQSRJQv/+/Q1TG7377rsAgMDAQFSrVk1kaahYsSIAIC4uznCscKZatWoZXatBT5Ik7N+/H4MHD4a/vz+mTp1qdMFSImeIvluSfas8cLvkjX0ykgP9/suSGxUsNjYWUVFRiI2NRVZWluFst+TkZADAli1bsHz5cpw/fx7Xrl3Djz/+iIkTJ2LQoEHw9PS0+H04Ypwswl8czeMOLYdWRzCwAaY+oaGhGD58OIKDgwEAL7/8MhYuXGi0zqVLl5CQkAAAuH37Nv744w8AOXPW5rZ//360adMGHh4e2Lt3LxYsWIDk5GRUqFABXbt2xeTJk+Hq6ur4D0UOo9Pp8M033xj+XvTWrl2LevXqYeLEiYIqI6JFixZh586dAIDWrVtj1apVSE9PR2BgoODKckaMAzlB9O3bt1G1alWn1/Dmm29i5MiRmD9/vsljlSpVwrx58/Dqq6/yxz1yKvatyFrsj5Et5LCvyasG/QUtCfjiiy8MFywHgIYNGwL4r5/t7u6OxYsXY/To0cjOzkbVqlUxbdo0fPTRR1a9D4PxfHAna4zhuDE57EyJyL5KlSqFNWvW5LtO7tFzlStXLnA0XYUKFXDw4EG71Efy0759e7Rp0wYHDhzAlClTMHv2bCQnJ2PSpElwd3fHuHHjRJdIpDl//fUXxo4dCwDw9fXF6tWrDdMfubu7C67uvxHjQM5oKBHBOAB8++23OHXqFI4dO2a0/M6dO4iJibH44tNE9sC+lWW0OiDJHOY1+WN+Yx73NcqxatUqrFq1Ks/HO3XqhE6dOhX6fTiVClmFB58c3Jma0to24XeBiID/Ro0/88wzmDhxIrZv344iRYoAAD799FOzozGJyHHS0tLQu3dvpKWlAQCWLVtmCKJr1KhRqAs62Yt+xDgAIfOM67m7u2P9+vV45plnjJZnZGRgzJgx6Ny5M+Lj4wVVZ7nHjx9j7969iIyMFF0K2Uhr/YjC4vZiX4xsw+8OmcNgPA/c0VJeuDMl7h+IKLegoCCEhITAzc0NrVq1wtatW+Hl5QUAGDVqFBYtWiS4QiL7ys7Oxr59+2Q5//Rnn32Gs2fPAgD69OmDnj17Cq7IlFyCcQB49tlnsXbtWri4uMDf3x+bNm1CqVKlAABhYWF4/vnnsW3bNqE1Pi0xMRE7d+7EhAkT0KJFC5QoUQJvvvmm0Uh8Ug72rWyj5e3GvpjluK2ICsZgnKym5Z2rlhsgluD2ISKt6tatm+Hfbdu2xR9//GG46MuwYcPwww8/iCotT5mZmaJLIIVycXHBzJkzERgYiPXr1yMrK0t0SQCA3bt3Y+7cuQCAKlWqmFwjQi4CAgLg4pLTDYuNjRVcDfDSSy9h2rRpqFOnDl599VWcPXsWbdu2BQD8888/6NatG4YPH44nT54Iqe/hw4f4448/MHbsWDRp0gQlS5ZE586dMWPGDBw/fhwZGRn44osvULp0aSH1ke3YdygcLW4/LWcRVDha/L6QZRiMm8GdbcG0uI24IyVAm3/7RGS9Dh06YNOmTfDw8AAADB48GIcOHRJc1X82b96Mb7/91miZJEk4ffo0Jk+eLMuRwCTGnDlz8NFHH2HBggXYsWMHrl27hszMTLz55puIjIxEz549Ubt2bSxfvtwwfYkIqamp6N+/P4Cc4P7nn3+Gj4+PsHry4+7ujmeffRaA+BHjehMmTMCoUaMA5Iwi3717N6ZPnw43t5xLUn3//fd4/fXXnVZPXFwcRo8ejQYNGqB06dJ45ZVXMGfOHJw+fRrZ2dkm63/yyScoWbIkKlasiHr16qFjx45Oq5Vsw76VfWhpO7IfZhtuN219T8h6vPgm2UxLF3PgjtRyar4gDBsVRPLw3HPP4ZNPPgEA+Pv7C64mb507d8aGDRvw+uuvo1+/fnjhhRdEl4T09HSMHz8e8+bNM8x/fufOHaxZswY//fQTLly4ACDnYjZBQUECK7XMqlWr8Oeff6Jbt27o0KGDIcSTq6SkJISEhGDNmjU4ePCgYT56Odu4cSOOHz9utMzd3d3ou3f16lUMGjQIkydPxqhRozB48GCnh9Le3t4ICQlBv379MHDgQLRs2dKp72+tvn37IjU1FQ0bNhRdCoCcHxO6d+9uuO/q6orx48ejXbt26N27N65fv+7UiwmXK1cO48aNQ6VKlRAaGoo///wz3/WzsrLw6NEjPHr0CACQkpLihCrJVuxb2Zea+1967IcRkaPIu/cgAHe41lF7OM5GGxGR/DRp0gRNmjQRXYZFunfvjpMnT6JBgwaGqRNEiY2NxVtvvYWTJ08CAM6dO4fOnTsjLCzMZATmrl27FBGMr169Gvv378eaNWvwzz//iC6nQKtWrcLIkSMBAD/99BM+/PBDsQVZwFzAmJGRgVu3bpksj4uLw7hx4/DNN9/go48+wvDhw1GmTBlnlAkACA4Oxrlz52Q7Ujy3L7/8UnQJFmnatCkiIyMRFhaGNm3aOPW9/f39MWLECIwYMQKXL19GaGgoQkNDce3aNZN133rrLUiShMTERCQlJcn6R1MtY9+KrMV8hgqL+x0qCKdSoUJT68GKO1DKTa1/50TkeI0aNRIeim/btg0NGzY0hOIAsGLFCuzcudMQivv5+WH48OEIDw/H5MmTRZVqsUePHuHw4cMAgI4dOxqmrZGzfv36wdfXFwAwb948s1NCyM2ZM2eQlJSEyMhIrFu3DtOmTcO7776bb/D46NEjfP3116hcuTI+/vhj3Lhxw2n1li5dGu7u7k57Py0oXrw43njjDaE11KxZE1OnTsWVK1dw4sQJDBs2DM8884zh8VatWmH9+vXYuXMnjh49io0bNwqslsxh38qx1Lh92f+yH61uSzV+L8j+GIznotWdhT2obdtxB1o4att+avv7JiLtyMzMxPjx49GtWzf8+++/Jo+7u7vj1VdfxebNm/H3339jwYIFaNSoEXQ6nYBqrRMWFma4gGjui5/KWfHixTFo0CAAwJUrV7Bt2zbBFVmmWLFiaNCgAd566y18/vnn+Prrr5Gamprn+uXKlcOLL76I3r17o3z58jh79qxsLtBJyqbT6dCsWTN8//33+Pvvv7Ft2zb07t0bK1asUMQPTVqltr6BXKlpO7P/RYWlpu8DORanUiF6CnegRESkBn///Td69eqFI0eO5LlOnz59sHz5cuEj2m2xZcsWADlBWefOnQVXY7mPP/4Yc+fORVZWFubOnWs0r7MSZGdno1+/fvD09ETLli1Ro0YNo1v16tVRrFgx0WWSBri7u6NLly7o0qULkpOTkZGRAU9PT9Fl0VPYt3IuLcw3TrZR+zS4RLZiMP7/8RfJwlPDjpYNN/tRS6OM+wYiUqKwsDC88847uH//fr7rhYSEIDs7G8uXL5f9hStzy8rKwvbt2wEAzZs3N5pSQe4qVKiAt956C2vXrsWBAwcQERGBRo2U037KzMzE5s2bFTGPN2kHf4yRJ/atxFB6P4z9Lyos7nvIGsrpAZEiqCEcJ9Jjo4xIvs6dO4cqVaowDHlKVlYWZsyYgUWLFsHPzw/Vq1dHyZIlUaJEiXz/m56erqhg/MSJE4apYZQyjUpuo0ePxtq1awEAc+fOxZo1awRXZDkPDw9FzOcuZ8nJydx3keoxmBJL6eG4XCWHRwIAigU2FFyJbbSQ13DfQ9ZSTg/IgRh+2ZdSd7bcgdofG2RE5Ci///47/ve//+HVV18VXYqsuLi4YOLEiZg4caLoUhxq69athn8rbSoSAGjcuDFatWqFw4cPY926dZgxYwbKly8PAJAkSRFzvJPtvvrqK8yYMUN0GUQOw36VPLAvZl/6UFz/b6WG40RkTHkTShIREZHmbdmyxTDHNP1HK4GqPhivWLEi6tevL7ga24wZMwZAztQkCxcuBACsW7cOUVFRAqsiR0tOTsbcuXNx69Yt0aUQOQRDcdKK3EE5yQP3P2QLBuNECnD+YproEoiIZOPOnTsIDw/Htm3bkJ2dLboccpLbt28DAG7cuIHz588DyJlGRak/BnTr1g3Vq1cHACxbtgyHDx9G//79cf36dcGVkSOdOHECGRkZCA0NFV0KERFZKK8QnOE4kfIxGFeh5PBI7qBVRB+KMxwnIsqxbds2AMDdu3fx559/Cq6GnOXNN9/E0aNHjaZRUeL84nqurq4YOXIkAODRo0do164dUlNTERMTI7YwcqjDhw8DAH7++WdIkiS4GiL74mhNU+zDWUeO09wWlK0weyFSNgbjKvJ0IC5yBy3HA5oSPd2QYsPKefg3TCRfuadQ4XQq2vHo0SN06NAB33//PQCgSJEiaNu2LZKTkxEfHy+4OutMmjQJzz33HJYvX25YlpmZCQAMxlVOH4xfuHABERFsaxCp1fmLabIY4MQfK5yD4TiRcmk+GFdD+MUR4trCcJyItCw1NRV79uwx3M89epjUTafTITU1FZcvXzbcr1+/Pp555hn8+++/gquzzqRJk+Dr64szZ86YPMZgXL3S09Nx4sQJw/2ff/5ZYDVE9sUANkfuQJyUz5qcRSmZjBoyMHO4DyJbaT4YVzJLAnGl7JxFk+NONL8GlZIaW3LctkSkXHv37kVqaqrh/pkzZxAbGyuwIhIlJSUF165dw+eff466deuKLscqXl5e2Lx5s2GO8dwYjKtXRESE0f5r7dq1hjMFiEjZCgrEldR/I9sxfyFSHgbjCsQR4gSwcUVE2mRuhDhHjWuDuYtsNmzYEJ988omAagrPz88P27dvR6lSpYyW37hxg3NPq5R+GhW9e/fuISwsTFA1RPaj5YEwHCGuXrZmLsxqiJSFwbiC2BqIi9oxq/UUHWewtHHFRhgVln6/khJ1VnQpRAWSJMlsCM55xrXJzc0NISEhcHd3F12KzWrUqIHNmzfDw8PDsOzJkyeKmzOdLPN0MA4Aq1evFlAJERWWLYG4qL6bln+4EIXhuHPxb5wKw010AVQw7lS1xZYGVv3ang6qhpSE+wpSu8jISPz9998my/ft24fk5GQUK1ZMQFXkLE+PGJ8wYQIaNGggphg7atWqFVauXIl33nnHsCwmJgblypUTWJV6JSYmIjs7GyVKlHDq+2ZnZ+Po0aMmy//v//4PCQkJ8PX1dWo9RPaitUCKA5McR04D6+zRr0oOj0SxwIZ2qIaIHIkjxmXMnlOmMDBTNzbQ7EsujTL9PsDSG5Ha6UeL5x5d6+HhgfT0dKMLcpL61atXDxMnThRdht307t0b06ZNM9y/ceOGuGJU7qOPPjKa59tZoqOjzV4k9smTJ9i4caPT6yEi69hryhT22+TPnv0q9tGI5E/TI8blEn49TU07z6LXIpBSrZHoMhSjMA0ljhxXBjV9v4mc7cKFC1ixYgUyMjIwZMgQAMCOHTtw6tQpHD9+HK+++qrYAsmh9CPGXVxcEBISAk9PdR3zJk2ahGvXruGnn37iBTgdJDQ0FGvWrMHChQud/t6HDx+Gp6cnXnvtNfz6668AgAEDBuDhw4dYu3Yt3n//fafXRFRYWhgtrpYgO/puSdQp+1B0GZokx5HjaspptLAfIsfSdDAuN44OzOS4Q5YDNe1I5RqOsyGWg6E4UeGsWbPGMK+0XvHixTF+/HhkZmYKrIycacyYMWjatKnoMuxOp9Phhx9+wM2bNxmMO8D169fx4YcfAgCKFCni9PcvUaIEoqOjIUmSIRivUaMGPv30U9y6dQuSJJm9wCwRieHIQFyufTZyXH+NWQyRfHEqFRngNAgE2K/xpZZRDURET3Nzy/v3/Pwec5SsrCxIkuT099UqnU6HGjVqYOrUqaJLcRgPDw/8/vvv8PLyEl2KqmRmZuLdd99FUlIS3NzchFywtVevXqhSpYrRXOIJCQkAgAoVKjAUJ5IJe02ZQvQ0Zj5E8sRgXCARgTh3xvJk78YXG3NERI6XlZWFr7/+WnQZmqGfQsXb21t0KQ5VsmRJfPPNN6LLUJUvv/wSx48fBwAULVpUaC0+Pj6Gf+uDcSIlunxPPWfdAs4PxNlfkx9nZCXMY4jkh8G4AFobIS7XudzVjo0t2zjq71VL33kirfDw8MDq1auxdOlS0aVowscff4wXXnhBdBlOkTs8pcI5fPgwvvrqK8N9EdOo5Obu7m6ogcE4kXhaGiEu5ylERWcGzuyrsV9oP3L+mybl0GwwLmLHq7VAnCzj6PnriIjIcWrVqoWhQ4fi999/F12K6g0YMEB0CaQwjx49wrvvvovs7GzDMtHBOPDfDx8MxonE0VIgTvLDXIhIPjQbjDuT3AJxOdVCjscGHxGR49SqVQuSJKF37944ePCg6HJUjXMwkzUkScLgwYMRGxtrtFwOwbh+nvHExETBlRBpj5wCcbnUoXWi8hHRuYzoUfpEcsFg3IHkFoiT/DirMSSHRpeWT3PifoBIvWrVqgUASEtLwyuvvIJz584JroiIAOCnn37C+vXrTZbLKRgXPWI8IyND6PsTOZOcAnEiPfYTbaflfIHsi8G4AyghEHd2fXL9NVJLO1M2BImI7E8fjAM5IVenTp1w8+ZNgRUR0dWrVzFs2DCzjzEY/8+GDRuwfft2oTUQOZrcA3Fn16al/q8l5JDbyKEGIi1jMG5HSgjEST5ENNDk3CiUA7n+gENE8pU7GAeAO3fuoGPHjnjw4IGgiojo/v372LJlC6KiouDm5gbgv6l45BCMy2WO8datW+Pll1/GrFmzIEmS0FqI7E3ugThRbsyRiMRhMG4HSg3ElVizWohspLGB6Fz8nhGpW5kyZQyjP/UuXbqEbt26ISUlRVBVRNrWvHlztG3bFhEREcjMzAQALFy4EC1atJBFMJ57jnGRgXRAQAAaN26McePG4b333kNqaqqwWojsRYmBuNLqVQu59dPkVg+RVmgyGLfnqFDuvCzH0bjywcYXEZF96HQ6k1HjAHDixAn07NmTc/gSCfTrr78CADw9PfHuu+9ix44daNeuneCq/gvGs7Ky8PjxY6G1vPLKKwCANWvWoHXr1vj777+F1kNUGOzjWEZu06mIyAnkmuPItS65kdvfMCmbJoNxe1HDTksNn0Fp5NJgk0sdRERKV7NmTZNlNWrUwOXLlzF9+nQBFRHRvXv3sHfvXgBA165d4ePjAx8fHwwZMkRwZTA6y0T0dCqvvvqq4d9//vknmjRpgpMnT4oriMhGSu/bKL1+sh9nZjQcvEjEYJxI03ixF8fiD09E2mBuxPinn36Ky5cv44svvhBQERFt3LgRWVlZAIBevXoJrsaYnILx2rVro0aNGob7cXFxaN26NVavXi2wKiIix1FCH00JNRKpBYNx0uxOV0RIK8eRAHKsSQT+Wk5EtqpVqxZ0Oh1++eUXFCtWDAAwc+ZMQyhHRM6nn0alaNGi6Nq1q+BqjOkvvgmID8Z1Op3RqHEASEtLQ9++ffHJJ59wP0aKoJb+jFo+B9mHVnOagmhtsB05HoNxcioth49s6BARqVOtWrUwZcoUvP322/jwww8BAFeuXMHGjRsFV0akTbdv38bhw4cB5MyhLYcLbuYmpxHjAEyCcb3Zs2ejW7duePTokVPrISLH02q4qLSwWWn1EikRg3EbqW0HpbbPQ9ZhaG9//E4RaUfdunUxadIkAMCoUaPg6ekJAJg+fTokSRJZGpEm/fbbb4bvntymUQHkF4w3a9YMZcqUMVnepUsX1KpVC8eOHRNQFZFl1NaPUdvnkROl9s+UWjeRUmguGNfyiGUSRwkNHCXUSEQkR25ubnBxyWlSlStXDv379wcAREVFYdeuXSJLI9Ik/TQqJUqUQHBwsOBqTOUOxhMTEwVWksPV1RUvv/yyyfKWLVti/vz56NKli4CqiEgLmM9YhuE4keNoLhgnorwxHCciKrxPPvnEEJR/8803gqsh0pbr16/j1KlTAIDXX3/dcAaHnMhtxDiQM+UMAHTu3Blly5YFAEyZMgVnz54VWRZRvth3IUupIVh21GdQ0o8TWp0CiByLwbgN1LBTNcdZn0tJO157UFqDzdH1yvFgZu+/SbXuI4jIMlWrVsXbb78NADh8+DCOHj0quCIi7Vi3bp3h33KcRgWQ18U39V566SWUKlUKixYtwrJlywAAGRkZ6Nu3LzIyMgRXR2RKaX0sazjrs8mxX0b5Yz+TyP4YjJMmsRGQPzU3NIny8/DhQ/Tp0we+vr7w9fVFnz59CrzoWL9+/aDT6YxuzZs3N1onLS0NH3/8Mfz8/FC0aFG8/PLLuH37tgM/CYk2fvx4w7+nT58usBIibdFPo/LMM8+gbdu2gqsxT44jxr29vbF582ZUqVIFr7zyCvr06QMgZ0qor7/+WnB1RES2U1uYrLbPQ5SXl19+GRUrVoSXlxfKlSuHPn364M6dO0brxMbGonv37ihatCj8/PwwfPhwpKenW/U+DMbJCHey9qXkgFnJtYvE75Cy9e7dG1FRUdi5cyd27tyJqKgoQziQn06dOiEuLs5w2759u9HjI0eOxKZNm/Drr7/iyJEjSE5ORrdu3ZCVleWoj0KC1a9fH927dwcAbNu2DWfOnBFcEVHhKOFCshcuXDBM/dGjRw+4ubkJrsg8Ly8vuLu7A5BPMA4ArVq1Mvx7wYIFCAgIAAB8/fXXiIhQzhmfWVlZuHjxIpKTk0WXQg6ihX6KFj4j2U6LfU4ObtSetm3bYv369bh06RI2btyIa9eu4c033zQ8npWVha5duyIlJQVHjhzBr7/+io0bN2LMmDFWvY+mgnGtTeFBYqmhMaOGz0BkqejoaOzcuRM//vgjgoKCEBQUhOXLl2Pr1q24dOlSvs/19PSEv7+/4VaqVCnDYwkJCVixYgXmzJmD9u3bo2HDhlizZg3OnTuHPXv2OPpjkUATJkww/HvGjBkCKyEqvG+++QZhYWGiy8hX7mlUevbsKbCS/Ol0OsOocTkF47mVLFkSP/74IwAgMzMTffv2RVqa/NqFjx8/xsmTJ7Fs2TIMGTIEzZs3R/HixfHhhx+iaNGiossjIsHUHCCr+bMRAcCoUaPQvHlzVKpUCS1atMD48eNx4sQJwxRvYWFhuHDhAtasWYOGDRuiffv2mDNnDpYvX27Vxc01FYzbgxZ2Plr4jGQ5huMkR4mJiUY3e3TWjx8/Dl9fXzRr1sywrHnz5vD19cWxY8fyfe6BAwdQpkwZ1KxZEwMHDsS9e/cMj4WHhyMjIwPBwcGGZQEBAahfv36Br0vKFhQUhDZt2gAA1q9fj6tXr4otiKgQihcvjo4dO2LQoEFWdTacRZIkwzQqAQEBeOGFFwRXlD8PDw8AkOW21OvcuTMGDBgAADh//jymTp0qtJ67d+9i165dmDlzJt5++23UqVMHxYsXR/PmzTFkyBAsW7YMJ0+eRHZ2Nn744QfodDqh9ZJjaKlvovbP6uiBi1rINbTwGUn+HNE3f9q///6L0NBQtGjRwnDW3fHjx1G/fn3DGW4A0LFjR6SlpSE8PNzi15bn+YVECqe2Rsz5i2moX9tTdBkOYc8GGRsm5l24nAZPb/t8J9JSc16nQoUKRssnT56MKVOmFOq14+PjUaZMGZPlZcqUQXx8fJ7P69y5M3r06IFKlSohJiYGn3/+Odq1a4fw8HB4enoiPj4eHh4eKFnS+PS/smXL5vu6pA4TJkzAgQMHkJ2djcWLF2Pu3LmiSzKSmJiI2NhY1K9fH0BOo3Pjxo0YOHCg4MpILubOnYvo6Gg888wzAIDly5dj165dWLFiBdq3by+4uv+cOXMGly9fBpAzWtzFRb7jfzIyMvDw4UMA8h0xrjd37lyEhYXh1q1bmDdvHkaMGIGyZcs6vY7U1FRs2LABixYtQnR0dL7rpqWl4aWXXsLNmzcZjquM2vpYRPaQHB6JYoENC/UaRa9FIKVaIztVRHKmlL653qeffoqFCxfi8ePHaN68ObZu3Wp4LD4+3qRNUrJkSXh4eFjVz5Zvi5GEYsBHT7NnQ5Tzg1Fh3bp1CwkJCYZb7ikrnjZlyhSTi2M+fTt9+jQAmO1AS5KUb8e6Z8+e6Nq1q2FO6R07duDy5cvYtm1bvp+hoNelvDVr1gweHh6oXLkyqlSpIrqcfHXo0AE9evTAsmXLZHkRzqFDh2LHjh0AgNOnT6NRo0YYNGgQ1q5dK7gyy4SGhqJ79+4YPXo0Hjx4ILqcAm3ZsgV9+/ZFy5YtFTFnNwCsWrUKP/74o9Hfb2xsLDp06IAPP/wQSUlJAqv7z/PPP48jR45g2LBhePfdd0WXk6+TJ08iNTUVU6dOxcyZM0WXky8fHx+EhITgf//7H44fPy4kFAdyLg760Ucf4a+//sLevXvx+uuvw9XVNc/1PTw8eIwlVeCPAbbRWp6h9s/L/EDeHNU3B4BPPvkEkZGRCAsLg6urK9577z2jNrQt/fenccS4FdS+s9EKR+9U1dx4UfPIcVIWHx8f+Pj4WLTusGHD0KtXr3zXqVy5Ms6ePYu7d++aPPbPP/9YFQSUK1cOlSpVwpUrVwAA/v7+SE9Px8OHD41Gjd+7dw8tWrSw+HXpP7du3UJ6ejpu3Lhh9VXHnU2n02H9+vWiyzBrzZo1CA0NRVZWFpYsWYKRI0catufKlSvRq1cv2QdLx48fN4wcET3NgyWOHz+O1atXAwBiYmJQtWpVwRXlLzk5Od8LWC5duhQ7d+5ESEgI2rZt68TKTLm4uKBly5Zo2bKl0DosoZ+r/cmTJ3jppZcEV1Ow9u3bIzw8PN8g2ll0Oh3atWuHdu3a4fbt21i2bBl++OEHoynMGjRoYJgChtRDzX0sIiIlc0TfXM/Pzw9+fn6oWbMm6tSpgwoVKuDEiRMICgqCv78/Tp48afTchw8fIiMjw6r+O0eMU54c+UOAWi+EqoUGmxY+oy34w5l8+fn5oXbt2vnevLy8EBQUhISEBJw6dcrw3JMnTyIhIcGqAPvBgwe4desWypUrBwAIDAyEu7s7du/ebVgnLi4O58+fZzBuo9yn0OX+N1nu2rVr+PDDDwEAv/32G4YOHWoIxUeOHImtW7fKPhQHYPQDVPHixQVXU7Dc1zB4uiEvR8WKFUNERARCQ0PzXOfGjRto164dhg0bhuTkZCdWp1z6YPy3335TzJkDcgjFn1a+fHl8+eWXiI2NNcw7CuScWj1kyBDB1RHZjyP7X2ociavVfplWPzcpi6V9c3P0bSb9HOZBQUE4f/484uLiDOuEhYXB09MTgYGBFtekmWBcrUEskQgMx0mN6tSpg06dOmHgwIE4ceIETpw4gYEDB6Jbt26oVauWYb3atWtj06ZNAHJGU44dOxbHjx/HjRs3cODAAXTv3h1+fn547bXXAAC+vr4YMGAAxowZg7179yIyMhLvvvsunnvuOVnNz6sUkiRhy5Ythvu5/02WycjIQO/evQ0hZlZWFoCcEHT9+vWYN2+e4cKAcqcPxmvUqCG4EsvkDsZPnDghsBLL/fnnnxaFjIsWLcLzzz+PgwcPOqEq5Xr48CH+/PNPAMDVq1dx7tw5wRUpn6enJ3r37o2jR48iIiICXbt2xZEjR0SXRXbEvgeRdqnxxxsq2KlTp7Bw4UJERUXh5s2b2L9/P3r37o1q1aohKCgIABAcHIy6deuiT58+iIyMxN69ezF27FgMHDjQ4hHsAKdSoQLY40IOWqG1BpsaplXhD2b0tNDQUAwfPhzBwcEAgJdffhkLFy40WufSpUuGi6W5urri3LlzWL16NR49eoRy5cqhbdu2WLdundHo1Xnz5sHNzQ1vvfUWUlNT8dJLL2HVqlVCR+AtXrwYs2bNQlxcHOrVq4f58+ejVatWZtf9/fffsWTJEkRFRSEtLQ316tXDlClT0LFjRydXDZw7dw6xsbGG+3v27MHjx49RpEgRp9eiVF988YXRmRF6tWrVgr+/v4CKbJOeno6bN28CUE4w7u/vj0qVKuHmzZuKGDEO5Iy+XbBgAbKzsy26nTp1CtWrV8ezzz4runRZ2rdvH7Kzsw33N2zYgOeff15gRerSsGFD/Pjjj4oZiU8F01ofKy9q6Hs5A0dNE6mDt7c3fv/9d0yePBkpKSkoV64cOnXqhF9//RWenjn7QldXV2zbtg1Dhw5Fy5Yt4e3tjd69e2P27NlWvReDcQtxB0tEeeH+QT1KlSqFNWvW5LtO7s62t7c3du3aVeDrenl54fvvv8f3339f6BrtYd26dRg5ciQWL16Mli1bYtmyZejcuTMuXLiAihUrmqx/6NAhdOjQAd988w1KlCiBlStXonv37jh58iQaNnTuj6dPjxB/8uQJ9u3bh27dujm1DqXat29fnhf7Cw8PR4cOHbBgwQIMHjzYyZVZ7/r164aAUSnBOJAzavzmzZuIjIxEWlqaoXEvV927dxddgqrop1HR27BhA6ZNmyaoGvVSwlRQRFrmiAFK7JNxYCOpx3PPPYd9+/YVuF7FihULPbWmZqZSIXIkjmSwHk+JIhJn7ty5GDBgAD744APUqVMH8+fPR4UKFbBkyRKz68+fPx/jxo1DkyZNUKNGDXzzzTeoUaOGkGlMzDV8OJ2KZe7fv48+ffqYHUnZpEkTLF68GHFxcYoIxYH/plEBlBWMN2/eHEDOiPeoqCixxZBTSZJk8mNqdHQ0Lly4IKgiInljH4vIeXg2NWkVg3EqEH95zZ+WG2xa/uxESpWeno7w8HDDdDF6wcHBOHbsmEWvkZ2djaSkJJQqVcoRJebp3r17Zqef2Lp1K0+bL4AkSRgwYADu3LljWFamTBmMGTMG586dw6lTp/Dhhx+iZEnl/Gip1GBcaRfgJPu5evWqYfqf3DZu3CigGiJSGva98uaszCJm31+I2feXU96LOJiOnIPBuAUYDKsHd6xkb9w/kNLcv38fWVlZKFu2rNHysmXLIj4+3qLXmDNnDlJSUvDWW2/luU5aWhoSExONboW1bds2swH4nTt3EBHBUS75WbJkCf744w+4ubnhlVdewebNm3H79m3Mnj0b9evXF12eTXIH49WrVxdYiXUaNmwIN7ec2QwZjGvL7t27zS7fsGGDkyshkj+GwCQ3uQNxhuNE6qGJYJynhBQewz/z2GBTLu4XSOuenn9VkiSL5mRdu3YtpkyZgnXr1qFMmTJ5rjd9+nT4+voabhUqVCh0zVu2bEGLFi0wYMAAw7JPP/0UNWvWLPTccmp2/vx5rFy5EnPmzMHt27exefNmvPLKK3B3dxddWqHog/Fnn31WURdf9fb2RoMGDQAAJ06cEFsMOZV+fvHc88r7+vri7NmzuHz5sqiyiGSHfay8cduYEpVVMBwnUgdNBOMkTwwmiYicz8/PD66uriajw+/du2cyivxp69atw4ABA7B+/Xq0b98+33UnTJiAhIQEw+3WrVuFrv2TTz7BkSNH0KJFC8OyN954AxcuXECXLl0K/fpq5e/vj1OnTmH06NEF/j9WEn0wrqRpVPT006lcv34d//zzj+Bq1O3o0aOiSwAAZGRk4J9//sEff/yBgQMHGpZfvXoVs2bNwvHjxwVWR0SkTM6cQsWa5SKpZVAjz/YnZ2EwTmQjZ/1aHx1xE9ERpvNRyoWWRy2opdFB2uLh4YHAwECTU/p3795tFDg/be3atejXrx9++eUXdO3atcD38fT0hI+Pj9GtsIKCgsyOand1dUWTJk0K/fpq5efnZ9HZAEry5MkTw48tSppGRS/3POOnTp0SWIm6ZWdn46OPPkJ6erroUgAAhw4dQvfu3Y1GjHt6emLs2LHo27evwMqI5EPLfQtL2XsbMYAkIi1jME4WYwgolpzDcSJSltGjR+PHH39ESEgIoqOjMWrUKMTGxmLIkCEAckZ7v/fee4b1165di/feew9z5sxB8+bNER8fj/j4eCQkJIj6CKRx165dM8w3r8QR482bNzf8m9OpOM62bdtw5swZxMTEiC4F7u7ucHV1BWA8lYpcQnsiImfhmeNEJCcMxomIiDSmZ8+emD9/PqZNm4YGDRrg0KFD2L59OypVqgQAiIuLQ2xsrGH9ZcuWITMzEx999BHKlStnuI0YMULURyCNu3r1quHfSgzGq1evjlKlSgHgBTgdRZIkTJ8+HYDxhVrlwMPDw/DvtDSOjiUikrOCpkuR43QqZJnzF9N4lgoxGC+IUkdJc+esDk+PElfbqHGetkckztChQ3Hjxg2kpaUhPDwcL774ouGxVatW4cCBA4b7Bw4cgCRJJrdVq1Y5v3AiGAedSgzGdTodmjZtCiBnKpXs7GzBFanP4cOHDfN2yzkY54hxov+oMaBSW/+N1E2ro/kZkGsbg3EV0ofiDMcdhztNIiIyJykpCRkZGaLLUL3cQWe1atUEVmI7/XQqCQkJuHTpkuBq1GfGjBmGf8stGM89lQpHjBOpH8Nx9WP2og4MyLXJpmB88eLFqFKlCry8vBAYGIjDhw/nue7vv/+ODh064JlnnoGPjw+CgoKwa9cumwum/HGHrH5sWBERydeVK1ewYsUK0WWonj7orFChAry9vQVXY5vcF+DkdCr2FRUVhR07dhjuyy0Y54hxsif2zeWL/TbHUurZ/SR/DMi1xepgfN26dRg5ciQmTpyIyMhItGrVCp07dzaaizS3Q4cOoUOHDti+fTvCw8PRtm1bdO/eHZGR3InZG0NxdWFDiohIee7evYspU6YgOTlZdCmqpg86lTiNip5+KhWAwbi9zZw50+i+3IJxjhgne2HfXL4c3ZdjaOcczFi0jQG5NlgdjM+dOxcDBgzABx98gDp16mD+/PmoUKEClixZYnb9+fPnY9y4cWjSpAlq1KiBb775BjVq1MCWLVsKXTz9x1k7bCX/Kqum+awZmhMRydO9e/dw9+5dzJ8/X3QpqvX48WPcvn0bgLKD8VKlSqFmzZoAgBMnThiWMygtnGvXrmH9+vVGy2JjY/HkyRNBFZniiHGyF/bNieSDIbp6MSBXN6uC8fT0dISHhyM4ONhoeXBwMI4dO2bRa2RnZyMpKQmlSpXKc520tDQkJiYa3Shv+e2AuXO2P+4QTWlxmyj5RyIicpy7d+8CAL799lv8888/gqtRp2vXrhn+reRgHPhvOpVz587h8ePH+OGHH7B48WLBVSnbrFmzTC5mKkmS0d+NaLlHjDMYJ1uprW+upv4EBzGRaOyrFsyWfQ4DcnWyKhi/f/8+srKyULZsWaPlZcuWRXx8vEWvMWfOHKSkpOCtt97Kc53p06fD19fXcKtQoYI1ZRIpniWNKTa4iIjkRx+MJyUl4euvvxZcjTrlnhZDicF4dnY2Vq5cib/++sswnUpWVha6d++OwYMHw8fHR3CFyhUXF4eVK1eafUxO06nkHjHOMwTIVuyby1NefTT23bSBAxPtQ+5n+zMcVxebLr6p0+mM7kuSZLLMnLVr12LKlClYt24dypQpk+d6EyZMQEJCguF269YtW8oEABS9FmHzc5WAO14iIiL50AfjQM4F0WJiYgRWox7p6em4fv06AOUH4y4uLoiNjUX9+vUxbtw4w/J9+/YBAPz8/ESVpngLFixAiRIl0LFjR8MyF5ec7o5cg3G5jRhPS0tDRkaG6DLICkrqmxMpCbMWyg9Hj6uHVcG4n58fXF1dTX6Bvnfvnskv1U9bt24dBgwYgPXr16N9+/b5ruvp6QkfHx+jmwhyP/2EO2riyAPbqP0HMyISJ3cwnpGRgc8//1xgNerh7u6O1q1bIyQkxBBw6nQ6VK1aFSkpKbKaJsMSY8eORbly5ZCammry2DPPPCOgInXo1asX7ty5g379+hmW/fzzz+jRo4esgnE5X3zz0aNHmDt3rugyyAJa65srAftmziP3rIa0gwG58lkVjHt4eCAwMBC7d+82Wr579260aNEiz+etXbsW/fr1wy+//IKuXbvaVikZsSYUZ4BORETkHLmDcQAIDQ1FZCQ7b4Wl0+lQrlw5DBgwAKtXrwYAFClSBO+88w6eeeYZ3L9/X3CF1ilatCimTZtm9jGOGLddgwYN4OrqisuXLxuWBQUFYd26dRg6dKjAyozJecR4dnY2Jk+ejEuXLokuhQqgpr65GkIlhuKkx/xFmxiQK5fVU6mMHj0aP/74I0JCQhAdHY1Ro0YhNjYWQ4YMAZBzqtV7771nWH/t2rV47733MGfOHDRv3hzx8fGIj49HQkKC/T6Fxoje0Wr511ln7OisbVSxEUZEJB/37t0zWTZhwgQBlahPzZo1AcAwzUNKSgo2btyIpk2bGi5iqST9+/dHvXr1TJYzGC88fTDu4eGBihUrQqfToUGDBmKLykXOI8azs7ORlpaGDz74wOQipiQ/7JsTEckLA3LlsToY79mzJ+bPn49p06ahQYMGOHToELZv345KlSoByLnoTWxsrGH9ZcuWITMzEx999BHKlStnuI0YMcJ+n0JDRIfiREREZF5WVhb++ecfk+W7du0yzB9NttMH40/79NNPnVyJfbi6uuLbb781Wubi4oISJUqIKUhF9MF49erV4erqKrgaU3IfMQ4AR44cwdKlSwVXQwVh31weLB2oZO8BTQzfiOSLAblyuNnypKFDh+Z5OuKqVauM7h84cMCWtyAzGIpTXqIjbqJOo0qiy9AMLZ81QUR5e/DggckIS39/fyQmJuLTTz/FqVOnLLogGplnLhh//vnn0alTJwHV2Efnzp3x0ksvYe/evQCA0qVLGy4WSbaRJMkwDUheP6aIJvcR43qffvopunXrhooVKwqsiArCvrlYPHtXnQqbvcTs+wtV2pmeFUby4MzAWv9e9Wt7FrAmicKWt0IwFNcGNqzIGWL2/WVyu3EwWnRZRIqnn1/8/fffh7e3NwCgZcuW+PfffzF79mw8ePBAZHmKV6tWLZNl48aNU/SPDTqdDrNmzTJ8Bl54s/Du3buHxMREAPINxuU8YlySJMO/k5OTMWTIEKNlRPam5BGV7LsRkaU4gly+GIxrhFyD9aLXIkSXoBqiG2bcycuPuQBcrvsCIjV49OgRfvjhB6xYsQINGzYEAPz555/w9PRE69atOXd0IdWoUcPofqVKldCzZ09B1dhPw4YN8e677wLg/OL2kPvCm3INxnOPGJdbMP70WS87duzAmjVrBFVDRGRKKWfvst9F5jAglx8G4wrAHao8cOdFcsYAnEi8F154AQMHDgQANG7cGAAQGxtr9oKcZL1ixYohICDAcH/MmDFwc7NpVkDZ+eqrr+Dp6clg3A5yB+PmzjKQg9wjxuU8lYreyJEjDWfEEFEO0YOSiEjZGJDLB4PxPMjlV0i5hlty2T5qYo/GFRto6sdR4ETylXtKjyZNmhj+ffr0aRHlqJJ+BHDp0qXx/vvvC67GfipWrIhRo0YxGLcDJYwYl/NUKuaC8X///RfDhw8XUA2RPBWmz8X+mvyppW/FzEYZGJCLx2BcxtSyQybKT/TdkqJLkC0G4ETKpR8xDuRMp0L2oQ86P/74YxQtWlRwNfY1fvx41K1bV3QZiqcPxn19fWU7Z7tSLr6Z2/r167F582bnFkOqxzCI1I79N7IUA3Jx1HH+qQo5YgfKKyNrQ3TETdRpVEl0GWQFNpiI1KdmzZrw8fFBYmIiR4zbUc2aNVGkSBEMGzZMdCl25+vriyFDhoguQ/H0wXjNmjVle2FWpY0Yb9asGQICAjBnzhy0bdsWvr6+Aiojkge5jfg+fzEN9Wt7FrxiAaLvlkSdsg/tUBERFcaFywzHnY3BuAwxJNMeuTWwKG+FPSWN328ibXBxcUFgYCD279+PP//8E5IkyTakU5JatWphwIABKF26tOhSHCL3SGKyXlZWFq5evQpAvtOoAICbmxt0Oh0kSZLdiHFJkgAAffr0waZNm5CcnIxixYrh999/N3qcSIvYZyNbcICidRx9RjlHZdPTOJWKzDA0kyel7TzZaCMiEk8/ncrdu3dx+/ZtwdWoQ926dTF69GjRZZBM3bx50zACW64X3gRyrkegHzUuxxHjI0eOxKpVq9C+fXsAwKFDh5CUlAQA/IGPiIRy9LzZzGOItIfBOBGRk7ChRaQtvACn/VWtWhWVK1cWXQbJlBIuvKmnPztAbsF47dq1MXfuXLi4uKBr164AgIyMDOzZs0dwZURi2XPgkZYHMRW9FiG6BCIiIwzGZcQZoZk930NJVznmBR5Jj40xInKW3ME4L8BJ5HhKCsb1I8blNpWKu7u7YVR4ly5dDMu3bdsmqiRSKSWdkavlIJvsQ0kDpNhfJq1RdTCupC+0knaUZF+OamiJaMApqYFLRORolSpVMsyFzRHjRI6XOxivUaOGwEoKJtepVHILCAhAw4YNAQDbt2/n/OKkSQzFiYjUTdXBuK2cPRKaoTgREZH66HQ6w6jx06dPM1QicjB9MB4QEIBixYoJriZ/Li453TC5jRh/mn46lbi4OERGKudsUSIiazGXIdImBuOCcecrf0oeBc0RDkREYumD8YcPH+LatWuCqyFSN30wLvdpVID/AnE5jxgH/gvGgZxR40Rawr4U2ROzHyJ5YjAukKgdI3fI8sHGlnbwe0dkP3IfYZlb48aNDf/mdCpEjpOamorY2FgAQK1atQRXU7DHjx8DkP/+rEmTJvDz8wPAecbJfpQw8MjR/TR7vr4Stqc9KOkaZyRPWvmukHUYjAvCkIycheG7/bAxRiQPf/313zH077//FlhJwXIH47wAJynVrl27cOXKFdFl5Ovq1auG6YrkPmI8MTHREIynpqYKriZ/rq6u6Ny5MwDg5MmT+OeffwRXZOzGjRuYOXMmJk2ahBEjRuD9999Hjx498Oabb+Lff/8VXR4RkRFn50DsvxIVjMG4AGoKxbmjJSIiZztz5ozh34cPHxZYScECAgIQEBAAALKen3fDhg0AgIyMDBw/flxwNSQnJ0+exPbt2/H6668jJSVFdDl5yn3hTbkH43v37jWE+A8ePBBcTcH006lIkoRjx44JrsZY5cqV0aRJE2zevBnfffcdVq5ciQ0bNiAuLg4lS5YUXR4pFAcWaY+aMhoisg6DcSJBnNngYuOOiNQiOzsbly5dMtyXc9ist2LFCpw+fRo7d+4UXYpZWVlZeP/997Fnzx4sXrwYLVq0QI8ePRAXFye6tAKtWrUKderUQfPmzRETEyO6nAJNmjQJDRo0QMOGDUWXYpH79++je/fu+O6773D+/HkMHjxYtheRffXVVxETE4Ndu3YhKChIdDn52rFjh+Hf9evXF1iJZTp27Ijly5fj9u3beOWVV0SXY6Jdu3aIjIzEggUL4OvrCwCoUKECdDqd4MpIidhvIiLSFjfRBWiNXH6JjNn3F6q0qye6DNnjHFRERPJy+vRpo1P5jxw5gsTERPj4+AisKn+dOnUSXUK+zp8/j6SkJLz//vtITk4GAOzfvx/e3t6CKyvY7du3cfHiRQA5Uz7I3d9//40zZ87Azc0N2dnZcHGR9xiVR48ewdPT03A/NDQUQUFB+OijjwRWZZ6rqysqV66MypUriy4lX5IkGQXjR44cQWpqqqy/byVKlMAHH3wguox8ubu7Y/jw4Xj77bfx2Wef4d133xVdEuWB/SvSOmYxRPIi79a4ysglFCdtctboBzZ2iciRtmzZYnQ/IyMDYWFhgqpRB/3UCLdu3cLDhw8BAP3790dKSgoSExNlO0IYyAlu9UqUKCGsDkv5+/sDADIzM2U///Hjx4+xaNEipKenGy0fNWoUp9sphPPnz+P27duG+ykpKdizZ4/AitTlmWeewfLly9GyZUvRpZACOXu0OEenExGJx2D8KY6aM5uheN6KXosQXYLTsRGkLfz+E9nP1q1bTZY9HZaTdczNGTx79myUL18en332mSKCcRcXFxQrVkxsMRbQB+MAEB8fL7CSghUpUgTz5s3DnTt3sHfvXgwePBh+fn7IyMhAjx49cO/ePdElKlLu0eJ6mzdvdn4hKufmxhOjyTrsn2mXiL4a+4fmRd/ltSHI+RiMOwF3eiQXbPARkZLdunULUVFRJsu3b9+OrKws5xekEnldTO+TTz7B999/L+vpPvTBuK+vr6zr1FNSMK7n6uqKdu3aYenSpYiLi8OuXbvQqVMnfPTRR8jMzBRdnuKYC8b/+OMP7sOIqFDUftauowYwknao/TtCtpN/D0Lh5ByK26s2HqSosJTwyzD/zrXh4cOH6NOnD3x9feHr64s+ffoYTdVgjk6nM3ubNWuWYZ02bdqYPN6rVy8Hfxr1MTdaHMi5QOCJEyecXI06xMfH4/r16ybLp0yZgpkzZ8r+4nX676cSplEBlBmM5+bm5obg4GD8+OOPCA0NRUZGhuiSFCUxMRFHjhwxWX7//n0cPXpUQEVEziXXYIqDh4iI5CstLQ0NGjSATqczGSRlrh++dOlSq16fwbgDyTkUp4LJteFWWGz4EeWtd+/eiIqKws6dO7Fz505ERUWhT58++T4nLi7O6BYSEgKdToc33njDaL2BAwcarbds2TJHfhRV2rZtG3r06IExY8YYls2ePRsvvvgip1Oxkbm5or/99ltMnjxZ9qE4wGBcJA8PD1lfMFKO9uzZg8zMTKO/1zJlygDgdCpEorBvRKIwLyKyzLhx4xAQEJDn4ytXrjTqZ/ft29eq1+fka0ROxsYXkTxFR0dj586dOHHiBJo1awYAWL58OYKCgnDp0iXUqlXL7PNyB10A8H//939o27YtqlatarS8SJEiJuuSdebPn4/q1asjJCTEsOzFF1/EmDFjcPXqVYGVKdfT06gsXLgQH330kaBqrKcPxkuWlP+ZR4C6gnG5y8rKgqurq+gyjJw5cwaLFy9G48aN0bRpUwDA5MmT8dxzz+GXX36BJEmK+EGKSC3k0C+LjriJOo0qiS5DsxhOE8nbjh07EBYWho0bN5qdjg7IGSBTmH42R4w7CHewJGdyaASKIOJCr9wXOEZiYqLRLS2t8Gd4HD9+HL6+voZQHACaN28OX1/fPOdgftrdu3exbds2DBgwwOSx0NBQ+Pn5oV69ehg7diySkpIKXbPWVK9e3abHKG/66Rt0Oh1WrFihqFAcUN6IcR8fH3h5eQFgMO5IkiRhxYoVosswMWXKFHz44YcoUqSIYVlmZiZatWqFJUuWMBQnIiK745Sg5GiO6JsDOX3rgQMH4ueffzZqOz1t2LBh8PPzQ5MmTbB06VJkZ2db9T4cMe4ASgrCYvb9hSrt6okug4hU7NKZW3D3KGaX18pITwYAVKhQwWj55MmTMWXKlEK9dnx8vOGU9tzKlCljcYD1008/oXjx4nj99deNlr/zzjuoUqUK/P39cf78eUyYMAFnzpzB7t27C1UzUWE8efIE4eHhcHV1xc8//4y3335bdElWkSRJccG4TqeDv78/bty4wWDcgU6fPo0ffvgBgwYNEl2KEX3wnXskOy+6SSSGVgcKkbwwjyFnU0rfXJIk9OvXD0OGDEHjxo1x48YNs+t9+eWXeOmll+Dt7Y29e/dizJgxuH//PiZNmmTxe6k2GBcxMhRQVihOzienBhhP2yMlu3XrFnx8fAz3PT0981x3ypQpmDp1ar6v9+effwKA2dF61pzaHhISgnfeeccwIlRv4MCBhn/Xr18fNWrUQOPGjREREYFGjRpZ9NpE9hYREQFJkvDbb7/htddeE12O1VJSUgyholKCcQAMxp0gJCQEZ86cwePHj/MdYSQKg3HSIjldv0lOfTIqGEc8E8mbI/rmx44dQ2JiIiZMmJDvurkD8AYNGgAApk2bxmDcVoXd4Wo5FE8Oj0SxwIaiyzAr+q71847KqeGmROcvpqF+7bx3hkSF5ePjY3Twzc+wYcPQq1evfNepXLkyzp49i7t375o89s8//6Bs2bIFvs/hw4dx6dIlrFu3rsB1GzVqBHd3d1y5coXBOAkTGRmJP/74A506dRJdik30o8UB5QXjAKdScZTU1FSsXbsWmZmZOH36NF588UXRJZlgME5EJJ5c8huOGnc85juO5Yi++VdffYUTJ06YhOyNGzfGO++8g59++snsc5s3b47ExETcvXvXoj48wGDcbuSyUyWyBkeNW4ajFJTNz88Pfn5+Ba4XFBSEhIQEnDp1ynBRtJMnTyIhIQEtWrQo8PkrVqxAYGAg/ve//xW47l9//YWMjAyUK1eu4A9A5CB9+vSxuBErR0oPxh88eID09HR4eHgIrkhdNm3ahISEBAA5145gME5EuclxtLi9+mSFHZwUfbck6pR9WOg6iIjyYmnf/LvvvsNXX31luH/nzh107NgR69atM7om2NMiIyPh5eVlVd+AwbgdKD0U5y+UziHHRhgR/adOnTro1KkTBg4ciGXLlgEABg0ahG7duqFWrVqG9WrXro3p06cbTT2RmJiI3377DXPmzDF53WvXriE0NBRdunSBn58fLly4gDFjxqBhw4Zo2bKl4z8YCZOUlITixYuLLiNPSg7FAeUH4wBw7949lC9fXmA16rNy5UrDv48fPy6wkrwxGCcSg/0xIiJlqFixotH9YsVy5kWvVq2aoe28ZcsWxMfHIygoCN7e3ti/fz8mTpyIQYMG5Tudy9Nc7Fc2ESkRG4iOo/QfzbQoNDQUzz33HIKDgxEcHIznn38eP//8s9E6ly5dMoxG1Pv1118hSZLZixd6eHhg79696NixI2rVqoXhw4cjODgYe/bsMQpHSH3mzZtnFN6SfakhGOd0KvZ18+ZN7N2713D/+PHjkCRJYEXmMRgnreE0BgSIuw6cEsit38j/V6QE7u7uWLx4MYKCgvD8889jwYIFmDZtmtnBavnhiPFCktsOjAqPDTci7SpVqhTWrFmT7zrmQpZBgwZh0KBBZtevUKECDh48aJf6SFkOHz6MEiVKYPjw4aJLUSUG4/S0n376yWgffe/ePcTExKBq1aoCqzLFYJzI+TgYiIhIuSpXrmzSD+/UqZNdrpXEEeNEREREdiZJEiIjI7FkyRJZjlhVAwbjlFt2djZWrVplslyO06kwGCciIiKSBwbjRERERHZ269YtPHjwABcvXsSBAwdEl6NKDMYpt0OHDiEmJsZkuRyDcTe3/07azczMFFgJERERkbYxGCcinlpIRGRnERH/zc24ZMkSgZWol1KD8bJlyxr+zWDcfkJCQswul2MwzhHjRERERPLAYJzICRg8ExFpS+5gfNOmTYiLixNYjTrpg3EXFxfDleqVwMvLyxDkMxi3j8TERPz+++/o3LkzqlevDgAoVqwYAgICcObMGaSkpAiu0BiDcSIisXitOCLSYzBOREREZGe5g/HMzEz8+OOPAqtRJ30wXqJECeh0OrHFWEk/nQqDcft49OgRzpw5g+3bt6NkyZIAgGrVquHixYsYM2YMzp49K7hCY0oJxuPj43HhwgXRZRAVGgcpERFRXhiMExEREdlZZGSk0f0ffviBcwnbWe5gXGkYjNtXxYoVUa1aNQDAgwcPAAClS5dG8eLFMXPmTDRv3lxkeSaUEoxfvHgRs2fPFl0GERERkcMwGCe7SQ6PLHglIiIilYuPj8edO3eMlt2+fRtbt24VVJE6MRgnc/79918AQKlSpQzL5HZGgYvLf10wOQfjly5dwpo1a/D333+LLoWIiFQu+m5J0SWQRjEYLwQ1zUulps9CREQk0tOjxfV4EU77UkMwnpKSguTkZMHVqEdmZqbh76J06dJii8mHTqczhONyD8YzMjKwYMEC0aUQkUZx8B0RORqDcSIiIiI7yj2/eG5hYWG4cuWKk6tRLzUE4wBHjduT/m8CMB4xLkf66VTkHowDwNKlS5GQkCC4GiIiIiL7YzBOREREZEcREREoU6YMgoODDctWr16N999/H6GhoQIrUw9JkhiMkwn9/OKAvEeMA8oKxpOSkrBs2TLB1RARERHZH4NxIgLAq7UTEdlLYGAgLl68iLffftuwrEqVKlixYgUmTZoksDL1SElJMQSKSgrG9dOmPB2Mx8bGYsqUKYKqUg/9/OKA/EeMu7m5AYBsL8qbnp6OmJgYw/358+cjLS1NYEWkVOcv8u+GiIjki8E4ERERkR199tlnKFmyJMqVK2dYFhcXB+C/MIwKJ/eUGUoKxidPnoxXX30VZ86cMSxbtGgR6tatywsc2gFHjNvPtWvXkJ2dbbgfFxfHM16IiIhIdRiMExERETlAQECA4d937twRWIn6KDUYHzBgAP7v//4Pn376qWHZgQMHkJKSgsDAQIGVqYOSRozLPRjXT6OS26xZs4zCciIisp+YfX+JLoFIkxiMExERETmAuRHjVDhhYWHIyMgwG4xnZGSIKcoKdevWRatWrcw+1rhxYydXoz4cMW4/5oLxixcvYuvWrQKqIVI/TmtJRCQGg3EiIgfgL/5EVLp0abi7uwPgiHF72bZtG55//nls3rzZsCwxMRETJkzAwIEDxRVmhSFDhpgsc3d3x3PPPSegGnXhiHH7MReMA8C3337r5EqIiIiIHIfB+P+XHB4pugRSKa3++s8L7RCR1ul0OsOocY4Yt48XXngBFy9exJw5cwzLhg0bhhkzZqBLly4CK7PcG2+8YTKauX79+vD09BRUkXrkHjHOYLxw9MG4/sc9Ly8vvPnmm4iIiMDRo0dFlkZkFa32xYiIyDIMxomIiIgcRD/POEeM20fLli3NLvf19cXLL7/s5Gps4+npif79+xst4zQq9qEfMV68eHFDoCtXcg/G79y5gx9++AEDBgwAADx58gQ//fQT7t27hzJlygiujoiIiMg+GIwT5aL1Uc4cUWGKZ5MQUWFwxLh9BQQEoEqVKibLe/bsCS8vLwEV2WbQoEFG93nhTfvQjxiX+/zigLyD8czMTOzZswcDBw5EjRo1DMuvX7+OYsWKGS0jIiIiUjIG40REREQOoh8x/vDhQ6SmpgquRh1eeOEFk2V9+/YVUIntatSogZdeeslwnyPG7UM/Ylzu06gAgJubGwB5BuNubm6oVq0aAKB69eqG5VeuXBFVEhEREZFDMBgnIiIichD9iHEAiI+PF1iJejw9nUr16tURFBQkqBrb6S/C6e7ujvr16wuuRh2UOGI8MzNTcCX5yz06/OrVqwIrISIiIrI/BuMkC0WvRYgugYiIyO70I8YBTqdiL0+PGH/vvfeg0+kEVWO7V155Bf7+/njuued44U07UdKIcTlPpZJblSpVDN8vBuNERESkNgzGiYiIiBwk94hxXoDTPurUqYMSJUoY7vfp00dcMYXg7u6OAQMGcBoVO0lPT0dSUhIAZY0Yl3sw7uXlhQoVKgDgVCpERESkPgzGiYiIiByEI8btz8XFxTCdSps2bVC5cmWxBRXCwIED0bRpU9FlqMLDhw8N/+aIcfvSzzPOEeNERESkNgzGySBm31+iSyAiIidZvHgxqlSpAi8vLwQGBuLw4cP5rn/w4EEEBgbCy8sLVatWxdKlS51UqbJxxLhj6KdTee+99wRXUjiVKlXC22+/LboMVdDPLw5wxLi96ecZv3XrFi8iTERERKrCYJyIiEhj1q1bh5EjR2LixImIjIxEq1at0LlzZ8TGxppdPyYmBl26dEGrVq0QGRmJzz77DMOHD8fGjRudXLnylC5dGu7u7gA4YtyeWrZsiSJFiuDNN98UXUqhFSlSRHQJqqCfXxzgiHF7048YB3KOB0RERERqwWCciIxER9wUXQIROdjcuXMxYMAAfPDBB6hTpw7mz5+PChUqYMmSJWbXX7p0KSpWrIj58+ejTp06+OCDD/D+++9j9uzZTq5ceVxcXAyjxjli3H6aNGmCt99+G8WLFxddiiZIkiS6hAJxxLjj5A7G5TzP+N27d/HVV1/ht99+w8WLF5GZmSm6JCIiIpI5BuNEREQakp6ejvDwcAQHBxstDw4OxrFjx8w+5/jx4ybrd+zYEadPn0ZGRobZ56SlpSExMdHoplX6YPzJkyeCK8nbmTNnIEmS4e8jOztbdEn58vLywpdffomsrCzZ1/o0SZJk/bfwtKtXr+Lbb7+V/Xc4JSUFbm5uADhi3N70wbi3t7fRyHy5KVu2LJo2bYq+ffsiKCgIKSkpoksiwTjgh4iICsJg3Eacj5sswcaY5aLvlhRdApEm3L9/H1lZWShbtqzR8rJlyyI+Pt7sc+Lj482un5mZifv375t9zvTp0+Hr62u4VahQwT4fAED//v2RlZWFrKwsNG7c2G6v6yjbtm1DWloaDh06JLqUPH333XcYPXo0jh07hsaNG6Ns2bLYtm2b6LLyVa5cOSxduhSurq7w8PDAuXPnRJdUoNdeew2urq6oWbOm6FIscv36ddSpUweTJk3Cjh07RJeTr969eyM9PR2JiYmK2C/s27cPWVlZiI6OFl1KgWrXro3bt28jOTkZ/fv3F11OvoKDgxEWFoapU6fC19dXdDlEREQkcwzGiUgTil6LEF0CkazodDqj+5IkmSwraH1zy/UmTJiAhIQEw+3WrVuFrNi4FhcXF7i4uORbs1yULl0aHh4eosvIl4eHB+bPn4+ePXsCyPkBpUSJEti/fz+2bt0quLq8paWlAQAyMjLg6ekpuJqCubq6QpIkJCcniy6lQA8ePMDbb7+NzMxMZGZmYtOmTaJLKpBOp0Px4sUN8/rLmZubG1xclNEVc3Nzw7PPPquYel944QV8/PHHossgAOcvpokugYiIKF/KaN2QYiSHR4ougYiI8uHn5wdXV1eT0eH37t0zGRWu5+/vb3Z9Nze3POfy9fT0hI+Pj9GN5EsfKt+7d8+w7IUXXkC7du1QokQJQVUVLPeUJEoIxosVKwYAipjioXTp0jhw4ADeffddAMD27dsNP0QQyZ0SfjQlIiIi8RiMExERaYiHhwcCAwOxe/duo+W7d+9GixYtzD4nKCjIZP2wsDA0btxYESMzqWB5jWgfOHAgXnjhBSdXY7ncwbiXl5fASixTtGhRADlz/ec1P7+ceHt7Y/Xq1ZgzZw5SUlKwd+9e0SUREREREdkNg3EiIiKNGT16NH788UeEhIQgOjoao0aNQmxsLIYMGQIgZxqU9957z7D+kCFDcPPmTYwePRrR0dEICQnBihUrMHbsWFEfgezMXDBepkwZzJgxQ0A1llNaMK4fMQ4oY9Q4kDPydvTo0di5cycOHjwouhwiIiIiIrtxE10AEZHa8OK8JHc9e/bEgwcPMG3aNMTFxaF+/frYvn07KlWqBACIi4tDbGysYf0qVapg+/btGDVqFBYtWoSAgAB89913eOONN0R9BLIzc8H4vHnzUKpUKQHVWC731B5KCMb1I8YBIDk5WdbT1DytQ4cOirioJRERERGRpRiME5GJ6IibqNOokugyiMiBhg4diqFDh5p9bNWqVSbLWrdujYgIXsRWrZ4Oxjt06IC3335bUDWWyz1iXO4XOAWMR4wr4QKcTytZsqToEoiIKB/nL6ahfm35X3ODzIvZ9xeqtKsnugwiTeFUKkREREQal/vClV5eXliyZIkiLl6nD8Y9PT0VUa8Sp1IhIiIiZUsOjxRdApFsMRgnIiIi0rjco60///xzVKtWTWA1ltMH40qYRgUwnUqFiIiIiIjEYTBOREREpHH6YLxu3bqKuqiqfo5xpQTjHDFOREQkFq8HRUS5MRgnIiIi0jh9ML5s2TJFzNWtl3sqFSVQ+hzjRERERERqwmCcjPDXUyIiIu3x8PDAwIED8cILL4guxSqcSoWIiMyJjrgpugQiIlIABuNEREREGlexYkXMmDFDdBlW41QqRERERERkKzfRBRCpFUcpEBGRUrRs2VJ0CTbhiHEiIiIiIrKVKkeMF70WIboEIsVjsE9ERHKn5DnGOWKciIiIiEgsm4LxxYsXo0qVKvDy8kJgYCAOHz6c7/oHDx5EYGAgvLy8ULVqVSxdutSmYomIiBzp66+/RosWLVCkSBGUKFHCoudIkoQpU6YgICAA3t7eaNOmDf76y/h6DWlpafj444/h5+eHokWL4uWXX8bt27cd8AmItEVpI8Z58U0isjf2zYmISK22bduGZs2awdvbG35+fnj99deNHo+NjUX37t1RtGhR+Pn5Yfjw4UhPT7fqPawOxtetW4eRI0di4sSJiIyMRKtWrdC5c2fExsaaXT8mJgZdunRBq1atEBkZic8++wzDhw/Hxo0brX1rIiIih0pPT0ePHj3w4YcfWvycb7/9FnPnzsXChQvx559/wt/fHx06dEBSUpJhnZEjR2LTpk349ddfceTIESQnJ6Nbt27IyspyxMcg0gylzTHu5eUFnU4HgME4ERUe++ZERKRWGzduRJ8+fdC/f3+cOXMGR48eRe/evQ2PZ2VloWvXrkhJScGRI0fw66+/YuPGjRgzZoxV72P1HONz587FgAED8MEHHwAA5s+fj127dmHJkiWYPn26yfpLly5FxYoVMX/+fABAnTp1cPr0acyePRtvvPGGtW9P5DDnL6aJLoGIBJs6dSoAYNWqVRatL0kS5s+fj4kTJxp+vf7pp59QtmxZ/PLLLxg8eDASEhKwYsUK/Pzzz2jfvj0AYM2aNahQoQL27NmDjh07OuSzEGmB0kaM63Q6FCtWDElJSZxKhYgKjX1zIiJSo8zMTIwYMQKzZs3CgAEDDMtr1apl+HdYWBguXLiAW7duISAgAAAwZ84c9OvXD19//TV8fHwsei+rgvH09HSEh4dj/PjxRsuDg4Nx7Ngxs885fvw4goODjZZ17NgRK1asQEZGBtzd3U2ek5aWZhgBBAAJCQkAgCQLOxBZj1MtWi+3lDTrhtonZ2Za/R5KkWTltnhatg3bHwAeJ1veQUz2SLR43dQUV4vWS0u1bzCeka78kWBpqZZvZ3NSU6yb8zU52fL3S0q3LlCwab8Qddbq5wDK3D+kZOaMXJYkye6vnWnH74L+tRITjf9WPD09hcwxHBMTg/j4eKPjnKenJ1q3bo1jx45h8ODBCA8PR0ZGhtE6AQEBqF+/Po4dO6aZYFz/t/X0/zuiwkhNzdm363Q6xfxtFSlSBElJSXj48KFiaibSOv131RHtJFuJ7punpuS//7pwWeygIyX2xQrb99Kztg+WmzX9McC6PpkzchpLyL2vVtg8Ji/W5jTW5DOAdRmNOZbmNrawd9bjSGlPcs46dtTxRil984iICPz9999wcXFBw4YNER8fjwYNGmD27NmoV68egJxjWv369Q2hOJBzTEtLS0N4eDjatm1r0XtZFYzfv38fWVlZKFu2rNHysmXLIj4+3uxz4uPjza6fmZmJ+/fvo1y5cibPmT59umHUXm7Pt3/NmnLJVifDRVdAMrFDdAHkdA8ePICvr69dXsvDwwP+/v7YvaaNXV5Pr1ixYqhQoYLRssmTJ2PKlCl2fR9L6I995o5zN2/eNKzj4eGBkiVLmqyT17FTjfRTyzz9/47IHtavX4/169eLLsMqu3btstv+loicIykpSTbfW9F98xFvVixE9WQO+14EgHkMAbBvvxxQXt/8+vXrAIApU6Zg7ty5qFy5MubMmYPWrVvj8uXLKFWqlNljWsmSJeHh4WFVP9vqqVQAGOZG1JMkyWRZQeubW643YcIEjB492nD/0aNHqFSpEmJjY2XTEBEtMTERFSpUwK1btyw+PUDtuE1McZuY4jYxLyEhARUrVkSpUqXs9ppeXl6IiYmx+uIXBTF3zMnvF+kpU6aY7dDl9ueff6Jx48Y212TtcdHSddQkICAAt27dQvHixe3yufldth23nW243WzHbWcbbjfb2XvbSZKEpKQko1FpcsG+uXj8rpriNjHFbWKK28SUI/rlgPL65tnZ2QCAiRMnGqb6WrlyJcqXL4/ffvsNgwcPBmD+2GVtP9uqYNzPzw+urq4myfu9e/dMUno9f39/s+u7ubmhdOnSZp+T17B7X19fflme4uPjw23yFG4TU9wmprhNzHNxsfqazPny8vISPvfvsGHD0KtXr3zXqVy5sk2v7e/vDyBnBFbuUVa5j4v+/v5IT0/Hw4cPjUaN37t3Dy1atLDpfZXIxcUF5cuXt/vr8rtsO24723C72Y7bzjbcbraz57aTWwjMvrn88LtqitvEFLeJKW4TU/bulwPK6pvrzzauW7euYbmnpyeqVq1quMC0v78/Tp48afTchw8fIiMjI8/joDlWbWkPDw8EBgZi9+7dRst3796dZ+c+KCjIZP2wsDA0btzY7BxmRERE9uTn54fatWvne7O1gVClSpWcU9JyHefS09Nx8OBBw3ExMDAQ7u7uRuvExcXh/PnzmgrGiYiIyH7YNyciIqWxtG8eGBgIT09PXLp0yfDcjIwM3LhxA5UqVQKQc0w7f/484uLiDOuEhYXB09MTgYGBFtdk9U8Qo0ePxo8//oiQkBBER0dj1KhRiI2NxZAhQwDknGr13nvvGdYfMmQIbt68idGjRyM6OhohISFYsWIFxo4da+1bExEROVRsbCyioqIQGxuLrKwsREVFISoqCsnJ/12kpHbt2ti0aROAnFO3Ro4ciW+++QabNm3C+fPn0a9fPxQpUgS9e/cGkDOiasCAARgzZgz27t2LyMhIvPvuu3juuefQvn17IZ+TiIiIlI99cyIiUiMfHx8MGTIEkydPRlhYGC5duoQPP/wQANCjRw8AORebrlu3Lvr06YPIyEjs3bsXY8eOxcCBA606A8HqOcZ79uyJBw8eYNq0aYiLi0P9+vWxfft2Q2IfFxdnGNYO5Iym2759O0aNGoVFixYhICAA3333nWGOGEt4enpi8uTJhb6qqZpwm5jiNjHFbWKK28Q8bpccX3zxBX766SfD/YYNGwIA9u/fjzZt2gAALl26hISEBMM648aNQ2pqKoYOHYqHDx+iWbNmCAsLQ/HixQ3rzJs3D25ubnjrrbeQmpqKl156CatWrYKrq+Ouvq52/Ju1HbedbbjdbMdtZxtuN9tpZduxby4P3CamuE1McZuY4jYxxW3yn1mzZsHNzQ19+vRBamoqmjVrhn379hmmJ3V1dcW2bdswdOhQtGzZEt7e3ujduzdmz55t1fvoJP3VNoiIiIiIiIiIiIiINMD+s7kTEREREREREREREckYg3EiIiIiIiIiIiIi0hQG40RERERERERERESkKQzGiYiIiIiIiIiIiEhTZBOML168GFWqVIGXlxcCAwNx+PDhfNc/ePAgAgMD4eXlhapVq2Lp0qVOqtR5rNkmv//+Ozp06IBnnnkGPj4+CAoKwq5du5xYrXNY+3eid/ToUbi5uaFBgwaOLVAAa7dJWloaJk6ciEqVKsHT0xPVqlVDSEiIk6p1Dmu3SWhoKP73v/+hSJEiKFeuHPr3748HDx44qVrHO3ToELp3746AgADodDps3ry5wOdoYR9L8se2gW3YfrAd2xm2Y3vENmyzWI/tGsfisdc8HltN8ZhpisdCUzzOGeMxTIYkGfj1118ld3d3afny5dKFCxekESNGSEWLFpVu3rxpdv3r169LRYoUkUaMGCFduHBBWr58ueTu7i5t2LDByZU7jrXbZMSIEdLMmTOlU6dOSZcvX5YmTJggubu7SxEREU6u3HGs3SZ6jx49kqpWrSoFBwdL//vf/5xTrJPYsk1efvllqVmzZtLu3bulmJgY6eTJk9LRo0edWLVjWbtNDh8+LLm4uEgLFiyQrl+/Lh0+fFiqV6+e9Oqrrzq5csfZvn27NHHiRGnjxo0SAGnTpk35rq+FfSzJH9sGtmH7wXZsZ9iO7RHbsM1iG7ZrHIfHXvN4bDXFY6YpHgtN8Thniscw+ZFFMN60aVNpyJAhRstq164tjR8/3uz648aNk2rXrm20bPDgwVLz5s0dVqOzWbtNzKlbt640depUe5cmjK3bpGfPntKkSZOkyZMnq+7ga+022bFjh+Tr6ys9ePDAGeUJYe02mTVrllS1alWjZd99951Uvnx5h9UokiUHXy3sY0n+2DawDdsPtmM7w3Zsj9iGbZbCY7vGvnjsNY/HVlM8ZprisdAUj3P54zFMHoRPpZKeno7w8HAEBwcbLQ8ODsaxY8fMPuf48eMm63fs2BGnT59GRkaGw2p1Flu2ydOys7ORlJSEUqVKOaJEp7N1m6xcuRLXrl3D5MmTHV2i09myTf744w80btwY3377LZ599lnUrFkTY8eORWpqqjNKdjhbtkmLFi1w+/ZtbN++HZIk4e7du9iwYQO6du3qjJJlSe37WJI/tg1sw/aD7djOsB3bI7Zhm8V5eHywDI+95vHYaorHTFM8Fpricc4+tLCfFc1NdAH3799HVlYWypYta7S8bNmyiI+PN/uc+Ph4s+tnZmbi/v37KFeunMPqdQZbtsnT5syZg5SUFLz11luOKNHpbNkmV65cwfjx43H48GG4uQn/U7c7W7bJ9evXceTIEXh5eWHTpk24f/8+hg4din///VcVc5nZsk1atGiB0NBQ9OzZE0+ePEFmZiZefvllfP/9984oWZbUvo8l+WPbwDZsP9iO7QzbsT1iG7ZZnIfHB8vw2Gsej62meMw0xWOhKR7n7EML+1nRhI8Y19PpdEb3JUkyWVbQ+uaWK5m120Rv7dq1mDJlCtatW4cyZco4qjwhLN0mWVlZ6N27N6ZOnYqaNWs6qzwhrPk7yc7Ohk6nQ2hoKJo2bYouXbpg7ty5WLVqlWp+mQas2yYXLlzA8OHD8cUXXyA8PBw7d+5ETEwMhgwZ4oxSZUsL+1iSP7YNbMP2g+3YzrAd2yO2YZvFOXh8sByPvebx2GqKx0xTPBaa4nGu8LSynxVF+E91fn5+cHV1NfnF6N69eya/iuj5+/ubXd/NzQ2lS5d2WK3OYss20Vu3bh0GDBiA3377De3bt3dkmU5l7TZJSkrC6dOnERkZiWHDhgHIOfBIkgQ3NzeEhYWhXbt2TqndUWz5OylXrhyeffZZ+Pr6GpbVqVMHkiTh9u3bqFGjhkNrdjRbtsn06dPRsmVLfPLJJwCA559/HkWLFkWrVq3w1VdfafIXWLXvY0n+2DawDdsPtmM7w3Zsj9iGbRbn4fHBi+vGHwAAA21JREFUMjz2msdjqykeM03xWGiKxzn70MJ+VjThI8Y9PDwQGBiI3bt3Gy3fvXs3WrRoYfY5QUFBJuuHhYWhcePGcHd3d1itzmLLNgFyfo3u168ffvnlF9XNwWTtNvHx8cG5c+cQFRVluA0ZMgS1atVCVFQUmjVr5qzSHcaWv5OWLVvizp07SE5ONiy7fPkyXFxcUL58eYfW6wy2bJPHjx/DxcV4V+jq6grgv19itUbt+1iSP7YNbMP2g+3YzrAd2yO2YZvFeXh8sAyPvebx2GqKx0xTPBaa4nHOPrSwnxXO4Zf3tMCvv/4qubu7SytWrJAuXLggjRw5UipatKh048YNSZIkafz48VKfPn0M61+/fl0qUqSINGrUKOnChQvSihUrJHd3d2nDhg2iPoLdWbtNfvnlF8nNzU1atGiRFBcXZ7g9evRI1EewO2u3ydPUeOVra7dJUlKSVL58eenNN9+U/vrrL+ngwYNSjRo1pA8++EDUR7A7a7fJypUrJTc3N2nx4sXStWvXpCNHjkiNGzeWmjZtKuoj2F1SUpIUGRkpRUZGSgCkuXPnSpGRkdLNmzclSdLmPpbkj20D27D9YDu2M2zH9oht2GaxDds1jsNjr3k8tpriMdMUj4WmeJwzxWOY/MgiGJckSVq0aJFUqVIlycPDQ2rUqJF08OBBw2N9+/aVWrdubbT+gQMHpIYNG0oeHh5S5cqVpSVLlji5YsezZpu0bt1aAmBy69u3r/MLdyBr/05yU+PBV5Ks3ybR0dFS+/btJW9vb6l8+fLS6NGjpcePHzu5aseydpt89913Ut26dSVvb2+pXLly0jvvvCPdvn3byVU7zv79+/PdP2h1H0vyx7aBbdh+sB3bGbZje8Q2bLNYj+0ax+Kx1zweW03xmGmKx0JTPM4Z4zFMfnSSpNHzEYiIiIiIiIiIiIhIk4TPMU5ERERERERERERE5EwMxomIiIiIiIiIiIhIUxiMExEREREREREREZGmMBgnIiIiIiIiIiIiIk1hME5EREREREREREREmsJgnIiIiIiIiIiIiIg0hcE4EREREREREREREWkKg3EiIiIiIiIiIiIi0hQG40RERERERERERESkKQzGiYiIiIiIiIiIiEhTGIwTERERERERERERkaYwGCciIiIiIiIiIiIiTfl/4kHeZeilR5cAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1500x500 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a uniform grid\n",
    "x = torch.linspace(0.0, 1.0, steps=10)\n",
    "XX, YY = torch.meshgrid(x, x, indexing=\"xy\")\n",
    "\n",
    "# Define scalar field y (potential field)\n",
    "y = torch.sin(2 * 4.1415 * XX) * torch.cos(1 * 3.1415 * YY)\n",
    "\n",
    "# Compute gradient (∇y)\n",
    "dy_dx, dy_dy = torch.gradient(y, spacing=(x, x))\n",
    "\n",
    "# Compute second derivatives (for Laplacian)\n",
    "d2y_dx2, _ = torch.gradient(dy_dx, spacing=(x, x))\n",
    "_, d2y_dy2 = torch.gradient(dy_dy, spacing=(x, x))\n",
    "\n",
    "# Compute divergence of the gradient field (Laplacian)\n",
    "laplacian_y = d2y_dx2 + d2y_dy2\n",
    "\n",
    "# Plot the scalar field y\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.contourf(XX, YY, y, cmap=\"coolwarm\")\n",
    "plt.colorbar()\n",
    "plt.title(\"Scalar Field $y$\")\n",
    "\n",
    "# Plot the gradient field (∇y) as quiver plot\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.quiver(XX, YY, dy_dx, dy_dy)\n",
    "plt.title(\"Gradient Field $\\\\nabla y$\")\n",
    "\n",
    "# Plot the Laplacian (divergence of gradient field)\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.contourf(XX, YY, laplacian_y, cmap=\"coolwarm\")\n",
    "plt.colorbar()\n",
    "plt.title(\"Laplacian $\\\\nabla \\\\cdot \\\\nabla y$\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define grid\n",
    "x = torch.linspace(0.0, 1.0, steps=20)\n",
    "XX, YY = torch.meshgrid(x, x, indexing=\"xy\")\n",
    "\n",
    "# Define stream function\n",
    "psi = torch.sin(2 * torch.pi * XX) * torch.cos(torch.pi * YY)\n",
    "\n",
    "# Compute velocity components\n",
    "u = -torch.pi * torch.sin(2 * torch.pi * XX) * torch.sin(torch.pi * YY)\n",
    "v = -2 * torch.pi * torch.cos(2 * torch.pi * XX) * torch.cos(torch.pi * YY)\n",
    "\n",
    "# Compute vorticity (curl)\n",
    "dv_dx = 4 * torch.pi**2 * torch.sin(2 * torch.pi * XX) * torch.cos(torch.pi * YY)\n",
    "du_dy = -torch.pi**2 * torch.sin(2 * torch.pi * XX) * torch.cos(torch.pi * YY)\n",
    "vorticity = dv_dx - du_dy  # Curl of velocity field\n",
    "\n",
    "# Plot streamlines\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.streamplot(XX.numpy(), YY.numpy(), u.numpy(), v.numpy(), color=\"blue\")\n",
    "plt.title(\"Streamlines of the Velocity Field\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()\n",
    "\n",
    "# Plot vorticity\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.contourf(XX.numpy(), YY.numpy(), vorticity.numpy(), cmap=\"coolwarm\")\n",
    "plt.colorbar(label=\"Vorticity\")\n",
    "plt.title(\"Vorticity Field (Curl of Velocity)\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
