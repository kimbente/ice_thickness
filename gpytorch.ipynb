{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07e03865",
   "metadata": {},
   "source": [
    "# dfGP\n",
    "\n",
    "- 1 run\n",
    "- curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6fe9e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "=== CURVE ===\n",
      "Training inputs shape: torch.Size([196, 2])\n",
      "Training observations shape: torch.Size([196, 2])\n",
      "Training inputs dtype: torch.float32\n",
      "Training inputs device: cuda:0\n",
      "Training observations device: cuda:0\n",
      "\n",
      "=== CURVE ===\n",
      "Test inputs shape: torch.Size([400, 2])\n",
      "Test observations shape: torch.Size([400, 2])\n",
      "Test inputs dtype: torch.float32\n",
      "Test inputs device: cuda:0\n",
      "Test observations device: cuda:0\n",
      "\n",
      "\n",
      "--- Training Run 1/1 ---\n",
      "\n",
      "Start Training\n",
      "curve dfGP Run 1/1, Epoch 1/2000, Training Loss (NLML): -128.7164, (RMSE): 0.0867\n",
      "curve dfGP Run 1/1, Epoch 2/2000, Training Loss (NLML): -289.4695, (RMSE): 0.0866\n",
      "curve dfGP Run 1/1, Epoch 3/2000, Training Loss (NLML): -369.8453, (RMSE): 0.0857\n",
      "curve dfGP Run 1/1, Epoch 4/2000, Training Loss (NLML): -412.5753, (RMSE): 0.0841\n",
      "curve dfGP Run 1/1, Epoch 5/2000, Training Loss (NLML): -435.9697, (RMSE): 0.0820\n",
      "curve dfGP Run 1/1, Epoch 6/2000, Training Loss (NLML): -449.1777, (RMSE): 0.0796\n",
      "curve dfGP Run 1/1, Epoch 7/2000, Training Loss (NLML): -456.9645, (RMSE): 0.0770\n",
      "curve dfGP Run 1/1, Epoch 8/2000, Training Loss (NLML): -462.1240, (RMSE): 0.0741\n",
      "curve dfGP Run 1/1, Epoch 9/2000, Training Loss (NLML): -465.6323, (RMSE): 0.0713\n",
      "curve dfGP Run 1/1, Epoch 10/2000, Training Loss (NLML): -468.5896, (RMSE): 0.0684\n",
      "curve dfGP Run 1/1, Epoch 11/2000, Training Loss (NLML): -471.2649, (RMSE): 0.0657\n",
      "curve dfGP Run 1/1, Epoch 12/2000, Training Loss (NLML): -474.0022, (RMSE): 0.0631\n",
      "curve dfGP Run 1/1, Epoch 13/2000, Training Loss (NLML): -476.8218, (RMSE): 0.0607\n",
      "curve dfGP Run 1/1, Epoch 14/2000, Training Loss (NLML): -479.9838, (RMSE): 0.0585\n",
      "curve dfGP Run 1/1, Epoch 15/2000, Training Loss (NLML): -483.4410, (RMSE): 0.0566\n",
      "curve dfGP Run 1/1, Epoch 16/2000, Training Loss (NLML): -487.3124, (RMSE): 0.0547\n",
      "curve dfGP Run 1/1, Epoch 17/2000, Training Loss (NLML): -491.6845, (RMSE): 0.0530\n",
      "curve dfGP Run 1/1, Epoch 18/2000, Training Loss (NLML): -496.7134, (RMSE): 0.0514\n",
      "curve dfGP Run 1/1, Epoch 19/2000, Training Loss (NLML): -502.3904, (RMSE): 0.0498\n",
      "curve dfGP Run 1/1, Epoch 20/2000, Training Loss (NLML): -508.8474, (RMSE): 0.0482\n",
      "curve dfGP Run 1/1, Epoch 21/2000, Training Loss (NLML): -516.0831, (RMSE): 0.0465\n",
      "curve dfGP Run 1/1, Epoch 22/2000, Training Loss (NLML): -524.2463, (RMSE): 0.0449\n",
      "curve dfGP Run 1/1, Epoch 23/2000, Training Loss (NLML): -533.3695, (RMSE): 0.0432\n",
      "curve dfGP Run 1/1, Epoch 24/2000, Training Loss (NLML): -543.5703, (RMSE): 0.0415\n",
      "curve dfGP Run 1/1, Epoch 25/2000, Training Loss (NLML): -554.8719, (RMSE): 0.0396\n",
      "curve dfGP Run 1/1, Epoch 26/2000, Training Loss (NLML): -567.3135, (RMSE): 0.0378\n",
      "curve dfGP Run 1/1, Epoch 27/2000, Training Loss (NLML): -581.0262, (RMSE): 0.0359\n",
      "curve dfGP Run 1/1, Epoch 28/2000, Training Loss (NLML): -595.9153, (RMSE): 0.0339\n",
      "curve dfGP Run 1/1, Epoch 29/2000, Training Loss (NLML): -612.1930, (RMSE): 0.0319\n",
      "curve dfGP Run 1/1, Epoch 30/2000, Training Loss (NLML): -629.4124, (RMSE): 0.0298\n",
      "curve dfGP Run 1/1, Epoch 31/2000, Training Loss (NLML): -648.2557, (RMSE): 0.0276\n",
      "curve dfGP Run 1/1, Epoch 32/2000, Training Loss (NLML): -666.9968, (RMSE): 0.0254\n",
      "curve dfGP Run 1/1, Epoch 33/2000, Training Loss (NLML): -684.5775, (RMSE): 0.0235\n",
      "curve dfGP Run 1/1, Epoch 34/2000, Training Loss (NLML): -697.6948, (RMSE): 0.0215\n",
      "curve dfGP Run 1/1, Epoch 35/2000, Training Loss (NLML): -702.4814, (RMSE): 0.0198\n",
      "curve dfGP Run 1/1, Epoch 36/2000, Training Loss (NLML): -689.3242, (RMSE): 0.0184\n",
      "curve dfGP Run 1/1, Epoch 37/2000, Training Loss (NLML): -682.1400, (RMSE): 0.0171\n",
      "curve dfGP Run 1/1, Epoch 38/2000, Training Loss (NLML): -682.8505, (RMSE): 0.0163\n",
      "curve dfGP Run 1/1, Epoch 39/2000, Training Loss (NLML): -703.6279, (RMSE): 0.0155\n",
      "curve dfGP Run 1/1, Epoch 40/2000, Training Loss (NLML): -713.7036, (RMSE): 0.0151\n",
      "curve dfGP Run 1/1, Epoch 41/2000, Training Loss (NLML): -712.7554, (RMSE): 0.0147\n",
      "curve dfGP Run 1/1, Epoch 42/2000, Training Loss (NLML): -706.7235, (RMSE): 0.0145\n",
      "curve dfGP Run 1/1, Epoch 43/2000, Training Loss (NLML): -699.9678, (RMSE): 0.0144\n",
      "curve dfGP Run 1/1, Epoch 44/2000, Training Loss (NLML): -694.6126, (RMSE): 0.0142\n",
      "curve dfGP Run 1/1, Epoch 45/2000, Training Loss (NLML): -692.5398, (RMSE): 0.0143\n",
      "curve dfGP Run 1/1, Epoch 46/2000, Training Loss (NLML): -693.8439, (RMSE): 0.0142\n",
      "curve dfGP Run 1/1, Epoch 47/2000, Training Loss (NLML): -697.6849, (RMSE): 0.0141\n",
      "curve dfGP Run 1/1, Epoch 48/2000, Training Loss (NLML): -702.3846, (RMSE): 0.0142\n",
      "curve dfGP Run 1/1, Epoch 49/2000, Training Loss (NLML): -707.0279, (RMSE): 0.0142\n",
      "curve dfGP Run 1/1, Epoch 50/2000, Training Loss (NLML): -710.5317, (RMSE): 0.0142\n",
      "curve dfGP Run 1/1, Epoch 51/2000, Training Loss (NLML): -710.7069, (RMSE): 0.0144\n",
      "curve dfGP Run 1/1, Epoch 52/2000, Training Loss (NLML): -709.2031, (RMSE): 0.0145\n",
      "curve dfGP Run 1/1, Epoch 53/2000, Training Loss (NLML): -709.9691, (RMSE): 0.0145\n",
      "curve dfGP Run 1/1, Epoch 54/2000, Training Loss (NLML): -710.0718, (RMSE): 0.0151\n",
      "curve dfGP Run 1/1, Epoch 55/2000, Training Loss (NLML): -713.1716, (RMSE): 0.0150\n",
      "curve dfGP Run 1/1, Epoch 56/2000, Training Loss (NLML): -714.6023, (RMSE): 0.0155\n",
      "curve dfGP Run 1/1, Epoch 57/2000, Training Loss (NLML): -716.2438, (RMSE): 0.0158\n",
      "curve dfGP Run 1/1, Epoch 58/2000, Training Loss (NLML): -716.6819, (RMSE): 0.0162\n",
      "curve dfGP Run 1/1, Epoch 59/2000, Training Loss (NLML): -715.2383, (RMSE): 0.0164\n",
      "curve dfGP Run 1/1, Epoch 60/2000, Training Loss (NLML): -713.9557, (RMSE): 0.0167\n",
      "curve dfGP Run 1/1, Epoch 61/2000, Training Loss (NLML): -713.6807, (RMSE): 0.0170\n",
      "curve dfGP Run 1/1, Epoch 62/2000, Training Loss (NLML): -713.5984, (RMSE): 0.0172\n",
      "curve dfGP Run 1/1, Epoch 63/2000, Training Loss (NLML): -714.1575, (RMSE): 0.0172\n",
      "curve dfGP Run 1/1, Epoch 64/2000, Training Loss (NLML): -714.9741, (RMSE): 0.0173\n",
      "curve dfGP Run 1/1, Epoch 65/2000, Training Loss (NLML): -715.7592, (RMSE): 0.0173\n",
      "curve dfGP Run 1/1, Epoch 66/2000, Training Loss (NLML): -717.0157, (RMSE): 0.0173\n",
      "curve dfGP Run 1/1, Epoch 67/2000, Training Loss (NLML): -716.5663, (RMSE): 0.0172\n",
      "curve dfGP Run 1/1, Epoch 68/2000, Training Loss (NLML): -716.9833, (RMSE): 0.0171\n",
      "curve dfGP Run 1/1, Epoch 69/2000, Training Loss (NLML): -716.6141, (RMSE): 0.0169\n",
      "curve dfGP Run 1/1, Epoch 70/2000, Training Loss (NLML): -718.2255, (RMSE): 0.0167\n",
      "curve dfGP Run 1/1, Epoch 71/2000, Training Loss (NLML): -716.4568, (RMSE): 0.0165\n",
      "curve dfGP Run 1/1, Epoch 72/2000, Training Loss (NLML): -718.5904, (RMSE): 0.0165\n",
      "curve dfGP Run 1/1, Epoch 73/2000, Training Loss (NLML): -719.0407, (RMSE): 0.0162\n",
      "curve dfGP Run 1/1, Epoch 74/2000, Training Loss (NLML): -719.4746, (RMSE): 0.0161\n",
      "curve dfGP Run 1/1, Epoch 75/2000, Training Loss (NLML): -719.3455, (RMSE): 0.0161\n",
      "curve dfGP Run 1/1, Epoch 76/2000, Training Loss (NLML): -719.6600, (RMSE): 0.0159\n",
      "curve dfGP Run 1/1, Epoch 77/2000, Training Loss (NLML): -718.7642, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 78/2000, Training Loss (NLML): -718.6846, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 79/2000, Training Loss (NLML): -718.9232, (RMSE): 0.0155\n",
      "curve dfGP Run 1/1, Epoch 80/2000, Training Loss (NLML): -718.9819, (RMSE): 0.0154\n",
      "curve dfGP Run 1/1, Epoch 81/2000, Training Loss (NLML): -718.1063, (RMSE): 0.0154\n",
      "curve dfGP Run 1/1, Epoch 82/2000, Training Loss (NLML): -719.8815, (RMSE): 0.0154\n",
      "curve dfGP Run 1/1, Epoch 83/2000, Training Loss (NLML): -719.7397, (RMSE): 0.0155\n",
      "curve dfGP Run 1/1, Epoch 84/2000, Training Loss (NLML): -719.0157, (RMSE): 0.0155\n",
      "curve dfGP Run 1/1, Epoch 85/2000, Training Loss (NLML): -720.0761, (RMSE): 0.0155\n",
      "curve dfGP Run 1/1, Epoch 86/2000, Training Loss (NLML): -720.1627, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 87/2000, Training Loss (NLML): -720.9291, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 88/2000, Training Loss (NLML): -720.7794, (RMSE): 0.0158\n",
      "curve dfGP Run 1/1, Epoch 89/2000, Training Loss (NLML): -720.9590, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 90/2000, Training Loss (NLML): -721.2158, (RMSE): 0.0159\n",
      "curve dfGP Run 1/1, Epoch 91/2000, Training Loss (NLML): -721.0662, (RMSE): 0.0159\n",
      "curve dfGP Run 1/1, Epoch 92/2000, Training Loss (NLML): -720.6832, (RMSE): 0.0161\n",
      "curve dfGP Run 1/1, Epoch 93/2000, Training Loss (NLML): -720.3549, (RMSE): 0.0160\n",
      "curve dfGP Run 1/1, Epoch 94/2000, Training Loss (NLML): -721.7789, (RMSE): 0.0161\n",
      "curve dfGP Run 1/1, Epoch 95/2000, Training Loss (NLML): -721.4946, (RMSE): 0.0160\n",
      "curve dfGP Run 1/1, Epoch 96/2000, Training Loss (NLML): -721.6167, (RMSE): 0.0161\n",
      "curve dfGP Run 1/1, Epoch 97/2000, Training Loss (NLML): -722.5543, (RMSE): 0.0161\n",
      "curve dfGP Run 1/1, Epoch 98/2000, Training Loss (NLML): -722.2950, (RMSE): 0.0160\n",
      "curve dfGP Run 1/1, Epoch 99/2000, Training Loss (NLML): -722.2122, (RMSE): 0.0160\n",
      "curve dfGP Run 1/1, Epoch 100/2000, Training Loss (NLML): -722.7587, (RMSE): 0.0159\n",
      "curve dfGP Run 1/1, Epoch 101/2000, Training Loss (NLML): -722.7599, (RMSE): 0.0159\n",
      "curve dfGP Run 1/1, Epoch 102/2000, Training Loss (NLML): -723.2505, (RMSE): 0.0158\n",
      "curve dfGP Run 1/1, Epoch 103/2000, Training Loss (NLML): -723.1692, (RMSE): 0.0158\n",
      "curve dfGP Run 1/1, Epoch 104/2000, Training Loss (NLML): -724.1226, (RMSE): 0.0158\n",
      "curve dfGP Run 1/1, Epoch 105/2000, Training Loss (NLML): -723.6230, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 106/2000, Training Loss (NLML): -724.1960, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 107/2000, Training Loss (NLML): -723.8630, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 108/2000, Training Loss (NLML): -724.1580, (RMSE): 0.0155\n",
      "curve dfGP Run 1/1, Epoch 109/2000, Training Loss (NLML): -724.4467, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 110/2000, Training Loss (NLML): -724.9418, (RMSE): 0.0155\n",
      "curve dfGP Run 1/1, Epoch 111/2000, Training Loss (NLML): -724.7828, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 112/2000, Training Loss (NLML): -724.9617, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 113/2000, Training Loss (NLML): -725.0981, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 114/2000, Training Loss (NLML): -725.4173, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 115/2000, Training Loss (NLML): -725.8109, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 116/2000, Training Loss (NLML): -726.3848, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 117/2000, Training Loss (NLML): -725.8368, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 118/2000, Training Loss (NLML): -727.0286, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 119/2000, Training Loss (NLML): -727.4151, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 120/2000, Training Loss (NLML): -727.1974, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 121/2000, Training Loss (NLML): -727.6952, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 122/2000, Training Loss (NLML): -727.9313, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 123/2000, Training Loss (NLML): -728.4576, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 124/2000, Training Loss (NLML): -728.0098, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 125/2000, Training Loss (NLML): -728.7683, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 126/2000, Training Loss (NLML): -728.8774, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 127/2000, Training Loss (NLML): -728.9758, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 128/2000, Training Loss (NLML): -729.4207, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 129/2000, Training Loss (NLML): -729.6725, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 130/2000, Training Loss (NLML): -729.8049, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 131/2000, Training Loss (NLML): -729.9254, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 132/2000, Training Loss (NLML): -730.4065, (RMSE): 0.0155\n",
      "curve dfGP Run 1/1, Epoch 133/2000, Training Loss (NLML): -730.6792, (RMSE): 0.0155\n",
      "curve dfGP Run 1/1, Epoch 134/2000, Training Loss (NLML): -731.4421, (RMSE): 0.0155\n",
      "curve dfGP Run 1/1, Epoch 135/2000, Training Loss (NLML): -731.2300, (RMSE): 0.0155\n",
      "curve dfGP Run 1/1, Epoch 136/2000, Training Loss (NLML): -731.7546, (RMSE): 0.0155\n",
      "curve dfGP Run 1/1, Epoch 137/2000, Training Loss (NLML): -731.6884, (RMSE): 0.0155\n",
      "curve dfGP Run 1/1, Epoch 138/2000, Training Loss (NLML): -732.0317, (RMSE): 0.0154\n",
      "curve dfGP Run 1/1, Epoch 139/2000, Training Loss (NLML): -732.3881, (RMSE): 0.0155\n",
      "curve dfGP Run 1/1, Epoch 140/2000, Training Loss (NLML): -732.6882, (RMSE): 0.0155\n",
      "curve dfGP Run 1/1, Epoch 141/2000, Training Loss (NLML): -732.5991, (RMSE): 0.0154\n",
      "curve dfGP Run 1/1, Epoch 142/2000, Training Loss (NLML): -733.3057, (RMSE): 0.0155\n",
      "curve dfGP Run 1/1, Epoch 143/2000, Training Loss (NLML): -734.0692, (RMSE): 0.0155\n",
      "curve dfGP Run 1/1, Epoch 144/2000, Training Loss (NLML): -733.9486, (RMSE): 0.0154\n",
      "curve dfGP Run 1/1, Epoch 145/2000, Training Loss (NLML): -734.3916, (RMSE): 0.0154\n",
      "curve dfGP Run 1/1, Epoch 146/2000, Training Loss (NLML): -734.9568, (RMSE): 0.0155\n",
      "curve dfGP Run 1/1, Epoch 147/2000, Training Loss (NLML): -735.1415, (RMSE): 0.0154\n",
      "curve dfGP Run 1/1, Epoch 148/2000, Training Loss (NLML): -735.5674, (RMSE): 0.0154\n",
      "curve dfGP Run 1/1, Epoch 149/2000, Training Loss (NLML): -736.1016, (RMSE): 0.0154\n",
      "curve dfGP Run 1/1, Epoch 150/2000, Training Loss (NLML): -736.3486, (RMSE): 0.0154\n",
      "curve dfGP Run 1/1, Epoch 151/2000, Training Loss (NLML): -736.5585, (RMSE): 0.0154\n",
      "curve dfGP Run 1/1, Epoch 152/2000, Training Loss (NLML): -736.9071, (RMSE): 0.0154\n",
      "curve dfGP Run 1/1, Epoch 153/2000, Training Loss (NLML): -737.4398, (RMSE): 0.0154\n",
      "curve dfGP Run 1/1, Epoch 154/2000, Training Loss (NLML): -738.0719, (RMSE): 0.0153\n",
      "curve dfGP Run 1/1, Epoch 155/2000, Training Loss (NLML): -738.2147, (RMSE): 0.0153\n",
      "curve dfGP Run 1/1, Epoch 156/2000, Training Loss (NLML): -738.0719, (RMSE): 0.0153\n",
      "curve dfGP Run 1/1, Epoch 157/2000, Training Loss (NLML): -738.7271, (RMSE): 0.0153\n",
      "curve dfGP Run 1/1, Epoch 158/2000, Training Loss (NLML): -739.5385, (RMSE): 0.0152\n",
      "curve dfGP Run 1/1, Epoch 159/2000, Training Loss (NLML): -739.6655, (RMSE): 0.0152\n",
      "curve dfGP Run 1/1, Epoch 160/2000, Training Loss (NLML): -740.0527, (RMSE): 0.0152\n",
      "curve dfGP Run 1/1, Epoch 161/2000, Training Loss (NLML): -740.3260, (RMSE): 0.0152\n",
      "curve dfGP Run 1/1, Epoch 162/2000, Training Loss (NLML): -740.7526, (RMSE): 0.0152\n",
      "curve dfGP Run 1/1, Epoch 163/2000, Training Loss (NLML): -741.1381, (RMSE): 0.0152\n",
      "curve dfGP Run 1/1, Epoch 164/2000, Training Loss (NLML): -741.8397, (RMSE): 0.0152\n",
      "curve dfGP Run 1/1, Epoch 165/2000, Training Loss (NLML): -742.0147, (RMSE): 0.0152\n",
      "curve dfGP Run 1/1, Epoch 166/2000, Training Loss (NLML): -742.2991, (RMSE): 0.0152\n",
      "curve dfGP Run 1/1, Epoch 167/2000, Training Loss (NLML): -743.0554, (RMSE): 0.0152\n",
      "curve dfGP Run 1/1, Epoch 168/2000, Training Loss (NLML): -743.5273, (RMSE): 0.0151\n",
      "curve dfGP Run 1/1, Epoch 169/2000, Training Loss (NLML): -744.0038, (RMSE): 0.0151\n",
      "curve dfGP Run 1/1, Epoch 170/2000, Training Loss (NLML): -744.2029, (RMSE): 0.0151\n",
      "curve dfGP Run 1/1, Epoch 171/2000, Training Loss (NLML): -744.4800, (RMSE): 0.0151\n",
      "curve dfGP Run 1/1, Epoch 172/2000, Training Loss (NLML): -744.9351, (RMSE): 0.0151\n",
      "curve dfGP Run 1/1, Epoch 173/2000, Training Loss (NLML): -745.3718, (RMSE): 0.0151\n",
      "curve dfGP Run 1/1, Epoch 174/2000, Training Loss (NLML): -745.8108, (RMSE): 0.0150\n",
      "curve dfGP Run 1/1, Epoch 175/2000, Training Loss (NLML): -746.2434, (RMSE): 0.0150\n",
      "curve dfGP Run 1/1, Epoch 176/2000, Training Loss (NLML): -746.6896, (RMSE): 0.0150\n",
      "curve dfGP Run 1/1, Epoch 177/2000, Training Loss (NLML): -747.2795, (RMSE): 0.0150\n",
      "curve dfGP Run 1/1, Epoch 178/2000, Training Loss (NLML): -747.6527, (RMSE): 0.0150\n",
      "curve dfGP Run 1/1, Epoch 179/2000, Training Loss (NLML): -747.8088, (RMSE): 0.0150\n",
      "curve dfGP Run 1/1, Epoch 180/2000, Training Loss (NLML): -748.3663, (RMSE): 0.0149\n",
      "curve dfGP Run 1/1, Epoch 181/2000, Training Loss (NLML): -748.6067, (RMSE): 0.0149\n",
      "curve dfGP Run 1/1, Epoch 182/2000, Training Loss (NLML): -748.8455, (RMSE): 0.0149\n",
      "curve dfGP Run 1/1, Epoch 183/2000, Training Loss (NLML): -749.1106, (RMSE): 0.0149\n",
      "curve dfGP Run 1/1, Epoch 184/2000, Training Loss (NLML): -749.4594, (RMSE): 0.0149\n",
      "curve dfGP Run 1/1, Epoch 185/2000, Training Loss (NLML): -749.6295, (RMSE): 0.0148\n",
      "curve dfGP Run 1/1, Epoch 186/2000, Training Loss (NLML): -749.8875, (RMSE): 0.0148\n",
      "curve dfGP Run 1/1, Epoch 187/2000, Training Loss (NLML): -750.1541, (RMSE): 0.0148\n",
      "curve dfGP Run 1/1, Epoch 188/2000, Training Loss (NLML): -750.2438, (RMSE): 0.0148\n",
      "curve dfGP Run 1/1, Epoch 189/2000, Training Loss (NLML): -750.3284, (RMSE): 0.0148\n",
      "curve dfGP Run 1/1, Epoch 190/2000, Training Loss (NLML): -750.4532, (RMSE): 0.0147\n",
      "curve dfGP Run 1/1, Epoch 191/2000, Training Loss (NLML): -750.5411, (RMSE): 0.0147\n",
      "curve dfGP Run 1/1, Epoch 192/2000, Training Loss (NLML): -750.4470, (RMSE): 0.0147\n",
      "curve dfGP Run 1/1, Epoch 193/2000, Training Loss (NLML): -750.4431, (RMSE): 0.0147\n",
      "curve dfGP Run 1/1, Epoch 194/2000, Training Loss (NLML): -750.5675, (RMSE): 0.0147\n",
      "curve dfGP Run 1/1, Epoch 195/2000, Training Loss (NLML): -750.4814, (RMSE): 0.0146\n",
      "curve dfGP Run 1/1, Epoch 196/2000, Training Loss (NLML): -750.4397, (RMSE): 0.0146\n",
      "curve dfGP Run 1/1, Epoch 197/2000, Training Loss (NLML): -750.3979, (RMSE): 0.0146\n",
      "curve dfGP Run 1/1, Epoch 198/2000, Training Loss (NLML): -750.2816, (RMSE): 0.0146\n",
      "curve dfGP Run 1/1, Epoch 199/2000, Training Loss (NLML): -750.2712, (RMSE): 0.0146\n",
      "curve dfGP Run 1/1, Epoch 200/2000, Training Loss (NLML): -750.3089, (RMSE): 0.0146\n",
      "curve dfGP Run 1/1, Epoch 201/2000, Training Loss (NLML): -750.2900, (RMSE): 0.0146\n",
      "curve dfGP Run 1/1, Epoch 202/2000, Training Loss (NLML): -750.2531, (RMSE): 0.0146\n",
      "curve dfGP Run 1/1, Epoch 203/2000, Training Loss (NLML): -750.3000, (RMSE): 0.0146\n",
      "curve dfGP Run 1/1, Epoch 204/2000, Training Loss (NLML): -750.3525, (RMSE): 0.0146\n",
      "curve dfGP Run 1/1, Epoch 205/2000, Training Loss (NLML): -750.3932, (RMSE): 0.0146\n",
      "curve dfGP Run 1/1, Epoch 206/2000, Training Loss (NLML): -750.3397, (RMSE): 0.0146\n",
      "curve dfGP Run 1/1, Epoch 207/2000, Training Loss (NLML): -750.3422, (RMSE): 0.0146\n",
      "curve dfGP Run 1/1, Epoch 208/2000, Training Loss (NLML): -750.3730, (RMSE): 0.0146\n",
      "curve dfGP Run 1/1, Epoch 209/2000, Training Loss (NLML): -750.4426, (RMSE): 0.0146\n",
      "curve dfGP Run 1/1, Epoch 210/2000, Training Loss (NLML): -750.4147, (RMSE): 0.0147\n",
      "curve dfGP Run 1/1, Epoch 211/2000, Training Loss (NLML): -750.5111, (RMSE): 0.0147\n",
      "curve dfGP Run 1/1, Epoch 212/2000, Training Loss (NLML): -750.4955, (RMSE): 0.0147\n",
      "curve dfGP Run 1/1, Epoch 213/2000, Training Loss (NLML): -750.5062, (RMSE): 0.0147\n",
      "curve dfGP Run 1/1, Epoch 214/2000, Training Loss (NLML): -750.5046, (RMSE): 0.0147\n",
      "curve dfGP Run 1/1, Epoch 215/2000, Training Loss (NLML): -750.5039, (RMSE): 0.0147\n",
      "curve dfGP Run 1/1, Epoch 216/2000, Training Loss (NLML): -750.5438, (RMSE): 0.0147\n",
      "curve dfGP Run 1/1, Epoch 217/2000, Training Loss (NLML): -750.5875, (RMSE): 0.0147\n",
      "curve dfGP Run 1/1, Epoch 218/2000, Training Loss (NLML): -750.5396, (RMSE): 0.0147\n",
      "curve dfGP Run 1/1, Epoch 219/2000, Training Loss (NLML): -750.4744, (RMSE): 0.0147\n",
      "curve dfGP Run 1/1, Epoch 220/2000, Training Loss (NLML): -750.5453, (RMSE): 0.0147\n",
      "curve dfGP Run 1/1, Epoch 221/2000, Training Loss (NLML): -750.4885, (RMSE): 0.0147\n",
      "curve dfGP Run 1/1, Epoch 222/2000, Training Loss (NLML): -750.5170, (RMSE): 0.0147\n",
      "curve dfGP Run 1/1, Epoch 223/2000, Training Loss (NLML): -750.5247, (RMSE): 0.0147\n",
      "curve dfGP Run 1/1, Epoch 224/2000, Training Loss (NLML): -750.5007, (RMSE): 0.0147\n",
      "curve dfGP Run 1/1, Epoch 225/2000, Training Loss (NLML): -750.5165, (RMSE): 0.0147\n",
      "curve dfGP Run 1/1, Epoch 226/2000, Training Loss (NLML): -750.5189, (RMSE): 0.0147\n",
      "curve dfGP Run 1/1, Epoch 227/2000, Training Loss (NLML): -750.5245, (RMSE): 0.0147\n",
      "curve dfGP Run 1/1, Epoch 228/2000, Training Loss (NLML): -750.4793, (RMSE): 0.0147\n",
      "curve dfGP Run 1/1, Epoch 229/2000, Training Loss (NLML): -750.4795, (RMSE): 0.0147\n",
      "curve dfGP Run 1/1, Epoch 230/2000, Training Loss (NLML): -750.4698, (RMSE): 0.0147\n",
      "curve dfGP Run 1/1, Epoch 231/2000, Training Loss (NLML): -750.5449, (RMSE): 0.0147\n",
      "curve dfGP Run 1/1, Epoch 232/2000, Training Loss (NLML): -750.5668, (RMSE): 0.0147\n",
      "curve dfGP Run 1/1, Epoch 233/2000, Training Loss (NLML): -750.4926, (RMSE): 0.0147\n",
      "curve dfGP Run 1/1, Epoch 234/2000, Training Loss (NLML): -750.4294, (RMSE): 0.0147\n",
      "curve dfGP Run 1/1, Epoch 235/2000, Training Loss (NLML): -750.5319, (RMSE): 0.0147\n",
      "curve dfGP Run 1/1, Epoch 236/2000, Training Loss (NLML): -750.5382, (RMSE): 0.0147\n",
      "curve dfGP Run 1/1, Epoch 237/2000, Training Loss (NLML): -750.5367, (RMSE): 0.0147\n",
      "curve dfGP Run 1/1, Epoch 238/2000, Training Loss (NLML): -750.5367, (RMSE): 0.0147\n",
      "curve dfGP Run 1/1, Epoch 239/2000, Training Loss (NLML): -750.4930, (RMSE): 0.0147\n",
      "curve dfGP Run 1/1, Epoch 240/2000, Training Loss (NLML): -750.5107, (RMSE): 0.0147\n",
      "curve dfGP Run 1/1, Epoch 241/2000, Training Loss (NLML): -750.5219, (RMSE): 0.0147\n",
      "curve dfGP Run 1/1, Epoch 242/2000, Training Loss (NLML): -750.5251, (RMSE): 0.0147\n",
      "curve dfGP Run 1/1, Epoch 243/2000, Training Loss (NLML): -750.5059, (RMSE): 0.0147\n",
      "curve dfGP Run 1/1, Epoch 244/2000, Training Loss (NLML): -750.5161, (RMSE): 0.0147\n",
      "curve dfGP Run 1/1, Epoch 245/2000, Training Loss (NLML): -750.5101, (RMSE): 0.0147\n",
      "curve dfGP Run 1/1, Epoch 246/2000, Training Loss (NLML): -750.5472, (RMSE): 0.0147\n",
      "curve dfGP Run 1/1, Epoch 247/2000, Training Loss (NLML): -750.5505, (RMSE): 0.0147\n",
      "curve dfGP Run 1/1, Epoch 248/2000, Training Loss (NLML): -750.5372, (RMSE): 0.0147\n",
      "curve dfGP Run 1/1, Epoch 249/2000, Training Loss (NLML): -750.5163, (RMSE): 0.0147\n",
      "curve dfGP Run 1/1, Epoch 250/2000, Training Loss (NLML): -750.5739, (RMSE): 0.0147\n",
      "curve dfGP Run 1/1, Epoch 251/2000, Training Loss (NLML): -750.4943, (RMSE): 0.0147\n",
      "curve dfGP Run 1/1, Epoch 252/2000, Training Loss (NLML): -750.5210, (RMSE): 0.0147\n",
      "curve dfGP Run 1/1, Epoch 253/2000, Training Loss (NLML): -750.4838, (RMSE): 0.0147\n",
      "curve dfGP Run 1/1, Epoch 254/2000, Training Loss (NLML): -750.4867, (RMSE): 0.0147\n",
      "curve dfGP Run 1/1, Epoch 255/2000, Training Loss (NLML): -750.5430, (RMSE): 0.0147\n",
      "curve dfGP Run 1/1, Epoch 256/2000, Training Loss (NLML): -750.5580, (RMSE): 0.0147\n",
      "curve dfGP Run 1/1, Epoch 257/2000, Training Loss (NLML): -750.5585, (RMSE): 0.0147\n",
      "curve dfGP Run 1/1, Epoch 258/2000, Training Loss (NLML): -750.5095, (RMSE): 0.0147\n",
      "curve dfGP Run 1/1, Epoch 259/2000, Training Loss (NLML): -750.5314, (RMSE): 0.0147\n",
      "curve dfGP Run 1/1, Epoch 260/2000, Training Loss (NLML): -750.5386, (RMSE): 0.0147\n",
      "curve dfGP Run 1/1, Epoch 261/2000, Training Loss (NLML): -750.5373, (RMSE): 0.0147\n",
      "curve dfGP Run 1/1, Epoch 262/2000, Training Loss (NLML): -750.5455, (RMSE): 0.0147\n",
      "curve dfGP Run 1/1, Epoch 263/2000, Training Loss (NLML): -750.5420, (RMSE): 0.0147\n",
      "curve dfGP Run 1/1, Epoch 264/2000, Training Loss (NLML): -750.4953, (RMSE): 0.0147\n",
      "curve dfGP Run 1/1, Epoch 265/2000, Training Loss (NLML): -750.5403, (RMSE): 0.0147\n",
      "curve dfGP Run 1/1, Epoch 266/2000, Training Loss (NLML): -750.4604, (RMSE): 0.0147\n",
      "curve dfGP Run 1/1, Epoch 267/2000, Training Loss (NLML): -750.5620, (RMSE): 0.0147\n",
      "Early stopping triggered after 267 epochs.\n",
      "tensor(0.)\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "# SIMULATED DATA EXPERIMENTS\n",
    "# # RUN WITH python run_sim_experiments_dfGP.py\n",
    "# \n",
    "#       ooooooooooooooooooooooooooooooooooooo\n",
    "#      8                                .d88\n",
    "#      8  oooooooooooooooooooooooooooood8888\n",
    "#      8  8888888888888888888888888P\"   8888    oooooooooooooooo\n",
    "#      8  8888888888888888888888P\"      8888    8              8\n",
    "#      8  8888888888888888888P\"         8888    8             d8\n",
    "#      8  8888888888888888P\"            8888    8            d88\n",
    "#      8  8888888888888P\"               8888    8           d888\n",
    "#      8  8888888888P\"                  8888    8          d8888\n",
    "#      8  8888888P\"                     8888    8         d88888\n",
    "#      8  8888P\"                        8888    8        d888888\n",
    "#      8  8888oooooooooooooooooooooocgmm8888    8       d8888888\n",
    "#      8 .od88888888888888888888888888888888    8      d88888888\n",
    "#      8888888888888888888888888888888888888    8     d888888888\n",
    "#                                               8    d8888888888\n",
    "#         ooooooooooooooooooooooooooooooo       8   d88888888888\n",
    "#        d                       ...oood8b      8  d888888888888\n",
    "#       d              ...oood888888888888b     8 d8888888888888\n",
    "#      d     ...oood88888888888888888888888b    8d88888888888888\n",
    "#     dood8888888888888888888888888888888888b\n",
    "#\n",
    "#\n",
    "# This artwork is a visual reminder that this script is for the sim experiments.\n",
    "\n",
    "model_name = \"dfGP\"\n",
    "\n",
    "# import configs to we can access the hypers with getattr\n",
    "import configs\n",
    "from configs import PATIENCE, MAX_NUM_EPOCHS, NUM_RUNS, WEIGHT_DECAY\n",
    "# also import x_test grid size and std noise for training data\n",
    "from configs import N_SIDE, STD_GAUSSIAN_NOISE\n",
    "from configs import TRACK_EMISSIONS_BOOL\n",
    "\n",
    "# Reiterating import for visibility\n",
    "MAX_NUM_EPOCHS = MAX_NUM_EPOCHS\n",
    "NUM_RUNS = 1\n",
    "WEIGHT_DECAY = WEIGHT_DECAY\n",
    "PATIENCE = PATIENCE\n",
    "\n",
    "# assign model-specific variable\n",
    "MODEL_LEARNING_RATE = getattr(configs, f\"{model_name}_SIM_LEARNING_RATE\")\n",
    "MODEL_SIM_RESULTS_DIR = getattr(configs, f\"{model_name}_SIM_RESULTS_DIR\")\n",
    "import os\n",
    "os.makedirs(MODEL_SIM_RESULTS_DIR, exist_ok = True)\n",
    "\n",
    "# imports for probabilistic models\n",
    "if model_name in [\"GP\", \"dfGP\", \"dfNGP\"]:\n",
    "    from GP_models import GP_predict\n",
    "    from metrics import compute_NLL_sparse, compute_NLL_full\n",
    "    from configs import L_RANGE, SIGMA_N_RANGE, GP_PATIENCE\n",
    "    # overwrite with GP_PATIENCE\n",
    "    PATIENCE = GP_PATIENCE\n",
    "\n",
    "    if model_name in [\"dfGP\", \"dfNGP\"]:\n",
    "        from configs import SIGMA_F_RANGE\n",
    "\n",
    "# universals \n",
    "from metrics import compute_RMSE, compute_MAE, compute_divergence_field\n",
    "\n",
    "# basics\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn # NOTE: we also use this module for GP params\n",
    "import torch.optim as optim\n",
    "import gpytorch\n",
    "\n",
    "# utilitarian\n",
    "from utils import set_seed, make_grid\n",
    "# reproducibility\n",
    "set_seed(42)\n",
    "import gc\n",
    "\n",
    "# setting device to GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# overwrite if needed: # device = 'cpu'\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "### START TIMING ###\n",
    "import time\n",
    "start_time = time.time()  # Start timing after imports\n",
    "\n",
    "### START TRACKING EXPERIMENT EMISSIONS ###\n",
    "if TRACK_EMISSIONS_BOOL:\n",
    "    from codecarbon import EmissionsTracker\n",
    "    tracker = EmissionsTracker(project_name = \"dfGP_simulation_experiments\", output_dir = MODEL_SIM_RESULTS_DIR)\n",
    "    tracker.start()\n",
    "\n",
    "### SIMULATION ###\n",
    "# Import all simulation functions\n",
    "from simulate import (\n",
    "    simulate_detailed_branching,\n",
    "    # simulate_detailed_convergence,\n",
    "    simulate_detailed_curve,\n",
    "    simulate_detailed_deflection,\n",
    "    simulate_detailed_edge,\n",
    "    simulate_detailed_ridges,\n",
    ")\n",
    "\n",
    "# Define simulations as a dictionary with names as keys to function objects\n",
    "# alphabectic order here\n",
    "simulations = {\n",
    "    \"curve\": simulate_detailed_curve,\n",
    "}\n",
    "\n",
    "########################\n",
    "### x_train & x_test ###\n",
    "########################\n",
    "\n",
    "# Load training inputs (once for all simulations)\n",
    "x_train = torch.load(\"data/sim_data/x_train_lines_discretised_0to1.pt\", weights_only = False).float()\n",
    "\n",
    "# Generate x_test (long) once for all simulations\n",
    "_, x_test = make_grid(N_SIDE)\n",
    "# x_test is long format (N_SIDE ** 2, 2)\n",
    "\n",
    "#################################\n",
    "### LOOP 1 - over SIMULATIONS ###\n",
    "#################################\n",
    "\n",
    "# Make y_train_dict: Iterate over all simulation functions\n",
    "for sim_name, sim_func in simulations.items():\n",
    "\n",
    "    ########################\n",
    "    ### y_train & y_test ###\n",
    "    ########################\n",
    "\n",
    "    # Generate training observations\n",
    "    # NOTE: sim_func() needs to be on CPU, so we move x_train to CPU\n",
    "    y_train = sim_func(x_train.cpu()).to(device)\n",
    "    y_test = sim_func(x_test.cpu()).to(device)\n",
    "    \n",
    "    x_test = x_test.to(device)\n",
    "    x_train = x_train.to(device)\n",
    "\n",
    "    # Print details\n",
    "    print(f\"=== {sim_name.upper()} ===\")\n",
    "    print(f\"Training inputs shape: {x_train.shape}\")\n",
    "    print(f\"Training observations shape: {y_train.shape}\")\n",
    "    print(f\"Training inputs dtype: {x_train.dtype}\")\n",
    "    print(f\"Training inputs device: {y_train.device}\")\n",
    "    print(f\"Training observations device: {y_train.device}\")\n",
    "    print()\n",
    "\n",
    "    # Print details\n",
    "    print(f\"=== {sim_name.upper()} ===\")\n",
    "    print(f\"Test inputs shape: {x_test.shape}\")\n",
    "    print(f\"Test observations shape: {y_test.shape}\")\n",
    "    print(f\"Test inputs dtype: {x_test.dtype}\")\n",
    "    print(f\"Test inputs device: {x_test.device}\")\n",
    "    print(f\"Test observations device: {y_test.device}\")\n",
    "    print()\n",
    "\n",
    "    # NOTE: This is different to the real data experiments\n",
    "    # calculate the mean magnitude of the test data as we use this to scale the noise\n",
    "    sim_mean_magnitude_for_noise = torch.norm(y_test, dim = -1).mean().to(device)\n",
    "    sim_noise = STD_GAUSSIAN_NOISE * sim_mean_magnitude_for_noise\n",
    "\n",
    "    # Store metrics for the simulation (used for *metrics_summary* report and *metrics_per_run*)\n",
    "    simulation_results = [] \n",
    "\n",
    "    ##################################\n",
    "    ### LOOP 2 - over training run ###\n",
    "    ##################################\n",
    "\n",
    "    # NOTE: GPs don't train on batches, use full data\n",
    "\n",
    "    for run in range(NUM_RUNS):\n",
    "\n",
    "        print(f\"\\n--- Training Run {run + 1}/{NUM_RUNS} ---\")\n",
    "\n",
    "        ### Initialise GP hyperparameters ###\n",
    "        # 3 learnable HPs\n",
    "        # NOTE: at every run this initialisation changes, introducing some randomness\n",
    "        # HACK: we need to use nn.Parameter for trainable hypers to avoid leaf variable error\n",
    "\n",
    "        # initialising (trainable) noise scalar from a uniform distribution over a predefined range\n",
    "        sigma_n = nn.Parameter(torch.empty(1, device = device).uniform_( * SIGMA_N_RANGE)) # Trainable\n",
    "\n",
    "        # initialising (trainable) output scalar from a uniform distribution over a predefined range\n",
    "        sigma_f = nn.Parameter(torch.empty(1, device = device).uniform_( * SIGMA_F_RANGE))\n",
    "\n",
    "        # initialising (trainable) lengthscales from a uniform distribution over a predefined range\n",
    "        # each dimension has its own lengthscale\n",
    "        l = nn.Parameter(torch.empty(2, device = device).uniform_( * L_RANGE))\n",
    "\n",
    "        # AdamW as optimizer for some regularisation/weight decay\n",
    "        optimizer = optim.AdamW([sigma_n, sigma_f, l], lr = MODEL_LEARNING_RATE, weight_decay = WEIGHT_DECAY)\n",
    "        # NOTE: No need to initialise GP model like we initialise a NN model in torch\n",
    "        \n",
    "        # _________________\n",
    "        # BEFORE EPOCH LOOP\n",
    "\n",
    "        # Export the convergence just for first run only\n",
    "        if run == 0:\n",
    "            # initialise tensors to store losses over epochs (for convergence plot)\n",
    "            train_losses_NLML_over_epochs = torch.zeros(MAX_NUM_EPOCHS) # objective\n",
    "            train_losses_RMSE_over_epochs = torch.zeros(MAX_NUM_EPOCHS) # by-product\n",
    "            # monitor performance transfer to test (only RMSE easy to calc without covar)\n",
    "            test_losses_RMSE_over_epochs = torch.zeros(MAX_NUM_EPOCHS)\n",
    "\n",
    "            sigma_n_over_epochs = torch.zeros(MAX_NUM_EPOCHS)\n",
    "            sigma_f_over_epochs = torch.zeros(MAX_NUM_EPOCHS)\n",
    "            l1_over_epochs = torch.zeros(MAX_NUM_EPOCHS)\n",
    "            l2_over_epochs = torch.zeros(MAX_NUM_EPOCHS)\n",
    "\n",
    "        # Early stopping variables\n",
    "        best_loss = float('inf')\n",
    "        # counter starts at 0\n",
    "        epochs_no_improve = 0\n",
    "\n",
    "        # Additive noise model: independent Gaussian noise\n",
    "        # For every run we have a FIXED NOISY TARGET. Draw from standard normal with appropriate std\n",
    "        y_train_noisy = y_train + (torch.randn(y_train.shape, device = device) * sim_noise)\n",
    "\n",
    "        ############################\n",
    "        ### LOOP 3 - over EPOCHS ###\n",
    "        ############################\n",
    "\n",
    "        print(\"\\nStart Training\")\n",
    "\n",
    "        for epoch in range(MAX_NUM_EPOCHS):\n",
    "\n",
    "            # For Run 1 we save a bunch of metrics and update, while for the rest we only update\n",
    "            if run == 0:\n",
    "                mean_pred_train, _, lml_train = GP_predict(\n",
    "                        x_train,\n",
    "                        y_train_noisy,\n",
    "                        x_train, # predict training data\n",
    "                        [sigma_n, sigma_f, l], # list of (initial) hypers\n",
    "                        mean_func = None, # no mean aka \"zero-mean function\"\n",
    "                        divergence_free_bool = True) # ensures we use a df kernel\n",
    "\n",
    "                # Compute test loss for loss convergence plot\n",
    "                mean_pred_test, _, _ = GP_predict(\n",
    "                        x_train,\n",
    "                        y_train_noisy,\n",
    "                        x_test.to(device), # have predictions for training data again\n",
    "                        # HACK: This is rather an eval, so we use detached hypers to avoid the computational tree\n",
    "                        [sigma_n.detach().clone(), sigma_f.detach().clone(), l.detach().clone()], \n",
    "                        mean_func = None, # no mean aka \"zero-mean function\"\n",
    "                        divergence_free_bool = True) # ensures we use a df kernel\n",
    "                \n",
    "                # UPDATE HYPERS (after test loss is computed to use same model)\n",
    "                optimizer.zero_grad() # don't accumulate gradients\n",
    "                # negative for NLML. loss is always on train\n",
    "                loss = - lml_train\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # NOTE: it is important to detach here \n",
    "                train_RMSE = compute_RMSE(y_train.detach(), mean_pred_train.detach())\n",
    "                test_RMSE = compute_RMSE(y_test.detach(), mean_pred_test.detach())\n",
    "\n",
    "                # Save losses for convergence plot\n",
    "                train_losses_NLML_over_epochs[epoch] = - lml_train\n",
    "                train_losses_RMSE_over_epochs[epoch] = train_RMSE\n",
    "                # NOTE: lml is always just given training data. There is no TEST NLML\n",
    "                test_losses_RMSE_over_epochs[epoch] = test_RMSE\n",
    "\n",
    "                # Save evolution of hyprs for convergence plot\n",
    "                sigma_n_over_epochs[epoch] = sigma_n[0]\n",
    "                sigma_f_over_epochs[epoch] = sigma_f[0]\n",
    "                l1_over_epochs[epoch] = l[0]\n",
    "                l2_over_epochs[epoch] = l[1]\n",
    "\n",
    "                print(f\"{sim_name} {model_name} Run {run + 1}/{NUM_RUNS}, Epoch {epoch + 1}/{MAX_NUM_EPOCHS}, Training Loss (NLML): {loss:.4f}, (RMSE): {train_RMSE:.4f}\")\n",
    "\n",
    "                # delete after printing and saving\n",
    "                # NOTE: keep loss for early stopping check\n",
    "                del mean_pred_train, mean_pred_test, lml_train, train_RMSE, test_RMSE\n",
    "                \n",
    "                # Free up memory every 20 epochs\n",
    "                if epoch % 20 == 0:\n",
    "                    gc.collect() and torch.cuda.empty_cache()\n",
    "            \n",
    "            # For all runs after the first we run a minimal version using only lml_train\n",
    "            else:\n",
    "                \n",
    "                # NOTE: We can use x_train[0:2] since the predictions doesn;t matter and we only care about lml_train\n",
    "                _, _, lml_train = GP_predict(\n",
    "                        x_train,\n",
    "                        y_train_noisy,\n",
    "                        x_train[0:2], # predictions don't matter and we output lml_train already\n",
    "                        [sigma_n, sigma_f, l], # list of (initial) hypers\n",
    "                        mean_func = None, # no mean aka \"zero-mean function\"\n",
    "                        divergence_free_bool = True) # ensures we use a df kernel\n",
    "                \n",
    "                # UPDATE HYPERS (after test loss is computed to use same model)\n",
    "                optimizer.zero_grad() # don't accumulate gradients\n",
    "                # negative for NLML\n",
    "                loss = - lml_train\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # After run 1 we only print lml, nothing else\n",
    "                print(f\"{sim_name} {model_name} Run {run + 1}/{NUM_RUNS}, Epoch {epoch + 1}/{MAX_NUM_EPOCHS}, Training Loss (NLML): {loss:.4f}\")\n",
    "\n",
    "                # NOTE: keep loss for early stopping check, del lml_train\n",
    "                del lml_train\n",
    "                \n",
    "                # Free up memory every 20 epochs\n",
    "                if epoch % 20 == 0:\n",
    "                    gc.collect() and torch.cuda.empty_cache()\n",
    "                \n",
    "            # EVERY EPOCH: Early stopping check\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                # reset counter if loss improves\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "\n",
    "            if epochs_no_improve >= PATIENCE:\n",
    "                print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
    "                # exit epoch loop\n",
    "                break\n",
    "\n",
    "        ##############################\n",
    "        ### END LOOP 3 over EPOCHS ###\n",
    "        ##############################\n",
    "\n",
    "        # for every run...\n",
    "        #######################################################\n",
    "        ### EVALUATE after all training for RUN is finished ###\n",
    "        #######################################################\n",
    "\n",
    "        # Evaluate the trained model after all epochs are finished or early stopping was triggered\n",
    "        # NOTE: Detach tuned hyperparameters from the computational graph\n",
    "        best_sigma_n = sigma_n.detach().clone()\n",
    "        best_sigma_f = sigma_f.detach().clone()\n",
    "        best_l = l.detach().clone()\n",
    "\n",
    "        # Need gradients for autograd divergence: We clone and detach\n",
    "        x_test_grad = x_test.to(device).clone().requires_grad_(True)\n",
    "\n",
    "        mean_pred_test, covar_pred_test, _ = GP_predict(\n",
    "            x_train,\n",
    "            y_train, # NOTE: use original y_train, not y_train_noisy\n",
    "            x_test_grad,\n",
    "            [best_sigma_n, best_sigma_f, best_l], # list of (initial) hypers\n",
    "            mean_func = None, # no mean aka \"zero-mean function\"\n",
    "            divergence_free_bool = True) # ensures we use a df kernel\n",
    "        \n",
    "        # Compute divergence field\n",
    "        dfGP_test_div_field = compute_divergence_field(mean_pred_test, x_test_grad)\n",
    "\n",
    "        # Only save mean_pred, covar_pred and divergence fields for the first run\n",
    "        if run == 0:\n",
    "\n",
    "            # (1) Save predictions from first run so we can visualise them later\n",
    "            torch.save(mean_pred_test, f\"{MODEL_SIM_RESULTS_DIR}/{sim_name}_{model_name}_test_mean_predictions.pt\")\n",
    "            torch.save(covar_pred_test, f\"{MODEL_SIM_RESULTS_DIR}/{sim_name}_{model_name}_test_covar_predictions.pt\")\n",
    "\n",
    "            # (2) Save best hyperparameters\n",
    "            # Stack tensors into a single tensor\n",
    "            best_hypers_tensor = torch.cat([\n",
    "                best_sigma_n.reshape(-1),  # Ensure 1D shape\n",
    "                best_sigma_f.reshape(-1),\n",
    "                best_l.reshape(-1),\n",
    "            ])\n",
    "\n",
    "            torch.save(best_hypers_tensor, f\"{MODEL_SIM_RESULTS_DIR}/{sim_name}_{model_name}_best_hypers.pt\")\n",
    "\n",
    "            # (3) Since all epoch training is finished, we can save the losses over epochs\n",
    "            df_losses = pd.DataFrame({\n",
    "                'Epoch': list(range(train_losses_NLML_over_epochs.shape[0])), # pythonic indexing\n",
    "                'Train Loss NLML': train_losses_NLML_over_epochs.tolist(),\n",
    "                'Train Loss RMSE': train_losses_RMSE_over_epochs.tolist(),\n",
    "                'Test Loss RMSE': test_losses_RMSE_over_epochs.tolist(),\n",
    "                'Sigma_n': sigma_n_over_epochs.tolist(),\n",
    "                'Sigma_f': sigma_f_over_epochs.tolist(),\n",
    "                'l1': l1_over_epochs.tolist(),\n",
    "                'l2': l2_over_epochs.tolist()\n",
    "                })\n",
    "            \n",
    "            df_losses.to_csv(f\"{MODEL_SIM_RESULTS_DIR}/{sim_name}_{model_name}_losses_over_epochs.csv\", index = False, float_format = \"%.5f\") # reduce to 5 decimals for readability\n",
    "\n",
    "            # (4) Save divergence field (computed above for all runs)\n",
    "            torch.save(dfGP_test_div_field, f\"{MODEL_SIM_RESULTS_DIR}/{sim_name}_{model_name}_test_prediction_divergence_field.pt\")\n",
    "\n",
    "        x_train_grad = x_train.to(device).clone().requires_grad_(True)\n",
    "\n",
    "        mean_pred_train, covar_pred_train, _ = GP_predict(\n",
    "                     x_train,\n",
    "                     y_train, # NOTE: use original y_train, not y_train_noisy\n",
    "                     x_train_grad,\n",
    "                     [best_sigma_n, best_sigma_f, best_l], # list of (initial) hypers\n",
    "                     mean_func = None, # no mean aka \"zero-mean function\"\n",
    "                     divergence_free_bool = True) # ensures we use a df kernel\n",
    "        \n",
    "        dfGP_train_div_field = compute_divergence_field(mean_pred_train, x_train_grad)\n",
    "\n",
    "        # Divergence: Convert field to metric: mean absolute divergence\n",
    "        # NOTE: It is important to use the absolute value of the divergence field, since positive and negative deviations are violations and shouldn't cancel each other out \n",
    "        dfGP_train_div = dfGP_train_div_field.abs().mean().item()\n",
    "        dfGP_test_div = dfGP_test_div_field.abs().mean().item()\n",
    "\n",
    "        # Compute metrics (convert tensors to float) for every run's tuned model\n",
    "        dfGP_train_RMSE = compute_RMSE(y_train, mean_pred_train).item()\n",
    "        dfGP_train_MAE = compute_MAE(y_train, mean_pred_train).item()\n",
    "        dfGP_train_sparse_NLL = compute_NLL_sparse(y_train, mean_pred_train, covar_pred_train).item()\n",
    "        dfGP_train_full_NLL, dfGP_train_jitter = compute_NLL_full(y_train, mean_pred_train, covar_pred_train)\n",
    "        # quantile coverage error\n",
    "        pred_dist_train = gpytorch.distributions.MultivariateNormal(mean_pred_train.T.reshape(-1), covar_pred_train)\n",
    "        dfGP_train_QCE = gpytorch.metrics.quantile_coverage_error(pred_dist_train, y_train.T.reshape(-1), quantile = 95).item()\n",
    "\n",
    "        dfGP_test_RMSE = compute_RMSE(y_test, mean_pred_test).item()\n",
    "        dfGP_test_MAE = compute_MAE(y_test, mean_pred_test).item()\n",
    "        dfGP_test_sparse_NLL = compute_NLL_sparse(y_test, mean_pred_test, covar_pred_test).item()\n",
    "        dfGP_test_full_NLL, dfGP_test_jitter = compute_NLL_full(y_test, mean_pred_test, covar_pred_test)\n",
    "        # quantile coverage error\n",
    "        pred_dist_test = gpytorch.distributions.MultivariateNormal(mean_pred_test.T.reshape(-1), covar_pred_test)\n",
    "        dfGP_test_QCE = gpytorch.metrics.quantile_coverage_error(pred_dist_test, y_test.T.reshape(-1), quantile = 95).item()\n",
    "\n",
    "        print(dfGP_train_jitter)\n",
    "        print(dfGP_test_jitter)\n",
    "\n",
    "        simulation_results.append([\n",
    "            run + 1,\n",
    "            dfGP_train_RMSE, dfGP_train_MAE, dfGP_train_sparse_NLL, dfGP_train_full_NLL.item(), dfGP_train_jitter.item(), dfGP_train_QCE, dfGP_train_div,\n",
    "            dfGP_test_RMSE, dfGP_test_MAE, dfGP_test_sparse_NLL, dfGP_test_full_NLL.item(), dfGP_test_jitter.item(), dfGP_test_QCE, dfGP_test_div\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f5334a",
   "metadata": {},
   "source": [
    "# GPytorch version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "aedcbd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpytorch\n",
    "from gpytorch.kernels import Kernel\n",
    "\n",
    "class DivergenceFreeSEKernel(Kernel):\n",
    "    \"\"\"\n",
    "    Divergence-free squared exponential (SE) kernel in 2D.\n",
    "\n",
    "    Returns a (2n x 2m) matrix for 2D vector fields, ensuring divergence-free structure.\n",
    "    \"\"\"\n",
    "    # This handles the lengthscale directly\n",
    "    # has_lengthscale = True\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # Register the outputscale / variance (sigma_f) as a learnable parameter\n",
    "        self.register_parameter(name = \"raw_outputscale\", \n",
    "                                parameter = torch.nn.Parameter(torch.tensor(0.0)))\n",
    "        \n",
    "        self.register_parameter(name = \"raw_lengthscale\",\n",
    "                                parameter = torch.nn.Parameter(torch.tensor([0.0, 0.0])))\n",
    "\n",
    "        # Register transform for positivity (softplus)\n",
    "        self.register_constraint(\"raw_outputscale\", gpytorch.constraints.Positive())\n",
    "\n",
    "        self.register_constraint(\"raw_lengthscale\", gpytorch.constraints.Positive())\n",
    "\n",
    "    @property\n",
    "    def outputscale(self):\n",
    "        return self.raw_outputscale_constraint.transform(self.raw_outputscale)\n",
    "    \n",
    "    @property\n",
    "    def lengthscale(self):\n",
    "        return self.raw_lengthscale_constraint.transform(self.raw_lengthscale)\n",
    "\n",
    "    @outputscale.setter\n",
    "    def outputscale(self, value):\n",
    "        self.initialize(raw_outputscale = self.raw_outputscale_constraint.inverse_transform(value))\n",
    "\n",
    "    @lengthscale.setter\n",
    "    def lengthscale(self, value):\n",
    "        self.initialize(raw_lengthscale = self.raw_lengthscale_constraint.inverse_transform(value))\n",
    "\n",
    "    def forward(self, x1, x2, diag = False, **params):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x1: torch.Size([2N, 1]) flattened, second explicit dim is automatic\n",
    "            x2: torch.Size([2M, 1])\n",
    "        Returns:\n",
    "            K: torch.Size([2N, 2M])\n",
    "        \"\"\"\n",
    "        mid_x1 = x1.shape[0] // 2\n",
    "        mid_x2 = x2.shape[0] // 2 \n",
    "\n",
    "        # torch.Size([N, 2])\n",
    "        x1 = torch.cat((x1[:mid_x1], x1[mid_x1:]), dim = 1).to(x1.device)\n",
    "        # torch.Size([M, 2])\n",
    "        x2 = torch.cat((x2[:mid_x2], x2[mid_x2:]), dim = 1).to(x2.device)\n",
    "\n",
    "        # Use the registered lengthscale parameter\n",
    "        # GPyTorch automatically expands it to shape [1, 2] if needed\n",
    "        l = self.lengthscale.squeeze().to(x1.device)  # Shape (2,)\n",
    "\n",
    "        lx1, lx2 = l[0].to(x1.device), l[1].to(x1.device)\n",
    "\n",
    "        sigma_f = self.outputscale\n",
    "\n",
    "        # Broadcast pairwise differences: shape [N, M, 2]\n",
    "        diff = (x1[:, None, :] - x2[None, :, :]).to(x1.device)\n",
    "\n",
    "        ### 2x2 block components ###\n",
    "        upper_left = (1 - diff[:, :, 1].square() / lx2.square()) / lx2.square()\n",
    "        lower_right = (1 - diff[:, :, 0].square() / lx1.square()) / lx1.square()\n",
    "        upper_right = (diff[:, :, 0] * diff[:, :, 1]) / (lx1.square() * lx2.square())\n",
    "        lower_left = upper_right\n",
    "\n",
    "        # Block matrix assembly\n",
    "        top = torch.cat((upper_left, upper_right), dim = 1)\n",
    "        bottom = torch.cat((lower_left, lower_right), dim = 1)\n",
    "        blocks = torch.cat((top, bottom), dim = 0)\n",
    "\n",
    "        # RBF/SE envelope (elementwise)\n",
    "        exp_term = torch.exp(-0.5 * (diff.square() / l.square()).sum(dim = -1))\n",
    "        # .tile(2, 2) forms (N, M) -> (2N, 2M) for the 2D vector field\n",
    "        K = sigma_f.square() * blocks * exp_term.tile(2, 2)\n",
    "\n",
    "        # Add this for Quantile Coverage Error (QCE) calculation\n",
    "        if diag:\n",
    "        # Return only the diagonal as a 1D tensor\n",
    "            return K.diag()\n",
    "\n",
    "        return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "e323ccd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DivergenceFreeGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "\n",
    "        self.mean_module = gpytorch.means.ZeroMean()\n",
    "        self.covar_module = DivergenceFreeSEKernel()\n",
    "        \n",
    "        # initialize hyperparameters by sampling from a uniform distribution over predefined ranges\n",
    "        # (raw_noise_constraint): GreaterThan(1.000E-04)\n",
    "        self.likelihood.noise = torch.empty(1, device = device).uniform_( * SIGMA_N_RANGE)\n",
    "        self.covar_module.outputscale = torch.empty(1, device = device).uniform_( * SIGMA_F_RANGE)\n",
    "        self.covar_module.lengthscale = torch.empty(2, device = device).uniform_( * L_RANGE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "72cc4591",
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = gpytorch.likelihoods.GaussianLikelihood().to(device)\n",
    "model = DivergenceFreeGPModel(x_train.T.reshape(-1), y_train_noisy.T.reshape(-1), likelihood).to(device)\n",
    "\n",
    "# Use ExactMarginalLogLikelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9d4608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D to 1D block\n",
    "# T.reshape(-1)\n",
    "# 1D block to 2D \n",
    "# .reshape(2, -1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "b819de52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "=== CURVE ===\n",
      "Training inputs shape: torch.Size([196, 2])\n",
      "Training observations shape: torch.Size([196, 2])\n",
      "Training inputs dtype: torch.float32\n",
      "Training inputs device: cuda:0\n",
      "Training observations device: cuda:0\n",
      "\n",
      "=== CURVE ===\n",
      "Test inputs shape: torch.Size([400, 2])\n",
      "Test observations shape: torch.Size([400, 2])\n",
      "Test inputs dtype: torch.float32\n",
      "Test inputs device: cuda:0\n",
      "Test observations device: cuda:0\n",
      "\n",
      "\n",
      "--- Training Run 1/1 ---\n",
      "\n",
      "Start Training\n",
      "curve dfGP Run 1/1, Epoch 1/2000, Training Loss (NLML): -0.1343, (RMSE): 0.1031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kim/anaconda3/lib/python3.11/site-packages/gpytorch/models/exact_gp.py:284: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curve dfGP Run 1/1, Epoch 2/2000, Training Loss (NLML): -0.1383, (RMSE): 0.1027\n",
      "curve dfGP Run 1/1, Epoch 3/2000, Training Loss (NLML): -0.1422, (RMSE): 0.1022\n",
      "curve dfGP Run 1/1, Epoch 4/2000, Training Loss (NLML): -0.1462, (RMSE): 0.1018\n",
      "curve dfGP Run 1/1, Epoch 5/2000, Training Loss (NLML): -0.1502, (RMSE): 0.1013\n",
      "curve dfGP Run 1/1, Epoch 6/2000, Training Loss (NLML): -0.1542, (RMSE): 0.1007\n",
      "curve dfGP Run 1/1, Epoch 7/2000, Training Loss (NLML): -0.1581, (RMSE): 0.1001\n",
      "curve dfGP Run 1/1, Epoch 8/2000, Training Loss (NLML): -0.1621, (RMSE): 0.0995\n",
      "curve dfGP Run 1/1, Epoch 9/2000, Training Loss (NLML): -0.1660, (RMSE): 0.0987\n",
      "curve dfGP Run 1/1, Epoch 10/2000, Training Loss (NLML): -0.1700, (RMSE): 0.0979\n",
      "curve dfGP Run 1/1, Epoch 11/2000, Training Loss (NLML): -0.1740, (RMSE): 0.0970\n",
      "curve dfGP Run 1/1, Epoch 12/2000, Training Loss (NLML): -0.1781, (RMSE): 0.0960\n",
      "curve dfGP Run 1/1, Epoch 13/2000, Training Loss (NLML): -0.1821, (RMSE): 0.0948\n",
      "curve dfGP Run 1/1, Epoch 14/2000, Training Loss (NLML): -0.1863, (RMSE): 0.0936\n",
      "curve dfGP Run 1/1, Epoch 15/2000, Training Loss (NLML): -0.1904, (RMSE): 0.0924\n",
      "curve dfGP Run 1/1, Epoch 16/2000, Training Loss (NLML): -0.1946, (RMSE): 0.0910\n",
      "curve dfGP Run 1/1, Epoch 17/2000, Training Loss (NLML): -0.1988, (RMSE): 0.0896\n",
      "curve dfGP Run 1/1, Epoch 18/2000, Training Loss (NLML): -0.2031, (RMSE): 0.0881\n",
      "curve dfGP Run 1/1, Epoch 19/2000, Training Loss (NLML): -0.2074, (RMSE): 0.0866\n",
      "curve dfGP Run 1/1, Epoch 20/2000, Training Loss (NLML): -0.2117, (RMSE): 0.0850\n",
      "curve dfGP Run 1/1, Epoch 21/2000, Training Loss (NLML): -0.2161, (RMSE): 0.0833\n",
      "curve dfGP Run 1/1, Epoch 22/2000, Training Loss (NLML): -0.2205, (RMSE): 0.0817\n",
      "curve dfGP Run 1/1, Epoch 23/2000, Training Loss (NLML): -0.2250, (RMSE): 0.0800\n",
      "curve dfGP Run 1/1, Epoch 24/2000, Training Loss (NLML): -0.2294, (RMSE): 0.0783\n",
      "curve dfGP Run 1/1, Epoch 25/2000, Training Loss (NLML): -0.2339, (RMSE): 0.0766\n",
      "curve dfGP Run 1/1, Epoch 26/2000, Training Loss (NLML): -0.2384, (RMSE): 0.0750\n",
      "curve dfGP Run 1/1, Epoch 27/2000, Training Loss (NLML): -0.2428, (RMSE): 0.0734\n",
      "curve dfGP Run 1/1, Epoch 28/2000, Training Loss (NLML): -0.2473, (RMSE): 0.0718\n",
      "curve dfGP Run 1/1, Epoch 29/2000, Training Loss (NLML): -0.2517, (RMSE): 0.0704\n",
      "curve dfGP Run 1/1, Epoch 30/2000, Training Loss (NLML): -0.2561, (RMSE): 0.0690\n",
      "curve dfGP Run 1/1, Epoch 31/2000, Training Loss (NLML): -0.2604, (RMSE): 0.0677\n",
      "curve dfGP Run 1/1, Epoch 32/2000, Training Loss (NLML): -0.2648, (RMSE): 0.0666\n",
      "curve dfGP Run 1/1, Epoch 33/2000, Training Loss (NLML): -0.2691, (RMSE): 0.0656\n",
      "curve dfGP Run 1/1, Epoch 34/2000, Training Loss (NLML): -0.2733, (RMSE): 0.0647\n",
      "curve dfGP Run 1/1, Epoch 35/2000, Training Loss (NLML): -0.2776, (RMSE): 0.0640\n",
      "curve dfGP Run 1/1, Epoch 36/2000, Training Loss (NLML): -0.2818, (RMSE): 0.0634\n",
      "curve dfGP Run 1/1, Epoch 37/2000, Training Loss (NLML): -0.2861, (RMSE): 0.0630\n",
      "curve dfGP Run 1/1, Epoch 38/2000, Training Loss (NLML): -0.2903, (RMSE): 0.0626\n",
      "curve dfGP Run 1/1, Epoch 39/2000, Training Loss (NLML): -0.2946, (RMSE): 0.0624\n",
      "curve dfGP Run 1/1, Epoch 40/2000, Training Loss (NLML): -0.2989, (RMSE): 0.0623\n",
      "curve dfGP Run 1/1, Epoch 41/2000, Training Loss (NLML): -0.3032, (RMSE): 0.0622\n",
      "curve dfGP Run 1/1, Epoch 42/2000, Training Loss (NLML): -0.3075, (RMSE): 0.0622\n",
      "curve dfGP Run 1/1, Epoch 43/2000, Training Loss (NLML): -0.3118, (RMSE): 0.0623\n",
      "curve dfGP Run 1/1, Epoch 44/2000, Training Loss (NLML): -0.3161, (RMSE): 0.0624\n",
      "curve dfGP Run 1/1, Epoch 45/2000, Training Loss (NLML): -0.3204, (RMSE): 0.0625\n",
      "curve dfGP Run 1/1, Epoch 46/2000, Training Loss (NLML): -0.3246, (RMSE): 0.0625\n",
      "curve dfGP Run 1/1, Epoch 47/2000, Training Loss (NLML): -0.3289, (RMSE): 0.0626\n",
      "curve dfGP Run 1/1, Epoch 48/2000, Training Loss (NLML): -0.3332, (RMSE): 0.0626\n",
      "curve dfGP Run 1/1, Epoch 49/2000, Training Loss (NLML): -0.3374, (RMSE): 0.0626\n",
      "curve dfGP Run 1/1, Epoch 50/2000, Training Loss (NLML): -0.3417, (RMSE): 0.0625\n",
      "curve dfGP Run 1/1, Epoch 51/2000, Training Loss (NLML): -0.3459, (RMSE): 0.0623\n",
      "curve dfGP Run 1/1, Epoch 52/2000, Training Loss (NLML): -0.3501, (RMSE): 0.0621\n",
      "curve dfGP Run 1/1, Epoch 53/2000, Training Loss (NLML): -0.3544, (RMSE): 0.0619\n",
      "curve dfGP Run 1/1, Epoch 54/2000, Training Loss (NLML): -0.3586, (RMSE): 0.0616\n",
      "curve dfGP Run 1/1, Epoch 55/2000, Training Loss (NLML): -0.3629, (RMSE): 0.0612\n",
      "curve dfGP Run 1/1, Epoch 56/2000, Training Loss (NLML): -0.3671, (RMSE): 0.0609\n",
      "curve dfGP Run 1/1, Epoch 57/2000, Training Loss (NLML): -0.3714, (RMSE): 0.0605\n",
      "curve dfGP Run 1/1, Epoch 58/2000, Training Loss (NLML): -0.3756, (RMSE): 0.0601\n",
      "curve dfGP Run 1/1, Epoch 59/2000, Training Loss (NLML): -0.3799, (RMSE): 0.0597\n",
      "curve dfGP Run 1/1, Epoch 60/2000, Training Loss (NLML): -0.3841, (RMSE): 0.0593\n",
      "curve dfGP Run 1/1, Epoch 61/2000, Training Loss (NLML): -0.3883, (RMSE): 0.0589\n",
      "curve dfGP Run 1/1, Epoch 62/2000, Training Loss (NLML): -0.3925, (RMSE): 0.0586\n",
      "curve dfGP Run 1/1, Epoch 63/2000, Training Loss (NLML): -0.3967, (RMSE): 0.0583\n",
      "curve dfGP Run 1/1, Epoch 64/2000, Training Loss (NLML): -0.4009, (RMSE): 0.0580\n",
      "curve dfGP Run 1/1, Epoch 65/2000, Training Loss (NLML): -0.4051, (RMSE): 0.0577\n",
      "curve dfGP Run 1/1, Epoch 66/2000, Training Loss (NLML): -0.4093, (RMSE): 0.0575\n",
      "curve dfGP Run 1/1, Epoch 67/2000, Training Loss (NLML): -0.4135, (RMSE): 0.0573\n",
      "curve dfGP Run 1/1, Epoch 68/2000, Training Loss (NLML): -0.4177, (RMSE): 0.0571\n",
      "curve dfGP Run 1/1, Epoch 69/2000, Training Loss (NLML): -0.4219, (RMSE): 0.0569\n",
      "curve dfGP Run 1/1, Epoch 70/2000, Training Loss (NLML): -0.4261, (RMSE): 0.0568\n",
      "curve dfGP Run 1/1, Epoch 71/2000, Training Loss (NLML): -0.4302, (RMSE): 0.0567\n",
      "curve dfGP Run 1/1, Epoch 72/2000, Training Loss (NLML): -0.4344, (RMSE): 0.0566\n",
      "curve dfGP Run 1/1, Epoch 73/2000, Training Loss (NLML): -0.4386, (RMSE): 0.0564\n",
      "curve dfGP Run 1/1, Epoch 74/2000, Training Loss (NLML): -0.4427, (RMSE): 0.0563\n",
      "curve dfGP Run 1/1, Epoch 75/2000, Training Loss (NLML): -0.4469, (RMSE): 0.0562\n",
      "curve dfGP Run 1/1, Epoch 76/2000, Training Loss (NLML): -0.4510, (RMSE): 0.0560\n",
      "curve dfGP Run 1/1, Epoch 77/2000, Training Loss (NLML): -0.4552, (RMSE): 0.0558\n",
      "curve dfGP Run 1/1, Epoch 78/2000, Training Loss (NLML): -0.4593, (RMSE): 0.0557\n",
      "curve dfGP Run 1/1, Epoch 79/2000, Training Loss (NLML): -0.4634, (RMSE): 0.0555\n",
      "curve dfGP Run 1/1, Epoch 80/2000, Training Loss (NLML): -0.4676, (RMSE): 0.0552\n",
      "curve dfGP Run 1/1, Epoch 81/2000, Training Loss (NLML): -0.4717, (RMSE): 0.0550\n",
      "curve dfGP Run 1/1, Epoch 82/2000, Training Loss (NLML): -0.4758, (RMSE): 0.0548\n",
      "curve dfGP Run 1/1, Epoch 83/2000, Training Loss (NLML): -0.4799, (RMSE): 0.0546\n",
      "curve dfGP Run 1/1, Epoch 84/2000, Training Loss (NLML): -0.4840, (RMSE): 0.0543\n",
      "curve dfGP Run 1/1, Epoch 85/2000, Training Loss (NLML): -0.4881, (RMSE): 0.0541\n",
      "curve dfGP Run 1/1, Epoch 86/2000, Training Loss (NLML): -0.4922, (RMSE): 0.0539\n",
      "curve dfGP Run 1/1, Epoch 87/2000, Training Loss (NLML): -0.4963, (RMSE): 0.0537\n",
      "curve dfGP Run 1/1, Epoch 88/2000, Training Loss (NLML): -0.5004, (RMSE): 0.0534\n",
      "curve dfGP Run 1/1, Epoch 89/2000, Training Loss (NLML): -0.5045, (RMSE): 0.0533\n",
      "curve dfGP Run 1/1, Epoch 90/2000, Training Loss (NLML): -0.5086, (RMSE): 0.0531\n",
      "curve dfGP Run 1/1, Epoch 91/2000, Training Loss (NLML): -0.5127, (RMSE): 0.0529\n",
      "curve dfGP Run 1/1, Epoch 92/2000, Training Loss (NLML): -0.5167, (RMSE): 0.0527\n",
      "curve dfGP Run 1/1, Epoch 93/2000, Training Loss (NLML): -0.5208, (RMSE): 0.0526\n",
      "curve dfGP Run 1/1, Epoch 94/2000, Training Loss (NLML): -0.5248, (RMSE): 0.0524\n",
      "curve dfGP Run 1/1, Epoch 95/2000, Training Loss (NLML): -0.5289, (RMSE): 0.0522\n",
      "curve dfGP Run 1/1, Epoch 96/2000, Training Loss (NLML): -0.5329, (RMSE): 0.0521\n",
      "curve dfGP Run 1/1, Epoch 97/2000, Training Loss (NLML): -0.5369, (RMSE): 0.0519\n",
      "curve dfGP Run 1/1, Epoch 98/2000, Training Loss (NLML): -0.5410, (RMSE): 0.0517\n",
      "curve dfGP Run 1/1, Epoch 99/2000, Training Loss (NLML): -0.5450, (RMSE): 0.0516\n",
      "curve dfGP Run 1/1, Epoch 100/2000, Training Loss (NLML): -0.5490, (RMSE): 0.0514\n",
      "curve dfGP Run 1/1, Epoch 101/2000, Training Loss (NLML): -0.5530, (RMSE): 0.0512\n",
      "curve dfGP Run 1/1, Epoch 102/2000, Training Loss (NLML): -0.5570, (RMSE): 0.0510\n",
      "curve dfGP Run 1/1, Epoch 103/2000, Training Loss (NLML): -0.5610, (RMSE): 0.0508\n",
      "curve dfGP Run 1/1, Epoch 104/2000, Training Loss (NLML): -0.5650, (RMSE): 0.0506\n",
      "curve dfGP Run 1/1, Epoch 105/2000, Training Loss (NLML): -0.5690, (RMSE): 0.0504\n",
      "curve dfGP Run 1/1, Epoch 106/2000, Training Loss (NLML): -0.5730, (RMSE): 0.0503\n",
      "curve dfGP Run 1/1, Epoch 107/2000, Training Loss (NLML): -0.5769, (RMSE): 0.0501\n",
      "curve dfGP Run 1/1, Epoch 108/2000, Training Loss (NLML): -0.5809, (RMSE): 0.0499\n",
      "curve dfGP Run 1/1, Epoch 109/2000, Training Loss (NLML): -0.5848, (RMSE): 0.0497\n",
      "curve dfGP Run 1/1, Epoch 110/2000, Training Loss (NLML): -0.5888, (RMSE): 0.0495\n",
      "curve dfGP Run 1/1, Epoch 111/2000, Training Loss (NLML): -0.5927, (RMSE): 0.0493\n",
      "curve dfGP Run 1/1, Epoch 112/2000, Training Loss (NLML): -0.5967, (RMSE): 0.0491\n",
      "curve dfGP Run 1/1, Epoch 113/2000, Training Loss (NLML): -0.6006, (RMSE): 0.0490\n",
      "curve dfGP Run 1/1, Epoch 114/2000, Training Loss (NLML): -0.6045, (RMSE): 0.0488\n",
      "curve dfGP Run 1/1, Epoch 115/2000, Training Loss (NLML): -0.6084, (RMSE): 0.0486\n",
      "curve dfGP Run 1/1, Epoch 116/2000, Training Loss (NLML): -0.6123, (RMSE): 0.0485\n",
      "curve dfGP Run 1/1, Epoch 117/2000, Training Loss (NLML): -0.6162, (RMSE): 0.0483\n",
      "curve dfGP Run 1/1, Epoch 118/2000, Training Loss (NLML): -0.6201, (RMSE): 0.0481\n",
      "curve dfGP Run 1/1, Epoch 119/2000, Training Loss (NLML): -0.6240, (RMSE): 0.0479\n",
      "curve dfGP Run 1/1, Epoch 120/2000, Training Loss (NLML): -0.6279, (RMSE): 0.0478\n",
      "curve dfGP Run 1/1, Epoch 121/2000, Training Loss (NLML): -0.6317, (RMSE): 0.0476\n",
      "curve dfGP Run 1/1, Epoch 122/2000, Training Loss (NLML): -0.6356, (RMSE): 0.0474\n",
      "curve dfGP Run 1/1, Epoch 123/2000, Training Loss (NLML): -0.6394, (RMSE): 0.0472\n",
      "curve dfGP Run 1/1, Epoch 124/2000, Training Loss (NLML): -0.6433, (RMSE): 0.0470\n",
      "curve dfGP Run 1/1, Epoch 125/2000, Training Loss (NLML): -0.6471, (RMSE): 0.0469\n",
      "curve dfGP Run 1/1, Epoch 126/2000, Training Loss (NLML): -0.6509, (RMSE): 0.0467\n",
      "curve dfGP Run 1/1, Epoch 127/2000, Training Loss (NLML): -0.6548, (RMSE): 0.0465\n",
      "curve dfGP Run 1/1, Epoch 128/2000, Training Loss (NLML): -0.6586, (RMSE): 0.0463\n",
      "curve dfGP Run 1/1, Epoch 129/2000, Training Loss (NLML): -0.6624, (RMSE): 0.0461\n",
      "curve dfGP Run 1/1, Epoch 130/2000, Training Loss (NLML): -0.6662, (RMSE): 0.0459\n",
      "curve dfGP Run 1/1, Epoch 131/2000, Training Loss (NLML): -0.6699, (RMSE): 0.0458\n",
      "curve dfGP Run 1/1, Epoch 132/2000, Training Loss (NLML): -0.6737, (RMSE): 0.0456\n",
      "curve dfGP Run 1/1, Epoch 133/2000, Training Loss (NLML): -0.6775, (RMSE): 0.0454\n",
      "curve dfGP Run 1/1, Epoch 134/2000, Training Loss (NLML): -0.6812, (RMSE): 0.0452\n",
      "curve dfGP Run 1/1, Epoch 135/2000, Training Loss (NLML): -0.6850, (RMSE): 0.0451\n",
      "curve dfGP Run 1/1, Epoch 136/2000, Training Loss (NLML): -0.6887, (RMSE): 0.0449\n",
      "curve dfGP Run 1/1, Epoch 137/2000, Training Loss (NLML): -0.6925, (RMSE): 0.0447\n",
      "curve dfGP Run 1/1, Epoch 138/2000, Training Loss (NLML): -0.6962, (RMSE): 0.0445\n",
      "curve dfGP Run 1/1, Epoch 139/2000, Training Loss (NLML): -0.6999, (RMSE): 0.0444\n",
      "curve dfGP Run 1/1, Epoch 140/2000, Training Loss (NLML): -0.7036, (RMSE): 0.0442\n",
      "curve dfGP Run 1/1, Epoch 141/2000, Training Loss (NLML): -0.7073, (RMSE): 0.0440\n",
      "curve dfGP Run 1/1, Epoch 142/2000, Training Loss (NLML): -0.7110, (RMSE): 0.0438\n",
      "curve dfGP Run 1/1, Epoch 143/2000, Training Loss (NLML): -0.7146, (RMSE): 0.0436\n",
      "curve dfGP Run 1/1, Epoch 144/2000, Training Loss (NLML): -0.7183, (RMSE): 0.0435\n",
      "curve dfGP Run 1/1, Epoch 145/2000, Training Loss (NLML): -0.7220, (RMSE): 0.0433\n",
      "curve dfGP Run 1/1, Epoch 146/2000, Training Loss (NLML): -0.7256, (RMSE): 0.0431\n",
      "curve dfGP Run 1/1, Epoch 147/2000, Training Loss (NLML): -0.7293, (RMSE): 0.0429\n",
      "curve dfGP Run 1/1, Epoch 148/2000, Training Loss (NLML): -0.7329, (RMSE): 0.0428\n",
      "curve dfGP Run 1/1, Epoch 149/2000, Training Loss (NLML): -0.7365, (RMSE): 0.0426\n",
      "curve dfGP Run 1/1, Epoch 150/2000, Training Loss (NLML): -0.7401, (RMSE): 0.0424\n",
      "curve dfGP Run 1/1, Epoch 151/2000, Training Loss (NLML): -0.7437, (RMSE): 0.0422\n",
      "curve dfGP Run 1/1, Epoch 152/2000, Training Loss (NLML): -0.7473, (RMSE): 0.0421\n",
      "curve dfGP Run 1/1, Epoch 153/2000, Training Loss (NLML): -0.7509, (RMSE): 0.0419\n",
      "curve dfGP Run 1/1, Epoch 154/2000, Training Loss (NLML): -0.7544, (RMSE): 0.0417\n",
      "curve dfGP Run 1/1, Epoch 155/2000, Training Loss (NLML): -0.7580, (RMSE): 0.0415\n",
      "curve dfGP Run 1/1, Epoch 156/2000, Training Loss (NLML): -0.7616, (RMSE): 0.0414\n",
      "curve dfGP Run 1/1, Epoch 157/2000, Training Loss (NLML): -0.7651, (RMSE): 0.0412\n",
      "curve dfGP Run 1/1, Epoch 158/2000, Training Loss (NLML): -0.7686, (RMSE): 0.0410\n",
      "curve dfGP Run 1/1, Epoch 159/2000, Training Loss (NLML): -0.7721, (RMSE): 0.0408\n",
      "curve dfGP Run 1/1, Epoch 160/2000, Training Loss (NLML): -0.7757, (RMSE): 0.0407\n",
      "curve dfGP Run 1/1, Epoch 161/2000, Training Loss (NLML): -0.7792, (RMSE): 0.0405\n",
      "curve dfGP Run 1/1, Epoch 162/2000, Training Loss (NLML): -0.7827, (RMSE): 0.0403\n",
      "curve dfGP Run 1/1, Epoch 163/2000, Training Loss (NLML): -0.7861, (RMSE): 0.0402\n",
      "curve dfGP Run 1/1, Epoch 164/2000, Training Loss (NLML): -0.7896, (RMSE): 0.0400\n",
      "curve dfGP Run 1/1, Epoch 165/2000, Training Loss (NLML): -0.7931, (RMSE): 0.0398\n",
      "curve dfGP Run 1/1, Epoch 166/2000, Training Loss (NLML): -0.7965, (RMSE): 0.0396\n",
      "curve dfGP Run 1/1, Epoch 167/2000, Training Loss (NLML): -0.8000, (RMSE): 0.0395\n",
      "curve dfGP Run 1/1, Epoch 168/2000, Training Loss (NLML): -0.8034, (RMSE): 0.0393\n",
      "curve dfGP Run 1/1, Epoch 169/2000, Training Loss (NLML): -0.8068, (RMSE): 0.0391\n",
      "curve dfGP Run 1/1, Epoch 170/2000, Training Loss (NLML): -0.8102, (RMSE): 0.0390\n",
      "curve dfGP Run 1/1, Epoch 171/2000, Training Loss (NLML): -0.8137, (RMSE): 0.0388\n",
      "curve dfGP Run 1/1, Epoch 172/2000, Training Loss (NLML): -0.8171, (RMSE): 0.0386\n",
      "curve dfGP Run 1/1, Epoch 173/2000, Training Loss (NLML): -0.8204, (RMSE): 0.0385\n",
      "curve dfGP Run 1/1, Epoch 174/2000, Training Loss (NLML): -0.8238, (RMSE): 0.0383\n",
      "curve dfGP Run 1/1, Epoch 175/2000, Training Loss (NLML): -0.8272, (RMSE): 0.0381\n",
      "curve dfGP Run 1/1, Epoch 176/2000, Training Loss (NLML): -0.8306, (RMSE): 0.0380\n",
      "curve dfGP Run 1/1, Epoch 177/2000, Training Loss (NLML): -0.8339, (RMSE): 0.0378\n",
      "curve dfGP Run 1/1, Epoch 178/2000, Training Loss (NLML): -0.8373, (RMSE): 0.0376\n",
      "curve dfGP Run 1/1, Epoch 179/2000, Training Loss (NLML): -0.8406, (RMSE): 0.0375\n",
      "curve dfGP Run 1/1, Epoch 180/2000, Training Loss (NLML): -0.8439, (RMSE): 0.0373\n",
      "curve dfGP Run 1/1, Epoch 181/2000, Training Loss (NLML): -0.8472, (RMSE): 0.0371\n",
      "curve dfGP Run 1/1, Epoch 182/2000, Training Loss (NLML): -0.8506, (RMSE): 0.0370\n",
      "curve dfGP Run 1/1, Epoch 183/2000, Training Loss (NLML): -0.8539, (RMSE): 0.0368\n",
      "curve dfGP Run 1/1, Epoch 184/2000, Training Loss (NLML): -0.8572, (RMSE): 0.0366\n",
      "curve dfGP Run 1/1, Epoch 185/2000, Training Loss (NLML): -0.8604, (RMSE): 0.0365\n",
      "curve dfGP Run 1/1, Epoch 186/2000, Training Loss (NLML): -0.8637, (RMSE): 0.0363\n",
      "curve dfGP Run 1/1, Epoch 187/2000, Training Loss (NLML): -0.8670, (RMSE): 0.0362\n",
      "curve dfGP Run 1/1, Epoch 188/2000, Training Loss (NLML): -0.8703, (RMSE): 0.0360\n",
      "curve dfGP Run 1/1, Epoch 189/2000, Training Loss (NLML): -0.8735, (RMSE): 0.0358\n",
      "curve dfGP Run 1/1, Epoch 190/2000, Training Loss (NLML): -0.8768, (RMSE): 0.0357\n",
      "curve dfGP Run 1/1, Epoch 191/2000, Training Loss (NLML): -0.8800, (RMSE): 0.0355\n",
      "curve dfGP Run 1/1, Epoch 192/2000, Training Loss (NLML): -0.8832, (RMSE): 0.0353\n",
      "curve dfGP Run 1/1, Epoch 193/2000, Training Loss (NLML): -0.8865, (RMSE): 0.0352\n",
      "curve dfGP Run 1/1, Epoch 194/2000, Training Loss (NLML): -0.8897, (RMSE): 0.0350\n",
      "curve dfGP Run 1/1, Epoch 195/2000, Training Loss (NLML): -0.8929, (RMSE): 0.0348\n",
      "curve dfGP Run 1/1, Epoch 196/2000, Training Loss (NLML): -0.8961, (RMSE): 0.0347\n",
      "curve dfGP Run 1/1, Epoch 197/2000, Training Loss (NLML): -0.8993, (RMSE): 0.0345\n",
      "curve dfGP Run 1/1, Epoch 198/2000, Training Loss (NLML): -0.9025, (RMSE): 0.0343\n",
      "curve dfGP Run 1/1, Epoch 199/2000, Training Loss (NLML): -0.9057, (RMSE): 0.0342\n",
      "curve dfGP Run 1/1, Epoch 200/2000, Training Loss (NLML): -0.9088, (RMSE): 0.0340\n",
      "curve dfGP Run 1/1, Epoch 201/2000, Training Loss (NLML): -0.9120, (RMSE): 0.0339\n",
      "curve dfGP Run 1/1, Epoch 202/2000, Training Loss (NLML): -0.9152, (RMSE): 0.0337\n",
      "curve dfGP Run 1/1, Epoch 203/2000, Training Loss (NLML): -0.9183, (RMSE): 0.0335\n",
      "curve dfGP Run 1/1, Epoch 204/2000, Training Loss (NLML): -0.9215, (RMSE): 0.0334\n",
      "curve dfGP Run 1/1, Epoch 205/2000, Training Loss (NLML): -0.9246, (RMSE): 0.0332\n",
      "curve dfGP Run 1/1, Epoch 206/2000, Training Loss (NLML): -0.9278, (RMSE): 0.0330\n",
      "curve dfGP Run 1/1, Epoch 207/2000, Training Loss (NLML): -0.9309, (RMSE): 0.0329\n",
      "curve dfGP Run 1/1, Epoch 208/2000, Training Loss (NLML): -0.9340, (RMSE): 0.0327\n",
      "curve dfGP Run 1/1, Epoch 209/2000, Training Loss (NLML): -0.9371, (RMSE): 0.0325\n",
      "curve dfGP Run 1/1, Epoch 210/2000, Training Loss (NLML): -0.9402, (RMSE): 0.0324\n",
      "curve dfGP Run 1/1, Epoch 211/2000, Training Loss (NLML): -0.9433, (RMSE): 0.0322\n",
      "curve dfGP Run 1/1, Epoch 212/2000, Training Loss (NLML): -0.9464, (RMSE): 0.0321\n",
      "curve dfGP Run 1/1, Epoch 213/2000, Training Loss (NLML): -0.9495, (RMSE): 0.0319\n",
      "curve dfGP Run 1/1, Epoch 214/2000, Training Loss (NLML): -0.9526, (RMSE): 0.0317\n",
      "curve dfGP Run 1/1, Epoch 215/2000, Training Loss (NLML): -0.9557, (RMSE): 0.0316\n",
      "curve dfGP Run 1/1, Epoch 216/2000, Training Loss (NLML): -0.9588, (RMSE): 0.0314\n",
      "curve dfGP Run 1/1, Epoch 217/2000, Training Loss (NLML): -0.9618, (RMSE): 0.0312\n",
      "curve dfGP Run 1/1, Epoch 218/2000, Training Loss (NLML): -0.9649, (RMSE): 0.0311\n",
      "curve dfGP Run 1/1, Epoch 219/2000, Training Loss (NLML): -0.9680, (RMSE): 0.0309\n",
      "curve dfGP Run 1/1, Epoch 220/2000, Training Loss (NLML): -0.9710, (RMSE): 0.0308\n",
      "curve dfGP Run 1/1, Epoch 221/2000, Training Loss (NLML): -0.9740, (RMSE): 0.0306\n",
      "curve dfGP Run 1/1, Epoch 222/2000, Training Loss (NLML): -0.9771, (RMSE): 0.0305\n",
      "curve dfGP Run 1/1, Epoch 223/2000, Training Loss (NLML): -0.9801, (RMSE): 0.0303\n",
      "curve dfGP Run 1/1, Epoch 224/2000, Training Loss (NLML): -0.9831, (RMSE): 0.0302\n",
      "curve dfGP Run 1/1, Epoch 225/2000, Training Loss (NLML): -0.9861, (RMSE): 0.0300\n",
      "curve dfGP Run 1/1, Epoch 226/2000, Training Loss (NLML): -0.9891, (RMSE): 0.0298\n",
      "curve dfGP Run 1/1, Epoch 227/2000, Training Loss (NLML): -0.9921, (RMSE): 0.0297\n",
      "curve dfGP Run 1/1, Epoch 228/2000, Training Loss (NLML): -0.9951, (RMSE): 0.0295\n",
      "curve dfGP Run 1/1, Epoch 229/2000, Training Loss (NLML): -0.9981, (RMSE): 0.0294\n",
      "curve dfGP Run 1/1, Epoch 230/2000, Training Loss (NLML): -1.0011, (RMSE): 0.0293\n",
      "curve dfGP Run 1/1, Epoch 231/2000, Training Loss (NLML): -1.0041, (RMSE): 0.0291\n",
      "curve dfGP Run 1/1, Epoch 232/2000, Training Loss (NLML): -1.0070, (RMSE): 0.0290\n",
      "curve dfGP Run 1/1, Epoch 233/2000, Training Loss (NLML): -1.0100, (RMSE): 0.0288\n",
      "curve dfGP Run 1/1, Epoch 234/2000, Training Loss (NLML): -1.0129, (RMSE): 0.0287\n",
      "curve dfGP Run 1/1, Epoch 235/2000, Training Loss (NLML): -1.0159, (RMSE): 0.0285\n",
      "curve dfGP Run 1/1, Epoch 236/2000, Training Loss (NLML): -1.0188, (RMSE): 0.0284\n",
      "curve dfGP Run 1/1, Epoch 237/2000, Training Loss (NLML): -1.0217, (RMSE): 0.0283\n",
      "curve dfGP Run 1/1, Epoch 238/2000, Training Loss (NLML): -1.0247, (RMSE): 0.0281\n",
      "curve dfGP Run 1/1, Epoch 239/2000, Training Loss (NLML): -1.0276, (RMSE): 0.0280\n",
      "curve dfGP Run 1/1, Epoch 240/2000, Training Loss (NLML): -1.0305, (RMSE): 0.0279\n",
      "curve dfGP Run 1/1, Epoch 241/2000, Training Loss (NLML): -1.0334, (RMSE): 0.0277\n",
      "curve dfGP Run 1/1, Epoch 242/2000, Training Loss (NLML): -1.0363, (RMSE): 0.0276\n",
      "curve dfGP Run 1/1, Epoch 243/2000, Training Loss (NLML): -1.0392, (RMSE): 0.0275\n",
      "curve dfGP Run 1/1, Epoch 244/2000, Training Loss (NLML): -1.0420, (RMSE): 0.0273\n",
      "curve dfGP Run 1/1, Epoch 245/2000, Training Loss (NLML): -1.0449, (RMSE): 0.0272\n",
      "curve dfGP Run 1/1, Epoch 246/2000, Training Loss (NLML): -1.0478, (RMSE): 0.0271\n",
      "curve dfGP Run 1/1, Epoch 247/2000, Training Loss (NLML): -1.0506, (RMSE): 0.0270\n",
      "curve dfGP Run 1/1, Epoch 248/2000, Training Loss (NLML): -1.0535, (RMSE): 0.0268\n",
      "curve dfGP Run 1/1, Epoch 249/2000, Training Loss (NLML): -1.0563, (RMSE): 0.0267\n",
      "curve dfGP Run 1/1, Epoch 250/2000, Training Loss (NLML): -1.0591, (RMSE): 0.0266\n",
      "curve dfGP Run 1/1, Epoch 251/2000, Training Loss (NLML): -1.0619, (RMSE): 0.0265\n",
      "curve dfGP Run 1/1, Epoch 252/2000, Training Loss (NLML): -1.0647, (RMSE): 0.0264\n",
      "curve dfGP Run 1/1, Epoch 253/2000, Training Loss (NLML): -1.0675, (RMSE): 0.0263\n",
      "curve dfGP Run 1/1, Epoch 254/2000, Training Loss (NLML): -1.0703, (RMSE): 0.0261\n",
      "curve dfGP Run 1/1, Epoch 255/2000, Training Loss (NLML): -1.0731, (RMSE): 0.0260\n",
      "curve dfGP Run 1/1, Epoch 256/2000, Training Loss (NLML): -1.0759, (RMSE): 0.0259\n",
      "curve dfGP Run 1/1, Epoch 257/2000, Training Loss (NLML): -1.0787, (RMSE): 0.0258\n",
      "curve dfGP Run 1/1, Epoch 258/2000, Training Loss (NLML): -1.0814, (RMSE): 0.0257\n",
      "curve dfGP Run 1/1, Epoch 259/2000, Training Loss (NLML): -1.0842, (RMSE): 0.0256\n",
      "curve dfGP Run 1/1, Epoch 260/2000, Training Loss (NLML): -1.0869, (RMSE): 0.0255\n",
      "curve dfGP Run 1/1, Epoch 261/2000, Training Loss (NLML): -1.0896, (RMSE): 0.0254\n",
      "curve dfGP Run 1/1, Epoch 262/2000, Training Loss (NLML): -1.0924, (RMSE): 0.0252\n",
      "curve dfGP Run 1/1, Epoch 263/2000, Training Loss (NLML): -1.0951, (RMSE): 0.0251\n",
      "curve dfGP Run 1/1, Epoch 264/2000, Training Loss (NLML): -1.0978, (RMSE): 0.0250\n",
      "curve dfGP Run 1/1, Epoch 265/2000, Training Loss (NLML): -1.1005, (RMSE): 0.0249\n",
      "curve dfGP Run 1/1, Epoch 266/2000, Training Loss (NLML): -1.1032, (RMSE): 0.0248\n",
      "curve dfGP Run 1/1, Epoch 267/2000, Training Loss (NLML): -1.1058, (RMSE): 0.0247\n",
      "curve dfGP Run 1/1, Epoch 268/2000, Training Loss (NLML): -1.1085, (RMSE): 0.0246\n",
      "curve dfGP Run 1/1, Epoch 269/2000, Training Loss (NLML): -1.1111, (RMSE): 0.0245\n",
      "curve dfGP Run 1/1, Epoch 270/2000, Training Loss (NLML): -1.1138, (RMSE): 0.0244\n",
      "curve dfGP Run 1/1, Epoch 271/2000, Training Loss (NLML): -1.1164, (RMSE): 0.0243\n",
      "curve dfGP Run 1/1, Epoch 272/2000, Training Loss (NLML): -1.1191, (RMSE): 0.0242\n",
      "curve dfGP Run 1/1, Epoch 273/2000, Training Loss (NLML): -1.1217, (RMSE): 0.0241\n",
      "curve dfGP Run 1/1, Epoch 274/2000, Training Loss (NLML): -1.1243, (RMSE): 0.0240\n",
      "curve dfGP Run 1/1, Epoch 275/2000, Training Loss (NLML): -1.1269, (RMSE): 0.0239\n",
      "curve dfGP Run 1/1, Epoch 276/2000, Training Loss (NLML): -1.1295, (RMSE): 0.0238\n",
      "curve dfGP Run 1/1, Epoch 277/2000, Training Loss (NLML): -1.1320, (RMSE): 0.0237\n",
      "curve dfGP Run 1/1, Epoch 278/2000, Training Loss (NLML): -1.1346, (RMSE): 0.0237\n",
      "curve dfGP Run 1/1, Epoch 279/2000, Training Loss (NLML): -1.1372, (RMSE): 0.0236\n",
      "curve dfGP Run 1/1, Epoch 280/2000, Training Loss (NLML): -1.1397, (RMSE): 0.0235\n",
      "curve dfGP Run 1/1, Epoch 281/2000, Training Loss (NLML): -1.1422, (RMSE): 0.0234\n",
      "curve dfGP Run 1/1, Epoch 282/2000, Training Loss (NLML): -1.1448, (RMSE): 0.0233\n",
      "curve dfGP Run 1/1, Epoch 283/2000, Training Loss (NLML): -1.1473, (RMSE): 0.0232\n",
      "curve dfGP Run 1/1, Epoch 284/2000, Training Loss (NLML): -1.1498, (RMSE): 0.0231\n",
      "curve dfGP Run 1/1, Epoch 285/2000, Training Loss (NLML): -1.1523, (RMSE): 0.0230\n",
      "curve dfGP Run 1/1, Epoch 286/2000, Training Loss (NLML): -1.1548, (RMSE): 0.0229\n",
      "curve dfGP Run 1/1, Epoch 287/2000, Training Loss (NLML): -1.1572, (RMSE): 0.0229\n",
      "curve dfGP Run 1/1, Epoch 288/2000, Training Loss (NLML): -1.1597, (RMSE): 0.0228\n",
      "curve dfGP Run 1/1, Epoch 289/2000, Training Loss (NLML): -1.1621, (RMSE): 0.0227\n",
      "curve dfGP Run 1/1, Epoch 290/2000, Training Loss (NLML): -1.1646, (RMSE): 0.0226\n",
      "curve dfGP Run 1/1, Epoch 291/2000, Training Loss (NLML): -1.1670, (RMSE): 0.0225\n",
      "curve dfGP Run 1/1, Epoch 292/2000, Training Loss (NLML): -1.1694, (RMSE): 0.0224\n",
      "curve dfGP Run 1/1, Epoch 293/2000, Training Loss (NLML): -1.1718, (RMSE): 0.0224\n",
      "curve dfGP Run 1/1, Epoch 294/2000, Training Loss (NLML): -1.1742, (RMSE): 0.0223\n",
      "curve dfGP Run 1/1, Epoch 295/2000, Training Loss (NLML): -1.1767, (RMSE): 0.0222\n",
      "curve dfGP Run 1/1, Epoch 296/2000, Training Loss (NLML): -1.1790, (RMSE): 0.0221\n",
      "curve dfGP Run 1/1, Epoch 297/2000, Training Loss (NLML): -1.1814, (RMSE): 0.0220\n",
      "curve dfGP Run 1/1, Epoch 298/2000, Training Loss (NLML): -1.1837, (RMSE): 0.0220\n",
      "curve dfGP Run 1/1, Epoch 299/2000, Training Loss (NLML): -1.1861, (RMSE): 0.0219\n",
      "curve dfGP Run 1/1, Epoch 300/2000, Training Loss (NLML): -1.1884, (RMSE): 0.0218\n",
      "curve dfGP Run 1/1, Epoch 301/2000, Training Loss (NLML): -1.1907, (RMSE): 0.0217\n",
      "curve dfGP Run 1/1, Epoch 302/2000, Training Loss (NLML): -1.1930, (RMSE): 0.0217\n",
      "curve dfGP Run 1/1, Epoch 303/2000, Training Loss (NLML): -1.1953, (RMSE): 0.0216\n",
      "curve dfGP Run 1/1, Epoch 304/2000, Training Loss (NLML): -1.1976, (RMSE): 0.0215\n",
      "curve dfGP Run 1/1, Epoch 305/2000, Training Loss (NLML): -1.1999, (RMSE): 0.0214\n",
      "curve dfGP Run 1/1, Epoch 306/2000, Training Loss (NLML): -1.2021, (RMSE): 0.0214\n",
      "curve dfGP Run 1/1, Epoch 307/2000, Training Loss (NLML): -1.2044, (RMSE): 0.0213\n",
      "curve dfGP Run 1/1, Epoch 308/2000, Training Loss (NLML): -1.2066, (RMSE): 0.0212\n",
      "curve dfGP Run 1/1, Epoch 309/2000, Training Loss (NLML): -1.2088, (RMSE): 0.0212\n",
      "curve dfGP Run 1/1, Epoch 310/2000, Training Loss (NLML): -1.2110, (RMSE): 0.0211\n",
      "curve dfGP Run 1/1, Epoch 311/2000, Training Loss (NLML): -1.2133, (RMSE): 0.0210\n",
      "curve dfGP Run 1/1, Epoch 312/2000, Training Loss (NLML): -1.2154, (RMSE): 0.0210\n",
      "curve dfGP Run 1/1, Epoch 313/2000, Training Loss (NLML): -1.2176, (RMSE): 0.0209\n",
      "curve dfGP Run 1/1, Epoch 314/2000, Training Loss (NLML): -1.2198, (RMSE): 0.0208\n",
      "curve dfGP Run 1/1, Epoch 315/2000, Training Loss (NLML): -1.2219, (RMSE): 0.0208\n",
      "curve dfGP Run 1/1, Epoch 316/2000, Training Loss (NLML): -1.2241, (RMSE): 0.0207\n",
      "curve dfGP Run 1/1, Epoch 317/2000, Training Loss (NLML): -1.2262, (RMSE): 0.0206\n",
      "curve dfGP Run 1/1, Epoch 318/2000, Training Loss (NLML): -1.2284, (RMSE): 0.0206\n",
      "curve dfGP Run 1/1, Epoch 319/2000, Training Loss (NLML): -1.2305, (RMSE): 0.0205\n",
      "curve dfGP Run 1/1, Epoch 320/2000, Training Loss (NLML): -1.2326, (RMSE): 0.0204\n",
      "curve dfGP Run 1/1, Epoch 321/2000, Training Loss (NLML): -1.2346, (RMSE): 0.0204\n",
      "curve dfGP Run 1/1, Epoch 322/2000, Training Loss (NLML): -1.2367, (RMSE): 0.0203\n",
      "curve dfGP Run 1/1, Epoch 323/2000, Training Loss (NLML): -1.2388, (RMSE): 0.0203\n",
      "curve dfGP Run 1/1, Epoch 324/2000, Training Loss (NLML): -1.2408, (RMSE): 0.0202\n",
      "curve dfGP Run 1/1, Epoch 325/2000, Training Loss (NLML): -1.2428, (RMSE): 0.0201\n",
      "curve dfGP Run 1/1, Epoch 326/2000, Training Loss (NLML): -1.2448, (RMSE): 0.0201\n",
      "curve dfGP Run 1/1, Epoch 327/2000, Training Loss (NLML): -1.2469, (RMSE): 0.0200\n",
      "curve dfGP Run 1/1, Epoch 328/2000, Training Loss (NLML): -1.2489, (RMSE): 0.0200\n",
      "curve dfGP Run 1/1, Epoch 329/2000, Training Loss (NLML): -1.2508, (RMSE): 0.0199\n",
      "curve dfGP Run 1/1, Epoch 330/2000, Training Loss (NLML): -1.2528, (RMSE): 0.0198\n",
      "curve dfGP Run 1/1, Epoch 331/2000, Training Loss (NLML): -1.2548, (RMSE): 0.0198\n",
      "curve dfGP Run 1/1, Epoch 332/2000, Training Loss (NLML): -1.2567, (RMSE): 0.0197\n",
      "curve dfGP Run 1/1, Epoch 333/2000, Training Loss (NLML): -1.2587, (RMSE): 0.0197\n",
      "curve dfGP Run 1/1, Epoch 334/2000, Training Loss (NLML): -1.2606, (RMSE): 0.0196\n",
      "curve dfGP Run 1/1, Epoch 335/2000, Training Loss (NLML): -1.2625, (RMSE): 0.0196\n",
      "curve dfGP Run 1/1, Epoch 336/2000, Training Loss (NLML): -1.2644, (RMSE): 0.0195\n",
      "curve dfGP Run 1/1, Epoch 337/2000, Training Loss (NLML): -1.2663, (RMSE): 0.0195\n",
      "curve dfGP Run 1/1, Epoch 338/2000, Training Loss (NLML): -1.2681, (RMSE): 0.0194\n",
      "curve dfGP Run 1/1, Epoch 339/2000, Training Loss (NLML): -1.2700, (RMSE): 0.0194\n",
      "curve dfGP Run 1/1, Epoch 340/2000, Training Loss (NLML): -1.2718, (RMSE): 0.0193\n",
      "curve dfGP Run 1/1, Epoch 341/2000, Training Loss (NLML): -1.2737, (RMSE): 0.0193\n",
      "curve dfGP Run 1/1, Epoch 342/2000, Training Loss (NLML): -1.2755, (RMSE): 0.0192\n",
      "curve dfGP Run 1/1, Epoch 343/2000, Training Loss (NLML): -1.2773, (RMSE): 0.0192\n",
      "curve dfGP Run 1/1, Epoch 344/2000, Training Loss (NLML): -1.2791, (RMSE): 0.0191\n",
      "curve dfGP Run 1/1, Epoch 345/2000, Training Loss (NLML): -1.2809, (RMSE): 0.0191\n",
      "curve dfGP Run 1/1, Epoch 346/2000, Training Loss (NLML): -1.2827, (RMSE): 0.0190\n",
      "curve dfGP Run 1/1, Epoch 347/2000, Training Loss (NLML): -1.2844, (RMSE): 0.0190\n",
      "curve dfGP Run 1/1, Epoch 348/2000, Training Loss (NLML): -1.2862, (RMSE): 0.0190\n",
      "curve dfGP Run 1/1, Epoch 349/2000, Training Loss (NLML): -1.2879, (RMSE): 0.0189\n",
      "curve dfGP Run 1/1, Epoch 350/2000, Training Loss (NLML): -1.2896, (RMSE): 0.0189\n",
      "curve dfGP Run 1/1, Epoch 351/2000, Training Loss (NLML): -1.2913, (RMSE): 0.0188\n",
      "curve dfGP Run 1/1, Epoch 352/2000, Training Loss (NLML): -1.2930, (RMSE): 0.0188\n",
      "curve dfGP Run 1/1, Epoch 353/2000, Training Loss (NLML): -1.2947, (RMSE): 0.0187\n",
      "curve dfGP Run 1/1, Epoch 354/2000, Training Loss (NLML): -1.2963, (RMSE): 0.0187\n",
      "curve dfGP Run 1/1, Epoch 355/2000, Training Loss (NLML): -1.2980, (RMSE): 0.0187\n",
      "curve dfGP Run 1/1, Epoch 356/2000, Training Loss (NLML): -1.2996, (RMSE): 0.0186\n",
      "curve dfGP Run 1/1, Epoch 357/2000, Training Loss (NLML): -1.3013, (RMSE): 0.0186\n",
      "curve dfGP Run 1/1, Epoch 358/2000, Training Loss (NLML): -1.3029, (RMSE): 0.0185\n",
      "curve dfGP Run 1/1, Epoch 359/2000, Training Loss (NLML): -1.3045, (RMSE): 0.0185\n",
      "curve dfGP Run 1/1, Epoch 360/2000, Training Loss (NLML): -1.3061, (RMSE): 0.0185\n",
      "curve dfGP Run 1/1, Epoch 361/2000, Training Loss (NLML): -1.3077, (RMSE): 0.0184\n",
      "curve dfGP Run 1/1, Epoch 362/2000, Training Loss (NLML): -1.3092, (RMSE): 0.0184\n",
      "curve dfGP Run 1/1, Epoch 363/2000, Training Loss (NLML): -1.3108, (RMSE): 0.0183\n",
      "curve dfGP Run 1/1, Epoch 364/2000, Training Loss (NLML): -1.3123, (RMSE): 0.0183\n",
      "curve dfGP Run 1/1, Epoch 365/2000, Training Loss (NLML): -1.3138, (RMSE): 0.0183\n",
      "curve dfGP Run 1/1, Epoch 366/2000, Training Loss (NLML): -1.3153, (RMSE): 0.0182\n",
      "curve dfGP Run 1/1, Epoch 367/2000, Training Loss (NLML): -1.3168, (RMSE): 0.0182\n",
      "curve dfGP Run 1/1, Epoch 368/2000, Training Loss (NLML): -1.3183, (RMSE): 0.0182\n",
      "curve dfGP Run 1/1, Epoch 369/2000, Training Loss (NLML): -1.3198, (RMSE): 0.0181\n",
      "curve dfGP Run 1/1, Epoch 370/2000, Training Loss (NLML): -1.3213, (RMSE): 0.0181\n",
      "curve dfGP Run 1/1, Epoch 371/2000, Training Loss (NLML): -1.3227, (RMSE): 0.0181\n",
      "curve dfGP Run 1/1, Epoch 372/2000, Training Loss (NLML): -1.3241, (RMSE): 0.0180\n",
      "curve dfGP Run 1/1, Epoch 373/2000, Training Loss (NLML): -1.3256, (RMSE): 0.0180\n",
      "curve dfGP Run 1/1, Epoch 374/2000, Training Loss (NLML): -1.3270, (RMSE): 0.0180\n",
      "curve dfGP Run 1/1, Epoch 375/2000, Training Loss (NLML): -1.3284, (RMSE): 0.0179\n",
      "curve dfGP Run 1/1, Epoch 376/2000, Training Loss (NLML): -1.3297, (RMSE): 0.0179\n",
      "curve dfGP Run 1/1, Epoch 377/2000, Training Loss (NLML): -1.3311, (RMSE): 0.0179\n",
      "curve dfGP Run 1/1, Epoch 378/2000, Training Loss (NLML): -1.3325, (RMSE): 0.0178\n",
      "curve dfGP Run 1/1, Epoch 379/2000, Training Loss (NLML): -1.3338, (RMSE): 0.0178\n",
      "curve dfGP Run 1/1, Epoch 380/2000, Training Loss (NLML): -1.3351, (RMSE): 0.0178\n",
      "curve dfGP Run 1/1, Epoch 381/2000, Training Loss (NLML): -1.3365, (RMSE): 0.0178\n",
      "curve dfGP Run 1/1, Epoch 382/2000, Training Loss (NLML): -1.3377, (RMSE): 0.0177\n",
      "curve dfGP Run 1/1, Epoch 383/2000, Training Loss (NLML): -1.3391, (RMSE): 0.0177\n",
      "curve dfGP Run 1/1, Epoch 384/2000, Training Loss (NLML): -1.3404, (RMSE): 0.0177\n",
      "curve dfGP Run 1/1, Epoch 385/2000, Training Loss (NLML): -1.3416, (RMSE): 0.0176\n",
      "curve dfGP Run 1/1, Epoch 386/2000, Training Loss (NLML): -1.3429, (RMSE): 0.0176\n",
      "curve dfGP Run 1/1, Epoch 387/2000, Training Loss (NLML): -1.3441, (RMSE): 0.0176\n",
      "curve dfGP Run 1/1, Epoch 388/2000, Training Loss (NLML): -1.3453, (RMSE): 0.0176\n",
      "curve dfGP Run 1/1, Epoch 389/2000, Training Loss (NLML): -1.3466, (RMSE): 0.0175\n",
      "curve dfGP Run 1/1, Epoch 390/2000, Training Loss (NLML): -1.3478, (RMSE): 0.0175\n",
      "curve dfGP Run 1/1, Epoch 391/2000, Training Loss (NLML): -1.3490, (RMSE): 0.0175\n",
      "curve dfGP Run 1/1, Epoch 392/2000, Training Loss (NLML): -1.3502, (RMSE): 0.0175\n",
      "curve dfGP Run 1/1, Epoch 393/2000, Training Loss (NLML): -1.3513, (RMSE): 0.0174\n",
      "curve dfGP Run 1/1, Epoch 394/2000, Training Loss (NLML): -1.3525, (RMSE): 0.0174\n",
      "curve dfGP Run 1/1, Epoch 395/2000, Training Loss (NLML): -1.3536, (RMSE): 0.0174\n",
      "curve dfGP Run 1/1, Epoch 396/2000, Training Loss (NLML): -1.3548, (RMSE): 0.0174\n",
      "curve dfGP Run 1/1, Epoch 397/2000, Training Loss (NLML): -1.3559, (RMSE): 0.0173\n",
      "curve dfGP Run 1/1, Epoch 398/2000, Training Loss (NLML): -1.3570, (RMSE): 0.0173\n",
      "curve dfGP Run 1/1, Epoch 399/2000, Training Loss (NLML): -1.3581, (RMSE): 0.0173\n",
      "curve dfGP Run 1/1, Epoch 400/2000, Training Loss (NLML): -1.3592, (RMSE): 0.0173\n",
      "curve dfGP Run 1/1, Epoch 401/2000, Training Loss (NLML): -1.3602, (RMSE): 0.0172\n",
      "curve dfGP Run 1/1, Epoch 402/2000, Training Loss (NLML): -1.3613, (RMSE): 0.0172\n",
      "curve dfGP Run 1/1, Epoch 403/2000, Training Loss (NLML): -1.3624, (RMSE): 0.0172\n",
      "curve dfGP Run 1/1, Epoch 404/2000, Training Loss (NLML): -1.3634, (RMSE): 0.0172\n",
      "curve dfGP Run 1/1, Epoch 405/2000, Training Loss (NLML): -1.3644, (RMSE): 0.0172\n",
      "curve dfGP Run 1/1, Epoch 406/2000, Training Loss (NLML): -1.3654, (RMSE): 0.0171\n",
      "curve dfGP Run 1/1, Epoch 407/2000, Training Loss (NLML): -1.3664, (RMSE): 0.0171\n",
      "curve dfGP Run 1/1, Epoch 408/2000, Training Loss (NLML): -1.3674, (RMSE): 0.0171\n",
      "curve dfGP Run 1/1, Epoch 409/2000, Training Loss (NLML): -1.3684, (RMSE): 0.0171\n",
      "curve dfGP Run 1/1, Epoch 410/2000, Training Loss (NLML): -1.3694, (RMSE): 0.0171\n",
      "curve dfGP Run 1/1, Epoch 411/2000, Training Loss (NLML): -1.3703, (RMSE): 0.0170\n",
      "curve dfGP Run 1/1, Epoch 412/2000, Training Loss (NLML): -1.3713, (RMSE): 0.0170\n",
      "curve dfGP Run 1/1, Epoch 413/2000, Training Loss (NLML): -1.3722, (RMSE): 0.0170\n",
      "curve dfGP Run 1/1, Epoch 414/2000, Training Loss (NLML): -1.3732, (RMSE): 0.0170\n",
      "curve dfGP Run 1/1, Epoch 415/2000, Training Loss (NLML): -1.3740, (RMSE): 0.0170\n",
      "curve dfGP Run 1/1, Epoch 416/2000, Training Loss (NLML): -1.3750, (RMSE): 0.0169\n",
      "curve dfGP Run 1/1, Epoch 417/2000, Training Loss (NLML): -1.3758, (RMSE): 0.0169\n",
      "curve dfGP Run 1/1, Epoch 418/2000, Training Loss (NLML): -1.3767, (RMSE): 0.0169\n",
      "curve dfGP Run 1/1, Epoch 419/2000, Training Loss (NLML): -1.3776, (RMSE): 0.0169\n",
      "curve dfGP Run 1/1, Epoch 420/2000, Training Loss (NLML): -1.3784, (RMSE): 0.0169\n",
      "curve dfGP Run 1/1, Epoch 421/2000, Training Loss (NLML): -1.3793, (RMSE): 0.0168\n",
      "curve dfGP Run 1/1, Epoch 422/2000, Training Loss (NLML): -1.3802, (RMSE): 0.0168\n",
      "curve dfGP Run 1/1, Epoch 423/2000, Training Loss (NLML): -1.3809, (RMSE): 0.0168\n",
      "curve dfGP Run 1/1, Epoch 424/2000, Training Loss (NLML): -1.3818, (RMSE): 0.0168\n",
      "curve dfGP Run 1/1, Epoch 425/2000, Training Loss (NLML): -1.3826, (RMSE): 0.0168\n",
      "curve dfGP Run 1/1, Epoch 426/2000, Training Loss (NLML): -1.3834, (RMSE): 0.0168\n",
      "curve dfGP Run 1/1, Epoch 427/2000, Training Loss (NLML): -1.3842, (RMSE): 0.0167\n",
      "curve dfGP Run 1/1, Epoch 428/2000, Training Loss (NLML): -1.3849, (RMSE): 0.0167\n",
      "curve dfGP Run 1/1, Epoch 429/2000, Training Loss (NLML): -1.3858, (RMSE): 0.0167\n",
      "curve dfGP Run 1/1, Epoch 430/2000, Training Loss (NLML): -1.3865, (RMSE): 0.0167\n",
      "curve dfGP Run 1/1, Epoch 431/2000, Training Loss (NLML): -1.3872, (RMSE): 0.0167\n",
      "curve dfGP Run 1/1, Epoch 432/2000, Training Loss (NLML): -1.3879, (RMSE): 0.0167\n",
      "curve dfGP Run 1/1, Epoch 433/2000, Training Loss (NLML): -1.3886, (RMSE): 0.0167\n",
      "curve dfGP Run 1/1, Epoch 434/2000, Training Loss (NLML): -1.3894, (RMSE): 0.0166\n",
      "curve dfGP Run 1/1, Epoch 435/2000, Training Loss (NLML): -1.3900, (RMSE): 0.0166\n",
      "curve dfGP Run 1/1, Epoch 436/2000, Training Loss (NLML): -1.3907, (RMSE): 0.0166\n",
      "curve dfGP Run 1/1, Epoch 437/2000, Training Loss (NLML): -1.3914, (RMSE): 0.0166\n",
      "curve dfGP Run 1/1, Epoch 438/2000, Training Loss (NLML): -1.3921, (RMSE): 0.0166\n",
      "curve dfGP Run 1/1, Epoch 439/2000, Training Loss (NLML): -1.3927, (RMSE): 0.0166\n",
      "curve dfGP Run 1/1, Epoch 440/2000, Training Loss (NLML): -1.3934, (RMSE): 0.0166\n",
      "curve dfGP Run 1/1, Epoch 441/2000, Training Loss (NLML): -1.3940, (RMSE): 0.0165\n",
      "curve dfGP Run 1/1, Epoch 442/2000, Training Loss (NLML): -1.3947, (RMSE): 0.0165\n",
      "curve dfGP Run 1/1, Epoch 443/2000, Training Loss (NLML): -1.3954, (RMSE): 0.0165\n",
      "curve dfGP Run 1/1, Epoch 444/2000, Training Loss (NLML): -1.3959, (RMSE): 0.0165\n",
      "curve dfGP Run 1/1, Epoch 445/2000, Training Loss (NLML): -1.3965, (RMSE): 0.0165\n",
      "curve dfGP Run 1/1, Epoch 446/2000, Training Loss (NLML): -1.3972, (RMSE): 0.0165\n",
      "curve dfGP Run 1/1, Epoch 447/2000, Training Loss (NLML): -1.3978, (RMSE): 0.0165\n",
      "curve dfGP Run 1/1, Epoch 448/2000, Training Loss (NLML): -1.3984, (RMSE): 0.0165\n",
      "curve dfGP Run 1/1, Epoch 449/2000, Training Loss (NLML): -1.3990, (RMSE): 0.0165\n",
      "curve dfGP Run 1/1, Epoch 450/2000, Training Loss (NLML): -1.3995, (RMSE): 0.0164\n",
      "curve dfGP Run 1/1, Epoch 451/2000, Training Loss (NLML): -1.4000, (RMSE): 0.0164\n",
      "curve dfGP Run 1/1, Epoch 452/2000, Training Loss (NLML): -1.4006, (RMSE): 0.0164\n",
      "curve dfGP Run 1/1, Epoch 453/2000, Training Loss (NLML): -1.4011, (RMSE): 0.0164\n",
      "curve dfGP Run 1/1, Epoch 454/2000, Training Loss (NLML): -1.4017, (RMSE): 0.0164\n",
      "curve dfGP Run 1/1, Epoch 455/2000, Training Loss (NLML): -1.4022, (RMSE): 0.0164\n",
      "curve dfGP Run 1/1, Epoch 456/2000, Training Loss (NLML): -1.4027, (RMSE): 0.0164\n",
      "curve dfGP Run 1/1, Epoch 457/2000, Training Loss (NLML): -1.4032, (RMSE): 0.0164\n",
      "curve dfGP Run 1/1, Epoch 458/2000, Training Loss (NLML): -1.4037, (RMSE): 0.0163\n",
      "curve dfGP Run 1/1, Epoch 459/2000, Training Loss (NLML): -1.4042, (RMSE): 0.0163\n",
      "curve dfGP Run 1/1, Epoch 460/2000, Training Loss (NLML): -1.4047, (RMSE): 0.0163\n",
      "curve dfGP Run 1/1, Epoch 461/2000, Training Loss (NLML): -1.4052, (RMSE): 0.0163\n",
      "curve dfGP Run 1/1, Epoch 462/2000, Training Loss (NLML): -1.4056, (RMSE): 0.0163\n",
      "curve dfGP Run 1/1, Epoch 463/2000, Training Loss (NLML): -1.4061, (RMSE): 0.0163\n",
      "curve dfGP Run 1/1, Epoch 464/2000, Training Loss (NLML): -1.4066, (RMSE): 0.0163\n",
      "curve dfGP Run 1/1, Epoch 465/2000, Training Loss (NLML): -1.4070, (RMSE): 0.0163\n",
      "curve dfGP Run 1/1, Epoch 466/2000, Training Loss (NLML): -1.4074, (RMSE): 0.0163\n",
      "curve dfGP Run 1/1, Epoch 467/2000, Training Loss (NLML): -1.4079, (RMSE): 0.0163\n",
      "curve dfGP Run 1/1, Epoch 468/2000, Training Loss (NLML): -1.4083, (RMSE): 0.0163\n",
      "curve dfGP Run 1/1, Epoch 469/2000, Training Loss (NLML): -1.4088, (RMSE): 0.0162\n",
      "curve dfGP Run 1/1, Epoch 470/2000, Training Loss (NLML): -1.4092, (RMSE): 0.0162\n",
      "curve dfGP Run 1/1, Epoch 471/2000, Training Loss (NLML): -1.4096, (RMSE): 0.0162\n",
      "curve dfGP Run 1/1, Epoch 472/2000, Training Loss (NLML): -1.4100, (RMSE): 0.0162\n",
      "curve dfGP Run 1/1, Epoch 473/2000, Training Loss (NLML): -1.4103, (RMSE): 0.0162\n",
      "curve dfGP Run 1/1, Epoch 474/2000, Training Loss (NLML): -1.4108, (RMSE): 0.0162\n",
      "curve dfGP Run 1/1, Epoch 475/2000, Training Loss (NLML): -1.4111, (RMSE): 0.0162\n",
      "curve dfGP Run 1/1, Epoch 476/2000, Training Loss (NLML): -1.4115, (RMSE): 0.0162\n",
      "curve dfGP Run 1/1, Epoch 477/2000, Training Loss (NLML): -1.4119, (RMSE): 0.0162\n",
      "curve dfGP Run 1/1, Epoch 478/2000, Training Loss (NLML): -1.4122, (RMSE): 0.0162\n",
      "curve dfGP Run 1/1, Epoch 479/2000, Training Loss (NLML): -1.4125, (RMSE): 0.0162\n",
      "curve dfGP Run 1/1, Epoch 480/2000, Training Loss (NLML): -1.4130, (RMSE): 0.0162\n",
      "curve dfGP Run 1/1, Epoch 481/2000, Training Loss (NLML): -1.4133, (RMSE): 0.0161\n",
      "curve dfGP Run 1/1, Epoch 482/2000, Training Loss (NLML): -1.4136, (RMSE): 0.0161\n",
      "curve dfGP Run 1/1, Epoch 483/2000, Training Loss (NLML): -1.4139, (RMSE): 0.0161\n",
      "curve dfGP Run 1/1, Epoch 484/2000, Training Loss (NLML): -1.4143, (RMSE): 0.0161\n",
      "curve dfGP Run 1/1, Epoch 485/2000, Training Loss (NLML): -1.4146, (RMSE): 0.0161\n",
      "curve dfGP Run 1/1, Epoch 486/2000, Training Loss (NLML): -1.4149, (RMSE): 0.0161\n",
      "curve dfGP Run 1/1, Epoch 487/2000, Training Loss (NLML): -1.4151, (RMSE): 0.0161\n",
      "curve dfGP Run 1/1, Epoch 488/2000, Training Loss (NLML): -1.4155, (RMSE): 0.0161\n",
      "curve dfGP Run 1/1, Epoch 489/2000, Training Loss (NLML): -1.4159, (RMSE): 0.0161\n",
      "curve dfGP Run 1/1, Epoch 490/2000, Training Loss (NLML): -1.4161, (RMSE): 0.0161\n",
      "curve dfGP Run 1/1, Epoch 491/2000, Training Loss (NLML): -1.4163, (RMSE): 0.0161\n",
      "curve dfGP Run 1/1, Epoch 492/2000, Training Loss (NLML): -1.4167, (RMSE): 0.0161\n",
      "curve dfGP Run 1/1, Epoch 493/2000, Training Loss (NLML): -1.4169, (RMSE): 0.0161\n",
      "curve dfGP Run 1/1, Epoch 494/2000, Training Loss (NLML): -1.4172, (RMSE): 0.0160\n",
      "curve dfGP Run 1/1, Epoch 495/2000, Training Loss (NLML): -1.4175, (RMSE): 0.0160\n",
      "curve dfGP Run 1/1, Epoch 496/2000, Training Loss (NLML): -1.4178, (RMSE): 0.0160\n",
      "curve dfGP Run 1/1, Epoch 497/2000, Training Loss (NLML): -1.4180, (RMSE): 0.0160\n",
      "curve dfGP Run 1/1, Epoch 498/2000, Training Loss (NLML): -1.4183, (RMSE): 0.0160\n",
      "curve dfGP Run 1/1, Epoch 499/2000, Training Loss (NLML): -1.4184, (RMSE): 0.0160\n",
      "curve dfGP Run 1/1, Epoch 500/2000, Training Loss (NLML): -1.4187, (RMSE): 0.0160\n",
      "curve dfGP Run 1/1, Epoch 501/2000, Training Loss (NLML): -1.4190, (RMSE): 0.0160\n",
      "curve dfGP Run 1/1, Epoch 502/2000, Training Loss (NLML): -1.4193, (RMSE): 0.0160\n",
      "curve dfGP Run 1/1, Epoch 503/2000, Training Loss (NLML): -1.4194, (RMSE): 0.0160\n",
      "curve dfGP Run 1/1, Epoch 504/2000, Training Loss (NLML): -1.4196, (RMSE): 0.0160\n",
      "curve dfGP Run 1/1, Epoch 505/2000, Training Loss (NLML): -1.4199, (RMSE): 0.0160\n",
      "curve dfGP Run 1/1, Epoch 506/2000, Training Loss (NLML): -1.4202, (RMSE): 0.0160\n",
      "curve dfGP Run 1/1, Epoch 507/2000, Training Loss (NLML): -1.4204, (RMSE): 0.0160\n",
      "curve dfGP Run 1/1, Epoch 508/2000, Training Loss (NLML): -1.4206, (RMSE): 0.0160\n",
      "curve dfGP Run 1/1, Epoch 509/2000, Training Loss (NLML): -1.4207, (RMSE): 0.0160\n",
      "curve dfGP Run 1/1, Epoch 510/2000, Training Loss (NLML): -1.4209, (RMSE): 0.0160\n",
      "curve dfGP Run 1/1, Epoch 511/2000, Training Loss (NLML): -1.4211, (RMSE): 0.0159\n",
      "curve dfGP Run 1/1, Epoch 512/2000, Training Loss (NLML): -1.4214, (RMSE): 0.0159\n",
      "curve dfGP Run 1/1, Epoch 513/2000, Training Loss (NLML): -1.4214, (RMSE): 0.0159\n",
      "curve dfGP Run 1/1, Epoch 514/2000, Training Loss (NLML): -1.4217, (RMSE): 0.0159\n",
      "curve dfGP Run 1/1, Epoch 515/2000, Training Loss (NLML): -1.4218, (RMSE): 0.0159\n",
      "curve dfGP Run 1/1, Epoch 516/2000, Training Loss (NLML): -1.4221, (RMSE): 0.0159\n",
      "curve dfGP Run 1/1, Epoch 517/2000, Training Loss (NLML): -1.4223, (RMSE): 0.0159\n",
      "curve dfGP Run 1/1, Epoch 518/2000, Training Loss (NLML): -1.4224, (RMSE): 0.0159\n",
      "curve dfGP Run 1/1, Epoch 519/2000, Training Loss (NLML): -1.4226, (RMSE): 0.0159\n",
      "curve dfGP Run 1/1, Epoch 520/2000, Training Loss (NLML): -1.4228, (RMSE): 0.0159\n",
      "curve dfGP Run 1/1, Epoch 521/2000, Training Loss (NLML): -1.4229, (RMSE): 0.0159\n",
      "curve dfGP Run 1/1, Epoch 522/2000, Training Loss (NLML): -1.4231, (RMSE): 0.0159\n",
      "curve dfGP Run 1/1, Epoch 523/2000, Training Loss (NLML): -1.4232, (RMSE): 0.0159\n",
      "curve dfGP Run 1/1, Epoch 524/2000, Training Loss (NLML): -1.4234, (RMSE): 0.0159\n",
      "curve dfGP Run 1/1, Epoch 525/2000, Training Loss (NLML): -1.4235, (RMSE): 0.0159\n",
      "curve dfGP Run 1/1, Epoch 526/2000, Training Loss (NLML): -1.4238, (RMSE): 0.0159\n",
      "curve dfGP Run 1/1, Epoch 527/2000, Training Loss (NLML): -1.4237, (RMSE): 0.0159\n",
      "curve dfGP Run 1/1, Epoch 528/2000, Training Loss (NLML): -1.4240, (RMSE): 0.0159\n",
      "curve dfGP Run 1/1, Epoch 529/2000, Training Loss (NLML): -1.4240, (RMSE): 0.0159\n",
      "curve dfGP Run 1/1, Epoch 530/2000, Training Loss (NLML): -1.4241, (RMSE): 0.0159\n",
      "curve dfGP Run 1/1, Epoch 531/2000, Training Loss (NLML): -1.4242, (RMSE): 0.0159\n",
      "curve dfGP Run 1/1, Epoch 532/2000, Training Loss (NLML): -1.4244, (RMSE): 0.0158\n",
      "curve dfGP Run 1/1, Epoch 533/2000, Training Loss (NLML): -1.4245, (RMSE): 0.0158\n",
      "curve dfGP Run 1/1, Epoch 534/2000, Training Loss (NLML): -1.4247, (RMSE): 0.0158\n",
      "curve dfGP Run 1/1, Epoch 535/2000, Training Loss (NLML): -1.4248, (RMSE): 0.0158\n",
      "curve dfGP Run 1/1, Epoch 536/2000, Training Loss (NLML): -1.4249, (RMSE): 0.0158\n",
      "curve dfGP Run 1/1, Epoch 537/2000, Training Loss (NLML): -1.4250, (RMSE): 0.0158\n",
      "curve dfGP Run 1/1, Epoch 538/2000, Training Loss (NLML): -1.4250, (RMSE): 0.0158\n",
      "curve dfGP Run 1/1, Epoch 539/2000, Training Loss (NLML): -1.4252, (RMSE): 0.0158\n",
      "curve dfGP Run 1/1, Epoch 540/2000, Training Loss (NLML): -1.4254, (RMSE): 0.0158\n",
      "curve dfGP Run 1/1, Epoch 541/2000, Training Loss (NLML): -1.4254, (RMSE): 0.0158\n",
      "curve dfGP Run 1/1, Epoch 542/2000, Training Loss (NLML): -1.4256, (RMSE): 0.0158\n",
      "curve dfGP Run 1/1, Epoch 543/2000, Training Loss (NLML): -1.4256, (RMSE): 0.0158\n",
      "curve dfGP Run 1/1, Epoch 544/2000, Training Loss (NLML): -1.4257, (RMSE): 0.0158\n",
      "curve dfGP Run 1/1, Epoch 545/2000, Training Loss (NLML): -1.4258, (RMSE): 0.0158\n",
      "curve dfGP Run 1/1, Epoch 546/2000, Training Loss (NLML): -1.4259, (RMSE): 0.0158\n",
      "curve dfGP Run 1/1, Epoch 547/2000, Training Loss (NLML): -1.4260, (RMSE): 0.0158\n",
      "curve dfGP Run 1/1, Epoch 548/2000, Training Loss (NLML): -1.4260, (RMSE): 0.0158\n",
      "curve dfGP Run 1/1, Epoch 549/2000, Training Loss (NLML): -1.4262, (RMSE): 0.0158\n",
      "curve dfGP Run 1/1, Epoch 550/2000, Training Loss (NLML): -1.4263, (RMSE): 0.0158\n",
      "curve dfGP Run 1/1, Epoch 551/2000, Training Loss (NLML): -1.4263, (RMSE): 0.0158\n",
      "curve dfGP Run 1/1, Epoch 552/2000, Training Loss (NLML): -1.4265, (RMSE): 0.0158\n",
      "curve dfGP Run 1/1, Epoch 553/2000, Training Loss (NLML): -1.4265, (RMSE): 0.0158\n",
      "curve dfGP Run 1/1, Epoch 554/2000, Training Loss (NLML): -1.4265, (RMSE): 0.0158\n",
      "curve dfGP Run 1/1, Epoch 555/2000, Training Loss (NLML): -1.4267, (RMSE): 0.0158\n",
      "curve dfGP Run 1/1, Epoch 556/2000, Training Loss (NLML): -1.4268, (RMSE): 0.0158\n",
      "curve dfGP Run 1/1, Epoch 557/2000, Training Loss (NLML): -1.4268, (RMSE): 0.0158\n",
      "curve dfGP Run 1/1, Epoch 558/2000, Training Loss (NLML): -1.4269, (RMSE): 0.0158\n",
      "curve dfGP Run 1/1, Epoch 559/2000, Training Loss (NLML): -1.4270, (RMSE): 0.0158\n",
      "curve dfGP Run 1/1, Epoch 560/2000, Training Loss (NLML): -1.4271, (RMSE): 0.0158\n",
      "curve dfGP Run 1/1, Epoch 561/2000, Training Loss (NLML): -1.4272, (RMSE): 0.0158\n",
      "curve dfGP Run 1/1, Epoch 562/2000, Training Loss (NLML): -1.4273, (RMSE): 0.0158\n",
      "curve dfGP Run 1/1, Epoch 563/2000, Training Loss (NLML): -1.4272, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 564/2000, Training Loss (NLML): -1.4273, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 565/2000, Training Loss (NLML): -1.4274, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 566/2000, Training Loss (NLML): -1.4274, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 567/2000, Training Loss (NLML): -1.4275, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 568/2000, Training Loss (NLML): -1.4276, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 569/2000, Training Loss (NLML): -1.4276, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 570/2000, Training Loss (NLML): -1.4277, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 571/2000, Training Loss (NLML): -1.4277, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 572/2000, Training Loss (NLML): -1.4276, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 573/2000, Training Loss (NLML): -1.4278, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 574/2000, Training Loss (NLML): -1.4278, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 575/2000, Training Loss (NLML): -1.4279, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 576/2000, Training Loss (NLML): -1.4280, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 577/2000, Training Loss (NLML): -1.4281, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 578/2000, Training Loss (NLML): -1.4279, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 579/2000, Training Loss (NLML): -1.4281, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 580/2000, Training Loss (NLML): -1.4281, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 581/2000, Training Loss (NLML): -1.4282, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 582/2000, Training Loss (NLML): -1.4282, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 583/2000, Training Loss (NLML): -1.4282, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 584/2000, Training Loss (NLML): -1.4283, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 585/2000, Training Loss (NLML): -1.4284, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 586/2000, Training Loss (NLML): -1.4282, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 587/2000, Training Loss (NLML): -1.4284, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 588/2000, Training Loss (NLML): -1.4283, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 589/2000, Training Loss (NLML): -1.4285, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 590/2000, Training Loss (NLML): -1.4286, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 591/2000, Training Loss (NLML): -1.4286, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 592/2000, Training Loss (NLML): -1.4286, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 593/2000, Training Loss (NLML): -1.4286, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 594/2000, Training Loss (NLML): -1.4287, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 595/2000, Training Loss (NLML): -1.4286, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 596/2000, Training Loss (NLML): -1.4287, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 597/2000, Training Loss (NLML): -1.4287, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 598/2000, Training Loss (NLML): -1.4288, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 599/2000, Training Loss (NLML): -1.4289, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 600/2000, Training Loss (NLML): -1.4288, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 601/2000, Training Loss (NLML): -1.4288, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 602/2000, Training Loss (NLML): -1.4288, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 603/2000, Training Loss (NLML): -1.4290, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 604/2000, Training Loss (NLML): -1.4289, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 605/2000, Training Loss (NLML): -1.4289, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 606/2000, Training Loss (NLML): -1.4289, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 607/2000, Training Loss (NLML): -1.4289, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 608/2000, Training Loss (NLML): -1.4291, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 609/2000, Training Loss (NLML): -1.4290, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 610/2000, Training Loss (NLML): -1.4291, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 611/2000, Training Loss (NLML): -1.4290, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 612/2000, Training Loss (NLML): -1.4290, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 613/2000, Training Loss (NLML): -1.4291, (RMSE): 0.0157\n",
      "curve dfGP Run 1/1, Epoch 614/2000, Training Loss (NLML): -1.4291, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 615/2000, Training Loss (NLML): -1.4291, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 616/2000, Training Loss (NLML): -1.4292, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 617/2000, Training Loss (NLML): -1.4292, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 618/2000, Training Loss (NLML): -1.4292, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 619/2000, Training Loss (NLML): -1.4291, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 620/2000, Training Loss (NLML): -1.4293, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 621/2000, Training Loss (NLML): -1.4293, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 622/2000, Training Loss (NLML): -1.4293, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 623/2000, Training Loss (NLML): -1.4293, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 624/2000, Training Loss (NLML): -1.4294, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 625/2000, Training Loss (NLML): -1.4293, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 626/2000, Training Loss (NLML): -1.4294, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 627/2000, Training Loss (NLML): -1.4294, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 628/2000, Training Loss (NLML): -1.4293, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 629/2000, Training Loss (NLML): -1.4293, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 630/2000, Training Loss (NLML): -1.4294, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 631/2000, Training Loss (NLML): -1.4293, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 632/2000, Training Loss (NLML): -1.4295, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 633/2000, Training Loss (NLML): -1.4295, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 634/2000, Training Loss (NLML): -1.4294, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 635/2000, Training Loss (NLML): -1.4294, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 636/2000, Training Loss (NLML): -1.4294, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 637/2000, Training Loss (NLML): -1.4294, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 638/2000, Training Loss (NLML): -1.4294, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 639/2000, Training Loss (NLML): -1.4295, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 640/2000, Training Loss (NLML): -1.4295, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 641/2000, Training Loss (NLML): -1.4295, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 642/2000, Training Loss (NLML): -1.4295, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 643/2000, Training Loss (NLML): -1.4295, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 644/2000, Training Loss (NLML): -1.4296, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 645/2000, Training Loss (NLML): -1.4295, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 646/2000, Training Loss (NLML): -1.4296, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 647/2000, Training Loss (NLML): -1.4296, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 648/2000, Training Loss (NLML): -1.4296, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 649/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 650/2000, Training Loss (NLML): -1.4296, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 651/2000, Training Loss (NLML): -1.4296, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 652/2000, Training Loss (NLML): -1.4295, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 653/2000, Training Loss (NLML): -1.4294, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 654/2000, Training Loss (NLML): -1.4295, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 655/2000, Training Loss (NLML): -1.4296, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 656/2000, Training Loss (NLML): -1.4296, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 657/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 658/2000, Training Loss (NLML): -1.4296, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 659/2000, Training Loss (NLML): -1.4296, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 660/2000, Training Loss (NLML): -1.4296, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 661/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 662/2000, Training Loss (NLML): -1.4296, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 663/2000, Training Loss (NLML): -1.4296, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 664/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 665/2000, Training Loss (NLML): -1.4296, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 666/2000, Training Loss (NLML): -1.4296, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 667/2000, Training Loss (NLML): -1.4296, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 668/2000, Training Loss (NLML): -1.4296, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 669/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 670/2000, Training Loss (NLML): -1.4296, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 671/2000, Training Loss (NLML): -1.4295, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 672/2000, Training Loss (NLML): -1.4296, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 673/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 674/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 675/2000, Training Loss (NLML): -1.4296, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 676/2000, Training Loss (NLML): -1.4296, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 677/2000, Training Loss (NLML): -1.4296, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 678/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 679/2000, Training Loss (NLML): -1.4296, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 680/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 681/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 682/2000, Training Loss (NLML): -1.4296, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 683/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 684/2000, Training Loss (NLML): -1.4296, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 685/2000, Training Loss (NLML): -1.4296, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 686/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 687/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 688/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 689/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 690/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 691/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 692/2000, Training Loss (NLML): -1.4296, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 693/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 694/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 695/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 696/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 697/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 698/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 699/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 700/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 701/2000, Training Loss (NLML): -1.4298, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 702/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 703/2000, Training Loss (NLML): -1.4298, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 704/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 705/2000, Training Loss (NLML): -1.4296, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 706/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 707/2000, Training Loss (NLML): -1.4296, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 708/2000, Training Loss (NLML): -1.4298, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 709/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 710/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 711/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 712/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 713/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 714/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 715/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 716/2000, Training Loss (NLML): -1.4298, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 717/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 718/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 719/2000, Training Loss (NLML): -1.4296, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 720/2000, Training Loss (NLML): -1.4295, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 721/2000, Training Loss (NLML): -1.4298, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 722/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 723/2000, Training Loss (NLML): -1.4298, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 724/2000, Training Loss (NLML): -1.4298, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 725/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 726/2000, Training Loss (NLML): -1.4296, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 727/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 728/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 729/2000, Training Loss (NLML): -1.4298, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 730/2000, Training Loss (NLML): -1.4296, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 731/2000, Training Loss (NLML): -1.4298, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 732/2000, Training Loss (NLML): -1.4296, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 733/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 734/2000, Training Loss (NLML): -1.4296, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 735/2000, Training Loss (NLML): -1.4296, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 736/2000, Training Loss (NLML): -1.4298, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 737/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 738/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 739/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 740/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 741/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 742/2000, Training Loss (NLML): -1.4296, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 743/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 744/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 745/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 746/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 747/2000, Training Loss (NLML): -1.4298, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 748/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 749/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 750/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 751/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 752/2000, Training Loss (NLML): -1.4298, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 753/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 754/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 755/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 756/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 757/2000, Training Loss (NLML): -1.4298, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 758/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 759/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 760/2000, Training Loss (NLML): -1.4298, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 761/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 762/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 763/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 764/2000, Training Loss (NLML): -1.4298, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 765/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 766/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 767/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 768/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 769/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 770/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 771/2000, Training Loss (NLML): -1.4296, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 772/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "curve dfGP Run 1/1, Epoch 773/2000, Training Loss (NLML): -1.4297, (RMSE): 0.0156\n",
      "Early stopping triggered after 773 epochs.\n",
      "dfGP_test_div_field shape: torch.Size([400])\n",
      "\n",
      "Results per run saved to results_sim/dfGP/curve_dfGP_metrics_per_run.csv\n",
      "\n",
      "Mean & Std saved to results_sim/dfGP/curve_dfGP_metrics_summary.csv\n",
      "Elapsed wall time: 30.2484 seconds\n",
      "Wall time saved to results_sim/dfGP/dfGP_run_wall_time.txt.\n"
     ]
    }
   ],
   "source": [
    "# SIMULATED DATA EXPERIMENTS\n",
    "# # RUN WITH python run_sim_experiments_dfGP.py\n",
    "# \n",
    "#       ooooooooooooooooooooooooooooooooooooo\n",
    "#      8                                .d88\n",
    "#      8  oooooooooooooooooooooooooooood8888\n",
    "#      8  8888888888888888888888888P\"   8888    oooooooooooooooo\n",
    "#      8  8888888888888888888888P\"      8888    8              8\n",
    "#      8  8888888888888888888P\"         8888    8             d8\n",
    "#      8  8888888888888888P\"            8888    8            d88\n",
    "#      8  8888888888888P\"               8888    8           d888\n",
    "#      8  8888888888P\"                  8888    8          d8888\n",
    "#      8  8888888P\"                     8888    8         d88888\n",
    "#      8  8888P\"                        8888    8        d888888\n",
    "#      8  8888oooooooooooooooooooooocgmm8888    8       d8888888\n",
    "#      8 .od88888888888888888888888888888888    8      d88888888\n",
    "#      8888888888888888888888888888888888888    8     d888888888\n",
    "#                                               8    d8888888888\n",
    "#         ooooooooooooooooooooooooooooooo       8   d88888888888\n",
    "#        d                       ...oood8b      8  d888888888888\n",
    "#       d              ...oood888888888888b     8 d8888888888888\n",
    "#      d     ...oood88888888888888888888888b    8d88888888888888\n",
    "#     dood8888888888888888888888888888888888b\n",
    "#\n",
    "#\n",
    "# This artwork is a visual reminder that this script is for the sim experiments.\n",
    "\n",
    "model_name = \"dfGP\"\n",
    "\n",
    "# import configs to we can access the hypers with getattr\n",
    "import configs\n",
    "from configs import PATIENCE, MAX_NUM_EPOCHS, NUM_RUNS, WEIGHT_DECAY\n",
    "# also import x_test grid size and std noise for training data\n",
    "from configs import N_SIDE, STD_GAUSSIAN_NOISE\n",
    "from configs import TRACK_EMISSIONS_BOOL\n",
    "\n",
    "# Reiterating import for visibility\n",
    "MAX_NUM_EPOCHS = MAX_NUM_EPOCHS\n",
    "NUM_RUNS = 1\n",
    "WEIGHT_DECAY = WEIGHT_DECAY\n",
    "PATIENCE = PATIENCE\n",
    "\n",
    "# assign model-specific variable\n",
    "MODEL_LEARNING_RATE = getattr(configs, f\"{model_name}_SIM_LEARNING_RATE\")\n",
    "MODEL_SIM_RESULTS_DIR = getattr(configs, f\"{model_name}_SIM_RESULTS_DIR\")\n",
    "import os\n",
    "os.makedirs(MODEL_SIM_RESULTS_DIR, exist_ok = True)\n",
    "\n",
    "# imports for probabilistic models\n",
    "if model_name in [\"GP\", \"dfGP\", \"dfNGP\"]:\n",
    "    from GP_models import GP_predict\n",
    "    from metrics import compute_NLL_sparse, compute_NLL_full\n",
    "    from configs import L_RANGE, SIGMA_N_RANGE, GP_PATIENCE\n",
    "    # overwrite with GP_PATIENCE\n",
    "    PATIENCE = GP_PATIENCE\n",
    "\n",
    "    if model_name in [\"dfGP\", \"dfNGP\"]:\n",
    "        from configs import SIGMA_F_RANGE\n",
    "\n",
    "# universals \n",
    "from metrics import compute_RMSE, compute_MAE, compute_divergence_field\n",
    "\n",
    "# basics\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn # NOTE: we also use this module for GP params\n",
    "import torch.optim as optim\n",
    "import gpytorch\n",
    "\n",
    "# utilitarian\n",
    "from utils import set_seed, make_grid\n",
    "# reproducibility\n",
    "set_seed(42)\n",
    "import gc\n",
    "\n",
    "# setting device to GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# overwrite if needed: # device = 'cpu'\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "### START TIMING ###\n",
    "import time\n",
    "start_time = time.time()  # Start timing after imports\n",
    "\n",
    "### START TRACKING EXPERIMENT EMISSIONS ###\n",
    "if TRACK_EMISSIONS_BOOL:\n",
    "    from codecarbon import EmissionsTracker\n",
    "    tracker = EmissionsTracker(project_name = \"dfGP_simulation_experiments\", output_dir = MODEL_SIM_RESULTS_DIR)\n",
    "    tracker.start()\n",
    "\n",
    "### SIMULATION ###\n",
    "# Import all simulation functions\n",
    "from simulate import (\n",
    "    simulate_detailed_branching,\n",
    "    # simulate_detailed_convergence,\n",
    "    simulate_detailed_curve,\n",
    "    simulate_detailed_deflection,\n",
    "    simulate_detailed_edge,\n",
    "    simulate_detailed_ridges,\n",
    ")\n",
    "\n",
    "# Define simulations as a dictionary with names as keys to function objects\n",
    "# alphabectic order here\n",
    "simulations = {\n",
    "    \"curve\": simulate_detailed_curve,\n",
    "}\n",
    "\n",
    "########################\n",
    "### x_train & x_test ###\n",
    "########################\n",
    "\n",
    "# Load training inputs (once for all simulations)\n",
    "x_train = torch.load(\"data/sim_data/x_train_lines_discretised_0to1.pt\", weights_only = False).float()\n",
    "\n",
    "# Generate x_test (long) once for all simulations\n",
    "_, x_test = make_grid(N_SIDE)\n",
    "# x_test is long format (N_SIDE ** 2, 2)\n",
    "\n",
    "#################################\n",
    "### LOOP 1 - over SIMULATIONS ###\n",
    "#################################\n",
    "\n",
    "# Make y_train_dict: Iterate over all simulation functions\n",
    "for sim_name, sim_func in simulations.items():\n",
    "\n",
    "    ########################\n",
    "    ### y_train & y_test ###\n",
    "    ########################\n",
    "\n",
    "    # Generate training observations\n",
    "    # NOTE: sim_func() needs to be on CPU, so we move x_train to CPU\n",
    "    y_train = sim_func(x_train.cpu()).to(device)\n",
    "    y_test = sim_func(x_test.cpu()).to(device)\n",
    "    \n",
    "    x_test = x_test.to(device)\n",
    "    x_train = x_train.to(device)\n",
    "\n",
    "    # Print details\n",
    "    print(f\"=== {sim_name.upper()} ===\")\n",
    "    print(f\"Training inputs shape: {x_train.shape}\")\n",
    "    print(f\"Training observations shape: {y_train.shape}\")\n",
    "    print(f\"Training inputs dtype: {x_train.dtype}\")\n",
    "    print(f\"Training inputs device: {y_train.device}\")\n",
    "    print(f\"Training observations device: {y_train.device}\")\n",
    "    print()\n",
    "\n",
    "    # Print details\n",
    "    print(f\"=== {sim_name.upper()} ===\")\n",
    "    print(f\"Test inputs shape: {x_test.shape}\")\n",
    "    print(f\"Test observations shape: {y_test.shape}\")\n",
    "    print(f\"Test inputs dtype: {x_test.dtype}\")\n",
    "    print(f\"Test inputs device: {x_test.device}\")\n",
    "    print(f\"Test observations device: {y_test.device}\")\n",
    "    print()\n",
    "\n",
    "    # NOTE: This is different to the real data experiments\n",
    "    # calculate the mean magnitude of the test data as we use this to scale the noise\n",
    "    sim_mean_magnitude_for_noise = torch.norm(y_test, dim = -1).mean().to(device)\n",
    "    sim_noise = STD_GAUSSIAN_NOISE * sim_mean_magnitude_for_noise\n",
    "\n",
    "    # Store metrics for the simulation (used for *metrics_summary* report and *metrics_per_run*)\n",
    "    simulation_results = [] \n",
    "\n",
    "    ##################################\n",
    "    ### LOOP 2 - over training run ###\n",
    "    ##################################\n",
    "\n",
    "    # NOTE: GPs don't train on batches, use full data\n",
    "\n",
    "    for run in range(NUM_RUNS):\n",
    "\n",
    "        print(f\"\\n--- Training Run {run + 1}/{NUM_RUNS} ---\")\n",
    "\n",
    "        # initialising (trainable) noise scalar from a uniform distribution over a predefined range\n",
    "        sigma_n = nn.Parameter(torch.empty(1, device = device).uniform_( * SIGMA_N_RANGE)) # Trainable\n",
    "\n",
    "        # Initialise the likelihood for the GP model\n",
    "        likelihood = gpytorch.likelihoods.GaussianLikelihood().to(device)\n",
    "\n",
    "        # Intialise fresh GP model\n",
    "        model = DivergenceFreeGPModel(\n",
    "            x_train.T.reshape(-1),\n",
    "            y_train_noisy.T.reshape(-1), \n",
    "            likelihood\n",
    "            ).to(device)\n",
    "\n",
    "        # Use ExactMarginalLogLikelihood\n",
    "        mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "        model.train()\n",
    "        likelihood.train()\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr = MODEL_LEARNING_RATE, weight_decay = WEIGHT_DECAY)\n",
    "        # _________________\n",
    "        # BEFORE EPOCH LOOP\n",
    "\n",
    "        # Export the convergence just for first run only\n",
    "        if run == 0:\n",
    "            # initialise tensors to store losses over epochs (for convergence plot)\n",
    "            train_losses_NLML_over_epochs = torch.zeros(MAX_NUM_EPOCHS) # objective\n",
    "            train_losses_RMSE_over_epochs = torch.zeros(MAX_NUM_EPOCHS) # by-product\n",
    "            # monitor performance transfer to test (only RMSE easy to calc without covar)\n",
    "            test_losses_RMSE_over_epochs = torch.zeros(MAX_NUM_EPOCHS)\n",
    "\n",
    "            sigma_n_over_epochs = torch.zeros(MAX_NUM_EPOCHS)\n",
    "            sigma_f_over_epochs = torch.zeros(MAX_NUM_EPOCHS)\n",
    "            l1_over_epochs = torch.zeros(MAX_NUM_EPOCHS)\n",
    "            l2_over_epochs = torch.zeros(MAX_NUM_EPOCHS)\n",
    "\n",
    "        # Early stopping variables\n",
    "        best_loss = float('inf')\n",
    "        # counter starts at 0\n",
    "        epochs_no_improve = 0\n",
    "\n",
    "        # Additive noise model: independent Gaussian noise\n",
    "        # For every run we have a FIXED NOISY TARGET. Draw from standard normal with appropriate std\n",
    "        y_train_noisy = y_train + (torch.randn(y_train.shape, device = device) * sim_noise)\n",
    "\n",
    "        ############################\n",
    "        ### LOOP 3 - over EPOCHS ###\n",
    "        ############################\n",
    "\n",
    "        print(\"\\nStart Training\")\n",
    "\n",
    "        for epoch in range(MAX_NUM_EPOCHS):\n",
    "        \n",
    "            # For Run 1 we save a bunch of metrics and update, while for the rest we only update\n",
    "            if run == 0:\n",
    "\n",
    "                model.train()\n",
    "                likelihood.train()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                # model outputs a multivariate normal distribution\n",
    "                output = model(x_train.T.reshape(-1).to(device))\n",
    "                loss = - mll(output, y_train_noisy.T.reshape(-1).to(device))  # negative log marginal likelihood\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                model.eval()\n",
    "                likelihood.eval()\n",
    "\n",
    "                train_pred = model(x_train.T.reshape(-1).to(device))\n",
    "                test_pred = model(x_test.T.reshape(-1).to(device))\n",
    "\n",
    "                train_RMSE = torch.sqrt(gpytorch.metrics.mean_squared_error(train_pred, y_train.T.reshape(-1).to(device)))\n",
    "                test_RMSE = torch.sqrt(gpytorch.metrics.mean_squared_error(test_pred, y_test.T.reshape(-1).to(device)))\n",
    "\n",
    "                # Add to list\n",
    "                train_losses_NLML_over_epochs[epoch] = loss.item()\n",
    "                train_losses_RMSE_over_epochs[epoch] = train_RMSE.item()\n",
    "                test_losses_RMSE_over_epochs[epoch] = test_RMSE.item()\n",
    "\n",
    "                sigma_n_over_epochs[epoch] = model.likelihood.noise.item()\n",
    "                sigma_f_over_epochs[epoch] = model.covar_module.outputscale.item()\n",
    "                l1_over_epochs[epoch] = model.covar_module.lengthscale[0].item()\n",
    "                l2_over_epochs[epoch] = model.covar_module.lengthscale[1].item()\n",
    "\n",
    "                print(f\"{sim_name} {model_name} Run {run + 1}/{NUM_RUNS}, Epoch {epoch + 1}/{MAX_NUM_EPOCHS}, Training Loss (NLML): {loss:.4f}, (RMSE): {train_RMSE:.4f}\")\n",
    "\n",
    "                del train_pred, test_pred, train_RMSE, test_RMSE\n",
    "\n",
    "                # Free up memory every 20 epochs\n",
    "                if epoch % 20 == 0:\n",
    "                    gc.collect() and torch.cuda.empty_cache()\n",
    "\n",
    "            else: \n",
    "                # After Run 1 we only update the model, no metrics\n",
    "\n",
    "                model.train()\n",
    "                likelihood.train()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                # model outputs a multivariate normal distribution\n",
    "                output = model(x_train.T.reshape(-1).to(device))\n",
    "                loss = - mll(output, y_train_noisy.T.reshape(-1).to(device))  # negative log marginal likelihood\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                print(f\"{sim_name} {model_name} Run {run + 1}/{NUM_RUNS}, Epoch {epoch + 1}/{MAX_NUM_EPOCHS}, Training Loss (NLML): {loss:.4f}\")\n",
    "\n",
    "            # EVERY EPOCH: Early stopping check\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                # reset counter if loss improves\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "\n",
    "            if epochs_no_improve >= PATIENCE:\n",
    "                print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
    "                # exit epoch loop\n",
    "                break\n",
    "\n",
    "        ##############################\n",
    "        ### END LOOP 3 over EPOCHS ###\n",
    "        ##############################\n",
    "\n",
    "        # for every run...\n",
    "        #######################################################\n",
    "        ### EVALUATE after all training for RUN is finished ###\n",
    "        #######################################################\n",
    "\n",
    "        model.eval()\n",
    "        likelihood.eval()\n",
    "\n",
    "        # Need gradients for autograd divergence: We clone and detach\n",
    "        x_test_grad = x_test.to(device).clone().requires_grad_(True)\n",
    "\n",
    "        # Likelihood\n",
    "        pred_dist_test = likelihood(model(x_test_grad.T.reshape(-1)))\n",
    "        \n",
    "        # Compute divergence field\n",
    "        dfGP_test_div_field = compute_divergence_field(pred_dist_test.mean.reshape(2, -1).T, x_test_grad)\n",
    "        print(f\"dfGP_test_div_field shape: {dfGP_test_div_field.shape}\")\n",
    "\n",
    "        # Only save mean_pred, covar_pred and divergence fields for the first run\n",
    "        if run == 0:\n",
    "\n",
    "            # (1) Save predictions from first run so we can visualise them later\n",
    "            torch.save(pred_dist_test.mean.reshape(2, -1).T, f\"{MODEL_SIM_RESULTS_DIR}/{sim_name}_{model_name}_test_mean_predictions.pt\")\n",
    "            torch.save(pred_dist_test.covariance_matrix, f\"{MODEL_SIM_RESULTS_DIR}/{sim_name}_{model_name}_test_covar_predictions.pt\")\n",
    "\n",
    "            # (2) Save best hyperparameters\n",
    "            # Stack tensors into a single tensor\n",
    "            best_hypers_tensor = torch.cat([\n",
    "                model.likelihood.noise.reshape(-1), \n",
    "                model.covar_module.outputscale.reshape(-1),\n",
    "                model.covar_module.lengthscale.reshape(-1), # flatten\n",
    "            ])\n",
    "\n",
    "            torch.save(best_hypers_tensor, f\"{MODEL_SIM_RESULTS_DIR}/{sim_name}_{model_name}_best_hypers.pt\")\n",
    "\n",
    "            # (3) Since all epoch training is finished, we can save the losses over epochs\n",
    "            df_losses = pd.DataFrame({\n",
    "                'Epoch': list(range(train_losses_NLML_over_epochs.shape[0])), # pythonic indexing\n",
    "                'Train Loss NLML': train_losses_NLML_over_epochs.tolist(),\n",
    "                'Train Loss RMSE': train_losses_RMSE_over_epochs.tolist(),\n",
    "                'Test Loss RMSE': test_losses_RMSE_over_epochs.tolist(),\n",
    "                'Sigma_n': sigma_n_over_epochs.tolist(),\n",
    "                'Sigma_f': sigma_f_over_epochs.tolist(),\n",
    "                'l1': l1_over_epochs.tolist(),\n",
    "                'l2': l2_over_epochs.tolist()\n",
    "                })\n",
    "            \n",
    "            df_losses.to_csv(f\"{MODEL_SIM_RESULTS_DIR}/{sim_name}_{model_name}_losses_over_epochs.csv\", index = False, float_format = \"%.5f\") # reduce to 5 decimals for readability\n",
    "\n",
    "            # (4) Save divergence field (computed above for all runs)\n",
    "            torch.save(dfGP_test_div_field, f\"{MODEL_SIM_RESULTS_DIR}/{sim_name}_{model_name}_test_prediction_divergence_field.pt\")\n",
    "\n",
    "        x_train_grad = x_train.to(device).clone().requires_grad_(True)\n",
    "\n",
    "        # NOTE: intialise new model without x_noisy\n",
    "\n",
    "        pred_dist_train = likelihood(model(x_train_grad.T.reshape(-1)))\n",
    "        \n",
    "        dfGP_train_div_field = compute_divergence_field(pred_dist_train.mean.reshape(2, -1).T, x_train_grad)\n",
    "\n",
    "        # Divergence: Convert field to metric: mean absolute divergence\n",
    "        # NOTE: It is important to use the absolute value of the divergence field, since positive and negative deviations are violations and shouldn't cancel each other out \n",
    "        dfGP_train_div = dfGP_train_div_field.abs().mean().item()\n",
    "        dfGP_test_div = dfGP_test_div_field.abs().mean().item()\n",
    "\n",
    "        # Compute metrics (convert tensors to float) for every run's tuned model\n",
    "        dfGP_train_RMSE = torch.sqrt(gpytorch.metrics.mean_squared_error(pred_dist_train, y_train.T.reshape(-1).to(device))).item()\n",
    "        dfGP_train_MAE = gpytorch.metrics.mean_absolute_error(pred_dist_train, y_train.T.reshape(-1).to(device)).item()\n",
    "        dfGP_train_NLPD = gpytorch.metrics.negative_log_predictive_density(pred_dist_train, y_train.T.reshape(-1).to(device)).item()\n",
    "        dfGP_train_QCE = gpytorch.metrics.quantile_coverage_error(pred_dist_train, y_train.T.reshape(-1), quantile = 95).item()\n",
    "\n",
    "        dfGP_test_RMSE = torch.sqrt(gpytorch.metrics.mean_squared_error(pred_dist_test, y_test.T.reshape(-1).to(device))).item()\n",
    "        dfGP_test_MAE = gpytorch.metrics.mean_absolute_error(pred_dist_test, y_test.T.reshape(-1).to(device)).item()\n",
    "        dfGP_test_NLPD = gpytorch.metrics.negative_log_predictive_density(pred_dist_test, y_test.T.reshape(-1).to(device)).item()\n",
    "        dfGP_test_QCE = gpytorch.metrics.quantile_coverage_error(pred_dist_test, y_test.T.reshape(-1), quantile = 95).item()\n",
    "\n",
    "        simulation_results.append([\n",
    "            run + 1,\n",
    "            dfGP_train_RMSE, dfGP_train_MAE, dfGP_train_NLPD, dfGP_train_QCE, dfGP_train_div,\n",
    "            dfGP_test_RMSE, dfGP_test_MAE, dfGP_test_NLPD, dfGP_test_QCE, dfGP_test_div\n",
    "        ])\n",
    "\n",
    "    ############################\n",
    "    ### END LOOP 2 over RUNS ###\n",
    "    ############################\n",
    "\n",
    "    # Convert results to a Pandas DataFrame\n",
    "    results_per_run = pd.DataFrame(\n",
    "        simulation_results, \n",
    "        columns = [\"Run\", \n",
    "                   \"Train RMSE\", \"Train MAE\", \"Train NLPD\", \"Train QCE\", \"Train MAD\",\n",
    "                   \"Test RMSE\", \"Test MAE\", \"Test NLPD\", \"Test QCE\", \"Test MAD\"])\n",
    "\n",
    "    # Compute mean and standard deviation for each metric\n",
    "    mean_std_df = results_per_run.iloc[:, 1:].agg([\"mean\", \"std\"]) # Exclude \"Run\" column\n",
    "\n",
    "    # Add sim_name and model_name as columns in the DataFrame _metrics_summary to be able to copy df\n",
    "    mean_std_df[\"simulation name\"] = sim_name\n",
    "    mean_std_df[\"model name\"] = model_name\n",
    "\n",
    "    # Save \"_metrics_per_run.csv\" to CSV\n",
    "    path_to_metrics_per_run = os.path.join(MODEL_SIM_RESULTS_DIR, f\"{sim_name}_{model_name}_metrics_per_run.csv\")\n",
    "    results_per_run.to_csv(path_to_metrics_per_run, index = False, float_format = \"%.5f\") # reduce to 5 decimals\n",
    "    print(f\"\\nResults per run saved to {path_to_metrics_per_run}\")\n",
    "\n",
    "    # Save \"_metrics_summary.csv\" to CSV\n",
    "    path_to_metrics_summary = os.path.join(MODEL_SIM_RESULTS_DIR, f\"{sim_name}_{model_name}_metrics_summary.csv\")\n",
    "    mean_std_df.to_csv(path_to_metrics_summary, float_format = \"%.5f\") # reduce to 5 decimals\n",
    "    print(f\"\\nMean & Std saved to {path_to_metrics_summary}\")\n",
    "\n",
    "###############################\n",
    "### END LOOP 1 over REGIONS ###\n",
    "###############################\n",
    "\n",
    "#############################\n",
    "### WALL time & GPU model ###\n",
    "#############################\n",
    "\n",
    "end_time = time.time()\n",
    "# compute elapsed time\n",
    "elapsed_time = end_time - start_time \n",
    "# convert elapsed time to minutes\n",
    "elapsed_time_minutes = elapsed_time / 60\n",
    "\n",
    "# also end emission tracking. Will be saved as emissions.csv\n",
    "if TRACK_EMISSIONS_BOOL:\n",
    "    tracker.stop()\n",
    "\n",
    "if device == \"cuda\":\n",
    "    # get name of GPU model\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "else:\n",
    "    gpu_name = \"N/A\"\n",
    "\n",
    "print(f\"Elapsed wall time: {elapsed_time:.4f} seconds\")\n",
    "\n",
    "# Define full path for the file\n",
    "wall_time_and_gpu_path = os.path.join(MODEL_SIM_RESULTS_DIR, model_name + \"_run_\" \"wall_time.txt\")\n",
    "\n",
    "# Save to the correct folder with both seconds and minutes\n",
    "with open(wall_time_and_gpu_path, \"w\") as f:\n",
    "    f.write(f\"Elapsed wall time: {elapsed_time:.4f} seconds\\n\")\n",
    "    f.write(f\"Elapsed wall time: {elapsed_time_minutes:.2f} minutes\\n\")\n",
    "    f.write(f\"Device used: {device}\\n\")\n",
    "    f.write(f\"GPU model: {gpu_name}\\n\")\n",
    "\n",
    "print(f\"Wall time saved to {wall_time_and_gpu_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ceceee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
