{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a6bb43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 10:03:11] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 10:03:11] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 10:03:11] No CPU tracking mode found. Falling back on CPU constant mode. \n",
      " Linux OS detected: Please ensure RAPL files exist at \\sys\\class\\powercap\\intel-rapl to measure CPU\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 10:03:13] We saw that you have a Intel(R) Xeon(R) CPU E5-2630 v4 @ 2.20GHz but we don't know it. Please contact us.\n",
      "[codecarbon INFO @ 10:03:13] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU E5-2630 v4 @ 2.20GHz\n",
      "[codecarbon INFO @ 10:03:13] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 10:03:13] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 10:03:13] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 10:03:13]   Platform system: Linux-5.15.0-134-generic-x86_64-with-glibc2.31\n",
      "[codecarbon INFO @ 10:03:13]   Python version: 3.9.20\n",
      "[codecarbon INFO @ 10:03:13]   CodeCarbon version: 2.8.4\n",
      "[codecarbon INFO @ 10:03:13]   Available RAM : 62.767 GB\n",
      "[codecarbon INFO @ 10:03:13]   CPU count: 40\n",
      "[codecarbon INFO @ 10:03:13]   CPU model: Intel(R) Xeon(R) CPU E5-2630 v4 @ 2.20GHz\n",
      "[codecarbon INFO @ 10:03:13]   GPU count: 1\n",
      "[codecarbon INFO @ 10:03:13]   GPU model: 1 x NVIDIA GeForce RTX 4090\n",
      "[codecarbon INFO @ 10:03:16] Emissions data (if any) will be saved to file /home/kim/ice_thickness/results_sim/dfNGP/emissions.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CURVE ===\n",
      "Training inputs shape: torch.Size([196, 2])\n",
      "Training observations shape: torch.Size([196, 2])\n",
      "Training inputs dtype: torch.float32\n",
      "Training inputs device: cuda:0\n",
      "Training observations device: cuda:0\n",
      "\n",
      "=== CURVE ===\n",
      "Test inputs shape: torch.Size([400, 2])\n",
      "Test observations shape: torch.Size([400, 2])\n",
      "Test inputs dtype: torch.float32\n",
      "Test inputs device: cuda:0\n",
      "Test observations device: cuda:0\n",
      "\n",
      "\n",
      "--- Training Run 1/1 ---\n",
      "\n",
      "Start Training\n",
      "curve dfNGP Run 1/1, Epoch 1/2000, Training Loss (NLML): -106.2010, (RMSE): 0.0887\n",
      "curve dfNGP Run 1/1, Epoch 2/2000, Training Loss (NLML): -264.0489, (RMSE): 0.0891\n",
      "curve dfNGP Run 1/1, Epoch 3/2000, Training Loss (NLML): -347.9341, (RMSE): 0.0888\n",
      "curve dfNGP Run 1/1, Epoch 4/2000, Training Loss (NLML): -391.5256, (RMSE): 0.0880\n",
      "curve dfNGP Run 1/1, Epoch 5/2000, Training Loss (NLML): -414.9910, (RMSE): 0.0867\n",
      "curve dfNGP Run 1/1, Epoch 6/2000, Training Loss (NLML): -427.9481, (RMSE): 0.0852\n",
      "curve dfNGP Run 1/1, Epoch 7/2000, Training Loss (NLML): -434.7980, (RMSE): 0.0833\n",
      "curve dfNGP Run 1/1, Epoch 8/2000, Training Loss (NLML): -439.0032, (RMSE): 0.0813\n",
      "curve dfNGP Run 1/1, Epoch 9/2000, Training Loss (NLML): -441.4507, (RMSE): 0.0791\n",
      "curve dfNGP Run 1/1, Epoch 10/2000, Training Loss (NLML): -443.1601, (RMSE): 0.0769\n",
      "curve dfNGP Run 1/1, Epoch 11/2000, Training Loss (NLML): -444.7952, (RMSE): 0.0746\n",
      "curve dfNGP Run 1/1, Epoch 12/2000, Training Loss (NLML): -446.4153, (RMSE): 0.0723\n",
      "curve dfNGP Run 1/1, Epoch 13/2000, Training Loss (NLML): -448.2961, (RMSE): 0.0700\n",
      "curve dfNGP Run 1/1, Epoch 14/2000, Training Loss (NLML): -450.6432, (RMSE): 0.0678\n",
      "curve dfNGP Run 1/1, Epoch 15/2000, Training Loss (NLML): -453.3616, (RMSE): 0.0658\n",
      "curve dfNGP Run 1/1, Epoch 16/2000, Training Loss (NLML): -456.5241, (RMSE): 0.0638\n",
      "curve dfNGP Run 1/1, Epoch 17/2000, Training Loss (NLML): -460.2236, (RMSE): 0.0620\n",
      "curve dfNGP Run 1/1, Epoch 18/2000, Training Loss (NLML): -464.3051, (RMSE): 0.0603\n",
      "curve dfNGP Run 1/1, Epoch 19/2000, Training Loss (NLML): -468.9203, (RMSE): 0.0587\n",
      "curve dfNGP Run 1/1, Epoch 20/2000, Training Loss (NLML): -474.0980, (RMSE): 0.0573\n",
      "curve dfNGP Run 1/1, Epoch 21/2000, Training Loss (NLML): -479.7424, (RMSE): 0.0559\n",
      "curve dfNGP Run 1/1, Epoch 22/2000, Training Loss (NLML): -485.9598, (RMSE): 0.0546\n",
      "curve dfNGP Run 1/1, Epoch 23/2000, Training Loss (NLML): -492.6682, (RMSE): 0.0534\n",
      "curve dfNGP Run 1/1, Epoch 24/2000, Training Loss (NLML): -499.9751, (RMSE): 0.0523\n",
      "curve dfNGP Run 1/1, Epoch 25/2000, Training Loss (NLML): -507.7997, (RMSE): 0.0511\n",
      "curve dfNGP Run 1/1, Epoch 26/2000, Training Loss (NLML): -516.2317, (RMSE): 0.0500\n",
      "curve dfNGP Run 1/1, Epoch 27/2000, Training Loss (NLML): -525.3463, (RMSE): 0.0488\n",
      "curve dfNGP Run 1/1, Epoch 28/2000, Training Loss (NLML): -535.1360, (RMSE): 0.0476\n",
      "curve dfNGP Run 1/1, Epoch 29/2000, Training Loss (NLML): -545.4932, (RMSE): 0.0463\n",
      "curve dfNGP Run 1/1, Epoch 30/2000, Training Loss (NLML): -556.5569, (RMSE): 0.0449\n",
      "curve dfNGP Run 1/1, Epoch 31/2000, Training Loss (NLML): -568.0829, (RMSE): 0.0433\n",
      "curve dfNGP Run 1/1, Epoch 32/2000, Training Loss (NLML): -580.1528, (RMSE): 0.0415\n",
      "curve dfNGP Run 1/1, Epoch 33/2000, Training Loss (NLML): -593.1990, (RMSE): 0.0398\n",
      "curve dfNGP Run 1/1, Epoch 34/2000, Training Loss (NLML): -606.2569, (RMSE): 0.0376\n",
      "curve dfNGP Run 1/1, Epoch 35/2000, Training Loss (NLML): -620.4673, (RMSE): 0.0354\n",
      "curve dfNGP Run 1/1, Epoch 36/2000, Training Loss (NLML): -634.5389, (RMSE): 0.0327\n",
      "curve dfNGP Run 1/1, Epoch 37/2000, Training Loss (NLML): -648.9081, (RMSE): 0.0302\n",
      "curve dfNGP Run 1/1, Epoch 38/2000, Training Loss (NLML): -661.0008, (RMSE): 0.0277\n",
      "curve dfNGP Run 1/1, Epoch 39/2000, Training Loss (NLML): -669.3953, (RMSE): 0.0254\n",
      "curve dfNGP Run 1/1, Epoch 40/2000, Training Loss (NLML): -673.2218, (RMSE): 0.0234\n",
      "curve dfNGP Run 1/1, Epoch 41/2000, Training Loss (NLML): -673.6406, (RMSE): 0.0221\n",
      "curve dfNGP Run 1/1, Epoch 42/2000, Training Loss (NLML): -678.8253, (RMSE): 0.0208\n",
      "curve dfNGP Run 1/1, Epoch 43/2000, Training Loss (NLML): -686.6597, (RMSE): 0.0199\n",
      "curve dfNGP Run 1/1, Epoch 44/2000, Training Loss (NLML): -696.4082, (RMSE): 0.0191\n",
      "curve dfNGP Run 1/1, Epoch 45/2000, Training Loss (NLML): -704.1534, (RMSE): 0.0181\n",
      "curve dfNGP Run 1/1, Epoch 46/2000, Training Loss (NLML): -704.5477, (RMSE): 0.0176\n",
      "curve dfNGP Run 1/1, Epoch 47/2000, Training Loss (NLML): -700.4869, (RMSE): 0.0171\n",
      "curve dfNGP Run 1/1, Epoch 48/2000, Training Loss (NLML): -695.7783, (RMSE): 0.0164\n",
      "curve dfNGP Run 1/1, Epoch 49/2000, Training Loss (NLML): -695.6412, (RMSE): 0.0159\n",
      "curve dfNGP Run 1/1, Epoch 50/2000, Training Loss (NLML): -694.9417, (RMSE): 0.0157\n",
      "curve dfNGP Run 1/1, Epoch 51/2000, Training Loss (NLML): -696.4907, (RMSE): 0.0153\n",
      "curve dfNGP Run 1/1, Epoch 52/2000, Training Loss (NLML): -697.7370, (RMSE): 0.0152\n",
      "curve dfNGP Run 1/1, Epoch 53/2000, Training Loss (NLML): -698.8665, (RMSE): 0.0149\n",
      "curve dfNGP Run 1/1, Epoch 54/2000, Training Loss (NLML): -699.9619, (RMSE): 0.0148\n",
      "curve dfNGP Run 1/1, Epoch 55/2000, Training Loss (NLML): -700.7329, (RMSE): 0.0149\n",
      "curve dfNGP Run 1/1, Epoch 56/2000, Training Loss (NLML): -696.0603, (RMSE): 0.0143\n",
      "curve dfNGP Run 1/1, Epoch 57/2000, Training Loss (NLML): -696.3082, (RMSE): 0.0144\n",
      "curve dfNGP Run 1/1, Epoch 58/2000, Training Loss (NLML): -697.6234, (RMSE): 0.0147\n",
      "curve dfNGP Run 1/1, Epoch 59/2000, Training Loss (NLML): -701.4504, (RMSE): 0.0146\n",
      "curve dfNGP Run 1/1, Epoch 60/2000, Training Loss (NLML): -704.7719, (RMSE): 0.0148\n",
      "curve dfNGP Run 1/1, Epoch 61/2000, Training Loss (NLML): -704.3808, (RMSE): 0.0149\n",
      "curve dfNGP Run 1/1, Epoch 62/2000, Training Loss (NLML): -702.7036, (RMSE): 0.0148\n",
      "curve dfNGP Run 1/1, Epoch 63/2000, Training Loss (NLML): -705.6594, (RMSE): 0.0150\n",
      "curve dfNGP Run 1/1, Epoch 64/2000, Training Loss (NLML): -704.2171, (RMSE): 0.0150\n",
      "curve dfNGP Run 1/1, Epoch 65/2000, Training Loss (NLML): -704.7480, (RMSE): 0.0150\n",
      "curve dfNGP Run 1/1, Epoch 66/2000, Training Loss (NLML): -705.7879, (RMSE): 0.0154\n",
      "curve dfNGP Run 1/1, Epoch 67/2000, Training Loss (NLML): -707.0218, (RMSE): 0.0153\n",
      "curve dfNGP Run 1/1, Epoch 68/2000, Training Loss (NLML): -708.5933, (RMSE): 0.0155\n",
      "curve dfNGP Run 1/1, Epoch 69/2000, Training Loss (NLML): -706.9390, (RMSE): 0.0157\n",
      "curve dfNGP Run 1/1, Epoch 70/2000, Training Loss (NLML): -707.0394, (RMSE): 0.0158\n",
      "curve dfNGP Run 1/1, Epoch 71/2000, Training Loss (NLML): -707.2078, (RMSE): 0.0160\n",
      "curve dfNGP Run 1/1, Epoch 72/2000, Training Loss (NLML): -705.7713, (RMSE): 0.0160\n",
      "curve dfNGP Run 1/1, Epoch 73/2000, Training Loss (NLML): -708.0522, (RMSE): 0.0162\n",
      "curve dfNGP Run 1/1, Epoch 74/2000, Training Loss (NLML): -709.9635, (RMSE): 0.0165\n",
      "curve dfNGP Run 1/1, Epoch 75/2000, Training Loss (NLML): -707.4552, (RMSE): 0.0164\n",
      "curve dfNGP Run 1/1, Epoch 76/2000, Training Loss (NLML): -708.6357, (RMSE): 0.0166\n",
      "curve dfNGP Run 1/1, Epoch 77/2000, Training Loss (NLML): -707.7606, (RMSE): 0.0168\n",
      "curve dfNGP Run 1/1, Epoch 78/2000, Training Loss (NLML): -708.0275, (RMSE): 0.0169\n",
      "curve dfNGP Run 1/1, Epoch 79/2000, Training Loss (NLML): -707.8895, (RMSE): 0.0167\n",
      "curve dfNGP Run 1/1, Epoch 80/2000, Training Loss (NLML): -707.6680, (RMSE): 0.0166\n",
      "curve dfNGP Run 1/1, Epoch 81/2000, Training Loss (NLML): -708.6689, (RMSE): 0.0167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 10:03:20] Energy consumed for RAM : 0.000785 kWh. RAM Power : 23.53756284713745 W\n",
      "[codecarbon INFO @ 10:03:20] Energy consumed for all CPUs : 0.001418 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:03:20] Energy consumed for all GPUs : 0.002214 kWh. Total GPU Power : 44.67618859031344 W\n",
      "[codecarbon INFO @ 10:03:20] 0.004417 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:03:20] 0.020176 g.CO2eq/s mean an estimation of 636.2745180677729 kg.CO2eq/year\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curve dfNGP Run 1/1, Epoch 82/2000, Training Loss (NLML): -709.9614, (RMSE): 0.0164\n",
      "curve dfNGP Run 1/1, Epoch 83/2000, Training Loss (NLML): -709.2744, (RMSE): 0.0166\n",
      "curve dfNGP Run 1/1, Epoch 84/2000, Training Loss (NLML): -710.2136, (RMSE): 0.0165\n",
      "curve dfNGP Run 1/1, Epoch 85/2000, Training Loss (NLML): -710.7921, (RMSE): 0.0164\n",
      "curve dfNGP Run 1/1, Epoch 86/2000, Training Loss (NLML): -709.9476, (RMSE): 0.0164\n",
      "curve dfNGP Run 1/1, Epoch 87/2000, Training Loss (NLML): -710.5165, (RMSE): 0.0165\n",
      "curve dfNGP Run 1/1, Epoch 88/2000, Training Loss (NLML): -709.3951, (RMSE): 0.0164\n",
      "curve dfNGP Run 1/1, Epoch 89/2000, Training Loss (NLML): -710.9632, (RMSE): 0.0162\n",
      "curve dfNGP Run 1/1, Epoch 90/2000, Training Loss (NLML): -712.7932, (RMSE): 0.0162\n",
      "curve dfNGP Run 1/1, Epoch 91/2000, Training Loss (NLML): -710.7435, (RMSE): 0.0163\n",
      "curve dfNGP Run 1/1, Epoch 92/2000, Training Loss (NLML): -711.4996, (RMSE): 0.0162\n",
      "curve dfNGP Run 1/1, Epoch 93/2000, Training Loss (NLML): -711.9746, (RMSE): 0.0161\n",
      "curve dfNGP Run 1/1, Epoch 94/2000, Training Loss (NLML): -712.0592, (RMSE): 0.0161\n",
      "curve dfNGP Run 1/1, Epoch 95/2000, Training Loss (NLML): -711.9464, (RMSE): 0.0161\n",
      "curve dfNGP Run 1/1, Epoch 96/2000, Training Loss (NLML): -712.1162, (RMSE): 0.0160\n",
      "curve dfNGP Run 1/1, Epoch 97/2000, Training Loss (NLML): -712.2051, (RMSE): 0.0160\n",
      "curve dfNGP Run 1/1, Epoch 98/2000, Training Loss (NLML): -711.7400, (RMSE): 0.0160\n",
      "curve dfNGP Run 1/1, Epoch 99/2000, Training Loss (NLML): -712.9339, (RMSE): 0.0159\n",
      "curve dfNGP Run 1/1, Epoch 100/2000, Training Loss (NLML): -714.2717, (RMSE): 0.0160\n",
      "curve dfNGP Run 1/1, Epoch 101/2000, Training Loss (NLML): -712.5302, (RMSE): 0.0158\n",
      "curve dfNGP Run 1/1, Epoch 102/2000, Training Loss (NLML): -713.9584, (RMSE): 0.0161\n",
      "curve dfNGP Run 1/1, Epoch 103/2000, Training Loss (NLML): -712.2301, (RMSE): 0.0159\n",
      "curve dfNGP Run 1/1, Epoch 104/2000, Training Loss (NLML): -713.7230, (RMSE): 0.0160\n",
      "curve dfNGP Run 1/1, Epoch 105/2000, Training Loss (NLML): -712.8723, (RMSE): 0.0160\n",
      "curve dfNGP Run 1/1, Epoch 106/2000, Training Loss (NLML): -712.9996, (RMSE): 0.0159\n",
      "curve dfNGP Run 1/1, Epoch 107/2000, Training Loss (NLML): -715.2940, (RMSE): 0.0161\n",
      "curve dfNGP Run 1/1, Epoch 108/2000, Training Loss (NLML): -713.2454, (RMSE): 0.0160\n",
      "curve dfNGP Run 1/1, Epoch 109/2000, Training Loss (NLML): -716.0030, (RMSE): 0.0161\n",
      "curve dfNGP Run 1/1, Epoch 110/2000, Training Loss (NLML): -713.6930, (RMSE): 0.0160\n",
      "curve dfNGP Run 1/1, Epoch 111/2000, Training Loss (NLML): -715.1446, (RMSE): 0.0160\n",
      "curve dfNGP Run 1/1, Epoch 112/2000, Training Loss (NLML): -714.8207, (RMSE): 0.0162\n",
      "curve dfNGP Run 1/1, Epoch 113/2000, Training Loss (NLML): -713.6859, (RMSE): 0.0161\n",
      "curve dfNGP Run 1/1, Epoch 114/2000, Training Loss (NLML): -715.0457, (RMSE): 0.0162\n",
      "curve dfNGP Run 1/1, Epoch 115/2000, Training Loss (NLML): -714.2927, (RMSE): 0.0162\n",
      "curve dfNGP Run 1/1, Epoch 116/2000, Training Loss (NLML): -713.9666, (RMSE): 0.0163\n",
      "curve dfNGP Run 1/1, Epoch 117/2000, Training Loss (NLML): -715.1559, (RMSE): 0.0161\n",
      "curve dfNGP Run 1/1, Epoch 118/2000, Training Loss (NLML): -714.4423, (RMSE): 0.0163\n",
      "curve dfNGP Run 1/1, Epoch 119/2000, Training Loss (NLML): -714.8312, (RMSE): 0.0162\n",
      "curve dfNGP Run 1/1, Epoch 120/2000, Training Loss (NLML): -714.6885, (RMSE): 0.0163\n",
      "curve dfNGP Run 1/1, Epoch 121/2000, Training Loss (NLML): -716.5641, (RMSE): 0.0163\n",
      "curve dfNGP Run 1/1, Epoch 122/2000, Training Loss (NLML): -715.6432, (RMSE): 0.0162\n",
      "curve dfNGP Run 1/1, Epoch 123/2000, Training Loss (NLML): -715.4800, (RMSE): 0.0162\n",
      "curve dfNGP Run 1/1, Epoch 124/2000, Training Loss (NLML): -716.2214, (RMSE): 0.0161\n",
      "curve dfNGP Run 1/1, Epoch 125/2000, Training Loss (NLML): -717.0180, (RMSE): 0.0163\n",
      "curve dfNGP Run 1/1, Epoch 126/2000, Training Loss (NLML): -716.0557, (RMSE): 0.0161\n",
      "curve dfNGP Run 1/1, Epoch 127/2000, Training Loss (NLML): -716.4651, (RMSE): 0.0163\n",
      "curve dfNGP Run 1/1, Epoch 128/2000, Training Loss (NLML): -717.9150, (RMSE): 0.0163\n",
      "curve dfNGP Run 1/1, Epoch 129/2000, Training Loss (NLML): -718.2153, (RMSE): 0.0162\n",
      "curve dfNGP Run 1/1, Epoch 130/2000, Training Loss (NLML): -717.1397, (RMSE): 0.0161\n",
      "curve dfNGP Run 1/1, Epoch 131/2000, Training Loss (NLML): -716.7192, (RMSE): 0.0161\n",
      "curve dfNGP Run 1/1, Epoch 132/2000, Training Loss (NLML): -717.8596, (RMSE): 0.0161\n",
      "curve dfNGP Run 1/1, Epoch 133/2000, Training Loss (NLML): -718.3108, (RMSE): 0.0162\n",
      "curve dfNGP Run 1/1, Epoch 134/2000, Training Loss (NLML): -719.1171, (RMSE): 0.0161\n",
      "curve dfNGP Run 1/1, Epoch 135/2000, Training Loss (NLML): -718.4258, (RMSE): 0.0161\n",
      "curve dfNGP Run 1/1, Epoch 136/2000, Training Loss (NLML): -718.6370, (RMSE): 0.0160\n",
      "curve dfNGP Run 1/1, Epoch 137/2000, Training Loss (NLML): -720.3548, (RMSE): 0.0161\n",
      "curve dfNGP Run 1/1, Epoch 138/2000, Training Loss (NLML): -719.1158, (RMSE): 0.0159\n",
      "curve dfNGP Run 1/1, Epoch 139/2000, Training Loss (NLML): -719.0547, (RMSE): 0.0161\n",
      "curve dfNGP Run 1/1, Epoch 140/2000, Training Loss (NLML): -721.0786, (RMSE): 0.0160\n",
      "curve dfNGP Run 1/1, Epoch 141/2000, Training Loss (NLML): -720.2319, (RMSE): 0.0159\n",
      "curve dfNGP Run 1/1, Epoch 142/2000, Training Loss (NLML): -720.5726, (RMSE): 0.0159\n",
      "curve dfNGP Run 1/1, Epoch 143/2000, Training Loss (NLML): -721.0020, (RMSE): 0.0160\n",
      "curve dfNGP Run 1/1, Epoch 144/2000, Training Loss (NLML): -720.5775, (RMSE): 0.0159\n",
      "curve dfNGP Run 1/1, Epoch 145/2000, Training Loss (NLML): -721.8551, (RMSE): 0.0160\n",
      "curve dfNGP Run 1/1, Epoch 146/2000, Training Loss (NLML): -721.4420, (RMSE): 0.0159\n",
      "curve dfNGP Run 1/1, Epoch 147/2000, Training Loss (NLML): -722.0396, (RMSE): 0.0160\n",
      "curve dfNGP Run 1/1, Epoch 148/2000, Training Loss (NLML): -721.4889, (RMSE): 0.0159\n",
      "curve dfNGP Run 1/1, Epoch 149/2000, Training Loss (NLML): -722.0184, (RMSE): 0.0159\n",
      "curve dfNGP Run 1/1, Epoch 150/2000, Training Loss (NLML): -723.4280, (RMSE): 0.0159\n",
      "curve dfNGP Run 1/1, Epoch 151/2000, Training Loss (NLML): -723.4459, (RMSE): 0.0159\n",
      "curve dfNGP Run 1/1, Epoch 152/2000, Training Loss (NLML): -722.9429, (RMSE): 0.0159\n",
      "curve dfNGP Run 1/1, Epoch 153/2000, Training Loss (NLML): -723.6925, (RMSE): 0.0160\n",
      "curve dfNGP Run 1/1, Epoch 154/2000, Training Loss (NLML): -724.3041, (RMSE): 0.0159\n",
      "curve dfNGP Run 1/1, Epoch 155/2000, Training Loss (NLML): -723.8724, (RMSE): 0.0157\n",
      "curve dfNGP Run 1/1, Epoch 156/2000, Training Loss (NLML): -723.7853, (RMSE): 0.0158\n",
      "curve dfNGP Run 1/1, Epoch 157/2000, Training Loss (NLML): -725.4001, (RMSE): 0.0158\n",
      "curve dfNGP Run 1/1, Epoch 158/2000, Training Loss (NLML): -725.4199, (RMSE): 0.0158\n",
      "curve dfNGP Run 1/1, Epoch 159/2000, Training Loss (NLML): -725.5557, (RMSE): 0.0158\n",
      "curve dfNGP Run 1/1, Epoch 160/2000, Training Loss (NLML): -725.1384, (RMSE): 0.0159\n",
      "curve dfNGP Run 1/1, Epoch 161/2000, Training Loss (NLML): -725.2693, (RMSE): 0.0158\n",
      "curve dfNGP Run 1/1, Epoch 162/2000, Training Loss (NLML): -725.7150, (RMSE): 0.0158\n",
      "curve dfNGP Run 1/1, Epoch 163/2000, Training Loss (NLML): -726.8788, (RMSE): 0.0158\n",
      "curve dfNGP Run 1/1, Epoch 164/2000, Training Loss (NLML): -726.3408, (RMSE): 0.0158\n",
      "curve dfNGP Run 1/1, Epoch 165/2000, Training Loss (NLML): -727.6254, (RMSE): 0.0159\n",
      "curve dfNGP Run 1/1, Epoch 166/2000, Training Loss (NLML): -727.5310, (RMSE): 0.0158\n",
      "curve dfNGP Run 1/1, Epoch 167/2000, Training Loss (NLML): -728.0262, (RMSE): 0.0158\n",
      "curve dfNGP Run 1/1, Epoch 168/2000, Training Loss (NLML): -728.7065, (RMSE): 0.0158\n",
      "curve dfNGP Run 1/1, Epoch 169/2000, Training Loss (NLML): -728.7355, (RMSE): 0.0157\n",
      "curve dfNGP Run 1/1, Epoch 170/2000, Training Loss (NLML): -728.4919, (RMSE): 0.0157\n",
      "curve dfNGP Run 1/1, Epoch 171/2000, Training Loss (NLML): -728.9226, (RMSE): 0.0158\n",
      "curve dfNGP Run 1/1, Epoch 172/2000, Training Loss (NLML): -729.5350, (RMSE): 0.0157\n",
      "curve dfNGP Run 1/1, Epoch 173/2000, Training Loss (NLML): -730.3382, (RMSE): 0.0157\n",
      "curve dfNGP Run 1/1, Epoch 174/2000, Training Loss (NLML): -730.6887, (RMSE): 0.0157\n",
      "curve dfNGP Run 1/1, Epoch 175/2000, Training Loss (NLML): -730.8641, (RMSE): 0.0156\n",
      "curve dfNGP Run 1/1, Epoch 176/2000, Training Loss (NLML): -730.4041, (RMSE): 0.0157\n",
      "curve dfNGP Run 1/1, Epoch 177/2000, Training Loss (NLML): -731.3967, (RMSE): 0.0156\n",
      "curve dfNGP Run 1/1, Epoch 178/2000, Training Loss (NLML): -731.7905, (RMSE): 0.0157\n",
      "curve dfNGP Run 1/1, Epoch 179/2000, Training Loss (NLML): -731.9627, (RMSE): 0.0156\n",
      "curve dfNGP Run 1/1, Epoch 180/2000, Training Loss (NLML): -732.7932, (RMSE): 0.0156\n",
      "curve dfNGP Run 1/1, Epoch 181/2000, Training Loss (NLML): -732.8541, (RMSE): 0.0156\n",
      "curve dfNGP Run 1/1, Epoch 182/2000, Training Loss (NLML): -733.5726, (RMSE): 0.0156\n",
      "curve dfNGP Run 1/1, Epoch 183/2000, Training Loss (NLML): -734.5491, (RMSE): 0.0156\n",
      "curve dfNGP Run 1/1, Epoch 184/2000, Training Loss (NLML): -734.4409, (RMSE): 0.0156\n",
      "curve dfNGP Run 1/1, Epoch 185/2000, Training Loss (NLML): -734.8683, (RMSE): 0.0156\n",
      "curve dfNGP Run 1/1, Epoch 186/2000, Training Loss (NLML): -735.4598, (RMSE): 0.0156\n",
      "curve dfNGP Run 1/1, Epoch 187/2000, Training Loss (NLML): -735.9979, (RMSE): 0.0156\n",
      "curve dfNGP Run 1/1, Epoch 188/2000, Training Loss (NLML): -736.9139, (RMSE): 0.0156\n",
      "curve dfNGP Run 1/1, Epoch 189/2000, Training Loss (NLML): -737.0816, (RMSE): 0.0155\n",
      "curve dfNGP Run 1/1, Epoch 190/2000, Training Loss (NLML): -737.5248, (RMSE): 0.0156\n",
      "curve dfNGP Run 1/1, Epoch 191/2000, Training Loss (NLML): -738.0991, (RMSE): 0.0155\n",
      "curve dfNGP Run 1/1, Epoch 192/2000, Training Loss (NLML): -738.8310, (RMSE): 0.0155\n",
      "curve dfNGP Run 1/1, Epoch 193/2000, Training Loss (NLML): -739.5903, (RMSE): 0.0155\n",
      "curve dfNGP Run 1/1, Epoch 194/2000, Training Loss (NLML): -739.9016, (RMSE): 0.0155\n",
      "curve dfNGP Run 1/1, Epoch 195/2000, Training Loss (NLML): -740.3426, (RMSE): 0.0155\n",
      "curve dfNGP Run 1/1, Epoch 196/2000, Training Loss (NLML): -740.8954, (RMSE): 0.0155\n",
      "curve dfNGP Run 1/1, Epoch 197/2000, Training Loss (NLML): -741.6131, (RMSE): 0.0155\n",
      "curve dfNGP Run 1/1, Epoch 198/2000, Training Loss (NLML): -742.6296, (RMSE): 0.0155\n",
      "curve dfNGP Run 1/1, Epoch 199/2000, Training Loss (NLML): -743.2386, (RMSE): 0.0155\n",
      "curve dfNGP Run 1/1, Epoch 200/2000, Training Loss (NLML): -743.9502, (RMSE): 0.0155\n",
      "curve dfNGP Run 1/1, Epoch 201/2000, Training Loss (NLML): -744.7517, (RMSE): 0.0155\n",
      "curve dfNGP Run 1/1, Epoch 202/2000, Training Loss (NLML): -745.6660, (RMSE): 0.0154\n",
      "curve dfNGP Run 1/1, Epoch 203/2000, Training Loss (NLML): -746.1887, (RMSE): 0.0154\n",
      "curve dfNGP Run 1/1, Epoch 204/2000, Training Loss (NLML): -747.0315, (RMSE): 0.0154\n",
      "curve dfNGP Run 1/1, Epoch 205/2000, Training Loss (NLML): -747.7437, (RMSE): 0.0154\n",
      "curve dfNGP Run 1/1, Epoch 206/2000, Training Loss (NLML): -748.7450, (RMSE): 0.0154\n",
      "curve dfNGP Run 1/1, Epoch 207/2000, Training Loss (NLML): -749.4648, (RMSE): 0.0154\n",
      "curve dfNGP Run 1/1, Epoch 208/2000, Training Loss (NLML): -750.2634, (RMSE): 0.0154\n",
      "curve dfNGP Run 1/1, Epoch 209/2000, Training Loss (NLML): -751.4385, (RMSE): 0.0154\n",
      "curve dfNGP Run 1/1, Epoch 210/2000, Training Loss (NLML): -752.2537, (RMSE): 0.0153\n",
      "curve dfNGP Run 1/1, Epoch 211/2000, Training Loss (NLML): -753.2765, (RMSE): 0.0153\n",
      "curve dfNGP Run 1/1, Epoch 212/2000, Training Loss (NLML): -754.4927, (RMSE): 0.0153\n",
      "curve dfNGP Run 1/1, Epoch 213/2000, Training Loss (NLML): -755.5433, (RMSE): 0.0153\n",
      "curve dfNGP Run 1/1, Epoch 214/2000, Training Loss (NLML): -756.8082, (RMSE): 0.0152\n",
      "curve dfNGP Run 1/1, Epoch 215/2000, Training Loss (NLML): -757.9931, (RMSE): 0.0153\n",
      "curve dfNGP Run 1/1, Epoch 216/2000, Training Loss (NLML): -759.2040, (RMSE): 0.0152\n",
      "curve dfNGP Run 1/1, Epoch 217/2000, Training Loss (NLML): -760.6156, (RMSE): 0.0152\n",
      "curve dfNGP Run 1/1, Epoch 218/2000, Training Loss (NLML): -762.1329, (RMSE): 0.0152\n",
      "curve dfNGP Run 1/1, Epoch 219/2000, Training Loss (NLML): -763.5043, (RMSE): 0.0152\n",
      "curve dfNGP Run 1/1, Epoch 220/2000, Training Loss (NLML): -765.0284, (RMSE): 0.0152\n",
      "curve dfNGP Run 1/1, Epoch 221/2000, Training Loss (NLML): -766.7555, (RMSE): 0.0152\n",
      "curve dfNGP Run 1/1, Epoch 222/2000, Training Loss (NLML): -768.5046, (RMSE): 0.0152\n",
      "curve dfNGP Run 1/1, Epoch 223/2000, Training Loss (NLML): -770.3926, (RMSE): 0.0152\n",
      "curve dfNGP Run 1/1, Epoch 224/2000, Training Loss (NLML): -772.3285, (RMSE): 0.0151\n",
      "curve dfNGP Run 1/1, Epoch 225/2000, Training Loss (NLML): -774.4109, (RMSE): 0.0151\n",
      "curve dfNGP Run 1/1, Epoch 226/2000, Training Loss (NLML): -776.6121, (RMSE): 0.0151\n",
      "curve dfNGP Run 1/1, Epoch 227/2000, Training Loss (NLML): -778.9307, (RMSE): 0.0151\n",
      "curve dfNGP Run 1/1, Epoch 228/2000, Training Loss (NLML): -781.3189, (RMSE): 0.0152\n",
      "curve dfNGP Run 1/1, Epoch 229/2000, Training Loss (NLML): -783.8381, (RMSE): 0.0152\n",
      "curve dfNGP Run 1/1, Epoch 230/2000, Training Loss (NLML): -786.3345, (RMSE): 0.0152\n",
      "curve dfNGP Run 1/1, Epoch 231/2000, Training Loss (NLML): -788.7557, (RMSE): 0.0152\n",
      "curve dfNGP Run 1/1, Epoch 232/2000, Training Loss (NLML): -791.0189, (RMSE): 0.0152\n",
      "curve dfNGP Run 1/1, Epoch 233/2000, Training Loss (NLML): -793.1206, (RMSE): 0.0152\n",
      "curve dfNGP Run 1/1, Epoch 234/2000, Training Loss (NLML): -795.3392, (RMSE): 0.0151\n",
      "curve dfNGP Run 1/1, Epoch 235/2000, Training Loss (NLML): -797.8359, (RMSE): 0.0149\n",
      "curve dfNGP Run 1/1, Epoch 236/2000, Training Loss (NLML): -800.1454, (RMSE): 0.0146\n",
      "curve dfNGP Run 1/1, Epoch 237/2000, Training Loss (NLML): -801.8325, (RMSE): 0.0143\n",
      "curve dfNGP Run 1/1, Epoch 238/2000, Training Loss (NLML): -803.0706, (RMSE): 0.0141\n",
      "curve dfNGP Run 1/1, Epoch 239/2000, Training Loss (NLML): -804.2409, (RMSE): 0.0140\n",
      "curve dfNGP Run 1/1, Epoch 240/2000, Training Loss (NLML): -805.2551, (RMSE): 0.0140\n",
      "curve dfNGP Run 1/1, Epoch 241/2000, Training Loss (NLML): -805.6887, (RMSE): 0.0141\n",
      "curve dfNGP Run 1/1, Epoch 242/2000, Training Loss (NLML): -806.1459, (RMSE): 0.0140\n",
      "curve dfNGP Run 1/1, Epoch 243/2000, Training Loss (NLML): -806.7017, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 244/2000, Training Loss (NLML): -806.6371, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 245/2000, Training Loss (NLML): -806.7446, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 246/2000, Training Loss (NLML): -806.9099, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 247/2000, Training Loss (NLML): -806.8427, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 248/2000, Training Loss (NLML): -807.1654, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 249/2000, Training Loss (NLML): -807.1356, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 250/2000, Training Loss (NLML): -807.4581, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 251/2000, Training Loss (NLML): -807.7646, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 252/2000, Training Loss (NLML): -808.0595, (RMSE): 0.0140\n",
      "curve dfNGP Run 1/1, Epoch 253/2000, Training Loss (NLML): -808.5444, (RMSE): 0.0140\n",
      "curve dfNGP Run 1/1, Epoch 254/2000, Training Loss (NLML): -808.8597, (RMSE): 0.0140\n",
      "curve dfNGP Run 1/1, Epoch 255/2000, Training Loss (NLML): -809.3879, (RMSE): 0.0140\n",
      "curve dfNGP Run 1/1, Epoch 256/2000, Training Loss (NLML): -809.7910, (RMSE): 0.0140\n",
      "curve dfNGP Run 1/1, Epoch 257/2000, Training Loss (NLML): -810.2629, (RMSE): 0.0140\n",
      "curve dfNGP Run 1/1, Epoch 258/2000, Training Loss (NLML): -810.7067, (RMSE): 0.0140\n",
      "curve dfNGP Run 1/1, Epoch 259/2000, Training Loss (NLML): -811.1208, (RMSE): 0.0140\n",
      "curve dfNGP Run 1/1, Epoch 260/2000, Training Loss (NLML): -811.5620, (RMSE): 0.0140\n",
      "curve dfNGP Run 1/1, Epoch 261/2000, Training Loss (NLML): -811.9811, (RMSE): 0.0141\n",
      "curve dfNGP Run 1/1, Epoch 262/2000, Training Loss (NLML): -812.3950, (RMSE): 0.0141\n",
      "curve dfNGP Run 1/1, Epoch 263/2000, Training Loss (NLML): -812.8138, (RMSE): 0.0140\n",
      "curve dfNGP Run 1/1, Epoch 264/2000, Training Loss (NLML): -813.2237, (RMSE): 0.0141\n",
      "curve dfNGP Run 1/1, Epoch 265/2000, Training Loss (NLML): -813.6511, (RMSE): 0.0141\n",
      "curve dfNGP Run 1/1, Epoch 266/2000, Training Loss (NLML): -814.1044, (RMSE): 0.0141\n",
      "curve dfNGP Run 1/1, Epoch 267/2000, Training Loss (NLML): -814.5333, (RMSE): 0.0140\n",
      "curve dfNGP Run 1/1, Epoch 268/2000, Training Loss (NLML): -815.0123, (RMSE): 0.0140\n",
      "curve dfNGP Run 1/1, Epoch 269/2000, Training Loss (NLML): -815.4738, (RMSE): 0.0140\n",
      "curve dfNGP Run 1/1, Epoch 270/2000, Training Loss (NLML): -815.9730, (RMSE): 0.0140\n",
      "curve dfNGP Run 1/1, Epoch 271/2000, Training Loss (NLML): -816.4872, (RMSE): 0.0140\n",
      "curve dfNGP Run 1/1, Epoch 272/2000, Training Loss (NLML): -816.9981, (RMSE): 0.0140\n",
      "curve dfNGP Run 1/1, Epoch 273/2000, Training Loss (NLML): -817.5412, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 274/2000, Training Loss (NLML): -818.0737, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 275/2000, Training Loss (NLML): -818.6405, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 276/2000, Training Loss (NLML): -819.1940, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 277/2000, Training Loss (NLML): -819.7672, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 278/2000, Training Loss (NLML): -820.3273, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 279/2000, Training Loss (NLML): -820.8978, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 280/2000, Training Loss (NLML): -821.4581, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 281/2000, Training Loss (NLML): -822.0169, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 282/2000, Training Loss (NLML): -822.5573, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 283/2000, Training Loss (NLML): -823.0841, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 284/2000, Training Loss (NLML): -823.5941, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 285/2000, Training Loss (NLML): -824.0681, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 286/2000, Training Loss (NLML): -824.5190, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 287/2000, Training Loss (NLML): -824.9209, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 288/2000, Training Loss (NLML): -825.2772, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 289/2000, Training Loss (NLML): -825.5629, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 290/2000, Training Loss (NLML): -825.7444, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 291/2000, Training Loss (NLML): -825.8701, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 292/2000, Training Loss (NLML): -826.1230, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 293/2000, Training Loss (NLML): -826.3161, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 294/2000, Training Loss (NLML): -826.3016, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 295/2000, Training Loss (NLML): -826.4467, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 296/2000, Training Loss (NLML): -826.5083, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 297/2000, Training Loss (NLML): -826.4963, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 298/2000, Training Loss (NLML): -826.6312, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 299/2000, Training Loss (NLML): -826.5886, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 300/2000, Training Loss (NLML): -826.6615, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 301/2000, Training Loss (NLML): -826.7527, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 302/2000, Training Loss (NLML): -826.7388, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 303/2000, Training Loss (NLML): -826.8458, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 304/2000, Training Loss (NLML): -826.9023, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 305/2000, Training Loss (NLML): -826.9851, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 306/2000, Training Loss (NLML): -827.0392, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 307/2000, Training Loss (NLML): -827.1440, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 308/2000, Training Loss (NLML): -827.2041, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 309/2000, Training Loss (NLML): -827.2673, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 310/2000, Training Loss (NLML): -827.3650, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 311/2000, Training Loss (NLML): -827.4031, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 312/2000, Training Loss (NLML): -827.4867, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 313/2000, Training Loss (NLML): -827.5347, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 314/2000, Training Loss (NLML): -827.6042, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 315/2000, Training Loss (NLML): -827.6370, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 316/2000, Training Loss (NLML): -827.7003, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 317/2000, Training Loss (NLML): -827.7373, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 318/2000, Training Loss (NLML): -827.7737, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 319/2000, Training Loss (NLML): -827.8231, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 320/2000, Training Loss (NLML): -827.8489, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 321/2000, Training Loss (NLML): -827.8878, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 322/2000, Training Loss (NLML): -827.9215, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 323/2000, Training Loss (NLML): -827.9553, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 324/2000, Training Loss (NLML): -827.9787, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 325/2000, Training Loss (NLML): -828.0194, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 326/2000, Training Loss (NLML): -828.0408, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 327/2000, Training Loss (NLML): -828.0729, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 328/2000, Training Loss (NLML): -828.1031, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 329/2000, Training Loss (NLML): -828.1297, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 330/2000, Training Loss (NLML): -828.1545, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 331/2000, Training Loss (NLML): -828.1844, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 332/2000, Training Loss (NLML): -828.2133, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 333/2000, Training Loss (NLML): -828.2350, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 334/2000, Training Loss (NLML): -828.2656, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 335/2000, Training Loss (NLML): -828.2890, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 336/2000, Training Loss (NLML): -828.3137, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 337/2000, Training Loss (NLML): -828.3385, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 338/2000, Training Loss (NLML): -828.3643, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 339/2000, Training Loss (NLML): -828.3879, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 340/2000, Training Loss (NLML): -828.4099, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 341/2000, Training Loss (NLML): -828.4344, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 342/2000, Training Loss (NLML): -828.4576, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 343/2000, Training Loss (NLML): -828.4789, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 344/2000, Training Loss (NLML): -828.5010, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 345/2000, Training Loss (NLML): -828.5212, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 346/2000, Training Loss (NLML): -828.5445, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 347/2000, Training Loss (NLML): -828.5640, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 348/2000, Training Loss (NLML): -828.5830, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 349/2000, Training Loss (NLML): -828.6037, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 350/2000, Training Loss (NLML): -828.6229, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 351/2000, Training Loss (NLML): -828.6407, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 352/2000, Training Loss (NLML): -828.6585, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 353/2000, Training Loss (NLML): -828.6768, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 354/2000, Training Loss (NLML): -828.6947, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 355/2000, Training Loss (NLML): -828.7125, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 356/2000, Training Loss (NLML): -828.7291, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 357/2000, Training Loss (NLML): -828.7456, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 358/2000, Training Loss (NLML): -828.7626, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 359/2000, Training Loss (NLML): -828.7792, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 360/2000, Training Loss (NLML): -828.7932, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 361/2000, Training Loss (NLML): -828.8096, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 362/2000, Training Loss (NLML): -828.8243, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 363/2000, Training Loss (NLML): -828.8391, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 364/2000, Training Loss (NLML): -828.8533, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 365/2000, Training Loss (NLML): -828.8678, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 366/2000, Training Loss (NLML): -828.8821, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 367/2000, Training Loss (NLML): -828.8955, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 368/2000, Training Loss (NLML): -828.9086, (RMSE): 0.0138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 10:03:31] Energy consumed for RAM : 0.000098 kWh. RAM Power : 23.53756284713745 W\n",
      "[codecarbon INFO @ 10:03:31] Energy consumed for all CPUs : 0.000177 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:03:31] Energy consumed for all GPUs : 0.000294 kWh. Total GPU Power : 70.64069941838757 W\n",
      "[codecarbon INFO @ 10:03:31] 0.000570 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curve dfNGP Run 1/1, Epoch 369/2000, Training Loss (NLML): -828.9220, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 370/2000, Training Loss (NLML): -828.9347, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 371/2000, Training Loss (NLML): -828.9470, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 372/2000, Training Loss (NLML): -828.9601, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 373/2000, Training Loss (NLML): -828.9716, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 374/2000, Training Loss (NLML): -828.9832, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 375/2000, Training Loss (NLML): -828.9954, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 376/2000, Training Loss (NLML): -829.0071, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 377/2000, Training Loss (NLML): -829.0189, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 378/2000, Training Loss (NLML): -829.0298, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 379/2000, Training Loss (NLML): -829.0398, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 380/2000, Training Loss (NLML): -829.0507, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 381/2000, Training Loss (NLML): -829.0617, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 382/2000, Training Loss (NLML): -829.0717, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 383/2000, Training Loss (NLML): -829.0828, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 384/2000, Training Loss (NLML): -829.0922, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 385/2000, Training Loss (NLML): -829.1021, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 386/2000, Training Loss (NLML): -829.1123, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 387/2000, Training Loss (NLML): -829.1215, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 388/2000, Training Loss (NLML): -829.1320, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 389/2000, Training Loss (NLML): -829.1406, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 390/2000, Training Loss (NLML): -829.1498, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 391/2000, Training Loss (NLML): -829.1588, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 392/2000, Training Loss (NLML): -829.1686, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 393/2000, Training Loss (NLML): -829.1770, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 394/2000, Training Loss (NLML): -829.1869, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 395/2000, Training Loss (NLML): -829.1957, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 396/2000, Training Loss (NLML): -829.2038, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 397/2000, Training Loss (NLML): -829.2135, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 398/2000, Training Loss (NLML): -829.2221, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 399/2000, Training Loss (NLML): -829.2312, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 400/2000, Training Loss (NLML): -829.2402, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 401/2000, Training Loss (NLML): -829.2482, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 402/2000, Training Loss (NLML): -829.2570, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 403/2000, Training Loss (NLML): -829.2649, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 404/2000, Training Loss (NLML): -829.2739, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 405/2000, Training Loss (NLML): -829.2825, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 406/2000, Training Loss (NLML): -829.2912, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 407/2000, Training Loss (NLML): -829.2991, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 408/2000, Training Loss (NLML): -829.3073, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 409/2000, Training Loss (NLML): -829.3157, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 410/2000, Training Loss (NLML): -829.3249, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 411/2000, Training Loss (NLML): -829.3328, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 412/2000, Training Loss (NLML): -829.3407, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 413/2000, Training Loss (NLML): -829.3492, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 414/2000, Training Loss (NLML): -829.3566, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 415/2000, Training Loss (NLML): -829.3657, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 416/2000, Training Loss (NLML): -829.3743, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 417/2000, Training Loss (NLML): -829.3818, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 418/2000, Training Loss (NLML): -829.3900, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 419/2000, Training Loss (NLML): -829.3984, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 420/2000, Training Loss (NLML): -829.4073, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 421/2000, Training Loss (NLML): -829.4156, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 422/2000, Training Loss (NLML): -829.4247, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 423/2000, Training Loss (NLML): -829.4315, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 424/2000, Training Loss (NLML): -829.4403, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 425/2000, Training Loss (NLML): -829.4479, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 426/2000, Training Loss (NLML): -829.4543, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 427/2000, Training Loss (NLML): -829.4613, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 428/2000, Training Loss (NLML): -829.4642, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 429/2000, Training Loss (NLML): -829.4649, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 430/2000, Training Loss (NLML): -829.4542, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 431/2000, Training Loss (NLML): -829.4410, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 432/2000, Training Loss (NLML): -829.3898, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 433/2000, Training Loss (NLML): -829.3899, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 434/2000, Training Loss (NLML): -829.3546, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 435/2000, Training Loss (NLML): -829.4546, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 436/2000, Training Loss (NLML): -829.5190, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 437/2000, Training Loss (NLML): -829.5403, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 438/2000, Training Loss (NLML): -829.5182, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 439/2000, Training Loss (NLML): -829.4843, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 440/2000, Training Loss (NLML): -829.5217, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 441/2000, Training Loss (NLML): -829.5612, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 442/2000, Training Loss (NLML): -829.5798, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 443/2000, Training Loss (NLML): -829.5730, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 444/2000, Training Loss (NLML): -829.5619, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 445/2000, Training Loss (NLML): -829.5840, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 446/2000, Training Loss (NLML): -829.6084, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 447/2000, Training Loss (NLML): -829.6197, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 448/2000, Training Loss (NLML): -829.6208, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 449/2000, Training Loss (NLML): -829.6201, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 450/2000, Training Loss (NLML): -829.6359, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 451/2000, Training Loss (NLML): -829.6533, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 452/2000, Training Loss (NLML): -829.6629, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 453/2000, Training Loss (NLML): -829.6664, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 454/2000, Training Loss (NLML): -829.6698, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 455/2000, Training Loss (NLML): -829.6827, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 456/2000, Training Loss (NLML): -829.6966, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 457/2000, Training Loss (NLML): -829.7064, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 458/2000, Training Loss (NLML): -829.7104, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 459/2000, Training Loss (NLML): -829.7157, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 460/2000, Training Loss (NLML): -829.7265, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 461/2000, Training Loss (NLML): -829.7399, (RMSE): 0.0138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 10:03:35] Energy consumed for RAM : 0.000883 kWh. RAM Power : 23.53756284713745 W\n",
      "[codecarbon INFO @ 10:03:35] Energy consumed for all CPUs : 0.001595 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:03:35] Energy consumed for all GPUs : 0.002512 kWh. Total GPU Power : 71.3491971386363 W\n",
      "[codecarbon INFO @ 10:03:35] 0.004990 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curve dfNGP Run 1/1, Epoch 462/2000, Training Loss (NLML): -829.7482, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 463/2000, Training Loss (NLML): -829.7542, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 464/2000, Training Loss (NLML): -829.7604, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 465/2000, Training Loss (NLML): -829.7695, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 466/2000, Training Loss (NLML): -829.7824, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 467/2000, Training Loss (NLML): -829.7919, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 468/2000, Training Loss (NLML): -829.7980, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 469/2000, Training Loss (NLML): -829.8053, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 470/2000, Training Loss (NLML): -829.8133, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 471/2000, Training Loss (NLML): -829.8246, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 472/2000, Training Loss (NLML): -829.8346, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 473/2000, Training Loss (NLML): -829.8429, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 474/2000, Training Loss (NLML): -829.8510, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 475/2000, Training Loss (NLML): -829.8586, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 476/2000, Training Loss (NLML): -829.8678, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 477/2000, Training Loss (NLML): -829.8786, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 478/2000, Training Loss (NLML): -829.8871, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 479/2000, Training Loss (NLML): -829.8954, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 480/2000, Training Loss (NLML): -829.9038, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 481/2000, Training Loss (NLML): -829.9131, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 482/2000, Training Loss (NLML): -829.9225, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 483/2000, Training Loss (NLML): -829.9325, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 484/2000, Training Loss (NLML): -829.9413, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 485/2000, Training Loss (NLML): -829.9510, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 486/2000, Training Loss (NLML): -829.9590, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 487/2000, Training Loss (NLML): -829.9692, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 488/2000, Training Loss (NLML): -829.9781, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 489/2000, Training Loss (NLML): -829.9879, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 490/2000, Training Loss (NLML): -829.9978, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 491/2000, Training Loss (NLML): -830.0074, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 492/2000, Training Loss (NLML): -830.0176, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 493/2000, Training Loss (NLML): -830.0264, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 494/2000, Training Loss (NLML): -830.0358, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 495/2000, Training Loss (NLML): -830.0458, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 496/2000, Training Loss (NLML): -830.0566, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 497/2000, Training Loss (NLML): -830.0649, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 498/2000, Training Loss (NLML): -830.0763, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 499/2000, Training Loss (NLML): -830.0872, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 500/2000, Training Loss (NLML): -830.0968, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 501/2000, Training Loss (NLML): -830.1066, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 502/2000, Training Loss (NLML): -830.1176, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 503/2000, Training Loss (NLML): -830.1277, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 504/2000, Training Loss (NLML): -830.1378, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 505/2000, Training Loss (NLML): -830.1492, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 506/2000, Training Loss (NLML): -830.1586, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 507/2000, Training Loss (NLML): -830.1697, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 508/2000, Training Loss (NLML): -830.1815, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 509/2000, Training Loss (NLML): -830.1915, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 510/2000, Training Loss (NLML): -830.2034, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 511/2000, Training Loss (NLML): -830.2140, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 512/2000, Training Loss (NLML): -830.2252, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 513/2000, Training Loss (NLML): -830.2358, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 514/2000, Training Loss (NLML): -830.2471, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 515/2000, Training Loss (NLML): -830.2561, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 516/2000, Training Loss (NLML): -830.2671, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 517/2000, Training Loss (NLML): -830.2780, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 518/2000, Training Loss (NLML): -830.2847, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 519/2000, Training Loss (NLML): -830.2900, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 520/2000, Training Loss (NLML): -830.2911, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 521/2000, Training Loss (NLML): -830.2909, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 522/2000, Training Loss (NLML): -830.2875, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 523/2000, Training Loss (NLML): -830.2999, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 524/2000, Training Loss (NLML): -830.3239, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 525/2000, Training Loss (NLML): -830.3591, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 526/2000, Training Loss (NLML): -830.3814, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 527/2000, Training Loss (NLML): -830.3846, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 528/2000, Training Loss (NLML): -830.3758, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 529/2000, Training Loss (NLML): -830.3854, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 530/2000, Training Loss (NLML): -830.3978, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 531/2000, Training Loss (NLML): -830.4253, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 532/2000, Training Loss (NLML): -830.4343, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 533/2000, Training Loss (NLML): -830.4418, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 534/2000, Training Loss (NLML): -830.4445, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 535/2000, Training Loss (NLML): -830.4701, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 536/2000, Training Loss (NLML): -830.4933, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 537/2000, Training Loss (NLML): -830.5175, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 538/2000, Training Loss (NLML): -830.5307, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 539/2000, Training Loss (NLML): -830.5403, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 540/2000, Training Loss (NLML): -830.5558, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 541/2000, Training Loss (NLML): -830.5718, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 542/2000, Training Loss (NLML): -830.5876, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 543/2000, Training Loss (NLML): -830.5994, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 544/2000, Training Loss (NLML): -830.6090, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 545/2000, Training Loss (NLML): -830.6162, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 546/2000, Training Loss (NLML): -830.6314, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 547/2000, Training Loss (NLML): -830.6455, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 548/2000, Training Loss (NLML): -830.6625, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 549/2000, Training Loss (NLML): -830.6740, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 550/2000, Training Loss (NLML): -830.6889, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 551/2000, Training Loss (NLML): -830.7031, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 552/2000, Training Loss (NLML): -830.7213, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 553/2000, Training Loss (NLML): -830.7385, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 554/2000, Training Loss (NLML): -830.7579, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 555/2000, Training Loss (NLML): -830.7745, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 556/2000, Training Loss (NLML): -830.7911, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 557/2000, Training Loss (NLML): -830.8068, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 558/2000, Training Loss (NLML): -830.8246, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 559/2000, Training Loss (NLML): -830.8414, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 560/2000, Training Loss (NLML): -830.8589, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 561/2000, Training Loss (NLML): -830.8761, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 562/2000, Training Loss (NLML): -830.8940, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 563/2000, Training Loss (NLML): -830.9114, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 564/2000, Training Loss (NLML): -830.9292, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 565/2000, Training Loss (NLML): -830.9464, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 566/2000, Training Loss (NLML): -830.9635, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 567/2000, Training Loss (NLML): -830.9825, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 568/2000, Training Loss (NLML): -831.0004, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 569/2000, Training Loss (NLML): -831.0195, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 570/2000, Training Loss (NLML): -831.0385, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 571/2000, Training Loss (NLML): -831.0584, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 572/2000, Training Loss (NLML): -831.0780, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 573/2000, Training Loss (NLML): -831.0980, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 574/2000, Training Loss (NLML): -831.1176, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 575/2000, Training Loss (NLML): -831.1376, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 576/2000, Training Loss (NLML): -831.1572, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 577/2000, Training Loss (NLML): -831.1779, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 578/2000, Training Loss (NLML): -831.1962, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 579/2000, Training Loss (NLML): -831.2117, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 580/2000, Training Loss (NLML): -831.2251, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 581/2000, Training Loss (NLML): -831.2349, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 582/2000, Training Loss (NLML): -831.2327, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 583/2000, Training Loss (NLML): -831.2372, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 584/2000, Training Loss (NLML): -831.2280, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 585/2000, Training Loss (NLML): -831.2596, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 586/2000, Training Loss (NLML): -831.2402, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 587/2000, Training Loss (NLML): -831.2527, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 588/2000, Training Loss (NLML): -831.1531, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 589/2000, Training Loss (NLML): -831.2830, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 590/2000, Training Loss (NLML): -831.3565, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 591/2000, Training Loss (NLML): -831.4630, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 592/2000, Training Loss (NLML): -831.4849, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 593/2000, Training Loss (NLML): -831.4327, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 594/2000, Training Loss (NLML): -831.4669, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 595/2000, Training Loss (NLML): -831.5271, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 596/2000, Training Loss (NLML): -831.6078, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 597/2000, Training Loss (NLML): -831.6100, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 598/2000, Training Loss (NLML): -831.5819, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 599/2000, Training Loss (NLML): -831.6371, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 600/2000, Training Loss (NLML): -831.6973, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 601/2000, Training Loss (NLML): -831.7324, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 602/2000, Training Loss (NLML): -831.7401, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 603/2000, Training Loss (NLML): -831.7495, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 604/2000, Training Loss (NLML): -831.7952, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 605/2000, Training Loss (NLML): -831.8417, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 606/2000, Training Loss (NLML): -831.8737, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 607/2000, Training Loss (NLML): -831.8867, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 608/2000, Training Loss (NLML): -831.9034, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 609/2000, Training Loss (NLML): -831.9541, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 610/2000, Training Loss (NLML): -831.9963, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 611/2000, Training Loss (NLML): -832.0190, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 612/2000, Training Loss (NLML): -832.0411, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 613/2000, Training Loss (NLML): -832.0715, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 614/2000, Training Loss (NLML): -832.1144, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 615/2000, Training Loss (NLML): -832.1533, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 616/2000, Training Loss (NLML): -832.1866, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 617/2000, Training Loss (NLML): -832.2151, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 618/2000, Training Loss (NLML): -832.2443, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 619/2000, Training Loss (NLML): -832.2834, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 620/2000, Training Loss (NLML): -832.3256, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 621/2000, Training Loss (NLML): -832.3613, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 622/2000, Training Loss (NLML): -832.3936, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 623/2000, Training Loss (NLML): -832.4286, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 624/2000, Training Loss (NLML): -832.4666, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 625/2000, Training Loss (NLML): -832.5050, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 626/2000, Training Loss (NLML): -832.5436, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 627/2000, Training Loss (NLML): -832.5779, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 628/2000, Training Loss (NLML): -832.6076, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 629/2000, Training Loss (NLML): -832.6314, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 630/2000, Training Loss (NLML): -832.6445, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 631/2000, Training Loss (NLML): -832.6603, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 632/2000, Training Loss (NLML): -832.6786, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 633/2000, Training Loss (NLML): -832.7323, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 634/2000, Training Loss (NLML): -832.8219, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 635/2000, Training Loss (NLML): -832.9055, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 636/2000, Training Loss (NLML): -832.9471, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 637/2000, Training Loss (NLML): -832.9573, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 638/2000, Training Loss (NLML): -832.9771, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 639/2000, Training Loss (NLML): -833.0277, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 640/2000, Training Loss (NLML): -833.1011, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 641/2000, Training Loss (NLML): -833.1627, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 642/2000, Training Loss (NLML): -833.1955, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 643/2000, Training Loss (NLML): -833.2174, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 644/2000, Training Loss (NLML): -833.2503, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 645/2000, Training Loss (NLML): -833.3088, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 646/2000, Training Loss (NLML): -833.3732, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 647/2000, Training Loss (NLML): -833.4243, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 648/2000, Training Loss (NLML): -833.4622, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 649/2000, Training Loss (NLML): -833.4973, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 650/2000, Training Loss (NLML): -833.5382, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 651/2000, Training Loss (NLML): -833.5916, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 652/2000, Training Loss (NLML): -833.6497, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 653/2000, Training Loss (NLML): -833.7058, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 654/2000, Training Loss (NLML): -833.7534, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 655/2000, Training Loss (NLML): -833.7945, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 656/2000, Training Loss (NLML): -833.8320, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 657/2000, Training Loss (NLML): -833.8713, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 658/2000, Training Loss (NLML): -833.8998, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 659/2000, Training Loss (NLML): -833.9401, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 660/2000, Training Loss (NLML): -833.9355, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 661/2000, Training Loss (NLML): -833.9782, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 662/2000, Training Loss (NLML): -833.9485, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 663/2000, Training Loss (NLML): -834.0814, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 664/2000, Training Loss (NLML): -834.1594, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 665/2000, Training Loss (NLML): -834.2725, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 666/2000, Training Loss (NLML): -834.3132, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 667/2000, Training Loss (NLML): -834.3064, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 668/2000, Training Loss (NLML): -834.3546, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 669/2000, Training Loss (NLML): -834.4075, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 670/2000, Training Loss (NLML): -834.5547, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 671/2000, Training Loss (NLML): -834.6542, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 672/2000, Training Loss (NLML): -834.6971, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 673/2000, Training Loss (NLML): -834.7108, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 674/2000, Training Loss (NLML): -834.7200, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 675/2000, Training Loss (NLML): -834.7969, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 676/2000, Training Loss (NLML): -834.8802, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 677/2000, Training Loss (NLML): -834.9807, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 678/2000, Training Loss (NLML): -835.0503, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 679/2000, Training Loss (NLML): -835.0909, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 680/2000, Training Loss (NLML): -835.1241, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 681/2000, Training Loss (NLML): -835.1559, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 682/2000, Training Loss (NLML): -835.2248, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 683/2000, Training Loss (NLML): -835.2916, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 684/2000, Training Loss (NLML): -835.3772, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 685/2000, Training Loss (NLML): -835.4551, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 686/2000, Training Loss (NLML): -835.5233, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 687/2000, Training Loss (NLML): -835.5806, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 688/2000, Training Loss (NLML): -835.6300, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 689/2000, Training Loss (NLML): -835.6812, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 690/2000, Training Loss (NLML): -835.7239, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 691/2000, Training Loss (NLML): -835.7803, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 692/2000, Training Loss (NLML): -835.8231, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 693/2000, Training Loss (NLML): -835.8966, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 694/2000, Training Loss (NLML): -835.9510, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 695/2000, Training Loss (NLML): -836.0397, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 696/2000, Training Loss (NLML): -836.1120, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 697/2000, Training Loss (NLML): -836.1985, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 698/2000, Training Loss (NLML): -836.2742, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 699/2000, Training Loss (NLML): -836.3476, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 700/2000, Training Loss (NLML): -836.4053, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 701/2000, Training Loss (NLML): -836.4379, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 702/2000, Training Loss (NLML): -836.4243, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 703/2000, Training Loss (NLML): -836.2799, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 704/2000, Training Loss (NLML): -836.1977, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 705/2000, Training Loss (NLML): -836.0324, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 706/2000, Training Loss (NLML): -836.5917, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 707/2000, Training Loss (NLML): -836.7120, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 708/2000, Training Loss (NLML): -836.7177, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 709/2000, Training Loss (NLML): -836.7882, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 710/2000, Training Loss (NLML): -836.9814, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 711/2000, Training Loss (NLML): -836.9810, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 712/2000, Training Loss (NLML): -836.8300, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 713/2000, Training Loss (NLML): -837.1026, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 714/2000, Training Loss (NLML): -837.2490, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 715/2000, Training Loss (NLML): -837.2444, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 716/2000, Training Loss (NLML): -837.3037, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 717/2000, Training Loss (NLML): -837.3928, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 718/2000, Training Loss (NLML): -837.4294, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 719/2000, Training Loss (NLML): -837.4222, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 720/2000, Training Loss (NLML): -837.5842, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 721/2000, Training Loss (NLML): -837.6857, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 722/2000, Training Loss (NLML): -837.6981, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 723/2000, Training Loss (NLML): -837.7554, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 724/2000, Training Loss (NLML): -837.8383, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 725/2000, Training Loss (NLML): -837.8685, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 726/2000, Training Loss (NLML): -837.8737, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 727/2000, Training Loss (NLML): -837.9857, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 728/2000, Training Loss (NLML): -838.0646, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 729/2000, Training Loss (NLML): -838.1124, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 730/2000, Training Loss (NLML): -838.1740, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 731/2000, Training Loss (NLML): -838.2616, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 732/2000, Training Loss (NLML): -838.3182, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 733/2000, Training Loss (NLML): -838.3582, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 734/2000, Training Loss (NLML): -838.4150, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 735/2000, Training Loss (NLML): -838.4716, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 736/2000, Training Loss (NLML): -838.5187, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 737/2000, Training Loss (NLML): -838.5565, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 738/2000, Training Loss (NLML): -838.6078, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 739/2000, Training Loss (NLML): -838.6408, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 740/2000, Training Loss (NLML): -838.6989, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 741/2000, Training Loss (NLML): -838.7297, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 742/2000, Training Loss (NLML): -838.7916, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 743/2000, Training Loss (NLML): -838.8258, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 744/2000, Training Loss (NLML): -838.9109, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 745/2000, Training Loss (NLML): -838.9761, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 746/2000, Training Loss (NLML): -839.0520, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 747/2000, Training Loss (NLML): -839.1129, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 748/2000, Training Loss (NLML): -839.1779, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 749/2000, Training Loss (NLML): -839.2350, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 750/2000, Training Loss (NLML): -839.2817, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 751/2000, Training Loss (NLML): -839.3241, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 752/2000, Training Loss (NLML): -839.3636, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 753/2000, Training Loss (NLML): -839.4045, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 754/2000, Training Loss (NLML): -839.4348, (RMSE): 0.0139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 10:03:46] Energy consumed for RAM : 0.000196 kWh. RAM Power : 23.53756284713745 W\n",
      "[codecarbon INFO @ 10:03:46] Energy consumed for all CPUs : 0.000354 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:03:46] Energy consumed for all GPUs : 0.000591 kWh. Total GPU Power : 71.13125075539614 W\n",
      "[codecarbon INFO @ 10:03:46] 0.001141 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curve dfNGP Run 1/1, Epoch 755/2000, Training Loss (NLML): -839.4733, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 756/2000, Training Loss (NLML): -839.4885, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 757/2000, Training Loss (NLML): -839.5261, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 758/2000, Training Loss (NLML): -839.5157, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 759/2000, Training Loss (NLML): -839.5728, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 760/2000, Training Loss (NLML): -839.5711, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 761/2000, Training Loss (NLML): -839.6718, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 762/2000, Training Loss (NLML): -839.7276, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 763/2000, Training Loss (NLML): -839.8317, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 764/2000, Training Loss (NLML): -839.9083, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 765/2000, Training Loss (NLML): -839.9690, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 766/2000, Training Loss (NLML): -840.0084, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 767/2000, Training Loss (NLML): -840.0337, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 768/2000, Training Loss (NLML): -840.0541, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 769/2000, Training Loss (NLML): -840.0624, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 770/2000, Training Loss (NLML): -840.0922, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 771/2000, Training Loss (NLML): -840.1004, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 772/2000, Training Loss (NLML): -840.1570, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 773/2000, Training Loss (NLML): -840.1869, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 774/2000, Training Loss (NLML): -840.2571, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 775/2000, Training Loss (NLML): -840.3090, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 776/2000, Training Loss (NLML): -840.3701, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 777/2000, Training Loss (NLML): -840.4203, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 778/2000, Training Loss (NLML): -840.4647, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 779/2000, Training Loss (NLML): -840.5042, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 780/2000, Training Loss (NLML): -840.5386, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 781/2000, Training Loss (NLML): -840.5692, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 782/2000, Training Loss (NLML): -840.5958, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 783/2000, Training Loss (NLML): -840.6207, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 784/2000, Training Loss (NLML): -840.6367, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 785/2000, Training Loss (NLML): -840.6526, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 786/2000, Training Loss (NLML): -840.6395, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 787/2000, Training Loss (NLML): -840.6439, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 788/2000, Training Loss (NLML): -840.5843, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 789/2000, Training Loss (NLML): -840.6273, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 790/2000, Training Loss (NLML): -840.5948, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 791/2000, Training Loss (NLML): -840.7464, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 792/2000, Training Loss (NLML): -840.8342, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 793/2000, Training Loss (NLML): -840.9449, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 794/2000, Training Loss (NLML): -840.9976, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 795/2000, Training Loss (NLML): -841.0095, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 796/2000, Training Loss (NLML): -841.0052, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 797/2000, Training Loss (NLML): -840.9882, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 798/2000, Training Loss (NLML): -841.0339, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 799/2000, Training Loss (NLML): -841.0637, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 800/2000, Training Loss (NLML): -841.1426, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 801/2000, Training Loss (NLML): -841.2011, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 802/2000, Training Loss (NLML): -841.2494, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 803/2000, Training Loss (NLML): -841.2793, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 804/2000, Training Loss (NLML): -841.2927, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 805/2000, Training Loss (NLML): -841.3044, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 806/2000, Training Loss (NLML): -841.3085, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 807/2000, Training Loss (NLML): -841.3447, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 808/2000, Training Loss (NLML): -841.3729, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 809/2000, Training Loss (NLML): -841.4280, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 810/2000, Training Loss (NLML): -841.4712, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 811/2000, Training Loss (NLML): -841.5151, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 812/2000, Training Loss (NLML): -841.5520, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 813/2000, Training Loss (NLML): -841.5863, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 814/2000, Training Loss (NLML): -841.6184, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 815/2000, Training Loss (NLML): -841.6498, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 816/2000, Training Loss (NLML): -841.6819, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 817/2000, Training Loss (NLML): -841.7124, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 818/2000, Training Loss (NLML): -841.7427, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 819/2000, Training Loss (NLML): -841.7695, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 820/2000, Training Loss (NLML): -841.7927, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 821/2000, Training Loss (NLML): -841.8095, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 822/2000, Training Loss (NLML): -841.8214, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 823/2000, Training Loss (NLML): -841.8142, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 824/2000, Training Loss (NLML): -841.8082, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 825/2000, Training Loss (NLML): -841.7638, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 826/2000, Training Loss (NLML): -841.7824, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 827/2000, Training Loss (NLML): -841.7506, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 828/2000, Training Loss (NLML): -841.8787, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 829/2000, Training Loss (NLML): -841.9318, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 830/2000, Training Loss (NLML): -842.0424, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 831/2000, Training Loss (NLML): -842.0872, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 832/2000, Training Loss (NLML): -842.1321, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 833/2000, Training Loss (NLML): -842.1561, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 834/2000, Training Loss (NLML): -842.1768, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 835/2000, Training Loss (NLML): -842.2096, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 836/2000, Training Loss (NLML): -842.2235, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 837/2000, Training Loss (NLML): -842.2643, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 838/2000, Training Loss (NLML): -842.2818, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 839/2000, Training Loss (NLML): -842.3303, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 840/2000, Training Loss (NLML): -842.3702, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 841/2000, Training Loss (NLML): -842.4259, (RMSE): 0.0139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 10:03:50] Energy consumed for RAM : 0.000981 kWh. RAM Power : 23.53756284713745 W\n",
      "[codecarbon INFO @ 10:03:50] Energy consumed for all CPUs : 0.001772 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:03:50] Energy consumed for all GPUs : 0.002808 kWh. Total GPU Power : 71.18097577357794 W\n",
      "[codecarbon INFO @ 10:03:50] 0.005562 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curve dfNGP Run 1/1, Epoch 842/2000, Training Loss (NLML): -842.4742, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 843/2000, Training Loss (NLML): -842.5045, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 844/2000, Training Loss (NLML): -842.5193, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 845/2000, Training Loss (NLML): -842.5245, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 846/2000, Training Loss (NLML): -842.5472, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 847/2000, Training Loss (NLML): -842.5691, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 848/2000, Training Loss (NLML): -842.6143, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 849/2000, Training Loss (NLML): -842.6437, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 850/2000, Training Loss (NLML): -842.6794, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 851/2000, Training Loss (NLML): -842.7012, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 852/2000, Training Loss (NLML): -842.7332, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 853/2000, Training Loss (NLML): -842.7520, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 854/2000, Training Loss (NLML): -842.7740, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 855/2000, Training Loss (NLML): -842.7849, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 856/2000, Training Loss (NLML): -842.8044, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 857/2000, Training Loss (NLML): -842.8270, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 858/2000, Training Loss (NLML): -842.8705, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 859/2000, Training Loss (NLML): -842.9043, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 860/2000, Training Loss (NLML): -842.9534, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 861/2000, Training Loss (NLML): -842.9727, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 862/2000, Training Loss (NLML): -843.0120, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 863/2000, Training Loss (NLML): -843.0310, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 864/2000, Training Loss (NLML): -843.0825, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 865/2000, Training Loss (NLML): -843.1198, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 866/2000, Training Loss (NLML): -843.1660, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 867/2000, Training Loss (NLML): -843.2024, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 868/2000, Training Loss (NLML): -843.2396, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 869/2000, Training Loss (NLML): -843.2748, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 870/2000, Training Loss (NLML): -843.3080, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 871/2000, Training Loss (NLML): -843.3380, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 872/2000, Training Loss (NLML): -843.3636, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 873/2000, Training Loss (NLML): -843.3868, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 874/2000, Training Loss (NLML): -843.4073, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 875/2000, Training Loss (NLML): -843.4332, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 876/2000, Training Loss (NLML): -843.4547, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 877/2000, Training Loss (NLML): -843.4836, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 878/2000, Training Loss (NLML): -843.5016, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 879/2000, Training Loss (NLML): -843.5251, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 880/2000, Training Loss (NLML): -843.5205, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 881/2000, Training Loss (NLML): -843.5328, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 882/2000, Training Loss (NLML): -843.4712, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 883/2000, Training Loss (NLML): -843.4665, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 884/2000, Training Loss (NLML): -843.2692, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 885/2000, Training Loss (NLML): -843.2863, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 886/2000, Training Loss (NLML): -843.1360, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 887/2000, Training Loss (NLML): -843.4621, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 888/2000, Training Loss (NLML): -843.7409, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 889/2000, Training Loss (NLML): -843.8195, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 890/2000, Training Loss (NLML): -843.7081, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 891/2000, Training Loss (NLML): -843.5988, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 892/2000, Training Loss (NLML): -843.7736, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 893/2000, Training Loss (NLML): -843.8691, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 894/2000, Training Loss (NLML): -843.9261, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 895/2000, Training Loss (NLML): -843.9330, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 896/2000, Training Loss (NLML): -843.9531, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 897/2000, Training Loss (NLML): -843.9838, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 898/2000, Training Loss (NLML): -843.9757, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 899/2000, Training Loss (NLML): -844.0668, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 900/2000, Training Loss (NLML): -844.1147, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 901/2000, Training Loss (NLML): -844.1353, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 902/2000, Training Loss (NLML): -844.1606, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 903/2000, Training Loss (NLML): -844.2059, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 904/2000, Training Loss (NLML): -844.2456, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 905/2000, Training Loss (NLML): -844.2343, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 906/2000, Training Loss (NLML): -844.2522, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 907/2000, Training Loss (NLML): -844.3031, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 908/2000, Training Loss (NLML): -844.3740, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 909/2000, Training Loss (NLML): -844.3976, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 910/2000, Training Loss (NLML): -844.4069, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 911/2000, Training Loss (NLML): -844.4393, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 912/2000, Training Loss (NLML): -844.4867, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 913/2000, Training Loss (NLML): -844.5133, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 914/2000, Training Loss (NLML): -844.5219, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 915/2000, Training Loss (NLML): -844.5486, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 916/2000, Training Loss (NLML): -844.5829, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 917/2000, Training Loss (NLML): -844.6154, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 918/2000, Training Loss (NLML): -844.6354, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 919/2000, Training Loss (NLML): -844.6648, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 920/2000, Training Loss (NLML): -844.7013, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 921/2000, Training Loss (NLML): -844.7389, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 922/2000, Training Loss (NLML): -844.7637, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 923/2000, Training Loss (NLML): -844.7898, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 924/2000, Training Loss (NLML): -844.8197, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 925/2000, Training Loss (NLML): -844.8568, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 926/2000, Training Loss (NLML): -844.8887, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 927/2000, Training Loss (NLML): -844.9142, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 928/2000, Training Loss (NLML): -844.9403, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 929/2000, Training Loss (NLML): -844.9698, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 930/2000, Training Loss (NLML): -845.0021, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 931/2000, Training Loss (NLML): -845.0317, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 932/2000, Training Loss (NLML): -845.0593, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 933/2000, Training Loss (NLML): -845.0870, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 934/2000, Training Loss (NLML): -845.1169, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 935/2000, Training Loss (NLML): -845.1489, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 936/2000, Training Loss (NLML): -845.1798, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 937/2000, Training Loss (NLML): -845.2083, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 938/2000, Training Loss (NLML): -845.2360, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 939/2000, Training Loss (NLML): -845.2621, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 940/2000, Training Loss (NLML): -845.2885, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 941/2000, Training Loss (NLML): -845.3102, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 942/2000, Training Loss (NLML): -845.3253, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 943/2000, Training Loss (NLML): -845.3118, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 944/2000, Training Loss (NLML): -845.2853, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 945/2000, Training Loss (NLML): -845.1180, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 946/2000, Training Loss (NLML): -845.0773, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 947/2000, Training Loss (NLML): -844.6366, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 948/2000, Training Loss (NLML): -845.1144, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 949/2000, Training Loss (NLML): -845.2494, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 950/2000, Training Loss (NLML): -845.4821, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 951/2000, Training Loss (NLML): -845.5358, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 952/2000, Training Loss (NLML): -845.4471, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 953/2000, Training Loss (NLML): -845.4843, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 954/2000, Training Loss (NLML): -845.4832, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 955/2000, Training Loss (NLML): -845.6398, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 956/2000, Training Loss (NLML): -845.6943, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 957/2000, Training Loss (NLML): -845.6676, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 958/2000, Training Loss (NLML): -845.6847, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 959/2000, Training Loss (NLML): -845.7104, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 960/2000, Training Loss (NLML): -845.8386, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 961/2000, Training Loss (NLML): -845.8988, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 962/2000, Training Loss (NLML): -845.8960, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 963/2000, Training Loss (NLML): -845.8771, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 964/2000, Training Loss (NLML): -845.8684, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 965/2000, Training Loss (NLML): -845.9459, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 966/2000, Training Loss (NLML): -846.0073, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 967/2000, Training Loss (NLML): -846.0790, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 968/2000, Training Loss (NLML): -846.1212, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 969/2000, Training Loss (NLML): -846.1400, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 970/2000, Training Loss (NLML): -846.1471, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 971/2000, Training Loss (NLML): -846.1448, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 972/2000, Training Loss (NLML): -846.1851, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 973/2000, Training Loss (NLML): -846.2235, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 974/2000, Training Loss (NLML): -846.2793, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 975/2000, Training Loss (NLML): -846.3180, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 976/2000, Training Loss (NLML): -846.3408, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 977/2000, Training Loss (NLML): -846.3615, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 978/2000, Training Loss (NLML): -846.3728, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 979/2000, Training Loss (NLML): -846.4061, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 980/2000, Training Loss (NLML): -846.4242, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 981/2000, Training Loss (NLML): -846.4645, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 982/2000, Training Loss (NLML): -846.4884, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 983/2000, Training Loss (NLML): -846.5233, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 984/2000, Training Loss (NLML): -846.5433, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 985/2000, Training Loss (NLML): -846.5635, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 986/2000, Training Loss (NLML): -846.5686, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 987/2000, Training Loss (NLML): -846.5802, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 988/2000, Training Loss (NLML): -846.5939, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 989/2000, Training Loss (NLML): -846.6368, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 990/2000, Training Loss (NLML): -846.6854, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 991/2000, Training Loss (NLML): -846.7546, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 992/2000, Training Loss (NLML): -846.8070, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 993/2000, Training Loss (NLML): -846.8553, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 994/2000, Training Loss (NLML): -846.8817, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 995/2000, Training Loss (NLML): -846.9109, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 996/2000, Training Loss (NLML): -846.9261, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 997/2000, Training Loss (NLML): -846.9622, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 998/2000, Training Loss (NLML): -846.9884, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 999/2000, Training Loss (NLML): -847.0358, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1000/2000, Training Loss (NLML): -847.0758, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1001/2000, Training Loss (NLML): -847.1219, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1002/2000, Training Loss (NLML): -847.1616, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1003/2000, Training Loss (NLML): -847.1982, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1004/2000, Training Loss (NLML): -847.2297, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1005/2000, Training Loss (NLML): -847.2589, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1006/2000, Training Loss (NLML): -847.2860, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1007/2000, Training Loss (NLML): -847.3116, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1008/2000, Training Loss (NLML): -847.3365, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1009/2000, Training Loss (NLML): -847.3563, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1010/2000, Training Loss (NLML): -847.3724, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1011/2000, Training Loss (NLML): -847.3711, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1012/2000, Training Loss (NLML): -847.3574, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1013/2000, Training Loss (NLML): -847.2961, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1014/2000, Training Loss (NLML): -847.2092, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1015/2000, Training Loss (NLML): -847.0865, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 1016/2000, Training Loss (NLML): -847.0045, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1017/2000, Training Loss (NLML): -847.1688, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1018/2000, Training Loss (NLML): -847.2560, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1019/2000, Training Loss (NLML): -847.3909, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1020/2000, Training Loss (NLML): -847.0807, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1021/2000, Training Loss (NLML): -847.3529, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1022/2000, Training Loss (NLML): -847.4691, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1023/2000, Training Loss (NLML): -847.7499, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1024/2000, Training Loss (NLML): -847.7493, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1025/2000, Training Loss (NLML): -847.5514, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1026/2000, Training Loss (NLML): -847.6158, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1027/2000, Training Loss (NLML): -847.6666, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1028/2000, Training Loss (NLML): -847.8540, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1029/2000, Training Loss (NLML): -847.8766, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1030/2000, Training Loss (NLML): -847.8329, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1031/2000, Training Loss (NLML): -847.8746, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1032/2000, Training Loss (NLML): -847.9120, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1033/2000, Training Loss (NLML): -847.9780, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1034/2000, Training Loss (NLML): -847.9603, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1035/2000, Training Loss (NLML): -847.9703, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1036/2000, Training Loss (NLML): -848.0082, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1037/2000, Training Loss (NLML): -848.0977, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1038/2000, Training Loss (NLML): -848.1741, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1039/2000, Training Loss (NLML): -848.1978, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1040/2000, Training Loss (NLML): -848.1832, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1041/2000, Training Loss (NLML): -848.1707, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1042/2000, Training Loss (NLML): -848.1980, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1043/2000, Training Loss (NLML): -848.2388, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1044/2000, Training Loss (NLML): -848.2864, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1045/2000, Training Loss (NLML): -848.2966, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1046/2000, Training Loss (NLML): -848.3123, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1047/2000, Training Loss (NLML): -848.3127, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1048/2000, Training Loss (NLML): -848.3671, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1049/2000, Training Loss (NLML): -848.4108, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1050/2000, Training Loss (NLML): -848.4587, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1051/2000, Training Loss (NLML): -848.4865, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1052/2000, Training Loss (NLML): -848.5048, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1053/2000, Training Loss (NLML): -848.5242, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1054/2000, Training Loss (NLML): -848.5514, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1055/2000, Training Loss (NLML): -848.5839, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1056/2000, Training Loss (NLML): -848.6152, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1057/2000, Training Loss (NLML): -848.6407, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1058/2000, Training Loss (NLML): -848.6609, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1059/2000, Training Loss (NLML): -848.6780, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1060/2000, Training Loss (NLML): -848.6951, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1061/2000, Training Loss (NLML): -848.7140, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1062/2000, Training Loss (NLML): -848.7299, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1063/2000, Training Loss (NLML): -848.7473, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1064/2000, Training Loss (NLML): -848.7507, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1065/2000, Training Loss (NLML): -848.7595, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1066/2000, Training Loss (NLML): -848.7275, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1067/2000, Training Loss (NLML): -848.7264, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1068/2000, Training Loss (NLML): -848.6243, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1069/2000, Training Loss (NLML): -848.6598, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1070/2000, Training Loss (NLML): -848.5435, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1071/2000, Training Loss (NLML): -848.7189, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1072/2000, Training Loss (NLML): -848.7883, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1073/2000, Training Loss (NLML): -848.9419, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1074/2000, Training Loss (NLML): -849.0090, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1075/2000, Training Loss (NLML): -849.0178, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1076/2000, Training Loss (NLML): -848.9982, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1077/2000, Training Loss (NLML): -848.9618, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1078/2000, Training Loss (NLML): -848.9891, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1079/2000, Training Loss (NLML): -848.9904, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1080/2000, Training Loss (NLML): -849.0725, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1081/2000, Training Loss (NLML): -849.1276, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1082/2000, Training Loss (NLML): -849.1740, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1083/2000, Training Loss (NLML): -849.1885, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1084/2000, Training Loss (NLML): -849.1796, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1085/2000, Training Loss (NLML): -849.1769, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1086/2000, Training Loss (NLML): -849.1613, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1087/2000, Training Loss (NLML): -849.1909, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1088/2000, Training Loss (NLML): -849.1947, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1089/2000, Training Loss (NLML): -849.2290, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1090/2000, Training Loss (NLML): -849.2416, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1091/2000, Training Loss (NLML): -849.2568, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1092/2000, Training Loss (NLML): -849.2531, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1093/2000, Training Loss (NLML): -849.2223, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1094/2000, Training Loss (NLML): -849.1981, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1095/2000, Training Loss (NLML): -849.1410, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1096/2000, Training Loss (NLML): -849.1925, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1097/2000, Training Loss (NLML): -849.2120, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1098/2000, Training Loss (NLML): -849.3505, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1099/2000, Training Loss (NLML): -849.4281, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1100/2000, Training Loss (NLML): -849.4951, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1101/2000, Training Loss (NLML): -849.5162, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1102/2000, Training Loss (NLML): -849.5095, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1103/2000, Training Loss (NLML): -849.4926, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1104/2000, Training Loss (NLML): -849.4734, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1105/2000, Training Loss (NLML): -849.5057, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1106/2000, Training Loss (NLML): -849.5314, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1107/2000, Training Loss (NLML): -849.5892, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1108/2000, Training Loss (NLML): -849.6221, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1109/2000, Training Loss (NLML): -849.6481, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1110/2000, Training Loss (NLML): -849.6588, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1111/2000, Training Loss (NLML): -849.6705, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1112/2000, Training Loss (NLML): -849.6786, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1113/2000, Training Loss (NLML): -849.6936, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1114/2000, Training Loss (NLML): -849.7107, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1115/2000, Training Loss (NLML): -849.7340, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1116/2000, Training Loss (NLML): -849.7607, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1117/2000, Training Loss (NLML): -849.7864, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1118/2000, Training Loss (NLML): -849.8103, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1119/2000, Training Loss (NLML): -849.8269, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1120/2000, Training Loss (NLML): -849.8402, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1121/2000, Training Loss (NLML): -849.8458, (RMSE): 0.0138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 10:04:01] Energy consumed for RAM : 0.000295 kWh. RAM Power : 23.53756284713745 W\n",
      "[codecarbon INFO @ 10:04:01] Energy consumed for all CPUs : 0.000532 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:04:01] Energy consumed for all GPUs : 0.000889 kWh. Total GPU Power : 71.0848605968606 W\n",
      "[codecarbon INFO @ 10:04:01] 0.001716 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curve dfNGP Run 1/1, Epoch 1122/2000, Training Loss (NLML): -849.8534, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1123/2000, Training Loss (NLML): -849.8462, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1124/2000, Training Loss (NLML): -849.8529, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1125/2000, Training Loss (NLML): -849.8297, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1126/2000, Training Loss (NLML): -849.8441, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1127/2000, Training Loss (NLML): -849.8120, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1128/2000, Training Loss (NLML): -849.8546, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1129/2000, Training Loss (NLML): -849.8428, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1130/2000, Training Loss (NLML): -849.9142, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1131/2000, Training Loss (NLML): -849.9423, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1132/2000, Training Loss (NLML): -850.0060, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1133/2000, Training Loss (NLML): -850.0459, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1134/2000, Training Loss (NLML): -850.0795, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1135/2000, Training Loss (NLML): -850.0995, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1136/2000, Training Loss (NLML): -850.1101, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1137/2000, Training Loss (NLML): -850.1144, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1138/2000, Training Loss (NLML): -850.1117, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1139/2000, Training Loss (NLML): -850.1144, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1140/2000, Training Loss (NLML): -850.1034, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1141/2000, Training Loss (NLML): -850.1118, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1142/2000, Training Loss (NLML): -850.0923, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1143/2000, Training Loss (NLML): -850.1051, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1144/2000, Training Loss (NLML): -850.0726, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1145/2000, Training Loss (NLML): -850.0813, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1146/2000, Training Loss (NLML): -850.0328, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1147/2000, Training Loss (NLML): -850.0325, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1148/2000, Training Loss (NLML): -850.0074, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1149/2000, Training Loss (NLML): -850.0303, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1150/2000, Training Loss (NLML): -850.1133, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1151/2000, Training Loss (NLML): -850.1722, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1152/2000, Training Loss (NLML): -850.2621, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1153/2000, Training Loss (NLML): -850.2579, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1154/2000, Training Loss (NLML): -850.2723, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1155/2000, Training Loss (NLML): -850.2317, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1156/2000, Training Loss (NLML): -850.3003, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1157/2000, Training Loss (NLML): -850.3488, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1158/2000, Training Loss (NLML): -850.4171, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1159/2000, Training Loss (NLML): -850.4558, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1160/2000, Training Loss (NLML): -850.4576, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1161/2000, Training Loss (NLML): -850.4423, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1162/2000, Training Loss (NLML): -850.4032, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1163/2000, Training Loss (NLML): -850.4189, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1164/2000, Training Loss (NLML): -850.4131, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1165/2000, Training Loss (NLML): -850.4755, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1166/2000, Training Loss (NLML): -850.5126, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1167/2000, Training Loss (NLML): -850.5605, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1168/2000, Training Loss (NLML): -850.5873, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1169/2000, Training Loss (NLML): -850.6042, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1170/2000, Training Loss (NLML): -850.6106, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1171/2000, Training Loss (NLML): -850.6150, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1172/2000, Training Loss (NLML): -850.6222, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1173/2000, Training Loss (NLML): -850.6299, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1174/2000, Training Loss (NLML): -850.6498, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1175/2000, Training Loss (NLML): -850.6685, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1176/2000, Training Loss (NLML): -850.6957, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1177/2000, Training Loss (NLML): -850.7196, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1178/2000, Training Loss (NLML): -850.7429, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1179/2000, Training Loss (NLML): -850.7627, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1180/2000, Training Loss (NLML): -850.7797, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1181/2000, Training Loss (NLML): -850.7933, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1182/2000, Training Loss (NLML): -850.8046, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1183/2000, Training Loss (NLML): -850.8121, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1184/2000, Training Loss (NLML): -850.8188, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1185/2000, Training Loss (NLML): -850.8188, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1186/2000, Training Loss (NLML): -850.8196, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1187/2000, Training Loss (NLML): -850.8037, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1188/2000, Training Loss (NLML): -850.7968, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1189/2000, Training Loss (NLML): -850.7455, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1190/2000, Training Loss (NLML): -850.7411, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1191/2000, Training Loss (NLML): -850.6447, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1192/2000, Training Loss (NLML): -850.7013, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1193/2000, Training Loss (NLML): -850.6273, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1194/2000, Training Loss (NLML): -850.7828, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1195/2000, Training Loss (NLML): -850.8280, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1196/2000, Training Loss (NLML): -850.9509, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1197/2000, Training Loss (NLML): -851.0104, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1198/2000, Training Loss (NLML): -851.0398, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1199/2000, Training Loss (NLML): -851.0371, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1200/2000, Training Loss (NLML): -851.0116, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1201/2000, Training Loss (NLML): -851.0016, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1202/2000, Training Loss (NLML): -850.9720, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1203/2000, Training Loss (NLML): -851.0118, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1204/2000, Training Loss (NLML): -851.0215, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1205/2000, Training Loss (NLML): -851.0841, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1206/2000, Training Loss (NLML): -851.1211, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1207/2000, Training Loss (NLML): -851.1654, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1208/2000, Training Loss (NLML): -851.1942, (RMSE): 0.0138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 10:04:05] Energy consumed for RAM : 0.001079 kWh. RAM Power : 23.53756284713745 W\n",
      "[codecarbon INFO @ 10:04:05] Energy consumed for all CPUs : 0.001949 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:04:05] Energy consumed for all GPUs : 0.003105 kWh. Total GPU Power : 71.086248726834 W\n",
      "[codecarbon INFO @ 10:04:05] 0.006133 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curve dfNGP Run 1/1, Epoch 1209/2000, Training Loss (NLML): -851.2146, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1210/2000, Training Loss (NLML): -851.2263, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1211/2000, Training Loss (NLML): -851.2321, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1212/2000, Training Loss (NLML): -851.2351, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1213/2000, Training Loss (NLML): -851.2292, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1214/2000, Training Loss (NLML): -851.2274, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1215/2000, Training Loss (NLML): -851.2084, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1216/2000, Training Loss (NLML): -851.2072, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1217/2000, Training Loss (NLML): -851.1747, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1218/2000, Training Loss (NLML): -851.1882, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1219/2000, Training Loss (NLML): -851.1665, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1220/2000, Training Loss (NLML): -851.2129, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1221/2000, Training Loss (NLML): -851.2391, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1222/2000, Training Loss (NLML): -851.3125, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1223/2000, Training Loss (NLML): -851.3719, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1224/2000, Training Loss (NLML): -851.4264, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1225/2000, Training Loss (NLML): -851.4596, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1226/2000, Training Loss (NLML): -851.4724, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1227/2000, Training Loss (NLML): -851.4723, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1228/2000, Training Loss (NLML): -851.4634, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1229/2000, Training Loss (NLML): -851.4617, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1230/2000, Training Loss (NLML): -851.4554, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1231/2000, Training Loss (NLML): -851.4749, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1232/2000, Training Loss (NLML): -851.4838, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1233/2000, Training Loss (NLML): -851.5182, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1234/2000, Training Loss (NLML): -851.5356, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1235/2000, Training Loss (NLML): -851.5657, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1236/2000, Training Loss (NLML): -851.5752, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1237/2000, Training Loss (NLML): -851.5902, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1238/2000, Training Loss (NLML): -851.5740, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1239/2000, Training Loss (NLML): -851.5723, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1240/2000, Training Loss (NLML): -851.5084, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1241/2000, Training Loss (NLML): -851.5160, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1242/2000, Training Loss (NLML): -851.4073, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1243/2000, Training Loss (NLML): -851.4932, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1244/2000, Training Loss (NLML): -851.4515, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1245/2000, Training Loss (NLML): -851.6014, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1246/2000, Training Loss (NLML): -851.6689, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1247/2000, Training Loss (NLML): -851.7567, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1248/2000, Training Loss (NLML): -851.7992, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1249/2000, Training Loss (NLML): -851.8092, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1250/2000, Training Loss (NLML): -851.8026, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1251/2000, Training Loss (NLML): -851.7756, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1252/2000, Training Loss (NLML): -851.7863, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1253/2000, Training Loss (NLML): -851.7731, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1254/2000, Training Loss (NLML): -851.8243, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1255/2000, Training Loss (NLML): -851.8500, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1256/2000, Training Loss (NLML): -851.8981, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1257/2000, Training Loss (NLML): -851.9254, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1258/2000, Training Loss (NLML): -851.9451, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1259/2000, Training Loss (NLML): -851.9534, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1260/2000, Training Loss (NLML): -851.9478, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1261/2000, Training Loss (NLML): -851.9432, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1262/2000, Training Loss (NLML): -851.9122, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1263/2000, Training Loss (NLML): -851.9094, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1264/2000, Training Loss (NLML): -851.8585, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1265/2000, Training Loss (NLML): -851.8785, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1266/2000, Training Loss (NLML): -851.8380, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1267/2000, Training Loss (NLML): -851.8984, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1268/2000, Training Loss (NLML): -851.9191, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1269/2000, Training Loss (NLML): -852.0049, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1270/2000, Training Loss (NLML): -852.0756, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1271/2000, Training Loss (NLML): -852.1381, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1272/2000, Training Loss (NLML): -852.1788, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1273/2000, Training Loss (NLML): -852.1888, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1274/2000, Training Loss (NLML): -852.1849, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1275/2000, Training Loss (NLML): -852.1635, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1276/2000, Training Loss (NLML): -852.1700, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1277/2000, Training Loss (NLML): -852.1632, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1278/2000, Training Loss (NLML): -852.2018, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1279/2000, Training Loss (NLML): -852.2281, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1280/2000, Training Loss (NLML): -852.2745, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1281/2000, Training Loss (NLML): -852.3121, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1282/2000, Training Loss (NLML): -852.3464, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1283/2000, Training Loss (NLML): -852.3721, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1284/2000, Training Loss (NLML): -852.3914, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1285/2000, Training Loss (NLML): -852.4052, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1286/2000, Training Loss (NLML): -852.4149, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1287/2000, Training Loss (NLML): -852.4220, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1288/2000, Training Loss (NLML): -852.4244, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1289/2000, Training Loss (NLML): -852.4294, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1290/2000, Training Loss (NLML): -852.4215, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1291/2000, Training Loss (NLML): -852.4272, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1292/2000, Training Loss (NLML): -852.4037, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1293/2000, Training Loss (NLML): -852.4161, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1294/2000, Training Loss (NLML): -852.3776, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1295/2000, Training Loss (NLML): -852.4137, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1296/2000, Training Loss (NLML): -852.3799, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1297/2000, Training Loss (NLML): -852.4525, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1298/2000, Training Loss (NLML): -852.4590, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1299/2000, Training Loss (NLML): -852.5426, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1300/2000, Training Loss (NLML): -852.5846, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1301/2000, Training Loss (NLML): -852.6421, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1302/2000, Training Loss (NLML): -852.6783, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1303/2000, Training Loss (NLML): -852.7040, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1304/2000, Training Loss (NLML): -852.7184, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1305/2000, Training Loss (NLML): -852.7230, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1306/2000, Training Loss (NLML): -852.7223, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1307/2000, Training Loss (NLML): -852.7104, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1308/2000, Training Loss (NLML): -852.7032, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1309/2000, Training Loss (NLML): -852.6698, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1310/2000, Training Loss (NLML): -852.6558, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1311/2000, Training Loss (NLML): -852.5972, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1312/2000, Training Loss (NLML): -852.5865, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1313/2000, Training Loss (NLML): -852.5481, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1314/2000, Training Loss (NLML): -852.5890, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1315/2000, Training Loss (NLML): -852.6674, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1316/2000, Training Loss (NLML): -852.7693, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1317/2000, Training Loss (NLML): -852.8779, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1318/2000, Training Loss (NLML): -852.9122, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1319/2000, Training Loss (NLML): -852.9022, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1320/2000, Training Loss (NLML): -852.8206, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1321/2000, Training Loss (NLML): -852.8323, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1322/2000, Training Loss (NLML): -852.7888, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1323/2000, Training Loss (NLML): -852.9076, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1324/2000, Training Loss (NLML): -852.9659, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1325/2000, Training Loss (NLML): -853.0448, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1326/2000, Training Loss (NLML): -853.0734, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1327/2000, Training Loss (NLML): -853.0834, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1328/2000, Training Loss (NLML): -853.0748, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1329/2000, Training Loss (NLML): -853.0798, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1330/2000, Training Loss (NLML): -853.0956, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1331/2000, Training Loss (NLML): -853.1298, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1332/2000, Training Loss (NLML): -853.1726, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1333/2000, Training Loss (NLML): -853.2144, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1334/2000, Training Loss (NLML): -853.2494, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1335/2000, Training Loss (NLML): -853.2727, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1336/2000, Training Loss (NLML): -853.2871, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1337/2000, Training Loss (NLML): -853.2947, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1338/2000, Training Loss (NLML): -853.2996, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1339/2000, Training Loss (NLML): -853.3021, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1340/2000, Training Loss (NLML): -853.3087, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1341/2000, Training Loss (NLML): -853.3073, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1342/2000, Training Loss (NLML): -853.3156, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1343/2000, Training Loss (NLML): -853.2932, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1344/2000, Training Loss (NLML): -853.2932, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1345/2000, Training Loss (NLML): -853.1957, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1346/2000, Training Loss (NLML): -853.2052, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1347/2000, Training Loss (NLML): -852.9906, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 1348/2000, Training Loss (NLML): -853.1752, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1349/2000, Training Loss (NLML): -853.1172, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1350/2000, Training Loss (NLML): -853.3795, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1351/2000, Training Loss (NLML): -853.4944, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1352/2000, Training Loss (NLML): -853.5652, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1353/2000, Training Loss (NLML): -853.5635, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1354/2000, Training Loss (NLML): -853.5037, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1355/2000, Training Loss (NLML): -853.4901, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1356/2000, Training Loss (NLML): -853.4366, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1357/2000, Training Loss (NLML): -853.5427, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1358/2000, Training Loss (NLML): -853.6044, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1359/2000, Training Loss (NLML): -853.6919, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1360/2000, Training Loss (NLML): -853.7372, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1361/2000, Training Loss (NLML): -853.7498, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1362/2000, Training Loss (NLML): -853.7426, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1363/2000, Training Loss (NLML): -853.7156, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1364/2000, Training Loss (NLML): -853.7157, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1365/2000, Training Loss (NLML): -853.6807, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1366/2000, Training Loss (NLML): -853.7252, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1367/2000, Training Loss (NLML): -853.7180, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1368/2000, Training Loss (NLML): -853.7809, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1369/2000, Training Loss (NLML): -853.7902, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1370/2000, Training Loss (NLML): -853.8156, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1371/2000, Training Loss (NLML): -853.7982, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1372/2000, Training Loss (NLML): -853.7599, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1373/2000, Training Loss (NLML): -853.7220, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1374/2000, Training Loss (NLML): -853.6458, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1375/2000, Training Loss (NLML): -853.6908, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1376/2000, Training Loss (NLML): -853.7220, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1377/2000, Training Loss (NLML): -853.8911, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1378/2000, Training Loss (NLML): -854.0156, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1379/2000, Training Loss (NLML): -854.1025, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1380/2000, Training Loss (NLML): -854.1185, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1381/2000, Training Loss (NLML): -854.0892, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1382/2000, Training Loss (NLML): -854.0538, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1383/2000, Training Loss (NLML): -854.0287, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1384/2000, Training Loss (NLML): -854.0707, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1385/2000, Training Loss (NLML): -854.1143, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1386/2000, Training Loss (NLML): -854.1968, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1387/2000, Training Loss (NLML): -854.2534, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1388/2000, Training Loss (NLML): -854.2900, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1389/2000, Training Loss (NLML): -854.3007, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1390/2000, Training Loss (NLML): -854.2993, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1391/2000, Training Loss (NLML): -854.2922, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1392/2000, Training Loss (NLML): -854.2921, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1393/2000, Training Loss (NLML): -854.2999, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1394/2000, Training Loss (NLML): -854.3217, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1395/2000, Training Loss (NLML): -854.3519, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1396/2000, Training Loss (NLML): -854.3947, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1397/2000, Training Loss (NLML): -854.4362, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1398/2000, Training Loss (NLML): -854.4801, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1399/2000, Training Loss (NLML): -854.5154, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1400/2000, Training Loss (NLML): -854.5442, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1401/2000, Training Loss (NLML): -854.5651, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1402/2000, Training Loss (NLML): -854.5803, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1403/2000, Training Loss (NLML): -854.5895, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1404/2000, Training Loss (NLML): -854.5980, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1405/2000, Training Loss (NLML): -854.5933, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1406/2000, Training Loss (NLML): -854.5978, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1407/2000, Training Loss (NLML): -854.5626, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1408/2000, Training Loss (NLML): -854.5680, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1409/2000, Training Loss (NLML): -854.4718, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1410/2000, Training Loss (NLML): -854.5151, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1411/2000, Training Loss (NLML): -854.3784, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1412/2000, Training Loss (NLML): -854.5387, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1413/2000, Training Loss (NLML): -854.5200, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1414/2000, Training Loss (NLML): -854.7019, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1415/2000, Training Loss (NLML): -854.7787, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1416/2000, Training Loss (NLML): -854.8524, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1417/2000, Training Loss (NLML): -854.8717, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1418/2000, Training Loss (NLML): -854.8541, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1419/2000, Training Loss (NLML): -854.8383, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1420/2000, Training Loss (NLML): -854.7922, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1421/2000, Training Loss (NLML): -854.8357, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1422/2000, Training Loss (NLML): -854.8334, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1423/2000, Training Loss (NLML): -854.9266, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1424/2000, Training Loss (NLML): -854.9730, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1425/2000, Training Loss (NLML): -855.0454, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1426/2000, Training Loss (NLML): -855.0902, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1427/2000, Training Loss (NLML): -855.1261, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1428/2000, Training Loss (NLML): -855.1472, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1429/2000, Training Loss (NLML): -855.1508, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1430/2000, Training Loss (NLML): -855.1520, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1431/2000, Training Loss (NLML): -855.1115, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1432/2000, Training Loss (NLML): -855.1064, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1433/2000, Training Loss (NLML): -854.9923, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 1434/2000, Training Loss (NLML): -855.0233, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1435/2000, Training Loss (NLML): -854.8607, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 1436/2000, Training Loss (NLML): -854.9971, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1437/2000, Training Loss (NLML): -854.9498, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 1438/2000, Training Loss (NLML): -855.1230, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1439/2000, Training Loss (NLML): -855.2042, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1440/2000, Training Loss (NLML): -855.2947, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1441/2000, Training Loss (NLML): -855.3649, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1442/2000, Training Loss (NLML): -855.3839, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1443/2000, Training Loss (NLML): -855.4286, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1444/2000, Training Loss (NLML): -855.4160, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1445/2000, Training Loss (NLML): -855.4655, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1446/2000, Training Loss (NLML): -855.4680, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1447/2000, Training Loss (NLML): -855.5237, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1448/2000, Training Loss (NLML): -855.5521, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1449/2000, Training Loss (NLML): -855.5760, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1450/2000, Training Loss (NLML): -855.6093, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1451/2000, Training Loss (NLML): -855.6158, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1452/2000, Training Loss (NLML): -855.6626, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1453/2000, Training Loss (NLML): -855.6707, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1454/2000, Training Loss (NLML): -855.7190, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1455/2000, Training Loss (NLML): -855.7281, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1456/2000, Training Loss (NLML): -855.7747, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1457/2000, Training Loss (NLML): -855.7881, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1458/2000, Training Loss (NLML): -855.8334, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1459/2000, Training Loss (NLML): -855.8478, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1460/2000, Training Loss (NLML): -855.8845, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1461/2000, Training Loss (NLML): -855.8905, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1462/2000, Training Loss (NLML): -855.9153, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1463/2000, Training Loss (NLML): -855.9052, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 1464/2000, Training Loss (NLML): -855.9109, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1465/2000, Training Loss (NLML): -855.8827, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 1466/2000, Training Loss (NLML): -855.8695, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1467/2000, Training Loss (NLML): -855.8540, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 1468/2000, Training Loss (NLML): -855.8655, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1469/2000, Training Loss (NLML): -855.9281, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 1470/2000, Training Loss (NLML): -856.0139, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1471/2000, Training Loss (NLML): -856.1327, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1472/2000, Training Loss (NLML): -856.2263, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1473/2000, Training Loss (NLML): -856.2914, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1474/2000, Training Loss (NLML): -856.3142, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1475/2000, Training Loss (NLML): -856.3114, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1476/2000, Training Loss (NLML): -856.2863, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1477/2000, Training Loss (NLML): -856.2844, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1478/2000, Training Loss (NLML): -856.2668, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 1479/2000, Training Loss (NLML): -856.3082, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1480/2000, Training Loss (NLML): -856.3240, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 1481/2000, Training Loss (NLML): -856.3937, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1482/2000, Training Loss (NLML): -856.4229, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 1483/2000, Training Loss (NLML): -856.4799, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1484/2000, Training Loss (NLML): -856.4772, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1485/2000, Training Loss (NLML): -856.4978, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1486/2000, Training Loss (NLML): -856.4109, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1487/2000, Training Loss (NLML): -856.4360, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1488/2000, Training Loss (NLML): -856.2628, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 1489/2000, Training Loss (NLML): -856.4264, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1490/2000, Training Loss (NLML): -856.3719, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1491/2000, Training Loss (NLML): -856.6011, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1492/2000, Training Loss (NLML): -856.7040, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1493/2000, Training Loss (NLML): -856.8214, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1494/2000, Training Loss (NLML): -856.8810, (RMSE): 0.0138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 10:04:16] Energy consumed for RAM : 0.000393 kWh. RAM Power : 23.53756284713745 W\n",
      "[codecarbon INFO @ 10:04:16] Energy consumed for all CPUs : 0.000709 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:04:16] Energy consumed for all GPUs : 0.001188 kWh. Total GPU Power : 71.74070149815307 W\n",
      "[codecarbon INFO @ 10:04:16] 0.002290 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curve dfNGP Run 1/1, Epoch 1495/2000, Training Loss (NLML): -856.8959, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1496/2000, Training Loss (NLML): -856.8857, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1497/2000, Training Loss (NLML): -856.8431, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1498/2000, Training Loss (NLML): -856.8578, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1499/2000, Training Loss (NLML): -856.8164, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1500/2000, Training Loss (NLML): -856.9033, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1501/2000, Training Loss (NLML): -856.9160, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1502/2000, Training Loss (NLML): -857.0223, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1503/2000, Training Loss (NLML): -857.0774, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1504/2000, Training Loss (NLML): -857.1508, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1505/2000, Training Loss (NLML): -857.2000, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1506/2000, Training Loss (NLML): -857.2402, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1507/2000, Training Loss (NLML): -857.2677, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1508/2000, Training Loss (NLML): -857.2848, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1509/2000, Training Loss (NLML): -857.2917, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1510/2000, Training Loss (NLML): -857.2679, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1511/2000, Training Loss (NLML): -857.2242, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 1512/2000, Training Loss (NLML): -857.0571, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1513/2000, Training Loss (NLML): -856.8607, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 1514/2000, Training Loss (NLML): -856.3737, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1515/2000, Training Loss (NLML): -856.2181, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 1516/2000, Training Loss (NLML): -856.3661, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1517/2000, Training Loss (NLML): -857.0199, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 1518/2000, Training Loss (NLML): -857.5153, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1519/2000, Training Loss (NLML): -857.3707, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1520/2000, Training Loss (NLML): -857.1108, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1521/2000, Training Loss (NLML): -856.9972, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1522/2000, Training Loss (NLML): -857.5328, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1523/2000, Training Loss (NLML): -857.7212, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1524/2000, Training Loss (NLML): -857.4856, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1525/2000, Training Loss (NLML): -857.3895, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1526/2000, Training Loss (NLML): -857.4387, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1527/2000, Training Loss (NLML): -857.7806, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1528/2000, Training Loss (NLML): -857.8306, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1529/2000, Training Loss (NLML): -857.7256, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1530/2000, Training Loss (NLML): -857.6610, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 1531/2000, Training Loss (NLML): -857.7800, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1532/2000, Training Loss (NLML): -857.9644, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1533/2000, Training Loss (NLML): -858.0319, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1534/2000, Training Loss (NLML): -857.9945, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1535/2000, Training Loss (NLML): -857.9031, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 1536/2000, Training Loss (NLML): -857.9340, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1537/2000, Training Loss (NLML): -857.9288, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 1538/2000, Training Loss (NLML): -857.9969, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1539/2000, Training Loss (NLML): -857.8419, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 1540/2000, Training Loss (NLML): -857.8707, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1541/2000, Training Loss (NLML): -857.5444, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 1542/2000, Training Loss (NLML): -857.9911, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1543/2000, Training Loss (NLML): -858.1205, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1544/2000, Training Loss (NLML): -858.3386, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1545/2000, Training Loss (NLML): -858.3970, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1546/2000, Training Loss (NLML): -858.3671, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1547/2000, Training Loss (NLML): -858.3499, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1548/2000, Training Loss (NLML): -858.2797, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1549/2000, Training Loss (NLML): -858.3975, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1550/2000, Training Loss (NLML): -858.4283, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1551/2000, Training Loss (NLML): -858.5255, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1552/2000, Training Loss (NLML): -858.5726, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1553/2000, Training Loss (NLML): -858.6232, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1554/2000, Training Loss (NLML): -858.6835, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1555/2000, Training Loss (NLML): -858.7253, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1556/2000, Training Loss (NLML): -858.7539, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1557/2000, Training Loss (NLML): -858.7438, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1558/2000, Training Loss (NLML): -858.7407, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1559/2000, Training Loss (NLML): -858.6927, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 1560/2000, Training Loss (NLML): -858.7222, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1561/2000, Training Loss (NLML): -858.6829, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 1562/2000, Training Loss (NLML): -858.7805, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1563/2000, Training Loss (NLML): -858.7734, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 1564/2000, Training Loss (NLML): -858.9012, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1565/2000, Training Loss (NLML): -858.9370, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1566/2000, Training Loss (NLML): -859.0469, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1567/2000, Training Loss (NLML): -859.1089, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1568/2000, Training Loss (NLML): -859.1768, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1569/2000, Training Loss (NLML): -859.2220, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1570/2000, Training Loss (NLML): -859.2576, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1571/2000, Training Loss (NLML): -859.2865, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1572/2000, Training Loss (NLML): -859.3086, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1573/2000, Training Loss (NLML): -859.3340, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1574/2000, Training Loss (NLML): -859.3387, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1575/2000, Training Loss (NLML): -859.3566, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1576/2000, Training Loss (NLML): -859.3270, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1577/2000, Training Loss (NLML): -859.3474, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1578/2000, Training Loss (NLML): -859.2743, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1579/2000, Training Loss (NLML): -859.3363, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1580/2000, Training Loss (NLML): -859.2463, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 1581/2000, Training Loss (NLML): -859.3878, (RMSE): 0.0138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 10:04:20] Energy consumed for RAM : 0.001178 kWh. RAM Power : 23.53756284713745 W\n",
      "[codecarbon INFO @ 10:04:20] Energy consumed for all CPUs : 0.002127 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:04:20] Energy consumed for all GPUs : 0.003403 kWh. Total GPU Power : 71.57266026276899 W\n",
      "[codecarbon INFO @ 10:04:20] 0.006708 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curve dfNGP Run 1/1, Epoch 1582/2000, Training Loss (NLML): -859.3627, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 1583/2000, Training Loss (NLML): -859.5253, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1584/2000, Training Loss (NLML): -859.5682, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1585/2000, Training Loss (NLML): -859.6666, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1586/2000, Training Loss (NLML): -859.7120, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1587/2000, Training Loss (NLML): -859.7499, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1588/2000, Training Loss (NLML): -859.7943, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1589/2000, Training Loss (NLML): -859.8131, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1590/2000, Training Loss (NLML): -859.8787, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1591/2000, Training Loss (NLML): -859.9081, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1592/2000, Training Loss (NLML): -859.9808, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1593/2000, Training Loss (NLML): -860.0199, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1594/2000, Training Loss (NLML): -860.0844, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1595/2000, Training Loss (NLML): -860.1210, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1596/2000, Training Loss (NLML): -860.1710, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1597/2000, Training Loss (NLML): -860.1959, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1598/2000, Training Loss (NLML): -860.2307, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1599/2000, Training Loss (NLML): -860.2225, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1600/2000, Training Loss (NLML): -860.2312, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1601/2000, Training Loss (NLML): -860.1193, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 1602/2000, Training Loss (NLML): -860.0878, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1603/2000, Training Loss (NLML): -859.7070, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 1604/2000, Training Loss (NLML): -859.7781, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1605/2000, Training Loss (NLML): -859.3082, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 1606/2000, Training Loss (NLML): -859.7363, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1607/2000, Training Loss (NLML): -859.9755, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1608/2000, Training Loss (NLML): -860.2612, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1609/2000, Training Loss (NLML): -860.4611, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1610/2000, Training Loss (NLML): -860.3713, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1611/2000, Training Loss (NLML): -860.4596, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1612/2000, Training Loss (NLML): -860.4132, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1613/2000, Training Loss (NLML): -860.6029, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1614/2000, Training Loss (NLML): -860.6827, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1615/2000, Training Loss (NLML): -860.7391, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1616/2000, Training Loss (NLML): -860.8104, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1617/2000, Training Loss (NLML): -860.7306, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1618/2000, Training Loss (NLML): -860.8009, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1619/2000, Training Loss (NLML): -860.7042, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 1620/2000, Training Loss (NLML): -860.9191, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1621/2000, Training Loss (NLML): -861.0148, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1622/2000, Training Loss (NLML): -861.1188, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1623/2000, Training Loss (NLML): -861.1687, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1624/2000, Training Loss (NLML): -861.1615, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1625/2000, Training Loss (NLML): -861.2013, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1626/2000, Training Loss (NLML): -861.1639, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1627/2000, Training Loss (NLML): -861.2377, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1628/2000, Training Loss (NLML): -861.1769, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1629/2000, Training Loss (NLML): -861.2753, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1630/2000, Training Loss (NLML): -861.2347, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1631/2000, Training Loss (NLML): -861.3743, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1632/2000, Training Loss (NLML): -861.4241, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1633/2000, Training Loss (NLML): -861.5470, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1634/2000, Training Loss (NLML): -861.6041, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1635/2000, Training Loss (NLML): -861.6469, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1636/2000, Training Loss (NLML): -861.6443, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1637/2000, Training Loss (NLML): -861.6271, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1638/2000, Training Loss (NLML): -861.6169, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1639/2000, Training Loss (NLML): -861.5588, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1640/2000, Training Loss (NLML): -861.5958, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1641/2000, Training Loss (NLML): -861.5215, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1642/2000, Training Loss (NLML): -861.6442, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1643/2000, Training Loss (NLML): -861.6674, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1644/2000, Training Loss (NLML): -861.8287, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1645/2000, Training Loss (NLML): -861.9561, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1646/2000, Training Loss (NLML): -862.0682, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1647/2000, Training Loss (NLML): -862.1644, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1648/2000, Training Loss (NLML): -862.1998, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1649/2000, Training Loss (NLML): -862.2321, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1650/2000, Training Loss (NLML): -862.2003, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1651/2000, Training Loss (NLML): -862.2361, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1652/2000, Training Loss (NLML): -862.1674, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1653/2000, Training Loss (NLML): -862.2520, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1654/2000, Training Loss (NLML): -862.1899, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1655/2000, Training Loss (NLML): -862.3154, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1656/2000, Training Loss (NLML): -862.2932, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1657/2000, Training Loss (NLML): -862.4250, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1658/2000, Training Loss (NLML): -862.4373, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1659/2000, Training Loss (NLML): -862.5392, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1660/2000, Training Loss (NLML): -862.5441, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1661/2000, Training Loss (NLML): -862.5890, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1662/2000, Training Loss (NLML): -862.5502, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1663/2000, Training Loss (NLML): -862.5090, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1664/2000, Training Loss (NLML): -862.4570, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1665/2000, Training Loss (NLML): -862.3221, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1666/2000, Training Loss (NLML): -862.4758, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1667/2000, Training Loss (NLML): -862.4449, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1668/2000, Training Loss (NLML): -862.8209, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1669/2000, Training Loss (NLML): -862.9480, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1670/2000, Training Loss (NLML): -863.1079, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1671/2000, Training Loss (NLML): -863.1139, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1672/2000, Training Loss (NLML): -863.1436, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1673/2000, Training Loss (NLML): -863.1123, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1674/2000, Training Loss (NLML): -863.1504, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1675/2000, Training Loss (NLML): -863.2003, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1676/2000, Training Loss (NLML): -863.2415, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1677/2000, Training Loss (NLML): -863.3457, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1678/2000, Training Loss (NLML): -863.3610, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1679/2000, Training Loss (NLML): -863.4346, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1680/2000, Training Loss (NLML): -863.3319, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1681/2000, Training Loss (NLML): -863.3728, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1682/2000, Training Loss (NLML): -863.0436, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1683/2000, Training Loss (NLML): -863.3113, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1684/2000, Training Loss (NLML): -863.1041, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1685/2000, Training Loss (NLML): -863.5518, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1686/2000, Training Loss (NLML): -863.7035, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1687/2000, Training Loss (NLML): -863.9065, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1688/2000, Training Loss (NLML): -864.0126, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1689/2000, Training Loss (NLML): -864.0482, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1690/2000, Training Loss (NLML): -864.0339, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1691/2000, Training Loss (NLML): -863.9484, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1692/2000, Training Loss (NLML): -863.9615, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1693/2000, Training Loss (NLML): -863.8442, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1694/2000, Training Loss (NLML): -864.0082, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1695/2000, Training Loss (NLML): -863.9940, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1696/2000, Training Loss (NLML): -864.1771, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1697/2000, Training Loss (NLML): -864.1781, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1698/2000, Training Loss (NLML): -864.2207, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1699/2000, Training Loss (NLML): -864.0128, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1700/2000, Training Loss (NLML): -863.7600, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1701/2000, Training Loss (NLML): -863.1873, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 1702/2000, Training Loss (NLML): -862.7697, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1703/2000, Training Loss (NLML): -863.3102, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1704/2000, Training Loss (NLML): -864.2020, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1705/2000, Training Loss (NLML): -864.7720, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1706/2000, Training Loss (NLML): -864.6090, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1707/2000, Training Loss (NLML): -864.1158, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1708/2000, Training Loss (NLML): -864.1498, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1709/2000, Training Loss (NLML): -864.6384, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1710/2000, Training Loss (NLML): -865.0029, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1711/2000, Training Loss (NLML): -864.9108, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1712/2000, Training Loss (NLML): -864.6105, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1713/2000, Training Loss (NLML): -864.6039, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1714/2000, Training Loss (NLML): -864.8517, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1715/2000, Training Loss (NLML): -865.1776, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1716/2000, Training Loss (NLML): -865.2991, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1717/2000, Training Loss (NLML): -865.2223, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1718/2000, Training Loss (NLML): -865.0723, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1719/2000, Training Loss (NLML): -864.9889, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1720/2000, Training Loss (NLML): -865.0657, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1721/2000, Training Loss (NLML): -865.2411, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1722/2000, Training Loss (NLML): -865.4561, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1723/2000, Training Loss (NLML): -865.5974, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1724/2000, Training Loss (NLML): -865.6482, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1725/2000, Training Loss (NLML): -865.6298, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1726/2000, Training Loss (NLML): -865.6017, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1727/2000, Training Loss (NLML): -865.6066, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1728/2000, Training Loss (NLML): -865.6664, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1729/2000, Training Loss (NLML): -865.7748, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1730/2000, Training Loss (NLML): -865.8788, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1731/2000, Training Loss (NLML): -865.9648, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1732/2000, Training Loss (NLML): -866.0073, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1733/2000, Training Loss (NLML): -866.0276, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1734/2000, Training Loss (NLML): -865.9977, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1735/2000, Training Loss (NLML): -865.9840, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1736/2000, Training Loss (NLML): -865.7166, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1737/2000, Training Loss (NLML): -865.6329, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1738/2000, Training Loss (NLML): -864.3242, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 1739/2000, Training Loss (NLML): -865.7506, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1740/2000, Training Loss (NLML): -865.9525, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1741/2000, Training Loss (NLML): -866.1964, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1742/2000, Training Loss (NLML): -866.2631, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1743/2000, Training Loss (NLML): -866.2101, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1744/2000, Training Loss (NLML): -866.3779, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1745/2000, Training Loss (NLML): -866.2699, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1746/2000, Training Loss (NLML): -866.3688, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1747/2000, Training Loss (NLML): -866.2550, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1748/2000, Training Loss (NLML): -866.4757, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1749/2000, Training Loss (NLML): -866.6787, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1750/2000, Training Loss (NLML): -866.8702, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1751/2000, Training Loss (NLML): -866.9379, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1752/2000, Training Loss (NLML): -866.8879, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1753/2000, Training Loss (NLML): -866.8159, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1754/2000, Training Loss (NLML): -866.6910, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1755/2000, Training Loss (NLML): -866.7515, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1756/2000, Training Loss (NLML): -866.5280, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1757/2000, Training Loss (NLML): -866.6930, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1758/2000, Training Loss (NLML): -866.1271, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1759/2000, Training Loss (NLML): -866.7421, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1760/2000, Training Loss (NLML): -866.6732, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1761/2000, Training Loss (NLML): -867.0618, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1762/2000, Training Loss (NLML): -867.1340, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1763/2000, Training Loss (NLML): -866.9677, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1764/2000, Training Loss (NLML): -867.1381, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1765/2000, Training Loss (NLML): -866.9958, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1766/2000, Training Loss (NLML): -867.4874, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1767/2000, Training Loss (NLML): -867.7039, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1768/2000, Training Loss (NLML): -867.7994, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1769/2000, Training Loss (NLML): -867.7771, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1770/2000, Training Loss (NLML): -867.6449, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1771/2000, Training Loss (NLML): -867.6339, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1772/2000, Training Loss (NLML): -867.3431, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1773/2000, Training Loss (NLML): -867.4838, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1774/2000, Training Loss (NLML): -867.1353, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1775/2000, Training Loss (NLML): -867.2413, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1776/2000, Training Loss (NLML): -867.0573, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1777/2000, Training Loss (NLML): -867.1294, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1778/2000, Training Loss (NLML): -867.4965, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1779/2000, Training Loss (NLML): -867.7904, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1780/2000, Training Loss (NLML): -868.1729, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1781/2000, Training Loss (NLML): -868.3059, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1782/2000, Training Loss (NLML): -868.3160, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1783/2000, Training Loss (NLML): -868.1619, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1784/2000, Training Loss (NLML): -868.1547, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1785/2000, Training Loss (NLML): -868.1637, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1786/2000, Training Loss (NLML): -868.4076, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1787/2000, Training Loss (NLML): -868.6481, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1788/2000, Training Loss (NLML): -868.7999, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1789/2000, Training Loss (NLML): -868.8315, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1790/2000, Training Loss (NLML): -868.7266, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1791/2000, Training Loss (NLML): -868.6499, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1792/2000, Training Loss (NLML): -868.2833, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1793/2000, Training Loss (NLML): -868.4672, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1794/2000, Training Loss (NLML): -868.0248, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1795/2000, Training Loss (NLML): -868.6102, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1796/2000, Training Loss (NLML): -868.5942, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1797/2000, Training Loss (NLML): -869.0070, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1798/2000, Training Loss (NLML): -869.1643, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1799/2000, Training Loss (NLML): -869.3308, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1800/2000, Training Loss (NLML): -869.3966, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1801/2000, Training Loss (NLML): -869.4034, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1802/2000, Training Loss (NLML): -869.3690, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1803/2000, Training Loss (NLML): -869.3103, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1804/2000, Training Loss (NLML): -869.3232, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1805/2000, Training Loss (NLML): -869.2526, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1806/2000, Training Loss (NLML): -869.3722, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1807/2000, Training Loss (NLML): -869.2615, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1808/2000, Training Loss (NLML): -869.4641, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1809/2000, Training Loss (NLML): -869.3302, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1810/2000, Training Loss (NLML): -869.6165, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1811/2000, Training Loss (NLML): -869.5712, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1812/2000, Training Loss (NLML): -869.8392, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1813/2000, Training Loss (NLML): -869.8690, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1814/2000, Training Loss (NLML): -870.0341, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1815/2000, Training Loss (NLML): -870.0564, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1816/2000, Training Loss (NLML): -870.1595, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1817/2000, Training Loss (NLML): -870.1620, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1818/2000, Training Loss (NLML): -870.2422, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1819/2000, Training Loss (NLML): -870.2026, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1820/2000, Training Loss (NLML): -870.2518, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1821/2000, Training Loss (NLML): -870.0596, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1822/2000, Training Loss (NLML): -870.0631, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1823/2000, Training Loss (NLML): -869.5096, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1824/2000, Training Loss (NLML): -869.6188, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1825/2000, Training Loss (NLML): -869.0449, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1826/2000, Training Loss (NLML): -869.4117, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1827/2000, Training Loss (NLML): -869.9905, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1828/2000, Training Loss (NLML): -870.4414, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1829/2000, Training Loss (NLML): -870.8395, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1830/2000, Training Loss (NLML): -870.6913, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1831/2000, Training Loss (NLML): -870.6036, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1832/2000, Training Loss (NLML): -870.3763, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1833/2000, Training Loss (NLML): -870.7625, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1834/2000, Training Loss (NLML): -871.0939, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1835/2000, Training Loss (NLML): -871.2485, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1836/2000, Training Loss (NLML): -871.2252, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1837/2000, Training Loss (NLML): -870.9094, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1838/2000, Training Loss (NLML): -870.9397, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1839/2000, Training Loss (NLML): -870.4280, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1840/2000, Training Loss (NLML): -871.0574, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1841/2000, Training Loss (NLML): -871.0845, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1842/2000, Training Loss (NLML): -871.4977, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1843/2000, Training Loss (NLML): -871.6356, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1844/2000, Training Loss (NLML): -871.7930, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1845/2000, Training Loss (NLML): -871.8716, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1846/2000, Training Loss (NLML): -871.9423, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1847/2000, Training Loss (NLML): -871.9806, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1848/2000, Training Loss (NLML): -872.0177, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1849/2000, Training Loss (NLML): -871.9891, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1850/2000, Training Loss (NLML): -871.9578, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1851/2000, Training Loss (NLML): -871.6505, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1852/2000, Training Loss (NLML): -871.5000, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1853/2000, Training Loss (NLML): -870.3538, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1854/2000, Training Loss (NLML): -871.0149, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1855/2000, Training Loss (NLML): -870.7650, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1856/2000, Training Loss (NLML): -871.6560, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1857/2000, Training Loss (NLML): -872.3265, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1858/2000, Training Loss (NLML): -872.4679, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1859/2000, Training Loss (NLML): -872.2909, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1860/2000, Training Loss (NLML): -871.6888, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1861/2000, Training Loss (NLML): -872.2964, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1862/2000, Training Loss (NLML): -872.6454, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1863/2000, Training Loss (NLML): -872.8619, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1864/2000, Training Loss (NLML): -872.8044, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1865/2000, Training Loss (NLML): -872.5038, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1866/2000, Training Loss (NLML): -872.6250, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1867/2000, Training Loss (NLML): -872.3711, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1868/2000, Training Loss (NLML): -872.8326, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1869/2000, Training Loss (NLML): -872.7319, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1870/2000, Training Loss (NLML): -873.0416, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1871/2000, Training Loss (NLML): -873.0265, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1872/2000, Training Loss (NLML): -873.2867, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1873/2000, Training Loss (NLML): -873.3922, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1874/2000, Training Loss (NLML): -873.5249, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1875/2000, Training Loss (NLML): -873.5563, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1876/2000, Training Loss (NLML): -873.5192, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1877/2000, Training Loss (NLML): -873.2933, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1878/2000, Training Loss (NLML): -872.8119, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1879/2000, Training Loss (NLML): -871.8571, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1880/2000, Training Loss (NLML): -870.7123, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1881/2000, Training Loss (NLML): -871.0787, (RMSE): 0.0139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 10:04:31] Energy consumed for RAM : 0.000491 kWh. RAM Power : 23.53756284713745 W\n",
      "[codecarbon INFO @ 10:04:31] Energy consumed for all CPUs : 0.000887 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:04:31] Energy consumed for all GPUs : 0.001487 kWh. Total GPU Power : 71.61217843750906 W\n",
      "[codecarbon INFO @ 10:04:31] 0.002866 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curve dfNGP Run 1/1, Epoch 1882/2000, Training Loss (NLML): -872.8179, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1883/2000, Training Loss (NLML): -873.9209, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1884/2000, Training Loss (NLML): -873.5155, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1885/2000, Training Loss (NLML): -872.9844, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1886/2000, Training Loss (NLML): -873.0436, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1887/2000, Training Loss (NLML): -874.0287, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1888/2000, Training Loss (NLML): -874.0649, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1889/2000, Training Loss (NLML): -873.2820, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1890/2000, Training Loss (NLML): -873.7723, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1891/2000, Training Loss (NLML): -874.2463, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1892/2000, Training Loss (NLML): -874.5261, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1893/2000, Training Loss (NLML): -874.3760, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1894/2000, Training Loss (NLML): -873.9143, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1895/2000, Training Loss (NLML): -874.0902, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1896/2000, Training Loss (NLML): -873.7727, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 1897/2000, Training Loss (NLML): -874.3978, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1898/2000, Training Loss (NLML): -874.0778, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 1899/2000, Training Loss (NLML): -874.3749, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1900/2000, Training Loss (NLML): -873.4177, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 1901/2000, Training Loss (NLML): -874.5055, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1902/2000, Training Loss (NLML): -874.5931, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1903/2000, Training Loss (NLML): -874.9421, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1904/2000, Training Loss (NLML): -875.1182, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1905/2000, Training Loss (NLML): -875.1949, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1906/2000, Training Loss (NLML): -875.2733, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1907/2000, Training Loss (NLML): -875.0868, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1908/2000, Training Loss (NLML): -875.1464, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1909/2000, Training Loss (NLML): -874.8545, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1910/2000, Training Loss (NLML): -875.2861, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1911/2000, Training Loss (NLML): -875.4344, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1912/2000, Training Loss (NLML): -875.7158, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1913/2000, Training Loss (NLML): -875.8489, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1914/2000, Training Loss (NLML): -875.9204, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1915/2000, Training Loss (NLML): -875.9508, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1916/2000, Training Loss (NLML): -875.9912, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1917/2000, Training Loss (NLML): -876.0431, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1918/2000, Training Loss (NLML): -876.0950, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1919/2000, Training Loss (NLML): -876.1434, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1920/2000, Training Loss (NLML): -876.0686, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1921/2000, Training Loss (NLML): -875.9658, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1922/2000, Training Loss (NLML): -875.1233, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 1923/2000, Training Loss (NLML): -875.4824, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1924/2000, Training Loss (NLML): -874.1631, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 1925/2000, Training Loss (NLML): -876.0154, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1926/2000, Training Loss (NLML): -876.2086, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1927/2000, Training Loss (NLML): -875.4503, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1928/2000, Training Loss (NLML): -876.1329, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1929/2000, Training Loss (NLML): -876.2010, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1930/2000, Training Loss (NLML): -876.7888, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1931/2000, Training Loss (NLML): -876.8285, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1932/2000, Training Loss (NLML): -876.5031, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1933/2000, Training Loss (NLML): -876.6633, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1934/2000, Training Loss (NLML): -876.5337, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1935/2000, Training Loss (NLML): -876.8054, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1936/2000, Training Loss (NLML): -876.6948, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1937/2000, Training Loss (NLML): -876.3484, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1938/2000, Training Loss (NLML): -876.4426, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1939/2000, Training Loss (NLML): -875.8875, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1940/2000, Training Loss (NLML): -876.1644, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1941/2000, Training Loss (NLML): -875.7507, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1942/2000, Training Loss (NLML): -875.5869, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 1943/2000, Training Loss (NLML): -876.4844, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1944/2000, Training Loss (NLML): -877.2869, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1945/2000, Training Loss (NLML): -877.8270, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1946/2000, Training Loss (NLML): -877.8082, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1947/2000, Training Loss (NLML): -877.3989, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1948/2000, Training Loss (NLML): -877.2460, (RMSE): 0.0137\n",
      "curve dfNGP Run 1/1, Epoch 1949/2000, Training Loss (NLML): -877.2313, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1950/2000, Training Loss (NLML): -877.5986, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1951/2000, Training Loss (NLML): -877.9413, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1952/2000, Training Loss (NLML): -878.0549, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1953/2000, Training Loss (NLML): -878.1622, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1954/2000, Training Loss (NLML): -878.1018, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1955/2000, Training Loss (NLML): -878.2097, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1956/2000, Training Loss (NLML): -877.9961, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 1957/2000, Training Loss (NLML): -878.0533, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1958/2000, Training Loss (NLML): -877.2688, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 1959/2000, Training Loss (NLML): -877.7904, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1960/2000, Training Loss (NLML): -877.1549, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 1961/2000, Training Loss (NLML): -878.0774, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1962/2000, Training Loss (NLML): -878.3157, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1963/2000, Training Loss (NLML): -878.5530, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1964/2000, Training Loss (NLML): -878.6772, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1965/2000, Training Loss (NLML): -878.4457, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1966/2000, Training Loss (NLML): -878.6693, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1967/2000, Training Loss (NLML): -878.5001, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1968/2000, Training Loss (NLML): -879.0114, (RMSE): 0.0138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 10:04:35] Energy consumed for RAM : 0.001276 kWh. RAM Power : 23.53756284713745 W\n",
      "[codecarbon INFO @ 10:04:35] Energy consumed for all CPUs : 0.002304 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:04:35] Energy consumed for all GPUs : 0.003703 kWh. Total GPU Power : 71.98451184154594 W\n",
      "[codecarbon INFO @ 10:04:35] 0.007283 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curve dfNGP Run 1/1, Epoch 1969/2000, Training Loss (NLML): -879.2042, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1970/2000, Training Loss (NLML): -879.3766, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1971/2000, Training Loss (NLML): -879.4149, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1972/2000, Training Loss (NLML): -879.3976, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1973/2000, Training Loss (NLML): -879.3198, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1974/2000, Training Loss (NLML): -879.0237, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1975/2000, Training Loss (NLML): -878.7893, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 1976/2000, Training Loss (NLML): -877.3785, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 1977/2000, Training Loss (NLML): -877.9762, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 1978/2000, Training Loss (NLML): -877.0732, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 1979/2000, Training Loss (NLML): -878.1144, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 1980/2000, Training Loss (NLML): -879.2379, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1981/2000, Training Loss (NLML): -879.5784, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1982/2000, Training Loss (NLML): -879.7601, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1983/2000, Training Loss (NLML): -879.0967, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1984/2000, Training Loss (NLML): -879.5242, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1985/2000, Training Loss (NLML): -879.7701, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1986/2000, Training Loss (NLML): -879.9771, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1987/2000, Training Loss (NLML): -880.1954, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1988/2000, Training Loss (NLML): -879.9036, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1989/2000, Training Loss (NLML): -880.0780, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1990/2000, Training Loss (NLML): -879.7936, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1991/2000, Training Loss (NLML): -880.1394, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1992/2000, Training Loss (NLML): -880.2000, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1993/2000, Training Loss (NLML): -880.3232, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 1994/2000, Training Loss (NLML): -880.4486, (RMSE): 0.0138\n",
      "curve dfNGP Run 1/1, Epoch 1995/2000, Training Loss (NLML): -880.3672, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 1996/2000, Training Loss (NLML): -880.5045, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 1997/2000, Training Loss (NLML): -880.1333, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 1998/2000, Training Loss (NLML): -880.4591, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 1999/2000, Training Loss (NLML): -879.8403, (RMSE): 0.0139\n",
      "curve dfNGP Run 1/1, Epoch 2000/2000, Training Loss (NLML): -880.7040, (RMSE): 0.0139\n"
     ]
    }
   ],
   "source": [
    "# SIMULATED DATA EXPERIMENTS\n",
    "# RUN WITH python run_sim_experiments_dfNGP.py\n",
    "# \n",
    "#       ooooooooooooooooooooooooooooooooooooo\n",
    "#      8                                .d88\n",
    "#      8  oooooooooooooooooooooooooooood8888\n",
    "#      8  8888888888888888888888888P\"   8888    oooooooooooooooo\n",
    "#      8  8888888888888888888888P\"      8888    8              8\n",
    "#      8  8888888888888888888P\"         8888    8             d8\n",
    "#      8  8888888888888888P\"            8888    8            d88\n",
    "#      8  8888888888888P\"               8888    8           d888\n",
    "#      8  8888888888P\"                  8888    8          d8888\n",
    "#      8  8888888P\"                     8888    8         d88888\n",
    "#      8  8888P\"                        8888    8        d888888\n",
    "#      8  8888oooooooooooooooooooooocgmm8888    8       d8888888\n",
    "#      8 .od88888888888888888888888888888888    8      d88888888\n",
    "#      8888888888888888888888888888888888888    8     d888888888\n",
    "#                                               8    d8888888888\n",
    "#         ooooooooooooooooooooooooooooooo       8   d88888888888\n",
    "#        d                       ...oood8b      8  d888888888888\n",
    "#       d              ...oood888888888888b     8 d8888888888888\n",
    "#      d     ...oood88888888888888888888888b    8d88888888888888\n",
    "#     dood8888888888888888888888888888888888b\n",
    "#\n",
    "#\n",
    "# This artwork is a visual reminder that this script is for the sim experiments.\n",
    "\n",
    "model_name = \"dfNGP\"\n",
    "\n",
    "# import configs to we can access the hypers with getattr\n",
    "import configs\n",
    "from configs import PATIENCE, MAX_NUM_EPOCHS, NUM_RUNS, WEIGHT_DECAY\n",
    "# also import x_test grid size and std noise for training data\n",
    "from configs import N_SIDE, STD_GAUSSIAN_NOISE\n",
    "\n",
    "# Reiterating import for visibility\n",
    "MAX_NUM_EPOCHS = MAX_NUM_EPOCHS\n",
    "NUM_RUNS = NUM_RUNS\n",
    "NUM_RUNS = 1\n",
    "WEIGHT_DECAY = WEIGHT_DECAY\n",
    "PATIENCE = PATIENCE\n",
    "\n",
    "# assign model-specific variable\n",
    "MODEL_LEARNING_RATE = getattr(configs, f\"{model_name}_SIM_LEARNING_RATE\")\n",
    "MODEL_SIM_RESULTS_DIR = getattr(configs, f\"{model_name}_SIM_RESULTS_DIR\")\n",
    "import os\n",
    "os.makedirs(MODEL_SIM_RESULTS_DIR, exist_ok = True)\n",
    "\n",
    "# imports for probabilistic models\n",
    "if model_name in [\"GP\", \"dfGP\", \"dfNGP\"]:\n",
    "    from GP_models import GP_predict\n",
    "    from metrics import compute_NLL_sparse, compute_NLL_full\n",
    "    from configs import L_RANGE, SIGMA_N_RANGE, GP_PATIENCE\n",
    "    # overwrite with GP_PATIENCE\n",
    "    PATIENCE = GP_PATIENCE\n",
    "    if model_name in [\"dfGP\", \"dfNGP\"]:\n",
    "        from configs import SIGMA_F_RANGE\n",
    "\n",
    "# for all models with NN components train on batches\n",
    "if model_name in [\"dfNGP\", \"dfNN\", \"PINN\"]:\n",
    "    from configs import BATCH_SIZE\n",
    "\n",
    "if model_name in [\"dfNGP\", \"dfNN\"]:\n",
    "    from NN_models import dfNN\n",
    "\n",
    "# universals \n",
    "from metrics import compute_RMSE, compute_MAE, compute_divergence_field\n",
    "\n",
    "# basics\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from codecarbon import EmissionsTracker\n",
    "\n",
    "# utilitarian\n",
    "from utils import set_seed, make_grid\n",
    "# reproducibility\n",
    "set_seed(42)\n",
    "import gc\n",
    "\n",
    "# setting device to GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# overwrite if needed: # device = 'cpu'\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "### START TIMING ###\n",
    "import time\n",
    "start_time = time.time()  # Start timing after imports\n",
    "\n",
    "### START TRACKING EXPERIMENT EMISSIONS ###\n",
    "tracker = EmissionsTracker(project_name = \"dfNGP_simulation_experiments\", output_dir = MODEL_SIM_RESULTS_DIR)\n",
    "tracker.start()\n",
    "\n",
    "### SIMULATION ###\n",
    "# Import all simulation functions\n",
    "from simulate import (\n",
    "    simulate_detailed_branching,\n",
    "    # simulate_detailed_convergence,\n",
    "    simulate_detailed_curve,\n",
    "    simulate_detailed_deflection,\n",
    "    simulate_detailed_edge,\n",
    "    simulate_detailed_ridges,\n",
    ")\n",
    "\n",
    "# Define simulations as a dictionary with names as keys to function objects\n",
    "# alphabectic order here\n",
    "simulations = {\n",
    "    \"curve\": simulate_detailed_curve,\n",
    "}\n",
    "\n",
    "########################\n",
    "### x_train & x_test ###\n",
    "########################\n",
    "\n",
    "# Load training inputs (once for all simulations)\n",
    "x_train = torch.load(\"data/sim_data/x_train_lines_discretised_0to1.pt\", weights_only = False).float()\n",
    "\n",
    "# Generate x_test (long) once for all simulations\n",
    "_, x_test = make_grid(N_SIDE)\n",
    "# x_test is long format (N_SIDE ** 2, 2)\n",
    "\n",
    "#################################\n",
    "### LOOP 1 - over SIMULATIONS ###\n",
    "#################################\n",
    "\n",
    "# Make y_train_dict: Iterate over all simulation functions\n",
    "for sim_name, sim_func in simulations.items():\n",
    "\n",
    "    ########################\n",
    "    ### y_train & y_test ###\n",
    "    ########################\n",
    "\n",
    "    # Generate training observations\n",
    "    # NOTE: sim_func() needs to be on CPU, so we move x_train to CPU\n",
    "    y_train = sim_func(x_train.cpu()).to(device)\n",
    "    y_test = sim_func(x_test.cpu()).to(device)\n",
    "    \n",
    "    # x_test = x_test.to(device).requires_grad_(True)\n",
    "    x_test = x_test.to(device)\n",
    "    # x_train = x_train.to(device).requires_grad_(True)\n",
    "    x_train = x_train.to(device)\n",
    "\n",
    "    # Print details\n",
    "    print(f\"=== {sim_name.upper()} ===\")\n",
    "    print(f\"Training inputs shape: {x_train.shape}\")\n",
    "    print(f\"Training observations shape: {y_train.shape}\")\n",
    "    print(f\"Training inputs dtype: {x_train.dtype}\")\n",
    "    print(f\"Training inputs device: {y_train.device}\")\n",
    "    print(f\"Training observations device: {y_train.device}\")\n",
    "    print()\n",
    "\n",
    "    # Print details\n",
    "    print(f\"=== {sim_name.upper()} ===\")\n",
    "    print(f\"Test inputs shape: {x_test.shape}\")\n",
    "    print(f\"Test observations shape: {y_test.shape}\")\n",
    "    print(f\"Test inputs dtype: {x_test.dtype}\")\n",
    "    print(f\"Test inputs device: {x_test.device}\")\n",
    "    print(f\"Test observations device: {y_test.device}\")\n",
    "    print()\n",
    "\n",
    "    # NOTE: This is different to the real data experiments\n",
    "    # calculate the mean magnitude of the test data as we use this to scale the noise\n",
    "    sim_mean_magnitude_for_noise = torch.norm(y_test, dim = -1).mean().to(device)\n",
    "    sim_noise = STD_GAUSSIAN_NOISE * sim_mean_magnitude_for_noise\n",
    "\n",
    "    # Store metrics for the simulation (used for *metrics_summary* report and *metrics_per_run*)\n",
    "    simulation_results = [] \n",
    "\n",
    "    ##################################\n",
    "    ### LOOP 2 - over training run ###\n",
    "    ##################################\n",
    "    \n",
    "    # NOTE: GPs and hense dfNGPs don't train on batches, use full data\n",
    "\n",
    "    for run in range(NUM_RUNS):\n",
    "\n",
    "        print(f\"\\n--- Training Run {run + 1}/{NUM_RUNS} ---\")\n",
    "\n",
    "        # initialise trainable dfGP params\n",
    "        sigma_n = nn.Parameter(torch.empty(1, device = device).uniform_( * SIGMA_N_RANGE))\n",
    "        sigma_f = nn.Parameter(torch.empty(1, device = device).uniform_( * SIGMA_F_RANGE))\n",
    "        l = nn.Parameter(torch.empty(2, device = device).uniform_( * L_RANGE))\n",
    "\n",
    "        # For every run initialise a (new) mean model\n",
    "        dfNN_mean_model = dfNN().to(device)\n",
    "\n",
    "        # NOTE: We don't need a criterion either\n",
    "\n",
    "        # AdamW as optimizer for some regularisation/weight decay\n",
    "        # HACK: create two param groups: one for the dfNN and one for the hypers\n",
    "        optimizer = optim.AdamW([\n",
    "            {\"params\": dfNN_mean_model.parameters(), \"weight_decay\": WEIGHT_DECAY, \"lr\": (0.1 * MODEL_LEARNING_RATE)},\n",
    "            {\"params\": [sigma_n, sigma_f, l], \"weight_decay\": WEIGHT_DECAY, \"lr\": MODEL_LEARNING_RATE},\n",
    "            ])\n",
    "\n",
    "        # _________________\n",
    "        # BEFORE EPOCH LOOP\n",
    "        \n",
    "        # Export the convergence just for first run only\n",
    "        if run == 0:\n",
    "            # initialise tensors to store losses over epochs (for convergence plot)\n",
    "            train_losses_NLML_over_epochs = torch.zeros(MAX_NUM_EPOCHS) # objective\n",
    "            train_losses_RMSE_over_epochs = torch.zeros(MAX_NUM_EPOCHS) # by-product\n",
    "            # monitor performance transfer to test (only RMSE easy to calc without covar)\n",
    "            test_losses_RMSE_over_epochs = torch.zeros(MAX_NUM_EPOCHS)\n",
    "\n",
    "            sigma_n_over_epochs = torch.zeros(MAX_NUM_EPOCHS)\n",
    "            sigma_f_over_epochs = torch.zeros(MAX_NUM_EPOCHS)\n",
    "            l1_over_epochs = torch.zeros(MAX_NUM_EPOCHS)\n",
    "            l2_over_epochs = torch.zeros(MAX_NUM_EPOCHS)\n",
    "\n",
    "        # Early stopping variables\n",
    "        best_loss = float('inf')\n",
    "        # counter starts at 0\n",
    "        epochs_no_improve = 0\n",
    "\n",
    "        # NOTE: This is a difference to the real data experiments\n",
    "        # Additive noise model: independent Gaussian noise\n",
    "        # For every run we have a FIXED NOISY TARGET. Draw from standard normal with appropriate std\n",
    "        y_train_noisy = y_train + (torch.randn(y_train.shape, device = device) * sim_noise)\n",
    "\n",
    "        ############################\n",
    "        ### LOOP 3 - over EPOCHS ###\n",
    "        ############################\n",
    "        \n",
    "        print(\"\\nStart Training\")\n",
    "\n",
    "        for epoch in range(MAX_NUM_EPOCHS):\n",
    "\n",
    "            # Assure model is in training mode\n",
    "            dfNN_mean_model.train()\n",
    "\n",
    "            # For Run 1 we save a bunch of metrics and update, while for the rest we only update\n",
    "            if run == 0:\n",
    "                mean_pred_train, _, lml_train = GP_predict(\n",
    "                        x_train,\n",
    "                        y_train_noisy,\n",
    "                        x_train, # predict training data\n",
    "                        [sigma_n, sigma_f, l], # list of (initial) hypers\n",
    "                        mean_func = dfNN_mean_model, # dfNN as mean function\n",
    "                        divergence_free_bool = True) # ensures we use a df kernel\n",
    "\n",
    "                # Compute test loss for loss convergence plot\n",
    "                mean_pred_test, _, _ = GP_predict(\n",
    "                        x_train,\n",
    "                        y_train_noisy,\n",
    "                        x_test.to(device), # have predictions for training data again\n",
    "                        # HACK: This is rather an eval, so we use detached hypers to avoid the computational tree\n",
    "                        [sigma_n.detach().clone(), sigma_f.detach().clone(), l.detach().clone()], # list of (initial) hypers\n",
    "                        mean_func = dfNN_mean_model, # dfNN as mean function\n",
    "                        divergence_free_bool = True) # ensures we use a df kernel\n",
    "                \n",
    "                # UPDATE HYPERS (after test loss is computed to use same model)\n",
    "                optimizer.zero_grad() # don't accumulate gradients\n",
    "                # negative for NLML. loss is always on train\n",
    "                loss = - lml_train\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # NOTE: it is important to detach here \n",
    "                train_RMSE = compute_RMSE(y_train.detach(), mean_pred_train.detach())\n",
    "                test_RMSE = compute_RMSE(y_test.detach(), mean_pred_test.detach())\n",
    "\n",
    "                # Save losses for convergence plot\n",
    "                train_losses_NLML_over_epochs[epoch] = - lml_train\n",
    "                train_losses_RMSE_over_epochs[epoch] = train_RMSE\n",
    "                # NOTE: lml is always just given training data. There is no TEST NLML\n",
    "                test_losses_RMSE_over_epochs[epoch] = test_RMSE\n",
    "\n",
    "                # Save evolution of hyprs for convergence plot\n",
    "                sigma_n_over_epochs[epoch] = sigma_n[0]\n",
    "                sigma_f_over_epochs[epoch] = sigma_f[0]\n",
    "                l1_over_epochs[epoch] = l[0]\n",
    "                l2_over_epochs[epoch] = l[1]\n",
    "\n",
    "                print(f\"{sim_name} {model_name} Run {run + 1}/{NUM_RUNS}, Epoch {epoch + 1}/{MAX_NUM_EPOCHS}, Training Loss (NLML): {loss:.4f}, (RMSE): {train_RMSE:.4f}\")\n",
    "\n",
    "                # delete after printing and saving\n",
    "                # NOTE: keep loss for early stopping check\n",
    "                del mean_pred_train, mean_pred_test, lml_train, train_RMSE, test_RMSE\n",
    "                \n",
    "                # Free up memory every 20 epochs\n",
    "                if epoch % 20 == 0:\n",
    "                    gc.collect() and torch.cuda.empty_cache()\n",
    "            \n",
    "             # For all runs after the first we run a minimal version using only lml_train\n",
    "            else:\n",
    "\n",
    "                # NOTE: We can use x_train[0:2] since the predictions doesn;t matter and we only care about lml_train\n",
    "                _, _, lml_train = GP_predict(\n",
    "                        x_train,\n",
    "                        y_train_noisy,\n",
    "                        x_train[0:2], # predictions don't matter and we output lml_train already\n",
    "                        [sigma_n, sigma_f, l], # list of (initial) hypers\n",
    "                        mean_func = dfNN_mean_model, # dfNN as mean function\n",
    "                        divergence_free_bool = True) # ensures we use a df kernel\n",
    "                \n",
    "                # UPDATE HYPERS (after test loss is computed to use same model)\n",
    "                optimizer.zero_grad() # don't accumulate gradients\n",
    "                # negative for NLML\n",
    "                loss = - lml_train\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # After run 1 we only print lml, nothing else\n",
    "                print(f\"{sim_name} {model_name} Run {run + 1}/{NUM_RUNS}, Epoch {epoch + 1}/{MAX_NUM_EPOCHS}, Training Loss (NLML): {loss:.4f}\")\n",
    "\n",
    "                # NOTE: keep loss for early stopping check, del lml_train\n",
    "                del lml_train\n",
    "                \n",
    "                # Free up memory every 20 epochs\n",
    "                if epoch % 20 == 0:\n",
    "                    gc.collect() and torch.cuda.empty_cache()\n",
    "\n",
    "            # EVERY EPOCH: Early stopping check\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                # reset counter if loss improves\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "\n",
    "            if epochs_no_improve >= PATIENCE:\n",
    "                print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
    "                # exit epoch loop\n",
    "                break\n",
    "\n",
    "        ##############################\n",
    "        ### END LOOP 3 over EPOCHS ###\n",
    "        ##############################\n",
    "\n",
    "        # for every run...\n",
    "        #######################################################\n",
    "        ### EVALUATE after all training for RUN is finished ###\n",
    "        #######################################################\n",
    "\n",
    "        # Evaluate the trained model after all epochs are finished or early stopping was triggered\n",
    "        # NOTE: Detach tuned hyperparameters from the computational graph\n",
    "        best_sigma_n = sigma_n.detach().clone()\n",
    "        best_sigma_f = sigma_f.detach().clone()\n",
    "        best_l = l.detach().clone()\n",
    "\n",
    "        # Need gradients for autograd divergence: We clone and detach\n",
    "        x_test_grad = x_test.to(device).clone().requires_grad_(True)\n",
    "\n",
    "        mean_pred_test, covar_pred_test, _ = GP_predict(\n",
    "            x_train,\n",
    "            y_train, # NOTE: use original y_train, not noisy\n",
    "            x_test_grad,\n",
    "            [best_sigma_n, best_sigma_f, best_l], # list of (initial) hypers\n",
    "            mean_func = dfNN_mean_model, # dfNN as mean function\n",
    "            divergence_free_bool = True) # ensures we use a df kernel\n",
    "        \n",
    "        # Compute divergence field\n",
    "        dfNGP_test_div_field = compute_divergence_field(mean_pred_test, x_test_grad)\n",
    "\n",
    "        # Only save mean_pred, covar_pred and divergence fields for the first run\n",
    "        if run == 0:\n",
    "\n",
    "            # (1) Save predictions from first run so we can visualise them later\n",
    "            torch.save(mean_pred_test, f\"{MODEL_SIM_RESULTS_DIR}/{sim_name}_{model_name}_test_mean_predictions.pt\")\n",
    "            torch.save(covar_pred_test, f\"{MODEL_SIM_RESULTS_DIR}/{sim_name}_{model_name}_test_covar_predictions.pt\")\n",
    "\n",
    "            # (2) Save best hyperparameters\n",
    "            # Stack tensors into a single tensor\n",
    "            best_hypers_tensor = torch.cat([\n",
    "                best_sigma_n.reshape(-1),  # Ensure 1D shape\n",
    "                best_sigma_f.reshape(-1),\n",
    "                best_l.reshape(-1),\n",
    "            ])\n",
    "\n",
    "            torch.save(best_hypers_tensor, f\"{MODEL_SIM_RESULTS_DIR}/{sim_name}_{model_name}_best_hypers.pt\")\n",
    "\n",
    "            # (3) Since all epoch training is finished, we can save the losses over epochs\n",
    "            df_losses = pd.DataFrame({\n",
    "                'Epoch': list(range(train_losses_NLML_over_epochs.shape[0])), # pythonic indexing\n",
    "                'Train Loss NLML': train_losses_NLML_over_epochs.tolist(),\n",
    "                'Train Loss RMSE': train_losses_RMSE_over_epochs.tolist(),\n",
    "                'Test Loss RMSE': test_losses_RMSE_over_epochs.tolist(),\n",
    "                'Sigma_n': sigma_n_over_epochs.tolist(),\n",
    "                'Sigma_f': sigma_f_over_epochs.tolist(),\n",
    "                'l1': l1_over_epochs.tolist(),\n",
    "                'l2': l2_over_epochs.tolist()\n",
    "                })\n",
    "            \n",
    "            df_losses.to_csv(f\"{MODEL_SIM_RESULTS_DIR}/{sim_name}_{model_name}_losses_over_epochs.csv\", index = False, float_format = \"%.5f\") # reduce to 5 decimals for readability\n",
    "\n",
    "            # (4) Save divergence field (computed above for all runs)\n",
    "            torch.save(dfNGP_test_div_field, f\"{MODEL_SIM_RESULTS_DIR}/{sim_name}_{model_name}_test_prediction_divergence_field.pt\")\n",
    "\n",
    "        x_train_grad = x_train.to(device).clone().requires_grad_(True)\n",
    "\n",
    "        mean_pred_train, covar_pred_train, _ = GP_predict(\n",
    "                     x_train,\n",
    "                     y_train, # NOTE: use original y_train, not noisy\n",
    "                     x_train_grad,\n",
    "                     [best_sigma_n, best_sigma_f, best_l], # list of (initial) hypers\n",
    "                     mean_func = dfNN_mean_model, # dfNN as mean function\n",
    "                     divergence_free_bool = True) # ensures we use a df kernel\n",
    "        \n",
    "        dfNGP_train_div_field = compute_divergence_field(mean_pred_train, x_train_grad)\n",
    "\n",
    "        # Divergence: Convert field to metric: mean absolute divergence\n",
    "        # NOTE: It is important to use the absolute value of the divergence field, since positive and negative deviations are violations and shouldn't cancel each other out \n",
    "        dfNGP_train_div = dfNGP_train_div_field.abs().mean().item()\n",
    "        dfNGP_test_div = dfNGP_test_div_field.abs().mean().item()\n",
    "\n",
    "        # Compute metrics (convert tensors to float) for every run's tuned model\n",
    "        dfNGP_train_RMSE = compute_RMSE(y_train, mean_pred_train).item()\n",
    "        dfNGP_train_MAE = compute_MAE(y_train, mean_pred_train).item()\n",
    "        dfNGP_train_NLL = compute_NLL_sparse(y_train, mean_pred_train, covar_pred_train).item()\n",
    "        dfNGP_train_full_NLL = compute_NLL_full(y_train, mean_pred_train, covar_pred_train).item()\n",
    "\n",
    "        dfNGP_test_RMSE = compute_RMSE(y_test, mean_pred_test).item()\n",
    "        dfNGP_test_MAE = compute_MAE(y_test, mean_pred_test).item()\n",
    "        dfNGP_test_NLL = compute_NLL_sparse(y_test, mean_pred_test, covar_pred_test).item()\n",
    "        dfNGP_test_full_NLL = compute_NLL_full(y_test, mean_pred_test, covar_pred_test).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f866d0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test, mean_pred_test, covar_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec5faf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([400, 2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 10:07:50] Energy consumed for RAM : 0.002550 kWh. RAM Power : 23.53756284713745 W\n",
      "[codecarbon INFO @ 10:07:50] Energy consumed for all CPUs : 0.004605 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:07:50] Energy consumed for all GPUs : 0.005766 kWh. Total GPU Power : 31.16293848889743 W\n",
      "[codecarbon INFO @ 10:07:50] 0.012921 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "mean_pred_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3972f010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_NLL_sparse(y_true, y_mean_pred, y_covar_pred):\n",
    "    \"\"\" Computes a sparse version of the Negative Log-Likelihood (NLL) for a 2D Gaussian distribution. This sparse version neglects cross-covariance terms and is more efficient for large datasets.\n",
    "    \n",
    "    NLL: The NLL quantifies how well the predicted Gaussian distribution fits the observed data.\n",
    "    Sparse format: each of the N points has its own 22 covariance matrix. (This is more than just the diagonal of the covariance matrix, but not the full covar.)\n",
    "\n",
    "    Args:\n",
    "        y_true (torch.Tensor): True observations of shape (N, 2).\n",
    "        y_mean_pred (torch.Tensor): Mean predictions of shape (N, 2).\n",
    "        y_covar_pred (torch.Tensor): Full predicted covariance matrix of shape (N * 2, N * 2).(BLOCK FORMAT) [u1, u2, u3, ..., v1, v2, v3, ...]\n",
    "            If N = 400, then y_covar_pred is torch.Size([800, 800]) so 640000 elements N x 2 x 2 = only 1600 elements.\n",
    "        jitter (float, optional): Small value added to the diagonal for numerical stability. Defaults to 0.5 * 1e-2 - quite high but we need to keep it consistent across all models.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor(): Negative Log-Likelihood (NLL) scalar.\n",
    "    \"\"\"\n",
    "    # Extract number of points\n",
    "    N = y_true.shape[0]\n",
    "\n",
    "    # Step 1: Sparsify the covariance matrix\n",
    "    # Change format of y_covar_pred from (N x 2, N x 2) to (N, 2, 2) so N (2, 2) matrices.\n",
    "    # NOTE: This is a sparse version of the covariance matrix, neglecting cross-covariance terms.\n",
    "\n",
    "    # extract diagonal of upper left quadrant: variance of the first output (y1) at each point.\n",
    "    var_y1_y1 = torch.diag(y_covar_pred[:N, :N])\n",
    "    # extract diagonal of ulower right quadrant: variance of the second output (y2) at each point\n",
    "    var_y2_y2 = torch.diag(y_covar_pred[N:, N:])\n",
    "\n",
    "    # extract diagonal of upper right quadrant: How much do y1 and y2 covary at this point\n",
    "    covar_y1_y2 = torch.diag(y_covar_pred[:N, N:])\n",
    "    # extract diagonal of lower left quadrant\n",
    "    covar_y2_y1 = torch.diag(y_covar_pred[N:, :N])\n",
    "\n",
    "    col1 = torch.cat([var_y1_y1.unsqueeze(-1), covar_y1_y2.unsqueeze(-1)], dim = -1)\n",
    "    col2 = torch.cat([covar_y2_y1.unsqueeze(-1), var_y2_y2.unsqueeze(-1)], dim = -1)\n",
    "\n",
    "    # At each point N, what is the predicted variance of y1 and y2 and \n",
    "    # what is the predicted covariance between y1 and y2? (symmetric)\n",
    "    covar_N22 = torch.cat([col1.unsqueeze(-1), col2.unsqueeze(-1)], dim = -1) # shape: torch.Size([N, 2, 2])\n",
    "\n",
    "\n",
    "    # STEP 2: Compute Mahalanobis distance efficiently\n",
    "    # Compute the difference between the true and predicted values (y - )\n",
    "    # NOTE: order is (true - pred) to match the Mahalanobis distance formula\n",
    "    # NOTE: we can also keep this shape\n",
    "    diff = y_true - y_mean_pred   # Shape: (N, 2)\n",
    "    \n",
    "    # Reshape diff to (N, 2, 1) to do matrix multiplication with (N, 2, 2)\n",
    "    diff = diff.unsqueeze(-1)  # shape: (N, 2, 1)\n",
    "\n",
    "    sigma_inverse = torch.inverse(covar_N22) # shape: torch.Size([N, 2, 2])\n",
    "\n",
    "    # Compute ( @ diff)  shape: (N, 2, 1)\n",
    "    maha_component = torch.matmul(sigma_inverse, diff)\n",
    "\n",
    "    # Compute (diff^T @  @ diff) for each point  shape: (N, 1, 1)\n",
    "    # transpose diff to (N, 1, 2) for matrix multiplication\n",
    "    mahalanobis_distances = torch.matmul(diff.transpose(1, 2), maha_component)\n",
    "\n",
    "    # Sum (N, ) distances to get a single value\n",
    "    mahalanobis_distances = mahalanobis_distances.squeeze().sum()\n",
    "\n",
    "    # STEP 3: Log determinant of the covariance matrix\n",
    "\n",
    "    # element-wise determinant of all 2x2 matrices: sum\n",
    "    sign, log_absdet = torch.slogdet(covar_N22)\n",
    "    if not torch.all(sign > 0):\n",
    "        print(\"Warning: Non-positive definite matrix encountered.\")\n",
    "        return torch.tensor(float(\"inf\"), device = covar_N22.device)\n",
    "    log_det_Sigma = log_absdet.sum()\n",
    "\n",
    "\n",
    "    # STEP 4: Compute normalisation term\n",
    "    d = N * 2  # Dimensionality (since we have two outputs per point)\n",
    "    normalisation_term = d * torch.log(torch.tensor(2 * torch.pi, device = y_true.device))\n",
    "\n",
    "    # Step 5: Combine 3 scalars into negative log-likelihood (NLL)\n",
    "    # Gaussian log-likelihood formula: 2D\n",
    "    # NOTE: Gaussian log-likelihood 2D formula\n",
    "    log_likelihood =  - 0.5 * (mahalanobis_distances + log_det_Sigma + normalisation_term)\n",
    "\n",
    "    # return the negative log-likelihood\n",
    "    # return - log_likelihood\n",
    "    return covar_N22\n",
    "\n",
    "func_N22 = compute_NLL_sparse(y_test, mean_pred_test, covar_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836d37b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa0029c4eb0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 10:05:46] Energy consumed for RAM : 0.000981 kWh. RAM Power : 23.53756284713745 W\n",
      "[codecarbon INFO @ 10:05:46] Energy consumed for all CPUs : 0.001772 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:05:46] Energy consumed for all GPUs : 0.002582 kWh. Total GPU Power : 35.5730267114239 W\n",
      "[codecarbon INFO @ 10:05:46] 0.005336 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:05:50] Energy consumed for RAM : 0.001766 kWh. RAM Power : 23.53756284713745 W\n",
      "[codecarbon INFO @ 10:05:50] Energy consumed for all CPUs : 0.003189 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:05:50] Energy consumed for all GPUs : 0.004792 kWh. Total GPU Power : 44.85939443707121 W\n",
      "[codecarbon INFO @ 10:05:50] 0.009747 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:06:01] Energy consumed for RAM : 0.001079 kWh. RAM Power : 23.53756284713745 W\n",
      "[codecarbon INFO @ 10:06:01] Energy consumed for all CPUs : 0.001949 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:06:01] Energy consumed for all GPUs : 0.002834 kWh. Total GPU Power : 60.549405691925074 W\n",
      "[codecarbon INFO @ 10:06:01] 0.005863 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:06:05] Energy consumed for RAM : 0.001864 kWh. RAM Power : 23.53756284713745 W\n",
      "[codecarbon INFO @ 10:06:05] Energy consumed for all CPUs : 0.003366 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:06:05] Energy consumed for all GPUs : 0.005006 kWh. Total GPU Power : 51.34915370168606 W\n",
      "[codecarbon INFO @ 10:06:05] 0.010236 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:06:16] Energy consumed for RAM : 0.001177 kWh. RAM Power : 23.53756284713745 W\n",
      "[codecarbon INFO @ 10:06:16] Energy consumed for all CPUs : 0.002126 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:06:16] Energy consumed for all GPUs : 0.002941 kWh. Total GPU Power : 25.604185303174923 W\n",
      "[codecarbon INFO @ 10:06:16] 0.006245 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:06:20] Energy consumed for RAM : 0.001962 kWh. RAM Power : 23.53756284713745 W\n",
      "[codecarbon INFO @ 10:06:20] Energy consumed for all CPUs : 0.003543 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:06:20] Energy consumed for all GPUs : 0.005112 kWh. Total GPU Power : 25.396129299305155 W\n",
      "[codecarbon INFO @ 10:06:20] 0.010616 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:06:31] Energy consumed for RAM : 0.001275 kWh. RAM Power : 23.53756284713745 W\n",
      "[codecarbon INFO @ 10:06:31] Energy consumed for all CPUs : 0.002303 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:06:31] Energy consumed for all GPUs : 0.003047 kWh. Total GPU Power : 25.4711958117064 W\n",
      "[codecarbon INFO @ 10:06:31] 0.006626 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:06:35] Energy consumed for RAM : 0.002060 kWh. RAM Power : 23.53756284713745 W\n",
      "[codecarbon INFO @ 10:06:35] Energy consumed for all CPUs : 0.003720 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:06:35] Energy consumed for all GPUs : 0.005218 kWh. Total GPU Power : 25.473183501174457 W\n",
      "[codecarbon INFO @ 10:06:35] 0.010998 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:06:46] Energy consumed for RAM : 0.001373 kWh. RAM Power : 23.53756284713745 W\n",
      "[codecarbon INFO @ 10:06:46] Energy consumed for all CPUs : 0.002480 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:06:46] Energy consumed for all GPUs : 0.003153 kWh. Total GPU Power : 25.34985428015535 W\n",
      "[codecarbon INFO @ 10:06:46] 0.007007 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:06:50] Energy consumed for RAM : 0.002158 kWh. RAM Power : 23.53756284713745 W\n",
      "[codecarbon INFO @ 10:06:50] Energy consumed for all CPUs : 0.003897 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:06:50] Energy consumed for all GPUs : 0.005323 kWh. Total GPU Power : 25.294624582589968 W\n",
      "[codecarbon INFO @ 10:06:50] 0.011378 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:07:01] Energy consumed for RAM : 0.001471 kWh. RAM Power : 23.53756284713745 W\n",
      "[codecarbon INFO @ 10:07:01] Energy consumed for all CPUs : 0.002657 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:07:01] Energy consumed for all GPUs : 0.003259 kWh. Total GPU Power : 25.42266641620302 W\n",
      "[codecarbon INFO @ 10:07:01] 0.007388 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:07:05] Energy consumed for RAM : 0.002256 kWh. RAM Power : 23.53756284713745 W\n",
      "[codecarbon INFO @ 10:07:05] Energy consumed for all CPUs : 0.004074 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:07:05] Energy consumed for all GPUs : 0.005429 kWh. Total GPU Power : 25.35997850974733 W\n",
      "[codecarbon INFO @ 10:07:05] 0.011759 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:07:16] Energy consumed for RAM : 0.001569 kWh. RAM Power : 23.53756284713745 W\n",
      "[codecarbon INFO @ 10:07:16] Energy consumed for all CPUs : 0.002834 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:07:16] Energy consumed for all GPUs : 0.003362 kWh. Total GPU Power : 24.855748647235554 W\n",
      "[codecarbon INFO @ 10:07:16] 0.007766 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:07:16] 0.015451 g.CO2eq/s mean an estimation of 487.2578528873868 kg.CO2eq/year\n",
      "[codecarbon INFO @ 10:07:20] Energy consumed for RAM : 0.002354 kWh. RAM Power : 23.53756284713745 W\n",
      "[codecarbon INFO @ 10:07:20] Energy consumed for all CPUs : 0.004251 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:07:20] Energy consumed for all GPUs : 0.005532 kWh. Total GPU Power : 24.80858677099064 W\n",
      "[codecarbon INFO @ 10:07:20] 0.012137 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:07:20] 0.015272 g.CO2eq/s mean an estimation of 481.62307203351327 kg.CO2eq/year\n",
      "[codecarbon INFO @ 10:07:31] Energy consumed for RAM : 0.001668 kWh. RAM Power : 23.53756284713745 W\n",
      "[codecarbon INFO @ 10:07:31] Energy consumed for all CPUs : 0.003011 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:07:31] Energy consumed for all GPUs : 0.003466 kWh. Total GPU Power : 24.847345786879092 W\n",
      "[codecarbon INFO @ 10:07:31] 0.008145 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(covar_pred_test.cpu().detach(), cmap='viridis')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
