{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "199ac2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "\n",
      "Training for REGION_LOWER_BYRD...\n",
      "=== REGION_LOWER_BYRD ===\n",
      "Training inputs shape: torch.Size([487, 2])\n",
      "Training observations shape: torch.Size([487, 2])\n",
      "Training inputs dtype: torch.float32\n",
      "\n",
      "=== REGION_LOWER_BYRD ===\n",
      "Test inputs shape: torch.Size([330, 2])\n",
      "Test observations shape: torch.Size([330, 2])\n",
      "Test inputs dtype: torch.float32\n",
      "\n",
      "\n",
      "--- Training Run 1/1 ---\n",
      "\n",
      "Start Training\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 1/2000, Training Loss (NLML): 4.9580\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 21/2000, Training Loss (NLML): 4.1327\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 41/2000, Training Loss (NLML): 3.3656\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 61/2000, Training Loss (NLML): 2.6027\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 81/2000, Training Loss (NLML): 1.9926\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 101/2000, Training Loss (NLML): 1.6149\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 121/2000, Training Loss (NLML): 1.3365\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 141/2000, Training Loss (NLML): 1.0834\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 161/2000, Training Loss (NLML): 0.8703\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 181/2000, Training Loss (NLML): 0.7143\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 201/2000, Training Loss (NLML): 0.6011\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 221/2000, Training Loss (NLML): 0.5255\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 241/2000, Training Loss (NLML): 0.4495\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 261/2000, Training Loss (NLML): 0.3807\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 281/2000, Training Loss (NLML): 0.3388\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 301/2000, Training Loss (NLML): 0.2828\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 321/2000, Training Loss (NLML): 0.2259\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 341/2000, Training Loss (NLML): 0.1882\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 361/2000, Training Loss (NLML): 0.1359\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 381/2000, Training Loss (NLML): 0.0843\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 401/2000, Training Loss (NLML): 0.0258\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 421/2000, Training Loss (NLML): -0.0438\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 441/2000, Training Loss (NLML): -0.0968\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 461/2000, Training Loss (NLML): -0.1254\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 481/2000, Training Loss (NLML): -0.1652\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 501/2000, Training Loss (NLML): -0.1869\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 521/2000, Training Loss (NLML): -0.1870\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 541/2000, Training Loss (NLML): -0.2237\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 561/2000, Training Loss (NLML): -0.2480\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 581/2000, Training Loss (NLML): -0.2512\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 601/2000, Training Loss (NLML): -0.2746\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 621/2000, Training Loss (NLML): -0.2858\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 641/2000, Training Loss (NLML): -0.2993\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 661/2000, Training Loss (NLML): -0.3263\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 681/2000, Training Loss (NLML): -0.3253\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 701/2000, Training Loss (NLML): -0.3380\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 721/2000, Training Loss (NLML): -0.3580\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 741/2000, Training Loss (NLML): -0.3623\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 761/2000, Training Loss (NLML): -0.3644\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 781/2000, Training Loss (NLML): -0.3822\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 801/2000, Training Loss (NLML): -0.3767\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 821/2000, Training Loss (NLML): -0.3861\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 841/2000, Training Loss (NLML): -0.3895\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 861/2000, Training Loss (NLML): -0.3911\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 881/2000, Training Loss (NLML): -0.4048\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 901/2000, Training Loss (NLML): -0.3965\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 921/2000, Training Loss (NLML): -0.4150\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 941/2000, Training Loss (NLML): -0.4175\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 961/2000, Training Loss (NLML): -0.4225\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 981/2000, Training Loss (NLML): -0.4277\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 1001/2000, Training Loss (NLML): -0.4394\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 1021/2000, Training Loss (NLML): -0.4376\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 1041/2000, Training Loss (NLML): -0.4386\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 1061/2000, Training Loss (NLML): -0.4482\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 1081/2000, Training Loss (NLML): -0.4548\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 1101/2000, Training Loss (NLML): -0.4536\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 1121/2000, Training Loss (NLML): -0.4612\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 1141/2000, Training Loss (NLML): -0.4686\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 1161/2000, Training Loss (NLML): -0.4785\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 1181/2000, Training Loss (NLML): -0.4762\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 1201/2000, Training Loss (NLML): -0.4850\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 1221/2000, Training Loss (NLML): -0.4858\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 1241/2000, Training Loss (NLML): -0.4957\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 1261/2000, Training Loss (NLML): -0.5042\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 1281/2000, Training Loss (NLML): -0.5036\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 1301/2000, Training Loss (NLML): -0.5125\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 1321/2000, Training Loss (NLML): -0.5044\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 1341/2000, Training Loss (NLML): -0.5122\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 1361/2000, Training Loss (NLML): -0.5212\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 1381/2000, Training Loss (NLML): -0.5153\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 1401/2000, Training Loss (NLML): -0.5181\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 1421/2000, Training Loss (NLML): -0.5206\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 1441/2000, Training Loss (NLML): -0.5139\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 1461/2000, Training Loss (NLML): -0.5242\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 1481/2000, Training Loss (NLML): -0.5263\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 1501/2000, Training Loss (NLML): -0.5327\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 1521/2000, Training Loss (NLML): -0.5344\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 1541/2000, Training Loss (NLML): -0.5509\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 1561/2000, Training Loss (NLML): -0.5467\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 1581/2000, Training Loss (NLML): -0.5493\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 1601/2000, Training Loss (NLML): -0.5484\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 1621/2000, Training Loss (NLML): -0.5512\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 1641/2000, Training Loss (NLML): -0.5510\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 1661/2000, Training Loss (NLML): -0.5525\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 1681/2000, Training Loss (NLML): -0.5566\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 1701/2000, Training Loss (NLML): -0.5728\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 1721/2000, Training Loss (NLML): -0.5646\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 1741/2000, Training Loss (NLML): -0.5794\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 1761/2000, Training Loss (NLML): -0.5728\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 1781/2000, Training Loss (NLML): -0.5735\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 1801/2000, Training Loss (NLML): -0.5658\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 1821/2000, Training Loss (NLML): -0.5764\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 1841/2000, Training Loss (NLML): -0.5776\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 1861/2000, Training Loss (NLML): -0.5771\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 1881/2000, Training Loss (NLML): -0.5860\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 1901/2000, Training Loss (NLML): -0.5943\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 1921/2000, Training Loss (NLML): -0.5837\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 1941/2000, Training Loss (NLML): -0.5854\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 1961/2000, Training Loss (NLML): -0.5916\n",
      "region_lower_byrd dfNGP Run 1/1, Epoch 1981/2000, Training Loss (NLML): -0.6047\n",
      "\n",
      "REGION_LOWER_BYRD dfNGP Run 1/1 Metrics:\n",
      "Train RMSE: 0.11949, Train MAE: 0.06429, Train NLL: -1.37325, Train QCE: 0.02125, Train MAD: 0.00000\n",
      "Test RMSE: 0.23642, Test MAE: 0.16173, Test NLL: 0.10519, Test QCE: 0.29848, Test MAD: 0.00000\n",
      "\n",
      "Training for REGION_MID_BYRD...\n",
      "=== REGION_MID_BYRD ===\n",
      "Training inputs shape: torch.Size([501, 2])\n",
      "Training observations shape: torch.Size([501, 2])\n",
      "Training inputs dtype: torch.float32\n",
      "\n",
      "=== REGION_MID_BYRD ===\n",
      "Test inputs shape: torch.Size([221, 2])\n",
      "Test observations shape: torch.Size([221, 2])\n",
      "Test inputs dtype: torch.float32\n",
      "\n",
      "\n",
      "--- Training Run 1/1 ---\n",
      "\n",
      "Start Training\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 1/2000, Training Loss (NLML): 14.0051\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 21/2000, Training Loss (NLML): 11.4029\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 41/2000, Training Loss (NLML): 8.6492\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 61/2000, Training Loss (NLML): 5.9788\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 81/2000, Training Loss (NLML): 4.1136\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 101/2000, Training Loss (NLML): 3.0389\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 121/2000, Training Loss (NLML): 2.2062\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 141/2000, Training Loss (NLML): 1.5667\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 161/2000, Training Loss (NLML): 1.1953\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 181/2000, Training Loss (NLML): 1.0173\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 201/2000, Training Loss (NLML): 0.9150\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 221/2000, Training Loss (NLML): 0.8438\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 241/2000, Training Loss (NLML): 0.7844\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 261/2000, Training Loss (NLML): 0.7349\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 281/2000, Training Loss (NLML): 0.6792\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 301/2000, Training Loss (NLML): 0.6267\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 321/2000, Training Loss (NLML): 0.5897\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 341/2000, Training Loss (NLML): 0.5387\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 361/2000, Training Loss (NLML): 0.5042\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 381/2000, Training Loss (NLML): 0.4927\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 401/2000, Training Loss (NLML): 0.4318\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 421/2000, Training Loss (NLML): 0.4108\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 441/2000, Training Loss (NLML): 0.3884\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 461/2000, Training Loss (NLML): 0.3327\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 481/2000, Training Loss (NLML): 0.3059\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 501/2000, Training Loss (NLML): 0.2717\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 521/2000, Training Loss (NLML): 0.2394\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 541/2000, Training Loss (NLML): 0.2260\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 561/2000, Training Loss (NLML): 0.1975\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 581/2000, Training Loss (NLML): 0.1584\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 601/2000, Training Loss (NLML): 0.1361\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 621/2000, Training Loss (NLML): 0.0922\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 641/2000, Training Loss (NLML): 0.0713\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 661/2000, Training Loss (NLML): 0.0365\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 681/2000, Training Loss (NLML): 0.0284\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 701/2000, Training Loss (NLML): -0.0270\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 721/2000, Training Loss (NLML): -0.0440\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 741/2000, Training Loss (NLML): -0.0907\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 761/2000, Training Loss (NLML): -0.1029\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 781/2000, Training Loss (NLML): -0.1375\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 801/2000, Training Loss (NLML): -0.1593\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 821/2000, Training Loss (NLML): -0.2127\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 841/2000, Training Loss (NLML): -0.2500\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 861/2000, Training Loss (NLML): -0.2566\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 881/2000, Training Loss (NLML): -0.2863\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 901/2000, Training Loss (NLML): -0.3132\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 921/2000, Training Loss (NLML): -0.3232\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 941/2000, Training Loss (NLML): -0.3429\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 961/2000, Training Loss (NLML): -0.3408\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 981/2000, Training Loss (NLML): -0.3444\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 1001/2000, Training Loss (NLML): -0.3557\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 1021/2000, Training Loss (NLML): -0.3711\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 1041/2000, Training Loss (NLML): -0.3859\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 1061/2000, Training Loss (NLML): -0.3687\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 1081/2000, Training Loss (NLML): -0.3991\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 1101/2000, Training Loss (NLML): -0.3935\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 1121/2000, Training Loss (NLML): -0.3905\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 1141/2000, Training Loss (NLML): -0.3918\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 1161/2000, Training Loss (NLML): -0.4034\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 1181/2000, Training Loss (NLML): -0.4024\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 1201/2000, Training Loss (NLML): -0.4364\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 1221/2000, Training Loss (NLML): -0.3943\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 1241/2000, Training Loss (NLML): -0.4135\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 1261/2000, Training Loss (NLML): -0.4223\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 1281/2000, Training Loss (NLML): -0.4309\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 1301/2000, Training Loss (NLML): -0.4076\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 1321/2000, Training Loss (NLML): -0.4301\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 1341/2000, Training Loss (NLML): -0.4268\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 1361/2000, Training Loss (NLML): -0.4505\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 1381/2000, Training Loss (NLML): -0.4320\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 1401/2000, Training Loss (NLML): -0.4255\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 1421/2000, Training Loss (NLML): -0.4461\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 1441/2000, Training Loss (NLML): -0.4464\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 1461/2000, Training Loss (NLML): -0.4411\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 1481/2000, Training Loss (NLML): -0.4321\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 1501/2000, Training Loss (NLML): -0.4447\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 1521/2000, Training Loss (NLML): -0.4458\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 1541/2000, Training Loss (NLML): -0.4325\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 1561/2000, Training Loss (NLML): -0.4367\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 1581/2000, Training Loss (NLML): -0.4467\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 1601/2000, Training Loss (NLML): -0.4399\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 1621/2000, Training Loss (NLML): -0.4613\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 1641/2000, Training Loss (NLML): -0.4453\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 1661/2000, Training Loss (NLML): -0.4485\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 1681/2000, Training Loss (NLML): -0.4498\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 1701/2000, Training Loss (NLML): -0.4592\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 1721/2000, Training Loss (NLML): -0.4656\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 1741/2000, Training Loss (NLML): -0.4279\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 1761/2000, Training Loss (NLML): -0.4651\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 1781/2000, Training Loss (NLML): -0.4617\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 1801/2000, Training Loss (NLML): -0.4661\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 1821/2000, Training Loss (NLML): -0.4486\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 1841/2000, Training Loss (NLML): -0.4347\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 1861/2000, Training Loss (NLML): -0.4607\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 1881/2000, Training Loss (NLML): -0.4780\n",
      "region_mid_byrd dfNGP Run 1/1, Epoch 1901/2000, Training Loss (NLML): -0.4556\n",
      "Early stopping triggered after 1904 epochs.\n",
      "\n",
      "REGION_MID_BYRD dfNGP Run 1/1 Metrics:\n",
      "Train RMSE: 0.07910, Train MAE: 0.04822, Train NLL: -1.89086, Train QCE: 0.01806, Train MAD: 0.00000\n",
      "Test RMSE: 0.43057, Test MAE: 0.32291, Test NLL: 0.81253, Test QCE: 0.24864, Test MAD: 0.00000\n",
      "\n",
      "Training for REGION_UPPER_BYRD...\n",
      "=== REGION_UPPER_BYRD ===\n",
      "Training inputs shape: torch.Size([504, 2])\n",
      "Training observations shape: torch.Size([504, 2])\n",
      "Training inputs dtype: torch.float32\n",
      "\n",
      "=== REGION_UPPER_BYRD ===\n",
      "Test inputs shape: torch.Size([242, 2])\n",
      "Test observations shape: torch.Size([242, 2])\n",
      "Test inputs dtype: torch.float32\n",
      "\n",
      "\n",
      "--- Training Run 1/1 ---\n",
      "\n",
      "Start Training\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 1/2000, Training Loss (NLML): 8.5401\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 21/2000, Training Loss (NLML): 6.8539\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 41/2000, Training Loss (NLML): 5.4251\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 61/2000, Training Loss (NLML): 4.2429\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 81/2000, Training Loss (NLML): 3.3964\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 101/2000, Training Loss (NLML): 2.6831\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 121/2000, Training Loss (NLML): 2.0230\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 141/2000, Training Loss (NLML): 1.4554\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 161/2000, Training Loss (NLML): 1.0144\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 181/2000, Training Loss (NLML): 0.7340\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 201/2000, Training Loss (NLML): 0.5739\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 221/2000, Training Loss (NLML): 0.4811\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 241/2000, Training Loss (NLML): 0.4491\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 261/2000, Training Loss (NLML): 0.4002\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 281/2000, Training Loss (NLML): 0.3741\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 301/2000, Training Loss (NLML): 0.3445\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 321/2000, Training Loss (NLML): 0.3167\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 341/2000, Training Loss (NLML): 0.2890\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 361/2000, Training Loss (NLML): 0.2676\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 381/2000, Training Loss (NLML): 0.2450\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 401/2000, Training Loss (NLML): 0.2115\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 421/2000, Training Loss (NLML): 0.1890\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 441/2000, Training Loss (NLML): 0.1662\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 461/2000, Training Loss (NLML): 0.1461\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 481/2000, Training Loss (NLML): 0.1224\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 501/2000, Training Loss (NLML): 0.0995\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 521/2000, Training Loss (NLML): 0.0774\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 541/2000, Training Loss (NLML): 0.0708\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 561/2000, Training Loss (NLML): 0.0379\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 581/2000, Training Loss (NLML): 0.0175\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 601/2000, Training Loss (NLML): 0.0171\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 621/2000, Training Loss (NLML): -0.0055\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 641/2000, Training Loss (NLML): -0.0213\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 661/2000, Training Loss (NLML): -0.0425\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 681/2000, Training Loss (NLML): -0.0592\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 701/2000, Training Loss (NLML): -0.0385\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 721/2000, Training Loss (NLML): -0.0659\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 741/2000, Training Loss (NLML): -0.0800\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 761/2000, Training Loss (NLML): -0.1017\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 781/2000, Training Loss (NLML): -0.0970\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 801/2000, Training Loss (NLML): -0.1082\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 821/2000, Training Loss (NLML): -0.1180\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 841/2000, Training Loss (NLML): -0.1287\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 861/2000, Training Loss (NLML): -0.1372\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 881/2000, Training Loss (NLML): -0.1410\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 901/2000, Training Loss (NLML): -0.1544\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 921/2000, Training Loss (NLML): -0.1643\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 941/2000, Training Loss (NLML): -0.1659\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 961/2000, Training Loss (NLML): -0.1769\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 981/2000, Training Loss (NLML): -0.1911\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 1001/2000, Training Loss (NLML): -0.1818\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 1021/2000, Training Loss (NLML): -0.1754\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 1041/2000, Training Loss (NLML): -0.2020\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 1061/2000, Training Loss (NLML): -0.1984\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 1081/2000, Training Loss (NLML): -0.2205\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 1101/2000, Training Loss (NLML): -0.2270\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 1121/2000, Training Loss (NLML): -0.2438\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 1141/2000, Training Loss (NLML): -0.2452\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 1161/2000, Training Loss (NLML): -0.2697\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 1181/2000, Training Loss (NLML): -0.2579\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 1201/2000, Training Loss (NLML): -0.2719\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 1221/2000, Training Loss (NLML): -0.2805\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 1241/2000, Training Loss (NLML): -0.2936\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 1261/2000, Training Loss (NLML): -0.3112\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 1281/2000, Training Loss (NLML): -0.3279\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 1301/2000, Training Loss (NLML): -0.3434\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 1321/2000, Training Loss (NLML): -0.3609\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 1341/2000, Training Loss (NLML): -0.3591\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 1361/2000, Training Loss (NLML): -0.3689\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 1381/2000, Training Loss (NLML): -0.3781\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 1401/2000, Training Loss (NLML): -0.3876\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 1421/2000, Training Loss (NLML): -0.4101\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 1441/2000, Training Loss (NLML): -0.4246\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 1461/2000, Training Loss (NLML): -0.4302\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 1481/2000, Training Loss (NLML): -0.4390\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 1501/2000, Training Loss (NLML): -0.4555\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 1521/2000, Training Loss (NLML): -0.4619\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 1541/2000, Training Loss (NLML): -0.4607\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 1561/2000, Training Loss (NLML): -0.4654\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 1581/2000, Training Loss (NLML): -0.4875\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 1601/2000, Training Loss (NLML): -0.4927\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 1621/2000, Training Loss (NLML): -0.4918\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 1641/2000, Training Loss (NLML): -0.4987\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 1661/2000, Training Loss (NLML): -0.5029\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 1681/2000, Training Loss (NLML): -0.5027\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 1701/2000, Training Loss (NLML): -0.5033\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 1721/2000, Training Loss (NLML): -0.5192\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 1741/2000, Training Loss (NLML): -0.5076\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 1761/2000, Training Loss (NLML): -0.5117\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 1781/2000, Training Loss (NLML): -0.5155\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 1801/2000, Training Loss (NLML): -0.5297\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 1821/2000, Training Loss (NLML): -0.5362\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 1841/2000, Training Loss (NLML): -0.5236\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 1861/2000, Training Loss (NLML): -0.5269\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 1881/2000, Training Loss (NLML): -0.5292\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 1901/2000, Training Loss (NLML): -0.5195\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 1921/2000, Training Loss (NLML): -0.5368\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 1941/2000, Training Loss (NLML): -0.5153\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 1961/2000, Training Loss (NLML): -0.5338\n",
      "region_upper_byrd dfNGP Run 1/1, Epoch 1981/2000, Training Loss (NLML): -0.5476\n",
      "\n",
      "REGION_UPPER_BYRD dfNGP Run 1/1 Metrics:\n",
      "Train RMSE: 0.09300, Train MAE: 0.06449, Train NLL: -1.73420, Train QCE: 0.01032, Train MAD: 0.00000\n",
      "Test RMSE: 0.37988, Test MAE: 0.24465, Test NLL: 1.79063, Test QCE: 0.18554, Test MAD: 0.00000\n"
     ]
    }
   ],
   "source": [
    "# REAL DATA EXPERIMENTS\n",
    "# RUN WITH python run_real_experiments_dfNGP.py\n",
    "#               _                 _   _      \n",
    "#              | |               | | (_)     \n",
    "#    __ _ _ __ | |_ __ _ _ __ ___| |_ _  ___ \n",
    "#   / _` | '_ \\| __/ _` | '__/ __| __| |/ __|\n",
    "#  | (_| | | | | || (_| | | | (__| |_| | (__ \n",
    "#   \\__,_|_| |_|\\__\\__,_|_|  \\___|\\__|_|\\___|\n",
    "# \n",
    "model_name = \"dfNGP\"\n",
    "from gpytorch_models import dfNGP\n",
    "\n",
    "# import configs to we can access the hypers with getattr\n",
    "import configs\n",
    "from configs import PATIENCE, MAX_NUM_EPOCHS, NUM_RUNS, WEIGHT_DECAY\n",
    "from configs import TRACK_EMISSIONS_BOOL\n",
    "from configs import SCALE_INPUT_region_lower_byrd, SCALE_INPUT_region_mid_byrd, SCALE_INPUT_region_upper_byrd\n",
    "from configs import REAL_L_RANGE, REAL_NOISE_VAR_RANGE, REAL_OUTPUTSCALE_VAR_RANGE\n",
    "\n",
    "SCALE_INPUT = {\n",
    "    \"region_lower_byrd\": SCALE_INPUT_region_lower_byrd,\n",
    "    \"region_mid_byrd\": SCALE_INPUT_region_mid_byrd,\n",
    "    \"region_upper_byrd\": SCALE_INPUT_region_upper_byrd,\n",
    "}\n",
    "\n",
    "# Reiterating import for visibility\n",
    "MAX_NUM_EPOCHS = MAX_NUM_EPOCHS\n",
    "# MAX_NUM_EPOCHS = 4200\n",
    "NUM_RUNS = NUM_RUNS\n",
    "NUM_RUNS = 1\n",
    "WEIGHT_DECAY = WEIGHT_DECAY\n",
    "PATIENCE = PATIENCE\n",
    "\n",
    "# assign model-specific variable\n",
    "MODEL_LEARNING_RATE = getattr(configs, f\"{model_name}_REAL_LEARNING_RATE\")\n",
    "MODEL_REAL_RESULTS_DIR = getattr(configs, f\"{model_name}_REAL_RESULTS_DIR\")\n",
    "import os\n",
    "os.makedirs(MODEL_REAL_RESULTS_DIR, exist_ok = True)\n",
    "\n",
    "# basics\n",
    "import pandas as pd\n",
    "import torch\n",
    "import gpytorch\n",
    "\n",
    "# universals \n",
    "from metrics import compute_divergence_field, quantile_coverage_error_2d\n",
    "from utils import set_seed, make_grid\n",
    "import gc\n",
    "import warnings\n",
    "set_seed(42)\n",
    "\n",
    "# setting device to GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# overwrite if needed: # device = 'cpu'\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#############################\n",
    "### LOOP 1 - over REGIONS ###\n",
    "#############################\n",
    "\n",
    "for region_name in [\"region_lower_byrd\", \"region_mid_byrd\", \"region_upper_byrd\"]:\n",
    "    SCALE_DOMAIN = SCALE_INPUT[region_name]\n",
    "\n",
    "    print(f\"\\nTraining for {region_name.upper()}...\")\n",
    "\n",
    "    # Store metrics for the current region (used for *metrics_summary* report and *metrics_per_run*)\n",
    "    region_results = []\n",
    "\n",
    "    ##########################################\n",
    "    ### x_train & y_train, x_test & x_test ###\n",
    "    ##########################################\n",
    "\n",
    "    # define paths based on region_name\n",
    "    path_to_training_tensor = \"data/real_data/\" + region_name + \"_train_tensor.pt\"\n",
    "    path_to_test_tensor = \"data/real_data/\" + region_name + \"_test_tensor.pt\"\n",
    "\n",
    "    # load and tranpose to have rows as points\n",
    "    train = torch.load(path_to_training_tensor, weights_only = False).T \n",
    "    test = torch.load(path_to_test_tensor, weights_only = False).T\n",
    "\n",
    "    # The train and test tensors have the following columns:\n",
    "    # [:, 0] = x\n",
    "    # [:, 1] = y\n",
    "    # [:, 2] = surface elevation (s)\n",
    "    # [:, 3] = ice flux in x direction (u)\n",
    "    # [:, 4] = ice flux in y direction (v)\n",
    "    # [:, 5] = ice velocity error in x direction (u_err)\n",
    "    # [:, 6] = ice velocity error in y direction (v_err)\n",
    "    # [:, 7] = ice velocity in x direction (u)\n",
    "    # [:, 8] = ice velocity in y direction (v)\n",
    "    # [:, 9] = thickness\n",
    "    # [:, 10] = source age\n",
    "    # [:, 11] = sqrt flux scale (used for scaling the fluxes)\n",
    "\n",
    "    # train\n",
    "    x_train = train[:, [0, 1]].to(device)\n",
    "    y_train = train[:, [3, 4]].to(device)\n",
    "\n",
    "    # test\n",
    "    x_test = test[:, [0, 1]].to(device)\n",
    "    y_test = test[:, [3, 4]].to(device)\n",
    "\n",
    "    # HACK: Scaling helps with numerical stability\n",
    "    # Units are not in km \n",
    "    x_test = x_test * SCALE_DOMAIN\n",
    "    x_train = x_train * SCALE_DOMAIN\n",
    "\n",
    "    # NOTE: Here we estimate the noise variance \n",
    "\n",
    "    # Print train details\n",
    "    print(f\"=== {region_name.upper()} ===\")\n",
    "    print(f\"Training inputs shape: {x_train.shape}\")\n",
    "    print(f\"Training observations shape: {y_train.shape}\")\n",
    "    print(f\"Training inputs dtype: {x_train.dtype}\")\n",
    "    print()\n",
    "\n",
    "    # Print test details\n",
    "    print(f\"=== {region_name.upper()} ===\")\n",
    "    print(f\"Test inputs shape: {x_test.shape}\")\n",
    "    print(f\"Test observations shape: {y_test.shape}\")\n",
    "    print(f\"Test inputs dtype: {x_test.dtype}\")\n",
    "    print()\n",
    "\n",
    "    ##################################\n",
    "    ### LOOP 2 - over training run ###\n",
    "    ##################################\n",
    "\n",
    "    # NOTE: GPs don't train on batches, use full data\n",
    "\n",
    "    for run in range(NUM_RUNS):\n",
    "\n",
    "        print(f\"\\n--- Training Run {run + 1}/{NUM_RUNS} ---\")\n",
    "\n",
    "        # Initialise the likelihood for the GP model (estimates noise)\n",
    "        # NOTE: we use a multitask likelihood for the dfNGP model but with a global noise term\n",
    "        likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(\n",
    "            num_tasks = 2,\n",
    "            has_global_noise = True, \n",
    "            has_task_noise = False, # HACK: This still needs to be manually turned off\n",
    "            ).to(device)\n",
    "\n",
    "        # NOTE: This was needed\n",
    "        x_train = x_train.clone().detach().requires_grad_(True)\n",
    "\n",
    "        model = dfNGP(\n",
    "            x_train,\n",
    "            y_train, \n",
    "            likelihood\n",
    "            ).to(device)\n",
    "        \n",
    "        ### REGISTER PRIORS & CONSTRAINTS ###\n",
    "        # PRIOR: outputscale variance\n",
    "        outputscale_prior = gpytorch.priors.SmoothedBoxPrior(\n",
    "            REAL_OUTPUTSCALE_VAR_RANGE[0], REAL_OUTPUTSCALE_VAR_RANGE[1]).to(device)\n",
    "        \n",
    "        model.covar_module.register_prior(\n",
    "            \"outputscale_prior\",\n",
    "            outputscale_prior,\n",
    "            \"raw_outputscale\"\n",
    "        )\n",
    "\n",
    "        # CONSTRAINT: Domain-informed noise variance constraint\n",
    "        model.likelihood.register_constraint(\n",
    "            \"raw_noise\", gpytorch.constraints.Interval(REAL_NOISE_VAR_RANGE[0], REAL_NOISE_VAR_RANGE[1])\n",
    "        )\n",
    "        \n",
    "        ### INITIALISE HYPERPARAMETERS ###\n",
    "        # Overwrite default lengthscale hyperparameter initialisation with REAL data lengthscale range init\n",
    "        model.base_kernel.lengthscale = torch.empty([1, 2], device = device).uniform_( * REAL_L_RANGE)\n",
    "\n",
    "        # Overwrite default outputscale variance initialisation with sample from prior\n",
    "        outputscale_sample = outputscale_prior.sample().to(device)\n",
    "        model.covar_module.outputscale = outputscale_sample\n",
    "        \n",
    "        # Overwrite default noise variance initialisation with REAL data noise range init\n",
    "        model.likelihood.noise = torch.empty(1, device = device).uniform_( * REAL_NOISE_VAR_RANGE)\n",
    "        \n",
    "        # NOTE: This part is different from dfGP\n",
    "        optimizer = torch.optim.AdamW([\n",
    "            {\"params\": model.mean_module.parameters(), \n",
    "             \"weight_decay\": WEIGHT_DECAY, \"lr\": 0.001},\n",
    "            {\"params\": list(model.covar_module.parameters()) + list(model.likelihood.parameters()), \n",
    "             \"weight_decay\": WEIGHT_DECAY, \"lr\": 0.005},\n",
    "            ])\n",
    "        \n",
    "        # Use ExactMarginalLogLikelihood\n",
    "        mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "        model.train()\n",
    "        likelihood.train()\n",
    "        # _________________\n",
    "        # BEFORE EPOCH LOOP\n",
    "        \n",
    "        # Early stopping variables\n",
    "        best_loss = float('inf')\n",
    "        # counter starts at 0\n",
    "        epochs_no_improve = 0\n",
    "\n",
    "        ############################\n",
    "        ### LOOP 3 - over EPOCHS ###\n",
    "        ############################\n",
    "        print(\"\\nStart Training\")\n",
    "\n",
    "        for epoch in range(MAX_NUM_EPOCHS):\n",
    "\n",
    "            # Set to train\n",
    "            model.train()\n",
    "            likelihood.train()\n",
    "\n",
    "            # Do a step\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            x_train = x_train.clone().detach().requires_grad_(True)\n",
    "            # model outputs a multivariate normal distribution\n",
    "            train_pred_dist = model(x_train.to(device))\n",
    "            # Train on noisy or targets\n",
    "            # NOTE: We only have observational y_train i.e. noisy data\n",
    "            loss = - mll(train_pred_dist, y_train.to(device))  # negative marginal log likelihood\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # For all runs after the first we run a minimal version using only lml_train\n",
    "\n",
    "            if epoch % 20 == 0:\n",
    "                # After run 1 we only print lml, nothing else\n",
    "                print(f\"{region_name} {model_name} Run {run + 1}/{NUM_RUNS}, Epoch {epoch + 1}/{MAX_NUM_EPOCHS}, Training Loss (NLML): {loss:.4f}\")\n",
    "                \n",
    "            # EVERY EPOCH: Early stopping check\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                # reset counter if loss improves\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "\n",
    "            if epochs_no_improve >= PATIENCE:\n",
    "                print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
    "                # exit epoch loop\n",
    "                break\n",
    "\n",
    "        ##############################\n",
    "        ### END LOOP 3 over EPOCHS ###\n",
    "        ##############################\n",
    "\n",
    "        # for every run...\n",
    "        #######################################################\n",
    "        ### EVALUATE after all training for RUN is finished ###\n",
    "        #######################################################\n",
    "\n",
    "        model.eval()\n",
    "        likelihood.eval()\n",
    "\n",
    "        ### --- dfNGP only: grid inference --- ###\n",
    "\n",
    "        if run == 0:\n",
    "\n",
    "            _, x_grid = make_grid(n_side = 30) \n",
    "            x_grid = x_grid * SCALE_DOMAIN # scale grid to match training data\n",
    "            x_grid.requires_grad_(True) # need gradients for divergence field\n",
    "\n",
    "            dist_grid = model(x_grid.to(device))\n",
    "            pred_dist_grid = likelihood(dist_grid)\n",
    "\n",
    "            torch.save(pred_dist_grid.mean, f\"{MODEL_REAL_RESULTS_DIR}/{region_name}_{model_name}_grid_mean_predictions.pt\")\n",
    "            torch.save(pred_dist_grid.covariance_matrix, f\"{MODEL_REAL_RESULTS_DIR}/{region_name}_{model_name}_grid_covar_predictions.pt\")\n",
    "            torch.save(dist_grid.covariance_matrix, f\"{MODEL_REAL_RESULTS_DIR}/{region_name}_{model_name}_grid_latent_covar_predictions.pt\")\n",
    "\n",
    "\n",
    "\n",
    "        ### ---------------------------------- ###\n",
    "\n",
    "        # Need gradients for autograd divergence: We clone and detach\n",
    "        x_test_grad = x_test.to(device).clone().requires_grad_(True)\n",
    "        x_train_grad = x_train.to(device).clone().requires_grad_(True)\n",
    "\n",
    "        # Underlying (latent) distribution and predictive distribution\n",
    "        dist_test = model(x_test_grad)\n",
    "        pred_dist_test = likelihood(dist_test)\n",
    "\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", gpytorch.utils.warnings.GPInputWarning)\n",
    "            dist_train = model(x_train_grad)\n",
    "            pred_dist_train = likelihood(dist_train)\n",
    "        \n",
    "        # Compute divergence field (from latent distribution)\n",
    "        test_div_field = compute_divergence_field(dist_test.mean, x_test_grad)\n",
    "        train_div_field = compute_divergence_field(dist_train.mean, x_train_grad)\n",
    "\n",
    "        # Only save mean_pred, covar_pred and divergence fields for the first run\n",
    "\n",
    "        # Compute TRAIN metrics (convert tensors to float) for every run's tuned model\n",
    "        # NOTE: gpytorch outputs metrics per task\n",
    "        train_RMSE = torch.sqrt(gpytorch.metrics.mean_squared_error(\n",
    "            pred_dist_train, y_train.to(device)).mean()).item()\n",
    "        train_MAE = gpytorch.metrics.mean_absolute_error(\n",
    "            pred_dist_train, y_train.to(device)).mean().item()\n",
    "        train_NLL = gpytorch.metrics.negative_log_predictive_density(\n",
    "            pred_dist_train, y_train.to(device)).item()\n",
    "        train_QCE = quantile_coverage_error_2d(\n",
    "            pred_dist_train, y_train.to(device), quantile = 95.0).item()\n",
    "        ## NOTE: It is important to use the absolute value of the divergence field, since both positive and negative deviations are violations and shouldn't cancel each other out \n",
    "        train_MAD = train_div_field.abs().mean().item()\n",
    "\n",
    "        # Compute TEST metrics (convert tensors to float) for every run's tuned model\n",
    "        test_RMSE = torch.sqrt(gpytorch.metrics.mean_squared_error(\n",
    "            pred_dist_test, y_test.to(device)).mean()).item()\n",
    "        test_MAE = gpytorch.metrics.mean_absolute_error(\n",
    "            pred_dist_test, y_test.to(device)).mean().item()\n",
    "        test_NLL = gpytorch.metrics.negative_log_predictive_density(\n",
    "            pred_dist_test, y_test.to(device)).item()\n",
    "        test_QCE = quantile_coverage_error_2d(\n",
    "            pred_dist_test, y_test.to(device), quantile = 95.0).item()\n",
    "        ## NOTE: It is important to use the absolute value of the divergence field, since both positive and negative deviations are violations and shouldn't cancel each other out \n",
    "        test_MAD = test_div_field.abs().mean().item()\n",
    "\n",
    "        region_results.append([\n",
    "            run + 1,\n",
    "            train_RMSE, train_MAE, train_NLL, train_QCE, train_MAD,\n",
    "            test_RMSE, test_MAE, test_NLL, test_QCE, test_MAD\n",
    "        ])\n",
    "\n",
    "        print(f\"\\n{region_name.upper()} {model_name} Run {run + 1}/{NUM_RUNS} Metrics:\")\n",
    "        print(f\"Train RMSE: {train_RMSE:.5f}, Train MAE: {train_MAE:.5f}, Train NLL: {train_NLL:.5f}, Train QCE: {train_QCE:.5f}, Train MAD: {train_MAD:.5f}\")\n",
    "        print(f\"Test RMSE: {test_RMSE:.5f}, Test MAE: {test_MAE:.5f}, Test NLL: {test_NLL:.5f}, Test QCE: {test_QCE:.5f}, Test MAD: {test_MAD:.5f}\")\n",
    "\n",
    "###############################\n",
    "### END LOOP 1 over REGIONS ###\n",
    "###############################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
