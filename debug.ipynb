{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6bb43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIMULATED DATA EXPERIMENTS\n",
    "# RUN WITH python run_sim_experiments_dfNGP.py\n",
    "# \n",
    "#       ooooooooooooooooooooooooooooooooooooo\n",
    "#      8                                .d88\n",
    "#      8  oooooooooooooooooooooooooooood8888\n",
    "#      8  8888888888888888888888888P\"   8888    oooooooooooooooo\n",
    "#      8  8888888888888888888888P\"      8888    8              8\n",
    "#      8  8888888888888888888P\"         8888    8             d8\n",
    "#      8  8888888888888888P\"            8888    8            d88\n",
    "#      8  8888888888888P\"               8888    8           d888\n",
    "#      8  8888888888P\"                  8888    8          d8888\n",
    "#      8  8888888P\"                     8888    8         d88888\n",
    "#      8  8888P\"                        8888    8        d888888\n",
    "#      8  8888oooooooooooooooooooooocgmm8888    8       d8888888\n",
    "#      8 .od88888888888888888888888888888888    8      d88888888\n",
    "#      8888888888888888888888888888888888888    8     d888888888\n",
    "#                                               8    d8888888888\n",
    "#         ooooooooooooooooooooooooooooooo       8   d88888888888\n",
    "#        d                       ...oood8b      8  d888888888888\n",
    "#       d              ...oood888888888888b     8 d8888888888888\n",
    "#      d     ...oood88888888888888888888888b    8d88888888888888\n",
    "#     dood8888888888888888888888888888888888b\n",
    "#\n",
    "#\n",
    "# This artwork is a visual reminder that this script is for the sim experiments.\n",
    "\n",
    "model_name = \"dfNGP\"\n",
    "\n",
    "# import configs to we can access the hypers with getattr\n",
    "import configs\n",
    "from configs import PATIENCE, MAX_NUM_EPOCHS, NUM_RUNS, WEIGHT_DECAY\n",
    "# also import x_test grid size and std noise for training data\n",
    "from configs import N_SIDE, STD_GAUSSIAN_NOISE\n",
    "\n",
    "# Reiterating import for visibility\n",
    "MAX_NUM_EPOCHS = MAX_NUM_EPOCHS\n",
    "NUM_RUNS = NUM_RUNS\n",
    "NUM_RUNS = 1\n",
    "WEIGHT_DECAY = WEIGHT_DECAY\n",
    "PATIENCE = PATIENCE\n",
    "\n",
    "# assign model-specific variable\n",
    "MODEL_LEARNING_RATE = getattr(configs, f\"{model_name}_SIM_LEARNING_RATE\")\n",
    "MODEL_SIM_RESULTS_DIR = getattr(configs, f\"{model_name}_SIM_RESULTS_DIR\")\n",
    "import os\n",
    "os.makedirs(MODEL_SIM_RESULTS_DIR, exist_ok = True)\n",
    "\n",
    "# imports for probabilistic models\n",
    "if model_name in [\"GP\", \"dfGP\", \"dfNGP\"]:\n",
    "    from GP_models import GP_predict\n",
    "    from metrics import compute_NLL_sparse, compute_NLL_full\n",
    "    from configs import L_RANGE, SIGMA_N_RANGE, GP_PATIENCE\n",
    "    # overwrite with GP_PATIENCE\n",
    "    PATIENCE = GP_PATIENCE\n",
    "    if model_name in [\"dfGP\", \"dfNGP\"]:\n",
    "        from configs import SIGMA_F_RANGE\n",
    "\n",
    "# for all models with NN components train on batches\n",
    "if model_name in [\"dfNGP\", \"dfNN\", \"PINN\"]:\n",
    "    from configs import BATCH_SIZE\n",
    "\n",
    "if model_name in [\"dfNGP\", \"dfNN\"]:\n",
    "    from NN_models import dfNN\n",
    "\n",
    "# universals \n",
    "from metrics import compute_RMSE, compute_MAE, compute_divergence_field\n",
    "\n",
    "# basics\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from codecarbon import EmissionsTracker\n",
    "\n",
    "# utilitarian\n",
    "from utils import set_seed, make_grid\n",
    "# reproducibility\n",
    "set_seed(42)\n",
    "import gc\n",
    "\n",
    "# setting device to GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# overwrite if needed: # device = 'cpu'\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "### START TIMING ###\n",
    "import time\n",
    "start_time = time.time()  # Start timing after imports\n",
    "\n",
    "### START TRACKING EXPERIMENT EMISSIONS ###\n",
    "tracker = EmissionsTracker(project_name = \"dfNGP_simulation_experiments\", output_dir = MODEL_SIM_RESULTS_DIR)\n",
    "tracker.start()\n",
    "\n",
    "### SIMULATION ###\n",
    "# Import all simulation functions\n",
    "from simulate import (\n",
    "    simulate_detailed_branching,\n",
    "    # simulate_detailed_convergence,\n",
    "    simulate_detailed_curve,\n",
    "    simulate_detailed_deflection,\n",
    "    simulate_detailed_edge,\n",
    "    simulate_detailed_ridges,\n",
    ")\n",
    "\n",
    "# Define simulations as a dictionary with names as keys to function objects\n",
    "# alphabectic order here\n",
    "simulations = {\n",
    "    \"curve\": simulate_detailed_curve,\n",
    "}\n",
    "\n",
    "########################\n",
    "### x_train & x_test ###\n",
    "########################\n",
    "\n",
    "# Load training inputs (once for all simulations)\n",
    "x_train = torch.load(\"data/sim_data/x_train_lines_discretised_0to1.pt\", weights_only = False).float()\n",
    "\n",
    "# Generate x_test (long) once for all simulations\n",
    "_, x_test = make_grid(N_SIDE)\n",
    "# x_test is long format (N_SIDE ** 2, 2)\n",
    "\n",
    "#################################\n",
    "### LOOP 1 - over SIMULATIONS ###\n",
    "#################################\n",
    "\n",
    "# Make y_train_dict: Iterate over all simulation functions\n",
    "for sim_name, sim_func in simulations.items():\n",
    "\n",
    "    ########################\n",
    "    ### y_train & y_test ###\n",
    "    ########################\n",
    "\n",
    "    # Generate training observations\n",
    "    # NOTE: sim_func() needs to be on CPU, so we move x_train to CPU\n",
    "    y_train = sim_func(x_train.cpu()).to(device)\n",
    "    y_test = sim_func(x_test.cpu()).to(device)\n",
    "    \n",
    "    # x_test = x_test.to(device).requires_grad_(True)\n",
    "    x_test = x_test.to(device)\n",
    "    # x_train = x_train.to(device).requires_grad_(True)\n",
    "    x_train = x_train.to(device)\n",
    "\n",
    "    # Print details\n",
    "    print(f\"=== {sim_name.upper()} ===\")\n",
    "    print(f\"Training inputs shape: {x_train.shape}\")\n",
    "    print(f\"Training observations shape: {y_train.shape}\")\n",
    "    print(f\"Training inputs dtype: {x_train.dtype}\")\n",
    "    print(f\"Training inputs device: {y_train.device}\")\n",
    "    print(f\"Training observations device: {y_train.device}\")\n",
    "    print()\n",
    "\n",
    "    # Print details\n",
    "    print(f\"=== {sim_name.upper()} ===\")\n",
    "    print(f\"Test inputs shape: {x_test.shape}\")\n",
    "    print(f\"Test observations shape: {y_test.shape}\")\n",
    "    print(f\"Test inputs dtype: {x_test.dtype}\")\n",
    "    print(f\"Test inputs device: {x_test.device}\")\n",
    "    print(f\"Test observations device: {y_test.device}\")\n",
    "    print()\n",
    "\n",
    "    # NOTE: This is different to the real data experiments\n",
    "    # calculate the mean magnitude of the test data as we use this to scale the noise\n",
    "    sim_mean_magnitude_for_noise = torch.norm(y_test, dim = -1).mean().to(device)\n",
    "    sim_noise = STD_GAUSSIAN_NOISE * sim_mean_magnitude_for_noise\n",
    "\n",
    "    # Store metrics for the simulation (used for *metrics_summary* report and *metrics_per_run*)\n",
    "    simulation_results = [] \n",
    "\n",
    "    ##################################\n",
    "    ### LOOP 2 - over training run ###\n",
    "    ##################################\n",
    "    \n",
    "    # NOTE: GPs and hense dfNGPs don't train on batches, use full data\n",
    "\n",
    "    for run in range(NUM_RUNS):\n",
    "\n",
    "        print(f\"\\n--- Training Run {run + 1}/{NUM_RUNS} ---\")\n",
    "\n",
    "        # initialise trainable dfGP params\n",
    "        sigma_n = nn.Parameter(torch.empty(1, device = device).uniform_( * SIGMA_N_RANGE))\n",
    "        sigma_f = nn.Parameter(torch.empty(1, device = device).uniform_( * SIGMA_F_RANGE))\n",
    "        l = nn.Parameter(torch.empty(2, device = device).uniform_( * L_RANGE))\n",
    "\n",
    "        # For every run initialise a (new) mean model\n",
    "        dfNN_mean_model = dfNN().to(device)\n",
    "\n",
    "        # NOTE: We don't need a criterion either\n",
    "\n",
    "        # AdamW as optimizer for some regularisation/weight decay\n",
    "        # HACK: create two param groups: one for the dfNN and one for the hypers\n",
    "        optimizer = optim.AdamW([\n",
    "            {\"params\": dfNN_mean_model.parameters(), \"weight_decay\": WEIGHT_DECAY, \"lr\": (0.1 * MODEL_LEARNING_RATE)},\n",
    "            {\"params\": [sigma_n, sigma_f, l], \"weight_decay\": WEIGHT_DECAY, \"lr\": MODEL_LEARNING_RATE},\n",
    "            ])\n",
    "\n",
    "        # _________________\n",
    "        # BEFORE EPOCH LOOP\n",
    "        \n",
    "        # Export the convergence just for first run only\n",
    "        if run == 0:\n",
    "            # initialise tensors to store losses over epochs (for convergence plot)\n",
    "            train_losses_NLML_over_epochs = torch.zeros(MAX_NUM_EPOCHS) # objective\n",
    "            train_losses_RMSE_over_epochs = torch.zeros(MAX_NUM_EPOCHS) # by-product\n",
    "            # monitor performance transfer to test (only RMSE easy to calc without covar)\n",
    "            test_losses_RMSE_over_epochs = torch.zeros(MAX_NUM_EPOCHS)\n",
    "\n",
    "            sigma_n_over_epochs = torch.zeros(MAX_NUM_EPOCHS)\n",
    "            sigma_f_over_epochs = torch.zeros(MAX_NUM_EPOCHS)\n",
    "            l1_over_epochs = torch.zeros(MAX_NUM_EPOCHS)\n",
    "            l2_over_epochs = torch.zeros(MAX_NUM_EPOCHS)\n",
    "\n",
    "        # Early stopping variables\n",
    "        best_loss = float('inf')\n",
    "        # counter starts at 0\n",
    "        epochs_no_improve = 0\n",
    "\n",
    "        # NOTE: This is a difference to the real data experiments\n",
    "        # Additive noise model: independent Gaussian noise\n",
    "        # For every run we have a FIXED NOISY TARGET. Draw from standard normal with appropriate std\n",
    "        y_train_noisy = y_train + (torch.randn(y_train.shape, device = device) * sim_noise)\n",
    "\n",
    "        ############################\n",
    "        ### LOOP 3 - over EPOCHS ###\n",
    "        ############################\n",
    "        \n",
    "        print(\"\\nStart Training\")\n",
    "\n",
    "        for epoch in range(MAX_NUM_EPOCHS):\n",
    "\n",
    "            # Assure model is in training mode\n",
    "            dfNN_mean_model.train()\n",
    "\n",
    "            # For Run 1 we save a bunch of metrics and update, while for the rest we only update\n",
    "            if run == 0:\n",
    "                mean_pred_train, _, lml_train = GP_predict(\n",
    "                        x_train,\n",
    "                        y_train_noisy,\n",
    "                        x_train, # predict training data\n",
    "                        [sigma_n, sigma_f, l], # list of (initial) hypers\n",
    "                        mean_func = dfNN_mean_model, # dfNN as mean function\n",
    "                        divergence_free_bool = True) # ensures we use a df kernel\n",
    "\n",
    "                # Compute test loss for loss convergence plot\n",
    "                mean_pred_test, _, _ = GP_predict(\n",
    "                        x_train,\n",
    "                        y_train_noisy,\n",
    "                        x_test.to(device), # have predictions for training data again\n",
    "                        # HACK: This is rather an eval, so we use detached hypers to avoid the computational tree\n",
    "                        [sigma_n.detach().clone(), sigma_f.detach().clone(), l.detach().clone()], # list of (initial) hypers\n",
    "                        mean_func = dfNN_mean_model, # dfNN as mean function\n",
    "                        divergence_free_bool = True) # ensures we use a df kernel\n",
    "                \n",
    "                # UPDATE HYPERS (after test loss is computed to use same model)\n",
    "                optimizer.zero_grad() # don't accumulate gradients\n",
    "                # negative for NLML. loss is always on train\n",
    "                loss = - lml_train\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # NOTE: it is important to detach here \n",
    "                train_RMSE = compute_RMSE(y_train.detach(), mean_pred_train.detach())\n",
    "                test_RMSE = compute_RMSE(y_test.detach(), mean_pred_test.detach())\n",
    "\n",
    "                # Save losses for convergence plot\n",
    "                train_losses_NLML_over_epochs[epoch] = - lml_train\n",
    "                train_losses_RMSE_over_epochs[epoch] = train_RMSE\n",
    "                # NOTE: lml is always just given training data. There is no TEST NLML\n",
    "                test_losses_RMSE_over_epochs[epoch] = test_RMSE\n",
    "\n",
    "                # Save evolution of hyprs for convergence plot\n",
    "                sigma_n_over_epochs[epoch] = sigma_n[0]\n",
    "                sigma_f_over_epochs[epoch] = sigma_f[0]\n",
    "                l1_over_epochs[epoch] = l[0]\n",
    "                l2_over_epochs[epoch] = l[1]\n",
    "\n",
    "                print(f\"{sim_name} {model_name} Run {run + 1}/{NUM_RUNS}, Epoch {epoch + 1}/{MAX_NUM_EPOCHS}, Training Loss (NLML): {loss:.4f}, (RMSE): {train_RMSE:.4f}\")\n",
    "\n",
    "                # delete after printing and saving\n",
    "                # NOTE: keep loss for early stopping check\n",
    "                del mean_pred_train, mean_pred_test, lml_train, train_RMSE, test_RMSE\n",
    "                \n",
    "                # Free up memory every 20 epochs\n",
    "                if epoch % 20 == 0:\n",
    "                    gc.collect() and torch.cuda.empty_cache()\n",
    "            \n",
    "             # For all runs after the first we run a minimal version using only lml_train\n",
    "            else:\n",
    "\n",
    "                # NOTE: We can use x_train[0:2] since the predictions doesn;t matter and we only care about lml_train\n",
    "                _, _, lml_train = GP_predict(\n",
    "                        x_train,\n",
    "                        y_train_noisy,\n",
    "                        x_train[0:2], # predictions don't matter and we output lml_train already\n",
    "                        [sigma_n, sigma_f, l], # list of (initial) hypers\n",
    "                        mean_func = dfNN_mean_model, # dfNN as mean function\n",
    "                        divergence_free_bool = True) # ensures we use a df kernel\n",
    "                \n",
    "                # UPDATE HYPERS (after test loss is computed to use same model)\n",
    "                optimizer.zero_grad() # don't accumulate gradients\n",
    "                # negative for NLML\n",
    "                loss = - lml_train\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # After run 1 we only print lml, nothing else\n",
    "                print(f\"{sim_name} {model_name} Run {run + 1}/{NUM_RUNS}, Epoch {epoch + 1}/{MAX_NUM_EPOCHS}, Training Loss (NLML): {loss:.4f}\")\n",
    "\n",
    "                # NOTE: keep loss for early stopping check, del lml_train\n",
    "                del lml_train\n",
    "                \n",
    "                # Free up memory every 20 epochs\n",
    "                if epoch % 20 == 0:\n",
    "                    gc.collect() and torch.cuda.empty_cache()\n",
    "\n",
    "            # EVERY EPOCH: Early stopping check\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                # reset counter if loss improves\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "\n",
    "            if epochs_no_improve >= PATIENCE:\n",
    "                print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
    "                # exit epoch loop\n",
    "                break\n",
    "\n",
    "        ##############################\n",
    "        ### END LOOP 3 over EPOCHS ###\n",
    "        ##############################\n",
    "\n",
    "        # for every run...\n",
    "        #######################################################\n",
    "        ### EVALUATE after all training for RUN is finished ###\n",
    "        #######################################################\n",
    "\n",
    "        # Evaluate the trained model after all epochs are finished or early stopping was triggered\n",
    "        # NOTE: Detach tuned hyperparameters from the computational graph\n",
    "        best_sigma_n = sigma_n.detach().clone()\n",
    "        best_sigma_f = sigma_f.detach().clone()\n",
    "        best_l = l.detach().clone()\n",
    "\n",
    "        # Need gradients for autograd divergence: We clone and detach\n",
    "        x_test_grad = x_test.to(device).clone().requires_grad_(True)\n",
    "\n",
    "        mean_pred_test, covar_pred_test, _ = GP_predict(\n",
    "            x_train,\n",
    "            y_train, # NOTE: use original y_train, not noisy\n",
    "            x_test_grad,\n",
    "            [best_sigma_n, best_sigma_f, best_l], # list of (initial) hypers\n",
    "            mean_func = dfNN_mean_model, # dfNN as mean function\n",
    "            divergence_free_bool = True) # ensures we use a df kernel\n",
    "        \n",
    "        # Compute divergence field\n",
    "        dfNGP_test_div_field = compute_divergence_field(mean_pred_test, x_test_grad)\n",
    "\n",
    "        # Only save mean_pred, covar_pred and divergence fields for the first run\n",
    "        if run == 0:\n",
    "\n",
    "            # (1) Save predictions from first run so we can visualise them later\n",
    "            torch.save(mean_pred_test, f\"{MODEL_SIM_RESULTS_DIR}/{sim_name}_{model_name}_test_mean_predictions.pt\")\n",
    "            torch.save(covar_pred_test, f\"{MODEL_SIM_RESULTS_DIR}/{sim_name}_{model_name}_test_covar_predictions.pt\")\n",
    "\n",
    "            # (2) Save best hyperparameters\n",
    "            # Stack tensors into a single tensor\n",
    "            best_hypers_tensor = torch.cat([\n",
    "                best_sigma_n.reshape(-1),  # Ensure 1D shape\n",
    "                best_sigma_f.reshape(-1),\n",
    "                best_l.reshape(-1),\n",
    "            ])\n",
    "\n",
    "            torch.save(best_hypers_tensor, f\"{MODEL_SIM_RESULTS_DIR}/{sim_name}_{model_name}_best_hypers.pt\")\n",
    "\n",
    "            # (3) Since all epoch training is finished, we can save the losses over epochs\n",
    "            df_losses = pd.DataFrame({\n",
    "                'Epoch': list(range(train_losses_NLML_over_epochs.shape[0])), # pythonic indexing\n",
    "                'Train Loss NLML': train_losses_NLML_over_epochs.tolist(),\n",
    "                'Train Loss RMSE': train_losses_RMSE_over_epochs.tolist(),\n",
    "                'Test Loss RMSE': test_losses_RMSE_over_epochs.tolist(),\n",
    "                'Sigma_n': sigma_n_over_epochs.tolist(),\n",
    "                'Sigma_f': sigma_f_over_epochs.tolist(),\n",
    "                'l1': l1_over_epochs.tolist(),\n",
    "                'l2': l2_over_epochs.tolist()\n",
    "                })\n",
    "            \n",
    "            df_losses.to_csv(f\"{MODEL_SIM_RESULTS_DIR}/{sim_name}_{model_name}_losses_over_epochs.csv\", index = False, float_format = \"%.5f\") # reduce to 5 decimals for readability\n",
    "\n",
    "            # (4) Save divergence field (computed above for all runs)\n",
    "            torch.save(dfNGP_test_div_field, f\"{MODEL_SIM_RESULTS_DIR}/{sim_name}_{model_name}_test_prediction_divergence_field.pt\")\n",
    "\n",
    "        x_train_grad = x_train.to(device).clone().requires_grad_(True)\n",
    "\n",
    "        mean_pred_train, covar_pred_train, _ = GP_predict(\n",
    "                     x_train,\n",
    "                     y_train, # NOTE: use original y_train, not noisy\n",
    "                     x_train_grad,\n",
    "                     [best_sigma_n, best_sigma_f, best_l], # list of (initial) hypers\n",
    "                     mean_func = dfNN_mean_model, # dfNN as mean function\n",
    "                     divergence_free_bool = True) # ensures we use a df kernel\n",
    "        \n",
    "        dfNGP_train_div_field = compute_divergence_field(mean_pred_train, x_train_grad)\n",
    "\n",
    "        # Divergence: Convert field to metric: mean absolute divergence\n",
    "        # NOTE: It is important to use the absolute value of the divergence field, since positive and negative deviations are violations and shouldn't cancel each other out \n",
    "        dfNGP_train_div = dfNGP_train_div_field.abs().mean().item()\n",
    "        dfNGP_test_div = dfNGP_test_div_field.abs().mean().item()\n",
    "\n",
    "        # Compute metrics (convert tensors to float) for every run's tuned model\n",
    "        dfNGP_train_RMSE = compute_RMSE(y_train, mean_pred_train).item()\n",
    "        dfNGP_train_MAE = compute_MAE(y_train, mean_pred_train).item()\n",
    "        dfNGP_train_NLL = compute_NLL_sparse(y_train, mean_pred_train, covar_pred_train).item()\n",
    "        dfNGP_train_full_NLL = compute_NLL_full(y_train, mean_pred_train, covar_pred_train).item()\n",
    "\n",
    "        dfNGP_test_RMSE = compute_RMSE(y_test, mean_pred_test).item()\n",
    "        dfNGP_test_MAE = compute_MAE(y_test, mean_pred_test).item()\n",
    "        dfNGP_test_NLL = compute_NLL_sparse(y_test, mean_pred_test, covar_pred_test).item()\n",
    "        dfNGP_test_full_NLL = compute_NLL_full(y_test, mean_pred_test, covar_pred_test).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f866d0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test, mean_pred_test, covar_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec5faf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_pred_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3972f010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_NLL_sparse(y_true, y_mean_pred, y_covar_pred):\n",
    "    \"\"\" Computes a sparse version of the Negative Log-Likelihood (NLL) for a 2D Gaussian distribution. This sparse version neglects cross-covariance terms and is more efficient for large datasets.\n",
    "    \n",
    "    NLL: The NLL quantifies how well the predicted Gaussian distribution fits the observed data.\n",
    "    Sparse format: each of the N points has its own 2×2 covariance matrix. (This is more than just the diagonal of the covariance matrix, but not the full covar.)\n",
    "\n",
    "    Args:\n",
    "        y_true (torch.Tensor): True observations of shape (N, 2).\n",
    "        y_mean_pred (torch.Tensor): Mean predictions of shape (N, 2).\n",
    "        y_covar_pred (torch.Tensor): Full predicted covariance matrix of shape (N * 2, N * 2).(BLOCK FORMAT) [u1, u2, u3, ..., v1, v2, v3, ...]\n",
    "            If N = 400, then y_covar_pred is torch.Size([800, 800]) so 640000 elements N x 2 x 2 = only 1600 elements.\n",
    "        jitter (float, optional): Small value added to the diagonal for numerical stability. Defaults to 0.5 * 1e-2 - quite high but we need to keep it consistent across all models.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor(): Negative Log-Likelihood (NLL) scalar.\n",
    "    \"\"\"\n",
    "    # Extract number of points\n",
    "    N = y_true.shape[0]\n",
    "\n",
    "    # Step 1: Sparsify the covariance matrix\n",
    "    # Change format of y_covar_pred from (N x 2, N x 2) to (N, 2, 2) so N (2, 2) matrices.\n",
    "    # NOTE: This is a sparse version of the covariance matrix, neglecting cross-covariance terms.\n",
    "\n",
    "    # extract diagonal of upper left quadrant: variance of the first output (y1) at each point.\n",
    "    var_y1_y1 = torch.diag(y_covar_pred[:N, :N])\n",
    "    # extract diagonal of ulower right quadrant: variance of the second output (y2) at each point\n",
    "    var_y2_y2 = torch.diag(y_covar_pred[N:, N:])\n",
    "\n",
    "    # extract diagonal of upper right quadrant: How much do y1 and y2 covary at this point\n",
    "    covar_y1_y2 = torch.diag(y_covar_pred[:N, N:])\n",
    "    # extract diagonal of lower left quadrant\n",
    "    covar_y2_y1 = torch.diag(y_covar_pred[N:, :N])\n",
    "\n",
    "    col1 = torch.cat([var_y1_y1.unsqueeze(-1), covar_y1_y2.unsqueeze(-1)], dim = -1)\n",
    "    col2 = torch.cat([covar_y2_y1.unsqueeze(-1), var_y2_y2.unsqueeze(-1)], dim = -1)\n",
    "\n",
    "    # At each point N, what is the predicted variance of y1 and y2 and \n",
    "    # what is the predicted covariance between y1 and y2? (symmetric)\n",
    "    covar_N22 = torch.cat([col1.unsqueeze(-1), col2.unsqueeze(-1)], dim = -1) # shape: torch.Size([N, 2, 2])\n",
    "\n",
    "\n",
    "    # STEP 2: Compute Mahalanobis distance efficiently\n",
    "    # Compute the difference between the true and predicted values (y - μ)\n",
    "    # NOTE: order is (true - pred) to match the Mahalanobis distance formula\n",
    "    # NOTE: we can also keep this shape\n",
    "    diff = y_true - y_mean_pred   # Shape: (N, 2)\n",
    "    \n",
    "    # Reshape diff to (N, 2, 1) to do matrix multiplication with (N, 2, 2)\n",
    "    diff = diff.unsqueeze(-1)  # shape: (N, 2, 1)\n",
    "\n",
    "    sigma_inverse = torch.inverse(covar_N22) # shape: torch.Size([N, 2, 2])\n",
    "\n",
    "    # Compute (Σ⁻¹ @ diff) → shape: (N, 2, 1)\n",
    "    maha_component = torch.matmul(sigma_inverse, diff)\n",
    "\n",
    "    # Compute (diff^T @ Σ⁻¹ @ diff) for each point → shape: (N, 1, 1)\n",
    "    # transpose diff to (N, 1, 2) for matrix multiplication\n",
    "    mahalanobis_distances = torch.matmul(diff.transpose(1, 2), maha_component)\n",
    "\n",
    "    # Sum (N, ) distances to get a single value\n",
    "    mahalanobis_distances = mahalanobis_distances.squeeze().sum()\n",
    "\n",
    "    # STEP 3: Log determinant of the covariance matrix\n",
    "\n",
    "    # element-wise determinant of all 2x2 matrices: sum\n",
    "    sign, log_absdet = torch.slogdet(covar_N22)\n",
    "    if not torch.all(sign > 0):\n",
    "        print(\"Warning: Non-positive definite matrix encountered.\")\n",
    "        return torch.tensor(float(\"inf\"), device = covar_N22.device)\n",
    "    log_det_Sigma = log_absdet.sum()\n",
    "\n",
    "\n",
    "    # STEP 4: Compute normalisation term\n",
    "    d = N * 2  # Dimensionality (since we have two outputs per point)\n",
    "    normalisation_term = d * torch.log(torch.tensor(2 * torch.pi, device = y_true.device))\n",
    "\n",
    "    # Step 5: Combine 3 scalars into negative log-likelihood (NLL)\n",
    "    # Gaussian log-likelihood formula: 2D\n",
    "    # NOTE: Gaussian log-likelihood 2D formula\n",
    "    log_likelihood =  - 0.5 * (mahalanobis_distances + log_det_Sigma + normalisation_term)\n",
    "\n",
    "    # return the negative log-likelihood\n",
    "    # return - log_likelihood\n",
    "    return covar_N22\n",
    "\n",
    "func_N22 = compute_NLL_sparse(y_test, mean_pred_test, covar_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836d37b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(covar_pred_test.cpu().detach(), cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9335bd0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "\n",
      "Training for REGION_UPPER_BYRD...\n",
      "torch.Size([1012])\n"
     ]
    }
   ],
   "source": [
    "# REAL DATA EXPERIMENTS\n",
    "# RUN WITH python run_real_experiments_dfGP.py\n",
    "#               _                 _   _      \n",
    "#              | |               | | (_)     \n",
    "#    __ _ _ __ | |_ __ _ _ __ ___| |_ _  ___ \n",
    "#   / _` | '_ \\| __/ _` | '__/ __| __| |/ __|\n",
    "#  | (_| | | | | || (_| | | | (__| |_| | (__ \n",
    "#   \\__,_|_| |_|\\__\\__,_|_|  \\___|\\__|_|\\___|\n",
    "# \n",
    "model_name = \"dfGP\"\n",
    "\n",
    "# import configs to we can access the hypers with getattr\n",
    "import configs\n",
    "from configs import PATIENCE, MAX_NUM_EPOCHS, NUM_RUNS, WEIGHT_DECAY\n",
    "\n",
    "# Reiterating import for visibility\n",
    "MAX_NUM_EPOCHS = MAX_NUM_EPOCHS\n",
    "NUM_RUNS = NUM_RUNS\n",
    "WEIGHT_DECAY = WEIGHT_DECAY\n",
    "PATIENCE = PATIENCE\n",
    "\n",
    "# TODO: Delete overwrite, run full\n",
    "NUM_RUNS = 1\n",
    "\n",
    "# assign model-specific variable\n",
    "MODEL_LEARNING_RATE = getattr(configs, f\"{model_name}_REAL_LEARNING_RATE\")\n",
    "MODEL_REAL_RESULTS_DIR = getattr(configs, f\"{model_name}_REAL_RESULTS_DIR\")\n",
    "import os\n",
    "os.makedirs(MODEL_REAL_RESULTS_DIR, exist_ok = True)\n",
    "\n",
    "# imports for probabilistic models\n",
    "if model_name in [\"GP\", \"dfGP\", \"dfNGP\"]:\n",
    "    from GP_models import GP_predict\n",
    "    from metrics import compute_NLL_sparse, compute_NLL_full\n",
    "    from configs import L_RANGE, GP_PATIENCE\n",
    "    # overwrite with GP_PATIENCE\n",
    "    PATIENCE = GP_PATIENCE\n",
    "    \n",
    "    if model_name in [\"dfGP\", \"dfNGP\"]:\n",
    "        from configs import SIGMA_F_RANGE\n",
    "\n",
    "# universals \n",
    "from metrics import compute_RMSE, compute_MAE, compute_divergence_field\n",
    "\n",
    "# basics\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from codecarbon import EmissionsTracker\n",
    "\n",
    "# utilitarian\n",
    "from utils import set_seed\n",
    "# reproducibility\n",
    "set_seed(42)\n",
    "import gc\n",
    "\n",
    "# setting device to GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# overwrite if needed: # device = 'cpu'\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "### START TIMING ###\n",
    "import time\n",
    "start_time = time.time()  # Start timing after imports\n",
    "\n",
    "### START TRACKING EXPERIMENT EMISSIONS ###\n",
    "# tracker = EmissionsTracker(project_name = \"dfGP_real_experiments\", output_dir = MODEL_REAL_RESULTS_DIR)\n",
    "# tracker.start()\n",
    "\n",
    "#############################\n",
    "### LOOP 1 - over REGIONS ###\n",
    "#############################\n",
    "\n",
    "for region_name in [\"region_upper_byrd\", \"region_mid_byrd\", \"region_lower_byrd\"]:\n",
    "\n",
    "    print(f\"\\nTraining for {region_name.upper()}...\")\n",
    "\n",
    "    # Store metrics for the current region (used for *metrics_summary* report and *metrics_per_run*)\n",
    "    region_results = []\n",
    "\n",
    "    ##########################################\n",
    "    ### x_train & y_train, x_test & x_test ###\n",
    "    ##########################################\n",
    "\n",
    "    # define paths based on region_name\n",
    "    path_to_training_tensor = \"data/real_data/\" + region_name + \"_train_tensor.pt\"\n",
    "    path_to_test_tensor = \"data/real_data/\" + region_name + \"_test_tensor.pt\"\n",
    "\n",
    "    # load and tranpose to have rows as points\n",
    "    train = torch.load(path_to_training_tensor, weights_only = False).T \n",
    "    test = torch.load(path_to_test_tensor, weights_only = False).T\n",
    "\n",
    "    # The train and test tensors have the following columns:\n",
    "    # [:, 0] = x\n",
    "    # [:, 1] = y\n",
    "    # [:, 2] = surface elevation (s)\n",
    "    # [:, 3] = ice flux in x direction (u)\n",
    "    # [:, 4] = ice flux in y direction (v)\n",
    "    # [:, 5] = ice flux error in x direction (u_err)\n",
    "    # [:, 6] = ice flux error in y direction (v_err)\n",
    "    # [:, 7] = source age\n",
    "\n",
    "    # train\n",
    "    x_train = train[:, [0, 1]].to(device)\n",
    "    y_train = train[:, [3, 4]].to(device)\n",
    "\n",
    "    # test\n",
    "    x_test = test[:, [0, 1]].to(device)\n",
    "    y_test = test[:, [3, 4]].to(device)\n",
    "\n",
    "    # local measurment errors as noise\n",
    "    train_noise_diag = torch.concat((train[:, 5], train[:, 6]), dim = 0).to(device) \n",
    "    print(train_noise_diag.shape)\n",
    "    # torch.log(region_train_tensor[7, :] + 3) * 0.01\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81e66af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "print(train[7, :].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
